{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Python utilities for Manubot: Manuscripts, open and automated Manubot is a workflow and set of tools for the next generation of scholarly publishing. This repository contains a Python package with several Manubot-related utilities, as described in the usage section below. Package documentation is available at https://manubot.github.io/manubot (auto-generated from the Python source code). The manubot cite command-line interface retrieves and formats bibliographic metadata for user-supplied persistent identifiers like DOIs or PubMed IDs. The manubot process command-line interface prepares scholarly manuscripts for Pandoc consumption. The manubot process command is used by Manubot manuscripts, which are based off the Rootstock template , to automate several aspects of manuscript generation. See Rootstock's manuscript usage guide for more information. Note: If you want to experience Manubot by editing an existing manuscript, see https://github.com/manubot/try-manubot . If you want to create a new manuscript, see https://github.com/manubot/rootstock . To cite the Manubot project or for more information on its design and history, see: Open collaborative writing with Manubot Daniel S. Himmelstein, Vincent Rubinetti, David R. Slochower, Dongbo Hu, Venkat S. Malladi, Casey S. Greene, Anthony Gitter PLOS Computational Biology (2019-06-24) https://doi.org/c7np DOI: 10.1371/journal.pcbi.1007128 \u00b7 PMID: 31233491 \u00b7 PMCID: PMC6611653 The Manubot version of this manuscript is available at https://greenelab.github.io/meta-review/ . Installation If you are using the manubot Python package as part of a manuscript repository, installation of this package is handled though the Rootstock's environment specification . For other use cases, this package can be installed via pip . Install the latest release version from PyPI : pip install --upgrade manubot Or install from the source code on GitHub , using the version specified by a commit hash: COMMIT = d2160151e52750895571079a6e257beb6e0b1278 pip install --upgrade git+https://github.com/manubot/manubot@ $COMMIT The --upgrade argument ensures pip updates an existing manubot installation if present. Some functions in this package require Pandoc , which must be installed separately on the system. The pandoc-manubot-cite filter depends on Pandoc as well as panflute (a Python package). Users must install a compatible version of panflute based on their Pandoc version. For example, on a system with Pandoc 2.9, install the appropriate panflute like pip install panflute==1.12.5 . Usage Installing the python package creates the manubot command line program. Here is the usage information as per manubot --help : usage : manubot [- h ] [-- version ] { process , cite , webpage } ... Manubot : the manuscript bot for scholarly writing optional arguments : - h , -- help show this help message and exit -- version show program ' s version number and exit subcommands : All operations are done through subcommands : { process , cite , webpage } process process manuscript content cite citekey to CSL JSON command line utility webpage deploy Manubot outputs to a webpage directory tree Note that all operations are done through the following sub-commands. Process The manubot process program is the primary interface to using Manubot. There are two required arguments: --content-directory and --output-directory , which specify the respective paths to the content and output directories. The content directory stores the manuscript source files. Files generated by Manubot are saved to the output directory. One common setup is to create a directory for a manuscript that contains both the content and output directory. Under this setup, you can run the Manubot using: manubot process \\ --skip-citations \\ --content-directory = content \\ --output-directory = output See manubot process --help for documentation of all command line arguments: usage : manubot process [- h ] -- content - directory CONTENT_DIRECTORY -- output - directory OUTPUT_DIRECTORY [-- template - variables - path TEMPLATE_VARIABLES_PATH ] -- skip - citations [-- cache - directory CACHE_DIRECTORY ] [-- clear - requests - cache ] [-- skip - remote ] [-- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL } ] Process manuscript content to create outputs for Pandoc consumption . Performs bibliographic processing and templating . optional arguments : - h , -- help show this help message and exit -- content - directory CONTENT_DIRECTORY Directory where manuscript content files are located . -- output - directory OUTPUT_DIRECTORY Directory to output files generated by this script . -- template - variables - path TEMPLATE_VARIABLES_PATH Path or URL of a file containing template variables for jinja2 . Serialization format is inferred from the file extension , with support for JSON , YAML , and TOML . If the format cannot be detected , the parser assumes JSON . Specify this argument multiple times to read multiple files . Variables can be applied to a namespace ( i . e . stored under a dictionary key ) like ` -- template - variables - path = namespace = path_or_url ` . Namespaces must match the regex ` [ a - zA - Z_ ][ a - zA - Z0 - 9 _ ]* ` . -- skip - citations Skip citation and reference processing . Support for citation and reference processing has been moved from ` manubot process ` to the pandoc - manubot - cite filter . Therefore this argument is now required . If citation - tags . tsv is found in content , these tags will be inserted in the markdown output using the reference - link syntax for citekey aliases . Appends content / manual - references * . * paths to Pandoc 's metadata . bibliography field . -- cache - directory CACHE_DIRECTORY Custom cache directory . If not specified , caches to output - directory . -- clear - requests - cache -- skip - remote Do not add the rootstock repository to the local git repository remotes . -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL } Set the logging level for stderr logging Manual references Manubot has the ability to rely on user-provided reference metadata rather than generating it. manubot process searches the content directory for files containing manually-provided reference metadata that match the glob manual-references*.* . These files are stored in the Pandoc metadata bibliography field, such that they can be loaded by pandoc-manubot-cite . Cite manubot cite is a command line utility to produce bibliographic metadata for citation keys. The utility either outputs metadata as CSL JSON items or produces formatted references if --render . Citation keys should be in the format prefix:accession . For example, the following example generates Markdown-formatted references for four persistent identifiers: manubot cite --format = markdown \\ doi:10.1098/rsif.2017.0387 pubmed:29424689 pmc:PMC5640425 arxiv:1806.05726 The following terminal recording demonstrates the main features of manubot cite (for a slightly outdated version): Additional usage information is available from manubot cite --help : usage : manubot cite [ - h ] [ -- output OUTPUT ] [ -- format { csljson , cslyaml , plain , markdown , docx , html , jats } | -- yml | -- txt | -- md ] [ -- csl CSL ] [ -- bibliography BIBLIOGRAPHY ] [ -- no - infer - prefix ] [ -- allow - invalid - csl - data ] [ -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL }] citekeys [ citekeys ... ] Generate bibliographic metadata in CSL JSON format for one or more citation keys . Optionally , render metadata into formatted references using Pandoc . Text outputs are UTF - 8 encoded . positional arguments : citekeys One or more ( space separated ) citation keys to generate bibliographic metadata for . optional arguments : - h , -- help show this help message and exit -- output OUTPUT Specify a file to write output , otherwise default to stdout . -- format { csljson , cslyaml , plain , markdown , docx , html , jats } Format to use for output file . csljson and cslyaml output the CSL data . All other choices render the references using Pandoc . If not specified , attempt to infer this from the -- output filename extension . Otherwise , default to csljson . -- yml Short for -- format = cslyaml . -- txt Short for -- format = plain . -- md Short for -- format = markdown . -- csl CSL URL or path with CSL XML style used to style references ( i . e . Pandoc 's --csl option). Defaults to Manubot 's style. -- bibliography BIBLIOGRAPHY File to read manual reference metadata . Specify multiple times to load multiple files . Similar to pandoc -- bibliography . -- no - infer - prefix Do not attempt to infer the prefix for citekeys without a known prefix . -- allow - invalid - csl - data Allow CSL Items that do not conform to the JSON Schema . Skips CSL pruning . -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL } Set the logging level for stderr logging Pandoc filter This package creates the pandoc-manubot-cite Pandoc filter, providing access to Manubot's cite-by-ID functionality from within a Pandoc workflow. Options are set via Pandoc metadata fields listed in the docs . usage : pandoc - manubot - cite [ -h ] [ --input [INPUT ] ] [ --output [OUTPUT ] ] target_format Pandoc filter for citation by persistent identifier . Filters are command - line programs that read and write a JSON - encoded abstract syntax tree for Pandoc . Unless you are debugging , run this filter as part of a pandoc command by specifying -- filter = pandoc - manubot - cite . positional arguments : target_format output format of the pandoc command , as per Pandoc ' s -- to option optional arguments : - h , -- help show this help message and exit -- input [ INPUT ] path read JSON input ( defaults to stdin ) --output [OUTPUT] path to write JSON output (defaults to stdout) Other Pandoc filters exist that do something similar: pandoc-url2cite , pandoc-url2cite-hs , & pwcite . Currently, pandoc-manubot-cite supports the most types of persistent identifiers. We're interested in creating as much compatibility as possible between these filters and their syntaxes. Manual references Manual references are loaded from the references and bibliography Pandoc metadata fields. If a manual reference filename ends with .json or .yaml , it's assumed to contain CSL Data (i.e. Citation Style Language JSON). Otherwise, the format is inferred from the extension and converted to CSL JSON using the pandoc-citeproc --bib2json utility . The standard citation key for manual references is inferred from the CSL JSON id or note field. When no prefix is provided, such as doi: , url: , or raw: , a raw: prefix is automatically added. If multiple manual reference files load metadata for the same standard citation id , precedence is assigned according to descending filename order. Webpage The manubot webpage command populates a webpage directory with Manubot output files. usage : manubot webpage [- h ] [-- checkout [ CHECKOUT ]] [-- version VERSION ] [-- timestamp ] [-- no - ots - cache | -- ots - cache OTS_CACHE ] [-- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL }] Update the webpage directory tree with Manubot output files . This command should be run from the root directory of a Manubot manuscript that follows the Rootstock layout , containing ` output ` and ` webpage ` directories . HTML and PDF outputs are copied to the webpage directory , which is structured as static source files for website hosting . optional arguments : - h , -- help show this help message and exit -- checkout [ CHECKOUT ] branch to checkout / v directory contents from . For example , -- checkout = upstream / gh - pages . -- checkout is equivalent to -- checkout = gh - pages . If -- checkout is ommitted , no checkout is performed . -- version VERSION Used to create webpage /v/ { version } directory . Generally a commit hash , tag , or 'local' . When omitted , version defaults to the commit hash on CI builds and 'local' elsewhere . -- timestamp timestamp versioned manuscripts in webpage / v using OpenTimestamps . Specify this flag to create timestamps for the current HTML and PDF outputs and upgrade any timestamps from past manuscript versions . -- no - ots - cache disable the timestamp cache . -- ots - cache OTS_CACHE location for the timestamp cache ( default : ci /cache/ ots ). -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL } Set the logging level for stderr logging Development Environment Create a development environment using: conda create --name manubot-dev --channel conda-forge \\ python = 3 .8 pandoc = 2 .8 conda activate manubot-dev # assumes conda >= 4.4 pip install --editable \".[webpage,dev]\" Commands Below are some common commands used for development. They assume the working directory is set to the repository's root, and the conda environment is activated. # run the test suite pytest # install pre-commit git hooks (once per local clone). # The pre-commit checks declared in .pre-commit-config.yaml will now # run on changed files during git commits. pre-commit install # run the pre-commit checks (required to pass CI) pre-commit run --all-files # commit despite failing pre-commit checks (will fail CI) git commit --no-verify # regenerate the README codeblocks for --help messages python manubot/tests/test_readme.py # generate the docs portray as_html --overwrite --output_dir = docs # process the example testing manuscript manubot process \\ --content-directory = manubot/process/tests/manuscripts/example/content \\ --output-directory = manubot/process/tests/manuscripts/example/output \\ --skip-citations \\ --log-level = INFO Release instructions This section is only relevant for project maintainers. GitHub Actions deploys releases to PyPI . To create a new release, bump the __version__ in manubot/__init__.py . Then, set the TAG and OLD_TAG environment variables: TAG = v $( python setup.py --version ) # fetch tags from the upstream remote # (assumes upstream is the manubot organization remote) git fetch --tags upstream main # get previous release tag, can hardcode like OLD_TAG=v0.3.1 OLD_TAG = $( git describe --tags --abbrev = 0 ) The following commands can help draft release notes: # check out a branch for a pull request as needed git checkout -b \"release- $TAG \" # create release notes file if it doesn't exist touch \"release-notes/ $TAG .md\" # commit list since previous tag echo $'\\n\\nCommits\\n-------\\n' >> \"release-notes/ $TAG .md\" git log --oneline --decorate = no --reverse $OLD_TAG ..HEAD >> \"release-notes/ $TAG .md\" # commit authors since previous tag echo $'\\n\\nCode authors\\n------------\\n' >> \"release-notes/ $TAG .md\" git log $OLD_TAG ..HEAD --format = '%aN <%aE>' | sort --unique >> \"release-notes/ $TAG .md\" After a commit with the above updates is part of upstream:main , for example after a PR is merged, use the GitHub interface to create a release with the new \"Tag version\". Monitor GitHub Actions and PyPI for successful deployment of the release. Goals & Acknowledgments Our goal is to create scholarly infrastructure that encourages open science and assists reproducibility. Accordingly, we hope for the Manubot software and philosophy to be adopted widely, by both academic and commercial entities. As such, Manubot is free/libre and open source software (see LICENSE.md ). We would like to thank the contributors and funders whose support makes this project possible. Specifically, Manubot development has been financially supported by: the Alfred P. Sloan Foundation in Grant G-2018-11163 to @dhimmel . the Gordon & Betty Moore Foundation ( @DDD-Moore ) in Grant GBMF4552 to @cgreene .","title":"Home"},{"location":"#python-utilities-for-manubot-manuscripts-open-and-automated","text":"Manubot is a workflow and set of tools for the next generation of scholarly publishing. This repository contains a Python package with several Manubot-related utilities, as described in the usage section below. Package documentation is available at https://manubot.github.io/manubot (auto-generated from the Python source code). The manubot cite command-line interface retrieves and formats bibliographic metadata for user-supplied persistent identifiers like DOIs or PubMed IDs. The manubot process command-line interface prepares scholarly manuscripts for Pandoc consumption. The manubot process command is used by Manubot manuscripts, which are based off the Rootstock template , to automate several aspects of manuscript generation. See Rootstock's manuscript usage guide for more information. Note: If you want to experience Manubot by editing an existing manuscript, see https://github.com/manubot/try-manubot . If you want to create a new manuscript, see https://github.com/manubot/rootstock . To cite the Manubot project or for more information on its design and history, see: Open collaborative writing with Manubot Daniel S. Himmelstein, Vincent Rubinetti, David R. Slochower, Dongbo Hu, Venkat S. Malladi, Casey S. Greene, Anthony Gitter PLOS Computational Biology (2019-06-24) https://doi.org/c7np DOI: 10.1371/journal.pcbi.1007128 \u00b7 PMID: 31233491 \u00b7 PMCID: PMC6611653 The Manubot version of this manuscript is available at https://greenelab.github.io/meta-review/ .","title":"Python utilities for Manubot: Manuscripts, open and automated"},{"location":"#installation","text":"If you are using the manubot Python package as part of a manuscript repository, installation of this package is handled though the Rootstock's environment specification . For other use cases, this package can be installed via pip . Install the latest release version from PyPI : pip install --upgrade manubot Or install from the source code on GitHub , using the version specified by a commit hash: COMMIT = d2160151e52750895571079a6e257beb6e0b1278 pip install --upgrade git+https://github.com/manubot/manubot@ $COMMIT The --upgrade argument ensures pip updates an existing manubot installation if present. Some functions in this package require Pandoc , which must be installed separately on the system. The pandoc-manubot-cite filter depends on Pandoc as well as panflute (a Python package). Users must install a compatible version of panflute based on their Pandoc version. For example, on a system with Pandoc 2.9, install the appropriate panflute like pip install panflute==1.12.5 .","title":"Installation"},{"location":"#usage","text":"Installing the python package creates the manubot command line program. Here is the usage information as per manubot --help : usage : manubot [- h ] [-- version ] { process , cite , webpage } ... Manubot : the manuscript bot for scholarly writing optional arguments : - h , -- help show this help message and exit -- version show program ' s version number and exit subcommands : All operations are done through subcommands : { process , cite , webpage } process process manuscript content cite citekey to CSL JSON command line utility webpage deploy Manubot outputs to a webpage directory tree Note that all operations are done through the following sub-commands.","title":"Usage"},{"location":"#process","text":"The manubot process program is the primary interface to using Manubot. There are two required arguments: --content-directory and --output-directory , which specify the respective paths to the content and output directories. The content directory stores the manuscript source files. Files generated by Manubot are saved to the output directory. One common setup is to create a directory for a manuscript that contains both the content and output directory. Under this setup, you can run the Manubot using: manubot process \\ --skip-citations \\ --content-directory = content \\ --output-directory = output See manubot process --help for documentation of all command line arguments: usage : manubot process [- h ] -- content - directory CONTENT_DIRECTORY -- output - directory OUTPUT_DIRECTORY [-- template - variables - path TEMPLATE_VARIABLES_PATH ] -- skip - citations [-- cache - directory CACHE_DIRECTORY ] [-- clear - requests - cache ] [-- skip - remote ] [-- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL } ] Process manuscript content to create outputs for Pandoc consumption . Performs bibliographic processing and templating . optional arguments : - h , -- help show this help message and exit -- content - directory CONTENT_DIRECTORY Directory where manuscript content files are located . -- output - directory OUTPUT_DIRECTORY Directory to output files generated by this script . -- template - variables - path TEMPLATE_VARIABLES_PATH Path or URL of a file containing template variables for jinja2 . Serialization format is inferred from the file extension , with support for JSON , YAML , and TOML . If the format cannot be detected , the parser assumes JSON . Specify this argument multiple times to read multiple files . Variables can be applied to a namespace ( i . e . stored under a dictionary key ) like ` -- template - variables - path = namespace = path_or_url ` . Namespaces must match the regex ` [ a - zA - Z_ ][ a - zA - Z0 - 9 _ ]* ` . -- skip - citations Skip citation and reference processing . Support for citation and reference processing has been moved from ` manubot process ` to the pandoc - manubot - cite filter . Therefore this argument is now required . If citation - tags . tsv is found in content , these tags will be inserted in the markdown output using the reference - link syntax for citekey aliases . Appends content / manual - references * . * paths to Pandoc 's metadata . bibliography field . -- cache - directory CACHE_DIRECTORY Custom cache directory . If not specified , caches to output - directory . -- clear - requests - cache -- skip - remote Do not add the rootstock repository to the local git repository remotes . -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL } Set the logging level for stderr logging","title":"Process"},{"location":"#manual-references","text":"Manubot has the ability to rely on user-provided reference metadata rather than generating it. manubot process searches the content directory for files containing manually-provided reference metadata that match the glob manual-references*.* . These files are stored in the Pandoc metadata bibliography field, such that they can be loaded by pandoc-manubot-cite .","title":"Manual references"},{"location":"#cite","text":"manubot cite is a command line utility to produce bibliographic metadata for citation keys. The utility either outputs metadata as CSL JSON items or produces formatted references if --render . Citation keys should be in the format prefix:accession . For example, the following example generates Markdown-formatted references for four persistent identifiers: manubot cite --format = markdown \\ doi:10.1098/rsif.2017.0387 pubmed:29424689 pmc:PMC5640425 arxiv:1806.05726 The following terminal recording demonstrates the main features of manubot cite (for a slightly outdated version): Additional usage information is available from manubot cite --help : usage : manubot cite [ - h ] [ -- output OUTPUT ] [ -- format { csljson , cslyaml , plain , markdown , docx , html , jats } | -- yml | -- txt | -- md ] [ -- csl CSL ] [ -- bibliography BIBLIOGRAPHY ] [ -- no - infer - prefix ] [ -- allow - invalid - csl - data ] [ -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL }] citekeys [ citekeys ... ] Generate bibliographic metadata in CSL JSON format for one or more citation keys . Optionally , render metadata into formatted references using Pandoc . Text outputs are UTF - 8 encoded . positional arguments : citekeys One or more ( space separated ) citation keys to generate bibliographic metadata for . optional arguments : - h , -- help show this help message and exit -- output OUTPUT Specify a file to write output , otherwise default to stdout . -- format { csljson , cslyaml , plain , markdown , docx , html , jats } Format to use for output file . csljson and cslyaml output the CSL data . All other choices render the references using Pandoc . If not specified , attempt to infer this from the -- output filename extension . Otherwise , default to csljson . -- yml Short for -- format = cslyaml . -- txt Short for -- format = plain . -- md Short for -- format = markdown . -- csl CSL URL or path with CSL XML style used to style references ( i . e . Pandoc 's --csl option). Defaults to Manubot 's style. -- bibliography BIBLIOGRAPHY File to read manual reference metadata . Specify multiple times to load multiple files . Similar to pandoc -- bibliography . -- no - infer - prefix Do not attempt to infer the prefix for citekeys without a known prefix . -- allow - invalid - csl - data Allow CSL Items that do not conform to the JSON Schema . Skips CSL pruning . -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL } Set the logging level for stderr logging","title":"Cite"},{"location":"#pandoc-filter","text":"This package creates the pandoc-manubot-cite Pandoc filter, providing access to Manubot's cite-by-ID functionality from within a Pandoc workflow. Options are set via Pandoc metadata fields listed in the docs . usage : pandoc - manubot - cite [ -h ] [ --input [INPUT ] ] [ --output [OUTPUT ] ] target_format Pandoc filter for citation by persistent identifier . Filters are command - line programs that read and write a JSON - encoded abstract syntax tree for Pandoc . Unless you are debugging , run this filter as part of a pandoc command by specifying -- filter = pandoc - manubot - cite . positional arguments : target_format output format of the pandoc command , as per Pandoc ' s -- to option optional arguments : - h , -- help show this help message and exit -- input [ INPUT ] path read JSON input ( defaults to stdin ) --output [OUTPUT] path to write JSON output (defaults to stdout) Other Pandoc filters exist that do something similar: pandoc-url2cite , pandoc-url2cite-hs , & pwcite . Currently, pandoc-manubot-cite supports the most types of persistent identifiers. We're interested in creating as much compatibility as possible between these filters and their syntaxes.","title":"Pandoc filter"},{"location":"#manual-references_1","text":"Manual references are loaded from the references and bibliography Pandoc metadata fields. If a manual reference filename ends with .json or .yaml , it's assumed to contain CSL Data (i.e. Citation Style Language JSON). Otherwise, the format is inferred from the extension and converted to CSL JSON using the pandoc-citeproc --bib2json utility . The standard citation key for manual references is inferred from the CSL JSON id or note field. When no prefix is provided, such as doi: , url: , or raw: , a raw: prefix is automatically added. If multiple manual reference files load metadata for the same standard citation id , precedence is assigned according to descending filename order.","title":"Manual references"},{"location":"#webpage","text":"The manubot webpage command populates a webpage directory with Manubot output files. usage : manubot webpage [- h ] [-- checkout [ CHECKOUT ]] [-- version VERSION ] [-- timestamp ] [-- no - ots - cache | -- ots - cache OTS_CACHE ] [-- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL }] Update the webpage directory tree with Manubot output files . This command should be run from the root directory of a Manubot manuscript that follows the Rootstock layout , containing ` output ` and ` webpage ` directories . HTML and PDF outputs are copied to the webpage directory , which is structured as static source files for website hosting . optional arguments : - h , -- help show this help message and exit -- checkout [ CHECKOUT ] branch to checkout / v directory contents from . For example , -- checkout = upstream / gh - pages . -- checkout is equivalent to -- checkout = gh - pages . If -- checkout is ommitted , no checkout is performed . -- version VERSION Used to create webpage /v/ { version } directory . Generally a commit hash , tag , or 'local' . When omitted , version defaults to the commit hash on CI builds and 'local' elsewhere . -- timestamp timestamp versioned manuscripts in webpage / v using OpenTimestamps . Specify this flag to create timestamps for the current HTML and PDF outputs and upgrade any timestamps from past manuscript versions . -- no - ots - cache disable the timestamp cache . -- ots - cache OTS_CACHE location for the timestamp cache ( default : ci /cache/ ots ). -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL } Set the logging level for stderr logging","title":"Webpage"},{"location":"#development","text":"","title":"Development"},{"location":"#environment","text":"Create a development environment using: conda create --name manubot-dev --channel conda-forge \\ python = 3 .8 pandoc = 2 .8 conda activate manubot-dev # assumes conda >= 4.4 pip install --editable \".[webpage,dev]\"","title":"Environment"},{"location":"#commands","text":"Below are some common commands used for development. They assume the working directory is set to the repository's root, and the conda environment is activated. # run the test suite pytest # install pre-commit git hooks (once per local clone). # The pre-commit checks declared in .pre-commit-config.yaml will now # run on changed files during git commits. pre-commit install # run the pre-commit checks (required to pass CI) pre-commit run --all-files # commit despite failing pre-commit checks (will fail CI) git commit --no-verify # regenerate the README codeblocks for --help messages python manubot/tests/test_readme.py # generate the docs portray as_html --overwrite --output_dir = docs # process the example testing manuscript manubot process \\ --content-directory = manubot/process/tests/manuscripts/example/content \\ --output-directory = manubot/process/tests/manuscripts/example/output \\ --skip-citations \\ --log-level = INFO","title":"Commands"},{"location":"#release-instructions","text":"This section is only relevant for project maintainers. GitHub Actions deploys releases to PyPI . To create a new release, bump the __version__ in manubot/__init__.py . Then, set the TAG and OLD_TAG environment variables: TAG = v $( python setup.py --version ) # fetch tags from the upstream remote # (assumes upstream is the manubot organization remote) git fetch --tags upstream main # get previous release tag, can hardcode like OLD_TAG=v0.3.1 OLD_TAG = $( git describe --tags --abbrev = 0 ) The following commands can help draft release notes: # check out a branch for a pull request as needed git checkout -b \"release- $TAG \" # create release notes file if it doesn't exist touch \"release-notes/ $TAG .md\" # commit list since previous tag echo $'\\n\\nCommits\\n-------\\n' >> \"release-notes/ $TAG .md\" git log --oneline --decorate = no --reverse $OLD_TAG ..HEAD >> \"release-notes/ $TAG .md\" # commit authors since previous tag echo $'\\n\\nCode authors\\n------------\\n' >> \"release-notes/ $TAG .md\" git log $OLD_TAG ..HEAD --format = '%aN <%aE>' | sort --unique >> \"release-notes/ $TAG .md\" After a commit with the above updates is part of upstream:main , for example after a PR is merged, use the GitHub interface to create a release with the new \"Tag version\". Monitor GitHub Actions and PyPI for successful deployment of the release.","title":"Release instructions"},{"location":"#goals-acknowledgments","text":"Our goal is to create scholarly infrastructure that encourages open science and assists reproducibility. Accordingly, we hope for the Manubot software and philosophy to be adopted widely, by both academic and commercial entities. As such, Manubot is free/libre and open source software (see LICENSE.md ). We would like to thank the contributors and funders whose support makes this project possible. Specifically, Manubot development has been financially supported by: the Alfred P. Sloan Foundation in Grant G-2018-11163 to @dhimmel . the Gordon & Betty Moore Foundation ( @DDD-Moore ) in Grant GBMF4552 to @cgreene .","title":"Goals &amp; Acknowledgments"},{"location":"LICENSE/","text":"BSD-2-Clause Plus Patent License Copyright \u00a9 2017\u20132020, Contributors & the Greene Lab at the University of Pennsylvania Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Subject to the terms and conditions of this license, each copyright holder and contributor hereby grants to those receiving rights under this license a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except for failure to satisfy the conditions of this license) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer this software, where such license applies only to those patent claims, already acquired or hereafter acquired, licensable by such copyright holder or contributor that are necessarily infringed by: (a) their Contribution(s) (the licensed copyrights of copyright holders and non-copyrightable additions of contributors, in source or binary form) alone; or (b) combination of their Contribution(s) with the work of authorship to which such Contribution(s) was added by such copyright holder or contributor, if, at the time the Contribution is added, such addition causes such combination to be necessarily infringed. The patent license shall not apply to any other combinations which include the Contribution. Except as expressly stated above, no rights or licenses from any copyright holder or contributor is granted under this license, whether expressly, by implication, estoppel or otherwise. DISCLAIMER THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"License"},{"location":"LICENSE/#bsd-2-clause-plus-patent-license","text":"Copyright \u00a9 2017\u20132020, Contributors & the Greene Lab at the University of Pennsylvania Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Subject to the terms and conditions of this license, each copyright holder and contributor hereby grants to those receiving rights under this license a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except for failure to satisfy the conditions of this license) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer this software, where such license applies only to those patent claims, already acquired or hereafter acquired, licensable by such copyright holder or contributor that are necessarily infringed by: (a) their Contribution(s) (the licensed copyrights of copyright holders and non-copyrightable additions of contributors, in source or binary form) alone; or (b) combination of their Contribution(s) with the work of authorship to which such Contribution(s) was added by such copyright holder or contributor, if, at the time the Contribution is added, such addition causes such combination to be necessarily infringed. The patent license shall not apply to any other combinations which include the Contribution. Except as expressly stated above, no rights or licenses from any copyright holder or contributor is granted under this license, whether expressly, by implication, estoppel or otherwise. DISCLAIMER THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"BSD-2-Clause Plus Patent License"},{"location":"media/terminal-recordings/","text":"Terminal recordings manubot cite Terminal recordings were created using asciinema . The manubot cite recording is online at https://asciinema.org/a/205085?speed=2. GIF and SVG outputs were created using asciicast2gif and svg-term-cli with the following commands: asciicast2gif -s 2.0 https://asciinema.org/a/205085.cast manubot-cite-cast.gif svg-term --window --cast=205085 --out=manubot-cite-cast.svg","title":"Terminal recordings"},{"location":"media/terminal-recordings/#terminal-recordings","text":"","title":"Terminal recordings"},{"location":"media/terminal-recordings/#manubot-cite","text":"Terminal recordings were created using asciinema . The manubot cite recording is online at https://asciinema.org/a/205085?speed=2. GIF and SVG outputs were created using asciicast2gif and svg-term-cli with the following commands: asciicast2gif -s 2.0 https://asciinema.org/a/205085.cast manubot-cite-cast.gif svg-term --window --cast=205085 --out=manubot-cite-cast.svg","title":"manubot cite"},{"location":"reference/manubot/","text":"Module manubot None None View Source __version__ = \"0.5.1\" Sub-modules manubot.cite manubot.command manubot.pandoc manubot.process manubot.tests manubot.util manubot.webpage","title":"Index"},{"location":"reference/manubot/#module-manubot","text":"None None View Source __version__ = \"0.5.1\"","title":"Module manubot"},{"location":"reference/manubot/#sub-modules","text":"manubot.cite manubot.command manubot.pandoc manubot.process manubot.tests manubot.util manubot.webpage","title":"Sub-modules"},{"location":"reference/manubot/command/","text":"Module manubot.command Manubot's command line interface None View Source \"\"\" Manubot's command line interface \"\"\" import argparse import logging import pathlib import sys import warnings import manubot from manubot.util import import_function def parse_arguments (): \"\"\" Read and process command line arguments. \"\"\" parser = argparse . ArgumentParser ( description = \"Manubot: the manuscript bot for scholarly writing\" ) parser . add_argument ( \"--version\" , action = \"version\" , version = f \"v { manubot . __version__ } \" ) subparsers = parser . add_subparsers ( title = \"subcommands\" , description = \"All operations are done through subcommands:\" ) # Require specifying a sub-command subparsers . required = True # https://bugs.python.org/issue26510 subparsers . dest = \"subcommand\" # https://bugs.python.org/msg186387 add_subparser_process ( subparsers ) add_subparser_cite ( subparsers ) add_subparser_webpage ( subparsers ) for subparser in subparsers . choices . values (): subparser . add_argument ( \"--log-level\" , default = \"WARNING\" , choices = [ \"DEBUG\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" ], help = \"Set the logging level for stderr logging\" , ) args = parser . parse_args () return args def add_subparser_process ( subparsers ): parser = subparsers . add_parser ( name = \"process\" , help = \"process manuscript content\" , description = \"Process manuscript content to create outputs for Pandoc consumption. \" \"Performs bibliographic processing and templating.\" , ) parser . add_argument ( \"--content-directory\" , type = pathlib . Path , required = True , help = \"Directory where manuscript content files are located.\" , ) parser . add_argument ( \"--output-directory\" , type = pathlib . Path , required = True , help = \"Directory to output files generated by this script.\" , ) parser . add_argument ( \"--template-variables-path\" , action = \"append\" , default = [], help = \"Path or URL of a file containing template variables for jinja2. \" \"Serialization format is inferred from the file extension, with support for JSON, YAML, and TOML. \" \"If the format cannot be detected, the parser assumes JSON. \" \"Specify this argument multiple times to read multiple files. \" \"Variables can be applied to a namespace (i.e. stored under a dictionary key) \" \"like `--template-variables-path=namespace=path_or_url`. \" \"Namespaces must match the regex `[a-zA-Z_][a-zA-Z0-9_]*`.\" , ) parser . add_argument ( \"--skip-citations\" , action = \"store_true\" , required = True , help = \"Skip citation and reference processing. \" \"Support for citation and reference processing has been moved from `manubot process` to the pandoc-manubot-cite filter. \" \"Therefore this argument is now required. \" \"If citation-tags.tsv is found in content, \" \"these tags will be inserted in the markdown output using the reference-link syntax for citekey aliases. \" \"Appends content/manual-references*.* paths to Pandoc's metadata.bibliography field.\" , ) parser . add_argument ( \"--cache-directory\" , type = pathlib . Path , help = \"Custom cache directory. If not specified, caches to output-directory.\" , ) parser . add_argument ( \"--clear-requests-cache\" , action = \"store_true\" ) parser . add_argument ( \"--skip-remote\" , action = \"store_true\" , help = \"Do not add the rootstock repository to the local git repository remotes.\" , ) parser . set_defaults ( function = \"manubot.process.process_command.cli_process\" ) def add_subparser_cite ( subparsers ): parser = subparsers . add_parser ( name = \"cite\" , help = \"citekey to CSL JSON command line utility\" , description = \"Generate bibliographic metadata in CSL JSON format for one or more citation keys. \" \"Optionally, render metadata into formatted references using Pandoc. \" \"Text outputs are UTF-8 encoded.\" , ) parser . add_argument ( \"--output\" , type = pathlib . Path , help = \"Specify a file to write output, otherwise default to stdout.\" , ) format_group = parser . add_mutually_exclusive_group () format_group . add_argument ( \"--format\" , choices = [ \"csljson\" , \"cslyaml\" , \"plain\" , \"markdown\" , \"docx\" , \"html\" , \"jats\" ], help = \"Format to use for output file. \" \"csljson and cslyaml output the CSL data. \" \"All other choices render the references using Pandoc. \" \"If not specified, attempt to infer this from the --output filename extension. \" \"Otherwise, default to csljson.\" , ) format_group . add_argument ( \"--yml\" , dest = \"format\" , action = \"store_const\" , const = \"cslyaml\" , help = \"Short for --format=cslyaml.\" , ) format_group . add_argument ( \"--txt\" , dest = \"format\" , action = \"store_const\" , const = \"plain\" , help = \"Short for --format=plain.\" , ) format_group . add_argument ( \"--md\" , dest = \"format\" , action = \"store_const\" , const = \"markdown\" , help = \"Short for --format=markdown.\" , ) parser . add_argument ( \"--csl\" , # redirects to the latest Manubot CSL Style. default = \"https://citation-style.manubot.org/\" , help = \"URL or path with CSL XML style used to style references \" \"(i.e. Pandoc's --csl option). \" \"Defaults to Manubot's style.\" , ) parser . add_argument ( \"--bibliography\" , default = [], action = \"append\" , help = \"File to read manual reference metadata. \" \"Specify multiple times to load multiple files. \" \"Similar to pandoc --bibliography.\" , ) parser . add_argument ( \"--no-infer-prefix\" , dest = \"infer_prefix\" , action = \"store_false\" , help = \"Do not attempt to infer the prefix for citekeys without a known prefix.\" , ) parser . add_argument ( \"--allow-invalid-csl-data\" , dest = \"prune_csl\" , action = \"store_false\" , help = \"Allow CSL Items that do not conform to the JSON Schema. Skips CSL pruning.\" , ) parser . add_argument ( \"citekeys\" , nargs = \"+\" , help = \"One or more (space separated) citation keys to generate bibliographic metadata for.\" , ) parser . set_defaults ( function = \"manubot.cite.cite_command.cli_cite\" ) def add_subparser_webpage ( subparsers ): parser = subparsers . add_parser ( name = \"webpage\" , help = \"deploy Manubot outputs to a webpage directory tree\" , description = \"Update the webpage directory tree with Manubot output files. \" \"This command should be run from the root directory of a Manubot manuscript that follows the Rootstock layout, containing `output` and `webpage` directories. \" \"HTML and PDF outputs are copied to the webpage directory, which is structured as static source files for website hosting.\" , ) parser . add_argument ( \"--checkout\" , nargs = \"?\" , const = \"gh-pages\" , default = None , help = \"branch to checkout /v directory contents from. \" \"For example, --checkout=upstream/gh-pages. \" \"--checkout is equivalent to --checkout=gh-pages. \" \"If --checkout is ommitted, no checkout is performed.\" , ) parser . add_argument ( \"--version\" , help = \"Used to create webpage/v/ {version} directory. \" \"Generally a commit hash, tag, or 'local'. \" \"When omitted, version defaults to the commit hash on CI builds and 'local' elsewhere.\" , ) parser . add_argument ( \"--timestamp\" , action = \"store_true\" , help = \"timestamp versioned manuscripts in webpage/v using OpenTimestamps. \" \"Specify this flag to create timestamps for the current HTML and PDF outputs and upgrade any timestamps from past manuscript versions.\" , ) cache_group = parser . add_mutually_exclusive_group () cache_group . add_argument ( \"--no-ots-cache\" , action = \"store_true\" , help = \"disable the timestamp cache.\" ) cache_group . add_argument ( \"--ots-cache\" , default = pathlib . Path ( \"ci/cache/ots\" ), type = pathlib . Path , help = \"location for the timestamp cache (default: ci/cache/ots).\" , ) parser . set_defaults ( function = \"manubot.webpage.webpage_command.cli_webpage\" ) def setup_logging_and_errors () -> dict : \"\"\" Configure warnings and logging. Set up an ErrorHandler to detect whether messages have been logged at or above the ERROR level. \"\"\" import errorhandler # Track if message gets logged with severity of error or greater # See https://stackoverflow.com/a/45446664/4651668 error_handler = errorhandler . ErrorHandler () # Log DeprecationWarnings warnings . simplefilter ( \"always\" , DeprecationWarning ) logging . captureWarnings ( True ) # Log to stderr logger = logging . getLogger () stream_handler = logging . StreamHandler ( stream = sys . stderr ) stream_handler . setFormatter ( logging . Formatter ( \"## {levelname} \\n {message} \" , style = \"{\" ) ) logger . addHandler ( stream_handler ) return { \"logger\" : logger , \"error_handler\" : error_handler , } def exit_if_error_handler_fired ( error_handler ): \"\"\" If a message has been logged with severity of ERROR or greater, exit Python with a nonzero code. \"\"\" if error_handler . fired : logging . critical ( \"Failure: exiting with code 1 due to logged errors\" ) raise SystemExit ( 1 ) def main (): \"\"\" Called as a console_scripts entry point in setup.cfg. This function defines the manubot command line script. \"\"\" diagnostics = setup_logging_and_errors () args = parse_arguments () diagnostics [ \"logger\" ] . setLevel ( getattr ( logging , args . log_level )) function = import_function ( args . function ) function ( args ) exit_if_error_handler_fired ( diagnostics [ \"error_handler\" ]) Functions add_subparser_cite def add_subparser_cite ( subparsers ) View Source def add_subparser_cite ( subparsers ): parser = subparsers . add_parser ( name = \"cite\" , help = \"citekey to CSL JSON command line utility\" , description = \"Generate bibliographic metadata in CSL JSON format for one or more citation keys. \" \"Optionally, render metadata into formatted references using Pandoc. \" \"Text outputs are UTF-8 encoded.\" , ) parser . add_argument ( \"--output\" , type = pathlib . Path , help = \"Specify a file to write output, otherwise default to stdout.\" , ) format_group = parser . add_mutually_exclusive_group () format_group . add_argument ( \"--format\" , choices = [ \"csljson\" , \"cslyaml\" , \"plain\" , \"markdown\" , \"docx\" , \"html\" , \"jats\" ], help = \"Format to use for output file. \" \"csljson and cslyaml output the CSL data. \" \"All other choices render the references using Pandoc. \" \"If not specified, attempt to infer this from the --output filename extension. \" \"Otherwise, default to csljson.\" , ) format_group . add_argument ( \"--yml\" , dest = \"format\" , action = \"store_const\" , const = \"cslyaml\" , help = \"Short for --format=cslyaml.\" , ) format_group . add_argument ( \"--txt\" , dest = \"format\" , action = \"store_const\" , const = \"plain\" , help = \"Short for --format=plain.\" , ) format_group . add_argument ( \"--md\" , dest = \"format\" , action = \"store_const\" , const = \"markdown\" , help = \"Short for --format=markdown.\" , ) parser . add_argument ( \"--csl\" , # redirects to the latest Manubot CSL Style. default = \"https://citation-style.manubot.org/\" , help = \"URL or path with CSL XML style used to style references \" \"(i.e. Pandoc's --csl option). \" \"Defaults to Manubot's style.\" , ) parser . add_argument ( \"--bibliography\" , default = [], action = \"append\" , help = \"File to read manual reference metadata. \" \"Specify multiple times to load multiple files. \" \"Similar to pandoc --bibliography.\" , ) parser . add_argument ( \"--no-infer-prefix\" , dest = \"infer_prefix\" , action = \"store_false\" , help = \"Do not attempt to infer the prefix for citekeys without a known prefix.\" , ) parser . add_argument ( \"--allow-invalid-csl-data\" , dest = \"prune_csl\" , action = \"store_false\" , help = \"Allow CSL Items that do not conform to the JSON Schema. Skips CSL pruning.\" , ) parser . add_argument ( \"citekeys\" , nargs = \"+\" , help = \"One or more (space separated) citation keys to generate bibliographic metadata for.\" , ) parser . set_defaults ( function = \"manubot.cite.cite_command.cli_cite\" ) add_subparser_process def add_subparser_process ( subparsers ) View Source def add_subparser_process ( subparsers ): parser = subparsers . add_parser ( name = \"process\" , help = \"process manuscript content\" , description = \"Process manuscript content to create outputs for Pandoc consumption. \" \"Performs bibliographic processing and templating.\" , ) parser . add_argument ( \"--content-directory\" , type = pathlib . Path , required = True , help = \"Directory where manuscript content files are located.\" , ) parser . add_argument ( \"--output-directory\" , type = pathlib . Path , required = True , help = \"Directory to output files generated by this script.\" , ) parser . add_argument ( \"--template-variables-path\" , action = \"append\" , default = [], help = \"Path or URL of a file containing template variables for jinja2. \" \"Serialization format is inferred from the file extension, with support for JSON, YAML, and TOML. \" \"If the format cannot be detected, the parser assumes JSON. \" \"Specify this argument multiple times to read multiple files. \" \"Variables can be applied to a namespace (i.e. stored under a dictionary key) \" \"like `--template-variables-path=namespace=path_or_url`. \" \"Namespaces must match the regex `[a-zA-Z_][a-zA-Z0-9_]*`.\" , ) parser . add_argument ( \"--skip-citations\" , action = \"store_true\" , required = True , help = \"Skip citation and reference processing. \" \"Support for citation and reference processing has been moved from `manubot process` to the pandoc-manubot-cite filter. \" \"Therefore this argument is now required. \" \"If citation-tags.tsv is found in content, \" \"these tags will be inserted in the markdown output using the reference-link syntax for citekey aliases. \" \"Appends content/manual-references*.* paths to Pandoc's metadata.bibliography field.\" , ) parser . add_argument ( \"--cache-directory\" , type = pathlib . Path , help = \"Custom cache directory. If not specified, caches to output-directory.\" , ) parser . add_argument ( \"--clear-requests-cache\" , action = \"store_true\" ) parser . add_argument ( \"--skip-remote\" , action = \"store_true\" , help = \"Do not add the rootstock repository to the local git repository remotes.\" , ) parser . set_defaults ( function = \"manubot.process.process_command.cli_process\" ) add_subparser_webpage def add_subparser_webpage ( subparsers ) View Source def add_subparser_webpage ( subparsers ) : parser = subparsers . add_parser ( name = \"webpage\" , help = \"deploy Manubot outputs to a webpage directory tree\" , description = \"Update the webpage directory tree with Manubot output files. \" \"This command should be run from the root directory of a Manubot manuscript that follows the Rootstock layout, containing `output` and `webpage` directories. \" \"HTML and PDF outputs are copied to the webpage directory, which is structured as static source files for website hosting.\" , ) parser . add_argument ( \"--checkout\" , nargs = \"?\" , const = \"gh-pages\" , default = None , help = \"branch to checkout /v directory contents from. \" \"For example, --checkout=upstream/gh-pages. \" \"--checkout is equivalent to --checkout=gh-pages. \" \"If --checkout is ommitted, no checkout is performed.\" , ) parser . add_argument ( \"--version\" , help = \"Used to create webpage/v/{version} directory. \" \"Generally a commit hash, tag, or 'local'. \" \"When omitted, version defaults to the commit hash on CI builds and 'local' elsewhere.\" , ) parser . add_argument ( \"--timestamp\" , action = \"store_true\" , help = \"timestamp versioned manuscripts in webpage/v using OpenTimestamps. \" \"Specify this flag to create timestamps for the current HTML and PDF outputs and upgrade any timestamps from past manuscript versions.\" , ) cache_group = parser . add_mutually_exclusive_group () cache_group . add_argument ( \"--no-ots-cache\" , action = \"store_true\" , help = \"disable the timestamp cache.\" ) cache_group . add_argument ( \"--ots-cache\" , default = pathlib . Path ( \"ci/cache/ots\" ), type = pathlib . Path , help = \"location for the timestamp cache (default: ci/cache/ots).\" , ) parser . set _defaults ( function = \"manubot.webpage.webpage_command.cli_webpage\" ) exit_if_error_handler_fired def exit_if_error_handler_fired ( error_handler ) If a message has been logged with severity of ERROR or greater, exit Python with a nonzero code. View Source def exit_if_error_handler_fired ( error_handler ) : \"\"\" If a message has been logged with severity of ERROR or greater , exit Python with a nonzero code . \"\"\" if error_handler . fired : logging . critical ( \" Failure: exiting with code 1 due to logged errors \" ) raise SystemExit ( 1 ) main def main ( ) Called as a console_scripts entry point in setup.cfg. This function defines the manubot command line script. View Source def main(): \"\"\" Called as a console_scripts entry point in setup.cfg. This function defines the manubot command line script. \"\"\" diagnostics = setup_logging_and_errors() args = parse_arguments() diagnostics[\"logger\"].setLevel(getattr(logging, args.log_level)) function = import_function(args.function) function(args) exit_if_error_handler_fired(diagnostics[\"error_handler\"]) parse_arguments def parse_arguments ( ) Read and process command line arguments. View Source def parse_arguments () : \"\"\" Read and process command line arguments . \"\"\" parser = argparse . ArgumentParser ( description = \" Manubot: the manuscript bot for scholarly writing \" ) parser . add_argument ( \" --version \" , action = \" version \" , version = f \" v{manubot.__version__} \" ) subparsers = parser . add_subparsers ( title = \" subcommands \" , description = \" All operations are done through subcommands: \" ) # Require specifying a sub - command subparsers . required = True # https : // bugs . python . org / issue26510 subparsers . dest = \" subcommand \" # https : // bugs . python . org / msg186387 add_subparser_process ( subparsers ) add_subparser_cite ( subparsers ) add_subparser_webpage ( subparsers ) for subparser in subparsers . choices . values () : subparser . add_argument ( \" --log-level \" , default = \" WARNING \" , choices = [ \" DEBUG \" , \" INFO \" , \" WARNING \" , \" ERROR \" , \" CRITICAL \" ], help = \" Set the logging level for stderr logging \" , ) args = parser . parse_args () return args setup_logging_and_errors def setup_logging_and_errors ( ) -> dict Configure warnings and logging. Set up an ErrorHandler to detect whether messages have been logged at or above the ERROR level. View Source def setup_logging_and_errors () -> dict : \"\"\" Configure warnings and logging. Set up an ErrorHandler to detect whether messages have been logged at or above the ERROR level. \"\"\" import errorhandler # Track if message gets logged with severity of error or greater # See https://stackoverflow.com/a/45446664/4651668 error_handler = errorhandler . ErrorHandler () # Log DeprecationWarnings warnings . simplefilter ( \"always\" , DeprecationWarning ) logging . captureWarnings ( True ) # Log to stderr logger = logging . getLogger () stream_handler = logging . StreamHandler ( stream = sys . stderr ) stream_handler . setFormatter ( logging . Formatter ( \"## {levelname} \\n {message} \" , style = \"{\" ) ) logger . addHandler ( stream_handler ) return { \"logger\" : logger , \"error_handler\" : error_handler , }","title":"Command"},{"location":"reference/manubot/command/#module-manubotcommand","text":"Manubot's command line interface None View Source \"\"\" Manubot's command line interface \"\"\" import argparse import logging import pathlib import sys import warnings import manubot from manubot.util import import_function def parse_arguments (): \"\"\" Read and process command line arguments. \"\"\" parser = argparse . ArgumentParser ( description = \"Manubot: the manuscript bot for scholarly writing\" ) parser . add_argument ( \"--version\" , action = \"version\" , version = f \"v { manubot . __version__ } \" ) subparsers = parser . add_subparsers ( title = \"subcommands\" , description = \"All operations are done through subcommands:\" ) # Require specifying a sub-command subparsers . required = True # https://bugs.python.org/issue26510 subparsers . dest = \"subcommand\" # https://bugs.python.org/msg186387 add_subparser_process ( subparsers ) add_subparser_cite ( subparsers ) add_subparser_webpage ( subparsers ) for subparser in subparsers . choices . values (): subparser . add_argument ( \"--log-level\" , default = \"WARNING\" , choices = [ \"DEBUG\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" ], help = \"Set the logging level for stderr logging\" , ) args = parser . parse_args () return args def add_subparser_process ( subparsers ): parser = subparsers . add_parser ( name = \"process\" , help = \"process manuscript content\" , description = \"Process manuscript content to create outputs for Pandoc consumption. \" \"Performs bibliographic processing and templating.\" , ) parser . add_argument ( \"--content-directory\" , type = pathlib . Path , required = True , help = \"Directory where manuscript content files are located.\" , ) parser . add_argument ( \"--output-directory\" , type = pathlib . Path , required = True , help = \"Directory to output files generated by this script.\" , ) parser . add_argument ( \"--template-variables-path\" , action = \"append\" , default = [], help = \"Path or URL of a file containing template variables for jinja2. \" \"Serialization format is inferred from the file extension, with support for JSON, YAML, and TOML. \" \"If the format cannot be detected, the parser assumes JSON. \" \"Specify this argument multiple times to read multiple files. \" \"Variables can be applied to a namespace (i.e. stored under a dictionary key) \" \"like `--template-variables-path=namespace=path_or_url`. \" \"Namespaces must match the regex `[a-zA-Z_][a-zA-Z0-9_]*`.\" , ) parser . add_argument ( \"--skip-citations\" , action = \"store_true\" , required = True , help = \"Skip citation and reference processing. \" \"Support for citation and reference processing has been moved from `manubot process` to the pandoc-manubot-cite filter. \" \"Therefore this argument is now required. \" \"If citation-tags.tsv is found in content, \" \"these tags will be inserted in the markdown output using the reference-link syntax for citekey aliases. \" \"Appends content/manual-references*.* paths to Pandoc's metadata.bibliography field.\" , ) parser . add_argument ( \"--cache-directory\" , type = pathlib . Path , help = \"Custom cache directory. If not specified, caches to output-directory.\" , ) parser . add_argument ( \"--clear-requests-cache\" , action = \"store_true\" ) parser . add_argument ( \"--skip-remote\" , action = \"store_true\" , help = \"Do not add the rootstock repository to the local git repository remotes.\" , ) parser . set_defaults ( function = \"manubot.process.process_command.cli_process\" ) def add_subparser_cite ( subparsers ): parser = subparsers . add_parser ( name = \"cite\" , help = \"citekey to CSL JSON command line utility\" , description = \"Generate bibliographic metadata in CSL JSON format for one or more citation keys. \" \"Optionally, render metadata into formatted references using Pandoc. \" \"Text outputs are UTF-8 encoded.\" , ) parser . add_argument ( \"--output\" , type = pathlib . Path , help = \"Specify a file to write output, otherwise default to stdout.\" , ) format_group = parser . add_mutually_exclusive_group () format_group . add_argument ( \"--format\" , choices = [ \"csljson\" , \"cslyaml\" , \"plain\" , \"markdown\" , \"docx\" , \"html\" , \"jats\" ], help = \"Format to use for output file. \" \"csljson and cslyaml output the CSL data. \" \"All other choices render the references using Pandoc. \" \"If not specified, attempt to infer this from the --output filename extension. \" \"Otherwise, default to csljson.\" , ) format_group . add_argument ( \"--yml\" , dest = \"format\" , action = \"store_const\" , const = \"cslyaml\" , help = \"Short for --format=cslyaml.\" , ) format_group . add_argument ( \"--txt\" , dest = \"format\" , action = \"store_const\" , const = \"plain\" , help = \"Short for --format=plain.\" , ) format_group . add_argument ( \"--md\" , dest = \"format\" , action = \"store_const\" , const = \"markdown\" , help = \"Short for --format=markdown.\" , ) parser . add_argument ( \"--csl\" , # redirects to the latest Manubot CSL Style. default = \"https://citation-style.manubot.org/\" , help = \"URL or path with CSL XML style used to style references \" \"(i.e. Pandoc's --csl option). \" \"Defaults to Manubot's style.\" , ) parser . add_argument ( \"--bibliography\" , default = [], action = \"append\" , help = \"File to read manual reference metadata. \" \"Specify multiple times to load multiple files. \" \"Similar to pandoc --bibliography.\" , ) parser . add_argument ( \"--no-infer-prefix\" , dest = \"infer_prefix\" , action = \"store_false\" , help = \"Do not attempt to infer the prefix for citekeys without a known prefix.\" , ) parser . add_argument ( \"--allow-invalid-csl-data\" , dest = \"prune_csl\" , action = \"store_false\" , help = \"Allow CSL Items that do not conform to the JSON Schema. Skips CSL pruning.\" , ) parser . add_argument ( \"citekeys\" , nargs = \"+\" , help = \"One or more (space separated) citation keys to generate bibliographic metadata for.\" , ) parser . set_defaults ( function = \"manubot.cite.cite_command.cli_cite\" ) def add_subparser_webpage ( subparsers ): parser = subparsers . add_parser ( name = \"webpage\" , help = \"deploy Manubot outputs to a webpage directory tree\" , description = \"Update the webpage directory tree with Manubot output files. \" \"This command should be run from the root directory of a Manubot manuscript that follows the Rootstock layout, containing `output` and `webpage` directories. \" \"HTML and PDF outputs are copied to the webpage directory, which is structured as static source files for website hosting.\" , ) parser . add_argument ( \"--checkout\" , nargs = \"?\" , const = \"gh-pages\" , default = None , help = \"branch to checkout /v directory contents from. \" \"For example, --checkout=upstream/gh-pages. \" \"--checkout is equivalent to --checkout=gh-pages. \" \"If --checkout is ommitted, no checkout is performed.\" , ) parser . add_argument ( \"--version\" , help = \"Used to create webpage/v/ {version} directory. \" \"Generally a commit hash, tag, or 'local'. \" \"When omitted, version defaults to the commit hash on CI builds and 'local' elsewhere.\" , ) parser . add_argument ( \"--timestamp\" , action = \"store_true\" , help = \"timestamp versioned manuscripts in webpage/v using OpenTimestamps. \" \"Specify this flag to create timestamps for the current HTML and PDF outputs and upgrade any timestamps from past manuscript versions.\" , ) cache_group = parser . add_mutually_exclusive_group () cache_group . add_argument ( \"--no-ots-cache\" , action = \"store_true\" , help = \"disable the timestamp cache.\" ) cache_group . add_argument ( \"--ots-cache\" , default = pathlib . Path ( \"ci/cache/ots\" ), type = pathlib . Path , help = \"location for the timestamp cache (default: ci/cache/ots).\" , ) parser . set_defaults ( function = \"manubot.webpage.webpage_command.cli_webpage\" ) def setup_logging_and_errors () -> dict : \"\"\" Configure warnings and logging. Set up an ErrorHandler to detect whether messages have been logged at or above the ERROR level. \"\"\" import errorhandler # Track if message gets logged with severity of error or greater # See https://stackoverflow.com/a/45446664/4651668 error_handler = errorhandler . ErrorHandler () # Log DeprecationWarnings warnings . simplefilter ( \"always\" , DeprecationWarning ) logging . captureWarnings ( True ) # Log to stderr logger = logging . getLogger () stream_handler = logging . StreamHandler ( stream = sys . stderr ) stream_handler . setFormatter ( logging . Formatter ( \"## {levelname} \\n {message} \" , style = \"{\" ) ) logger . addHandler ( stream_handler ) return { \"logger\" : logger , \"error_handler\" : error_handler , } def exit_if_error_handler_fired ( error_handler ): \"\"\" If a message has been logged with severity of ERROR or greater, exit Python with a nonzero code. \"\"\" if error_handler . fired : logging . critical ( \"Failure: exiting with code 1 due to logged errors\" ) raise SystemExit ( 1 ) def main (): \"\"\" Called as a console_scripts entry point in setup.cfg. This function defines the manubot command line script. \"\"\" diagnostics = setup_logging_and_errors () args = parse_arguments () diagnostics [ \"logger\" ] . setLevel ( getattr ( logging , args . log_level )) function = import_function ( args . function ) function ( args ) exit_if_error_handler_fired ( diagnostics [ \"error_handler\" ])","title":"Module manubot.command"},{"location":"reference/manubot/command/#functions","text":"","title":"Functions"},{"location":"reference/manubot/command/#add_subparser_cite","text":"def add_subparser_cite ( subparsers ) View Source def add_subparser_cite ( subparsers ): parser = subparsers . add_parser ( name = \"cite\" , help = \"citekey to CSL JSON command line utility\" , description = \"Generate bibliographic metadata in CSL JSON format for one or more citation keys. \" \"Optionally, render metadata into formatted references using Pandoc. \" \"Text outputs are UTF-8 encoded.\" , ) parser . add_argument ( \"--output\" , type = pathlib . Path , help = \"Specify a file to write output, otherwise default to stdout.\" , ) format_group = parser . add_mutually_exclusive_group () format_group . add_argument ( \"--format\" , choices = [ \"csljson\" , \"cslyaml\" , \"plain\" , \"markdown\" , \"docx\" , \"html\" , \"jats\" ], help = \"Format to use for output file. \" \"csljson and cslyaml output the CSL data. \" \"All other choices render the references using Pandoc. \" \"If not specified, attempt to infer this from the --output filename extension. \" \"Otherwise, default to csljson.\" , ) format_group . add_argument ( \"--yml\" , dest = \"format\" , action = \"store_const\" , const = \"cslyaml\" , help = \"Short for --format=cslyaml.\" , ) format_group . add_argument ( \"--txt\" , dest = \"format\" , action = \"store_const\" , const = \"plain\" , help = \"Short for --format=plain.\" , ) format_group . add_argument ( \"--md\" , dest = \"format\" , action = \"store_const\" , const = \"markdown\" , help = \"Short for --format=markdown.\" , ) parser . add_argument ( \"--csl\" , # redirects to the latest Manubot CSL Style. default = \"https://citation-style.manubot.org/\" , help = \"URL or path with CSL XML style used to style references \" \"(i.e. Pandoc's --csl option). \" \"Defaults to Manubot's style.\" , ) parser . add_argument ( \"--bibliography\" , default = [], action = \"append\" , help = \"File to read manual reference metadata. \" \"Specify multiple times to load multiple files. \" \"Similar to pandoc --bibliography.\" , ) parser . add_argument ( \"--no-infer-prefix\" , dest = \"infer_prefix\" , action = \"store_false\" , help = \"Do not attempt to infer the prefix for citekeys without a known prefix.\" , ) parser . add_argument ( \"--allow-invalid-csl-data\" , dest = \"prune_csl\" , action = \"store_false\" , help = \"Allow CSL Items that do not conform to the JSON Schema. Skips CSL pruning.\" , ) parser . add_argument ( \"citekeys\" , nargs = \"+\" , help = \"One or more (space separated) citation keys to generate bibliographic metadata for.\" , ) parser . set_defaults ( function = \"manubot.cite.cite_command.cli_cite\" )","title":"add_subparser_cite"},{"location":"reference/manubot/command/#add_subparser_process","text":"def add_subparser_process ( subparsers ) View Source def add_subparser_process ( subparsers ): parser = subparsers . add_parser ( name = \"process\" , help = \"process manuscript content\" , description = \"Process manuscript content to create outputs for Pandoc consumption. \" \"Performs bibliographic processing and templating.\" , ) parser . add_argument ( \"--content-directory\" , type = pathlib . Path , required = True , help = \"Directory where manuscript content files are located.\" , ) parser . add_argument ( \"--output-directory\" , type = pathlib . Path , required = True , help = \"Directory to output files generated by this script.\" , ) parser . add_argument ( \"--template-variables-path\" , action = \"append\" , default = [], help = \"Path or URL of a file containing template variables for jinja2. \" \"Serialization format is inferred from the file extension, with support for JSON, YAML, and TOML. \" \"If the format cannot be detected, the parser assumes JSON. \" \"Specify this argument multiple times to read multiple files. \" \"Variables can be applied to a namespace (i.e. stored under a dictionary key) \" \"like `--template-variables-path=namespace=path_or_url`. \" \"Namespaces must match the regex `[a-zA-Z_][a-zA-Z0-9_]*`.\" , ) parser . add_argument ( \"--skip-citations\" , action = \"store_true\" , required = True , help = \"Skip citation and reference processing. \" \"Support for citation and reference processing has been moved from `manubot process` to the pandoc-manubot-cite filter. \" \"Therefore this argument is now required. \" \"If citation-tags.tsv is found in content, \" \"these tags will be inserted in the markdown output using the reference-link syntax for citekey aliases. \" \"Appends content/manual-references*.* paths to Pandoc's metadata.bibliography field.\" , ) parser . add_argument ( \"--cache-directory\" , type = pathlib . Path , help = \"Custom cache directory. If not specified, caches to output-directory.\" , ) parser . add_argument ( \"--clear-requests-cache\" , action = \"store_true\" ) parser . add_argument ( \"--skip-remote\" , action = \"store_true\" , help = \"Do not add the rootstock repository to the local git repository remotes.\" , ) parser . set_defaults ( function = \"manubot.process.process_command.cli_process\" )","title":"add_subparser_process"},{"location":"reference/manubot/command/#add_subparser_webpage","text":"def add_subparser_webpage ( subparsers ) View Source def add_subparser_webpage ( subparsers ) : parser = subparsers . add_parser ( name = \"webpage\" , help = \"deploy Manubot outputs to a webpage directory tree\" , description = \"Update the webpage directory tree with Manubot output files. \" \"This command should be run from the root directory of a Manubot manuscript that follows the Rootstock layout, containing `output` and `webpage` directories. \" \"HTML and PDF outputs are copied to the webpage directory, which is structured as static source files for website hosting.\" , ) parser . add_argument ( \"--checkout\" , nargs = \"?\" , const = \"gh-pages\" , default = None , help = \"branch to checkout /v directory contents from. \" \"For example, --checkout=upstream/gh-pages. \" \"--checkout is equivalent to --checkout=gh-pages. \" \"If --checkout is ommitted, no checkout is performed.\" , ) parser . add_argument ( \"--version\" , help = \"Used to create webpage/v/{version} directory. \" \"Generally a commit hash, tag, or 'local'. \" \"When omitted, version defaults to the commit hash on CI builds and 'local' elsewhere.\" , ) parser . add_argument ( \"--timestamp\" , action = \"store_true\" , help = \"timestamp versioned manuscripts in webpage/v using OpenTimestamps. \" \"Specify this flag to create timestamps for the current HTML and PDF outputs and upgrade any timestamps from past manuscript versions.\" , ) cache_group = parser . add_mutually_exclusive_group () cache_group . add_argument ( \"--no-ots-cache\" , action = \"store_true\" , help = \"disable the timestamp cache.\" ) cache_group . add_argument ( \"--ots-cache\" , default = pathlib . Path ( \"ci/cache/ots\" ), type = pathlib . Path , help = \"location for the timestamp cache (default: ci/cache/ots).\" , ) parser . set _defaults ( function = \"manubot.webpage.webpage_command.cli_webpage\" )","title":"add_subparser_webpage"},{"location":"reference/manubot/command/#exit_if_error_handler_fired","text":"def exit_if_error_handler_fired ( error_handler ) If a message has been logged with severity of ERROR or greater, exit Python with a nonzero code. View Source def exit_if_error_handler_fired ( error_handler ) : \"\"\" If a message has been logged with severity of ERROR or greater , exit Python with a nonzero code . \"\"\" if error_handler . fired : logging . critical ( \" Failure: exiting with code 1 due to logged errors \" ) raise SystemExit ( 1 )","title":"exit_if_error_handler_fired"},{"location":"reference/manubot/command/#main","text":"def main ( ) Called as a console_scripts entry point in setup.cfg. This function defines the manubot command line script. View Source def main(): \"\"\" Called as a console_scripts entry point in setup.cfg. This function defines the manubot command line script. \"\"\" diagnostics = setup_logging_and_errors() args = parse_arguments() diagnostics[\"logger\"].setLevel(getattr(logging, args.log_level)) function = import_function(args.function) function(args) exit_if_error_handler_fired(diagnostics[\"error_handler\"])","title":"main"},{"location":"reference/manubot/command/#parse_arguments","text":"def parse_arguments ( ) Read and process command line arguments. View Source def parse_arguments () : \"\"\" Read and process command line arguments . \"\"\" parser = argparse . ArgumentParser ( description = \" Manubot: the manuscript bot for scholarly writing \" ) parser . add_argument ( \" --version \" , action = \" version \" , version = f \" v{manubot.__version__} \" ) subparsers = parser . add_subparsers ( title = \" subcommands \" , description = \" All operations are done through subcommands: \" ) # Require specifying a sub - command subparsers . required = True # https : // bugs . python . org / issue26510 subparsers . dest = \" subcommand \" # https : // bugs . python . org / msg186387 add_subparser_process ( subparsers ) add_subparser_cite ( subparsers ) add_subparser_webpage ( subparsers ) for subparser in subparsers . choices . values () : subparser . add_argument ( \" --log-level \" , default = \" WARNING \" , choices = [ \" DEBUG \" , \" INFO \" , \" WARNING \" , \" ERROR \" , \" CRITICAL \" ], help = \" Set the logging level for stderr logging \" , ) args = parser . parse_args () return args","title":"parse_arguments"},{"location":"reference/manubot/command/#setup_logging_and_errors","text":"def setup_logging_and_errors ( ) -> dict Configure warnings and logging. Set up an ErrorHandler to detect whether messages have been logged at or above the ERROR level. View Source def setup_logging_and_errors () -> dict : \"\"\" Configure warnings and logging. Set up an ErrorHandler to detect whether messages have been logged at or above the ERROR level. \"\"\" import errorhandler # Track if message gets logged with severity of error or greater # See https://stackoverflow.com/a/45446664/4651668 error_handler = errorhandler . ErrorHandler () # Log DeprecationWarnings warnings . simplefilter ( \"always\" , DeprecationWarning ) logging . captureWarnings ( True ) # Log to stderr logger = logging . getLogger () stream_handler = logging . StreamHandler ( stream = sys . stderr ) stream_handler . setFormatter ( logging . Formatter ( \"## {levelname} \\n {message} \" , style = \"{\" ) ) logger . addHandler ( stream_handler ) return { \"logger\" : logger , \"error_handler\" : error_handler , }","title":"setup_logging_and_errors"},{"location":"reference/manubot/util/","text":"Module manubot.util None None View Source import importlib import json import logging import os import pathlib import platform import shlex import shutil import subprocess import sys import typing from types import ModuleType if typing . TYPE_CHECKING : # allow type annotations of lazy-imported packages import yaml # Email address that forwards to Manubot maintainers contact_email : str = \"contact@manubot.org\" def import_function ( name : str ): \"\"\" Import a function in a module specified by name. For example, if name were 'manubot.cite.cite_command.cli_cite', the cli_cite function would be returned as an object. See https://stackoverflow.com/a/8790232/4651668. \"\"\" module_name , function_name = name . rsplit ( \".\" , 1 ) module = importlib . import_module ( module_name ) return getattr ( module , function_name ) def get_manubot_user_agent () -> str : \"\"\" Return a User-Agent string for web request headers to help services identify requests as coming from Manubot. \"\"\" try : from manubot import __version__ as manubot_version except ImportError : manubot_version = \"\" return ( f \"manubot/ { manubot_version } \" f \"( { platform . system () } ; Python/ { sys . version_info . major } . { sys . version_info . minor } ) \" f \"< { contact_email } >\" ) def shlex_join ( split_command ) -> str : \"\"\" Backport shlex.join for Python < 3.8. Also cast all args to str to increase versatility. https://github.com/python/cpython/pull/7605 https://bugs.python.org/issue22454 \"\"\" return \" \" . join ( shlex . quote ( str ( arg )) for arg in split_command ) \"\"\"Valid schemes for HTTP URL detection\"\"\" _http_url_schemes : set = { \"http\" , \"https\" } def is_http_url ( string : str ) -> bool : \"\"\" Return whether `string` is an HTTP(s) Uniform Resource Locator (URL). \"\"\" from urllib.parse import urlparse parsed_url = urlparse ( string ) return parsed_url . scheme in _http_url_schemes def read_serialized_data ( path : str ): \"\"\" Read seralized data from a local file path or web-address. If file format extension is not detected in path, assumes JSON. If a URL does not contain the appropriate suffix, one workaround is to hack the fragment like https://example.org#/variables.toml \"\"\" import requests path_str = os . fspath ( path ) path_obj = pathlib . Path ( path ) supported_suffixes = { \".json\" , \".yaml\" , \".yml\" , \".toml\" } suffixes = set ( path_obj . suffixes ) if is_http_url ( path_str ): headers = { \"User-Agent\" : get_manubot_user_agent ()} response = requests . get ( path_str , headers = headers ) if not suffixes & supported_suffixes : # if URL has no supported suffixes, evaluate suffixes of final redirect suffixes = set ( pathlib . Path ( response . url ) . suffixes ) text = response . text else : text = path_obj . read_text ( encoding = \"utf-8-sig\" ) if { \".yaml\" , \".yml\" } & suffixes : import yaml try : return yaml . safe_load ( text ) except yaml . parser . ParserError as error : _lint_yaml ( path ) raise error if \".toml\" in suffixes : import toml return toml . loads ( text ) if \".json\" not in suffixes : logging . info ( f \"read_serialized_data cannot infer serialization format from the extension of { path_str !r} . \" f \"Supported extensions are { ', ' . join ( supported_suffixes ) } . \" \"Assuming JSON.\" ) return json . loads ( text ) \"\"\" yamllint configuration as per https://yamllint.readthedocs.io/en/stable/configuration.html \"\"\" _yamllint_config = { \"extends\" : \"relaxed\" , \"rules\" : { \"line-length\" : \"disable\" , \"trailing-spaces\" : { \"level\" : \"warning\" }}, } def _lint_yaml ( path ): if not shutil . which ( \"yamllint\" ): logging . info ( f \"yamllint executable not found, skipping linting for { path } \" ) return args = [ \"yamllint\" , \"--config-data\" , json . dumps ( _yamllint_config , indent = None ), os . fspath ( path ), ] sys . stderr . write ( f \"yamllint { path } : \\n \" ) subprocess . run ( args , stdout = sys . stderr ) def read_serialized_dict ( path : str ) -> dict : \"\"\" Read serialized data, confirming that the top-level object is a dictionary. Delegates to `read_serialized_data`. \"\"\" data = read_serialized_data ( path ) if isinstance ( data , dict ): return data raise TypeError ( f \"Expected data encoded by { path !r} to be a dictionary at the top-level. \" f \"Received { data . __class__ . __name__ !r} instead.\" ) def _yaml_str_representer ( dumper : \"yaml.Dumper\" , data : str ): \"\"\" Use YAML literal block style for multiline strings. Based on https://stackoverflow.com/a/33300001/4651668 \"\"\" if len ( data . splitlines ()) > 1 : # use literal block style for multiline strings return dumper . represent_scalar ( \"tag:yaml.org,2002:str\" , data , style = \"|\" ) return dumper . represent_scalar ( \"tag:yaml.org,2002:str\" , data ) def get_configured_yaml () -> ModuleType : \"\"\" Return imported YAML library with Manubot configuration. The representers are only applied to yaml.dump, not yaml.safe_dump. \"\"\" import yaml from manubot.cite.csl_item import CSL_Item yaml . add_representer ( str , _yaml_str_representer ) # CSL_Item: pyyaml chokes on dict subclass # https://github.com/yaml/pyyaml/issues/142 # https://stackoverflow.com/a/50181505/4651668 yaml . add_representer ( CSL_Item , lambda dumper , data : dumper . represent_mapping ( tag = \"tag:yaml.org,2002:map\" , mapping = data . items () ), ) return yaml Variables contact_email Functions get_configured_yaml def get_configured_yaml ( ) -> module Return imported YAML library with Manubot configuration. The representers are only applied to yaml.dump, not yaml.safe_dump. View Source def get_configured_yaml () -> ModuleType : \"\"\" Return imported YAML library with Manubot configuration. The representers are only applied to yaml.dump, not yaml.safe_dump. \"\"\" import yaml from manubot.cite.csl_item import CSL_Item yaml . add_representer ( str , _yaml_str_representer ) # CSL_Item: pyyaml chokes on dict subclass # https://github.com/yaml/pyyaml/issues/142 # https://stackoverflow.com/a/50181505/4651668 yaml . add_representer ( CSL_Item , lambda dumper , data : dumper . represent_mapping ( tag = \"tag:yaml.org,2002:map\" , mapping = data . items () ), ) return yaml get_manubot_user_agent def get_manubot_user_agent ( ) -> str Return a User-Agent string for web request headers to help services identify requests as coming from Manubot. View Source def get_manubot_user_agent () -> str : \"\"\" Return a User-Agent string for web request headers to help services identify requests as coming from Manubot. \"\"\" try : from manubot import __version__ as manubot_version except ImportError : manubot_version = \"\" return ( f \"manubot/ { manubot_version } \" f \"( { platform . system () } ; Python/ { sys . version_info . major } . { sys . version_info . minor } ) \" f \"< { contact_email } >\" ) import_function def import_function ( name : str ) Import a function in a module specified by name. For example, if name were 'manubot.cite.cite_command.cli_cite', the cli_cite function would be returned as an object. See https://stackoverflow.com/a/8790232/4651668. View Source def import_function ( name : str ) : \"\"\" Import a function in a module specified by name . For example , if name were ' manubot.cite.cite_command.cli_cite ' , the cli_cite function would be returned as an object . See https : // stackoverflow . com / a / 8790232 / 4651668 . \"\"\" module_name , function_name = name . rsplit ( \" . \" , 1 ) module = importlib . import_module ( module_name ) return getattr ( module , function_name ) is_http_url def is_http_url ( string : str ) -> bool Return whether string is an HTTP(s) Uniform Resource Locator (URL). View Source def is_http_url ( string : str ) -> bool : \"\"\" Return whether `string` is an HTTP(s) Uniform Resource Locator (URL). \"\"\" from urllib.parse import urlparse parsed_url = urlparse ( string ) return parsed_url . scheme in _http_url_schemes read_serialized_data def read_serialized_data ( path : str ) Read seralized data from a local file path or web-address. If file format extension is not detected in path, assumes JSON. If a URL does not contain the appropriate suffix, one workaround is to hack the fragment like https://example.org#/variables.toml View Source def read_serialized_data ( path : str ): \"\"\" Read seralized data from a local file path or web-address. If file format extension is not detected in path, assumes JSON. If a URL does not contain the appropriate suffix, one workaround is to hack the fragment like https://example.org#/variables.toml \"\"\" import requests path_str = os . fspath ( path ) path_obj = pathlib . Path ( path ) supported_suffixes = { \".json\" , \".yaml\" , \".yml\" , \".toml\" } suffixes = set ( path_obj . suffixes ) if is_http_url ( path_str ): headers = { \"User-Agent\" : get_manubot_user_agent ()} response = requests . get ( path_str , headers = headers ) if not suffixes & supported_suffixes : # if URL has no supported suffixes, evaluate suffixes of final redirect suffixes = set ( pathlib . Path ( response . url ) . suffixes ) text = response . text else : text = path_obj . read_text ( encoding = \"utf-8-sig\" ) if { \".yaml\" , \".yml\" } & suffixes : import yaml try : return yaml . safe_load ( text ) except yaml . parser . ParserError as error : _lint_yaml ( path ) raise error if \".toml\" in suffixes : import toml return toml . loads ( text ) if \".json\" not in suffixes : logging . info ( f \"read_serialized_data cannot infer serialization format from the extension of { path_str !r} . \" f \"Supported extensions are { ', ' . join ( supported_suffixes ) } . \" \"Assuming JSON.\" ) return json . loads ( text ) read_serialized_dict def read_serialized_dict ( path : str ) -> dict Read serialized data, confirming that the top-level object is a dictionary. Delegates to read_serialized_data . View Source def read_serialized_dict ( path : str ) -> dict : \" \"\" Read serialized data, confirming that the top-level object is a dictionary. Delegates to `read_serialized_data`. \"\" \" data = read_serialized_data ( path ) if isinstance ( data , dict ) : return data raise TypeError ( f \"Expected data encoded by {path!r} to be a dictionary at the top-level. \" f \"Received {data.__class__.__name__!r} instead.\" ) shlex_join def shlex_join ( split_command ) -> str Backport shlex.join for Python < 3.8. Also cast all args to str to increase versatility. https://github.com/python/cpython/pull/7605 https://bugs.python.org/issue22454 View Source def shlex_join ( split_command ) -> str : \"\"\" Backport shlex.join for Python < 3.8. Also cast all args to str to increase versatility. https://github.com/python/cpython/pull/7605 https://bugs.python.org/issue22454 \"\"\" return \" \" . join ( shlex . quote ( str ( arg )) for arg in split_command )","title":"Util"},{"location":"reference/manubot/util/#module-manubotutil","text":"None None View Source import importlib import json import logging import os import pathlib import platform import shlex import shutil import subprocess import sys import typing from types import ModuleType if typing . TYPE_CHECKING : # allow type annotations of lazy-imported packages import yaml # Email address that forwards to Manubot maintainers contact_email : str = \"contact@manubot.org\" def import_function ( name : str ): \"\"\" Import a function in a module specified by name. For example, if name were 'manubot.cite.cite_command.cli_cite', the cli_cite function would be returned as an object. See https://stackoverflow.com/a/8790232/4651668. \"\"\" module_name , function_name = name . rsplit ( \".\" , 1 ) module = importlib . import_module ( module_name ) return getattr ( module , function_name ) def get_manubot_user_agent () -> str : \"\"\" Return a User-Agent string for web request headers to help services identify requests as coming from Manubot. \"\"\" try : from manubot import __version__ as manubot_version except ImportError : manubot_version = \"\" return ( f \"manubot/ { manubot_version } \" f \"( { platform . system () } ; Python/ { sys . version_info . major } . { sys . version_info . minor } ) \" f \"< { contact_email } >\" ) def shlex_join ( split_command ) -> str : \"\"\" Backport shlex.join for Python < 3.8. Also cast all args to str to increase versatility. https://github.com/python/cpython/pull/7605 https://bugs.python.org/issue22454 \"\"\" return \" \" . join ( shlex . quote ( str ( arg )) for arg in split_command ) \"\"\"Valid schemes for HTTP URL detection\"\"\" _http_url_schemes : set = { \"http\" , \"https\" } def is_http_url ( string : str ) -> bool : \"\"\" Return whether `string` is an HTTP(s) Uniform Resource Locator (URL). \"\"\" from urllib.parse import urlparse parsed_url = urlparse ( string ) return parsed_url . scheme in _http_url_schemes def read_serialized_data ( path : str ): \"\"\" Read seralized data from a local file path or web-address. If file format extension is not detected in path, assumes JSON. If a URL does not contain the appropriate suffix, one workaround is to hack the fragment like https://example.org#/variables.toml \"\"\" import requests path_str = os . fspath ( path ) path_obj = pathlib . Path ( path ) supported_suffixes = { \".json\" , \".yaml\" , \".yml\" , \".toml\" } suffixes = set ( path_obj . suffixes ) if is_http_url ( path_str ): headers = { \"User-Agent\" : get_manubot_user_agent ()} response = requests . get ( path_str , headers = headers ) if not suffixes & supported_suffixes : # if URL has no supported suffixes, evaluate suffixes of final redirect suffixes = set ( pathlib . Path ( response . url ) . suffixes ) text = response . text else : text = path_obj . read_text ( encoding = \"utf-8-sig\" ) if { \".yaml\" , \".yml\" } & suffixes : import yaml try : return yaml . safe_load ( text ) except yaml . parser . ParserError as error : _lint_yaml ( path ) raise error if \".toml\" in suffixes : import toml return toml . loads ( text ) if \".json\" not in suffixes : logging . info ( f \"read_serialized_data cannot infer serialization format from the extension of { path_str !r} . \" f \"Supported extensions are { ', ' . join ( supported_suffixes ) } . \" \"Assuming JSON.\" ) return json . loads ( text ) \"\"\" yamllint configuration as per https://yamllint.readthedocs.io/en/stable/configuration.html \"\"\" _yamllint_config = { \"extends\" : \"relaxed\" , \"rules\" : { \"line-length\" : \"disable\" , \"trailing-spaces\" : { \"level\" : \"warning\" }}, } def _lint_yaml ( path ): if not shutil . which ( \"yamllint\" ): logging . info ( f \"yamllint executable not found, skipping linting for { path } \" ) return args = [ \"yamllint\" , \"--config-data\" , json . dumps ( _yamllint_config , indent = None ), os . fspath ( path ), ] sys . stderr . write ( f \"yamllint { path } : \\n \" ) subprocess . run ( args , stdout = sys . stderr ) def read_serialized_dict ( path : str ) -> dict : \"\"\" Read serialized data, confirming that the top-level object is a dictionary. Delegates to `read_serialized_data`. \"\"\" data = read_serialized_data ( path ) if isinstance ( data , dict ): return data raise TypeError ( f \"Expected data encoded by { path !r} to be a dictionary at the top-level. \" f \"Received { data . __class__ . __name__ !r} instead.\" ) def _yaml_str_representer ( dumper : \"yaml.Dumper\" , data : str ): \"\"\" Use YAML literal block style for multiline strings. Based on https://stackoverflow.com/a/33300001/4651668 \"\"\" if len ( data . splitlines ()) > 1 : # use literal block style for multiline strings return dumper . represent_scalar ( \"tag:yaml.org,2002:str\" , data , style = \"|\" ) return dumper . represent_scalar ( \"tag:yaml.org,2002:str\" , data ) def get_configured_yaml () -> ModuleType : \"\"\" Return imported YAML library with Manubot configuration. The representers are only applied to yaml.dump, not yaml.safe_dump. \"\"\" import yaml from manubot.cite.csl_item import CSL_Item yaml . add_representer ( str , _yaml_str_representer ) # CSL_Item: pyyaml chokes on dict subclass # https://github.com/yaml/pyyaml/issues/142 # https://stackoverflow.com/a/50181505/4651668 yaml . add_representer ( CSL_Item , lambda dumper , data : dumper . represent_mapping ( tag = \"tag:yaml.org,2002:map\" , mapping = data . items () ), ) return yaml","title":"Module manubot.util"},{"location":"reference/manubot/util/#variables","text":"contact_email","title":"Variables"},{"location":"reference/manubot/util/#functions","text":"","title":"Functions"},{"location":"reference/manubot/util/#get_configured_yaml","text":"def get_configured_yaml ( ) -> module Return imported YAML library with Manubot configuration. The representers are only applied to yaml.dump, not yaml.safe_dump. View Source def get_configured_yaml () -> ModuleType : \"\"\" Return imported YAML library with Manubot configuration. The representers are only applied to yaml.dump, not yaml.safe_dump. \"\"\" import yaml from manubot.cite.csl_item import CSL_Item yaml . add_representer ( str , _yaml_str_representer ) # CSL_Item: pyyaml chokes on dict subclass # https://github.com/yaml/pyyaml/issues/142 # https://stackoverflow.com/a/50181505/4651668 yaml . add_representer ( CSL_Item , lambda dumper , data : dumper . represent_mapping ( tag = \"tag:yaml.org,2002:map\" , mapping = data . items () ), ) return yaml","title":"get_configured_yaml"},{"location":"reference/manubot/util/#get_manubot_user_agent","text":"def get_manubot_user_agent ( ) -> str Return a User-Agent string for web request headers to help services identify requests as coming from Manubot. View Source def get_manubot_user_agent () -> str : \"\"\" Return a User-Agent string for web request headers to help services identify requests as coming from Manubot. \"\"\" try : from manubot import __version__ as manubot_version except ImportError : manubot_version = \"\" return ( f \"manubot/ { manubot_version } \" f \"( { platform . system () } ; Python/ { sys . version_info . major } . { sys . version_info . minor } ) \" f \"< { contact_email } >\" )","title":"get_manubot_user_agent"},{"location":"reference/manubot/util/#import_function","text":"def import_function ( name : str ) Import a function in a module specified by name. For example, if name were 'manubot.cite.cite_command.cli_cite', the cli_cite function would be returned as an object. See https://stackoverflow.com/a/8790232/4651668. View Source def import_function ( name : str ) : \"\"\" Import a function in a module specified by name . For example , if name were ' manubot.cite.cite_command.cli_cite ' , the cli_cite function would be returned as an object . See https : // stackoverflow . com / a / 8790232 / 4651668 . \"\"\" module_name , function_name = name . rsplit ( \" . \" , 1 ) module = importlib . import_module ( module_name ) return getattr ( module , function_name )","title":"import_function"},{"location":"reference/manubot/util/#is_http_url","text":"def is_http_url ( string : str ) -> bool Return whether string is an HTTP(s) Uniform Resource Locator (URL). View Source def is_http_url ( string : str ) -> bool : \"\"\" Return whether `string` is an HTTP(s) Uniform Resource Locator (URL). \"\"\" from urllib.parse import urlparse parsed_url = urlparse ( string ) return parsed_url . scheme in _http_url_schemes","title":"is_http_url"},{"location":"reference/manubot/util/#read_serialized_data","text":"def read_serialized_data ( path : str ) Read seralized data from a local file path or web-address. If file format extension is not detected in path, assumes JSON. If a URL does not contain the appropriate suffix, one workaround is to hack the fragment like https://example.org#/variables.toml View Source def read_serialized_data ( path : str ): \"\"\" Read seralized data from a local file path or web-address. If file format extension is not detected in path, assumes JSON. If a URL does not contain the appropriate suffix, one workaround is to hack the fragment like https://example.org#/variables.toml \"\"\" import requests path_str = os . fspath ( path ) path_obj = pathlib . Path ( path ) supported_suffixes = { \".json\" , \".yaml\" , \".yml\" , \".toml\" } suffixes = set ( path_obj . suffixes ) if is_http_url ( path_str ): headers = { \"User-Agent\" : get_manubot_user_agent ()} response = requests . get ( path_str , headers = headers ) if not suffixes & supported_suffixes : # if URL has no supported suffixes, evaluate suffixes of final redirect suffixes = set ( pathlib . Path ( response . url ) . suffixes ) text = response . text else : text = path_obj . read_text ( encoding = \"utf-8-sig\" ) if { \".yaml\" , \".yml\" } & suffixes : import yaml try : return yaml . safe_load ( text ) except yaml . parser . ParserError as error : _lint_yaml ( path ) raise error if \".toml\" in suffixes : import toml return toml . loads ( text ) if \".json\" not in suffixes : logging . info ( f \"read_serialized_data cannot infer serialization format from the extension of { path_str !r} . \" f \"Supported extensions are { ', ' . join ( supported_suffixes ) } . \" \"Assuming JSON.\" ) return json . loads ( text )","title":"read_serialized_data"},{"location":"reference/manubot/util/#read_serialized_dict","text":"def read_serialized_dict ( path : str ) -> dict Read serialized data, confirming that the top-level object is a dictionary. Delegates to read_serialized_data . View Source def read_serialized_dict ( path : str ) -> dict : \" \"\" Read serialized data, confirming that the top-level object is a dictionary. Delegates to `read_serialized_data`. \"\" \" data = read_serialized_data ( path ) if isinstance ( data , dict ) : return data raise TypeError ( f \"Expected data encoded by {path!r} to be a dictionary at the top-level. \" f \"Received {data.__class__.__name__!r} instead.\" )","title":"read_serialized_dict"},{"location":"reference/manubot/util/#shlex_join","text":"def shlex_join ( split_command ) -> str Backport shlex.join for Python < 3.8. Also cast all args to str to increase versatility. https://github.com/python/cpython/pull/7605 https://bugs.python.org/issue22454 View Source def shlex_join ( split_command ) -> str : \"\"\" Backport shlex.join for Python < 3.8. Also cast all args to str to increase versatility. https://github.com/python/cpython/pull/7605 https://bugs.python.org/issue22454 \"\"\" return \" \" . join ( shlex . quote ( str ( arg )) for arg in split_command )","title":"shlex_join"},{"location":"reference/manubot/cite/","text":"Module manubot.cite None None View Source __all__ = [ \"citekey_to_csl_item\" , ] from manubot.cite.citekey import citekey_to_csl_item Sub-modules manubot.cite.arxiv manubot.cite.citations manubot.cite.cite_command manubot.cite.citekey manubot.cite.citeproc manubot.cite.csl_item manubot.cite.curie manubot.cite.doi manubot.cite.handlers manubot.cite.isbn manubot.cite.pubmed manubot.cite.tests manubot.cite.unpaywall manubot.cite.url manubot.cite.wikidata manubot.cite.zotero Functions citekey_to_csl_item def citekey_to_csl_item ( citekey , prune = True , manual_refs = {}, log_level : Union [ str , int ] = 'WARNING' ) Generate a CSL_Item for the input citekey. View Source def citekey_to_csl_item ( citekey , prune = True , manual_refs = {}, log_level : tp . Union [ str , int ] = \"WARNING\" ): \"\"\" Generate a CSL_Item for the input citekey. \"\"\" from manubot import __version__ as manubot_version # https://stackoverflow.com/a/35704430/4651668 log_level = logging . _checkLevel ( log_level ) if not isinstance ( citekey , CiteKey ): citekey = CiteKey ( citekey ) if citekey . standard_id in manual_refs : return manual_refs [ citekey . standard_id ] try : csl_item = citekey . csl_item except Exception as error : logging . log ( log_level , f \"Generating csl_item for { citekey . standard_id !r} failed \" f \"due to a { error . __class__ . __name__ } : \\n { error } \" , ) logging . info ( error , exc_info = True ) return None # update csl_item with manubot generated metadata note_text = f \"This CSL Item was generated by Manubot v { manubot_version } from its persistent identifier (standard_id).\" note_dict = { \"standard_id\" : citekey . standard_id } csl_item . note_append_text ( note_text ) csl_item . note_append_dict ( note_dict ) csl_item . set_id ( citekey . short_id ) csl_item . clean ( prune = prune ) return csl_item","title":"Index"},{"location":"reference/manubot/cite/#module-manubotcite","text":"None None View Source __all__ = [ \"citekey_to_csl_item\" , ] from manubot.cite.citekey import citekey_to_csl_item","title":"Module manubot.cite"},{"location":"reference/manubot/cite/#sub-modules","text":"manubot.cite.arxiv manubot.cite.citations manubot.cite.cite_command manubot.cite.citekey manubot.cite.citeproc manubot.cite.csl_item manubot.cite.curie manubot.cite.doi manubot.cite.handlers manubot.cite.isbn manubot.cite.pubmed manubot.cite.tests manubot.cite.unpaywall manubot.cite.url manubot.cite.wikidata manubot.cite.zotero","title":"Sub-modules"},{"location":"reference/manubot/cite/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/#citekey_to_csl_item","text":"def citekey_to_csl_item ( citekey , prune = True , manual_refs = {}, log_level : Union [ str , int ] = 'WARNING' ) Generate a CSL_Item for the input citekey. View Source def citekey_to_csl_item ( citekey , prune = True , manual_refs = {}, log_level : tp . Union [ str , int ] = \"WARNING\" ): \"\"\" Generate a CSL_Item for the input citekey. \"\"\" from manubot import __version__ as manubot_version # https://stackoverflow.com/a/35704430/4651668 log_level = logging . _checkLevel ( log_level ) if not isinstance ( citekey , CiteKey ): citekey = CiteKey ( citekey ) if citekey . standard_id in manual_refs : return manual_refs [ citekey . standard_id ] try : csl_item = citekey . csl_item except Exception as error : logging . log ( log_level , f \"Generating csl_item for { citekey . standard_id !r} failed \" f \"due to a { error . __class__ . __name__ } : \\n { error } \" , ) logging . info ( error , exc_info = True ) return None # update csl_item with manubot generated metadata note_text = f \"This CSL Item was generated by Manubot v { manubot_version } from its persistent identifier (standard_id).\" note_dict = { \"standard_id\" : citekey . standard_id } csl_item . note_append_text ( note_text ) csl_item . note_append_dict ( note_dict ) csl_item . set_id ( citekey . short_id ) csl_item . clean ( prune = prune ) return csl_item","title":"citekey_to_csl_item"},{"location":"reference/manubot/cite/arxiv/","text":"Module manubot.cite.arxiv None None View Source import logging import re import xml.etree.ElementTree import requests from manubot.util import get_manubot_user_agent from .csl_item import CSL_Item from .handlers import Handler class Handler_arXiv ( Handler ): standard_prefix = \"arxiv\" prefixes = [ \"arxiv\" , ] accession_pattern = re . compile ( r \"(?P<versionless_id>[0-9] {4} \\.[0-9]{4,5}|[a-z\\-]+(\\.[A-Z] {2} )?/[0-9] {7} )(?P<version>v[0-9]+)?\" ) def inspect ( self , citekey ): # https://arxiv.org/help/arxiv_identifier if not self . _get_pattern () . fullmatch ( citekey . accession ): return \"arXiv identifiers must conform to syntax described at https://arxiv.org/help/arxiv_identifier.\" def get_csl_item ( self , citekey ): return get_arxiv_csl_item ( citekey . standard_accession ) class CSL_Item_arXiv ( CSL_Item ): def _set_invariant_fields ( self ): # Set journal/publisher to arXiv self [ \"container-title\" ] = \"arXiv\" self [ \"publisher\" ] = \"arXiv\" # Set CSL type to report for preprint self [ \"type\" ] = \"report\" return self def log_journal_doi ( self , arxiv_id , journal_ref = None ): if \"DOI\" not in self : return msg = f \"arXiv article { arxiv_id } published at https://doi.org/ { self [ 'DOI' ] } \" if journal_ref : msg += f \" \u2014 { journal_ref } \" logging . info ( msg ) def set_identifier_fields ( self , arxiv_id ): self . set_id ( f \"arxiv: { arxiv_id } \" ) self [ \"URL\" ] = f \"https://arxiv.org/abs/ { arxiv_id } \" self [ \"number\" ] = arxiv_id _ , version = split_arxiv_id_version ( arxiv_id ) if version : self [ \"version\" ] = version def split_arxiv_id_version ( arxiv_id : str ): \"\"\" Return (versionless_id, version) tuple. Version refers to the verion suffix like 'v2' or None. \"\"\" match = re . match ( Handler_arXiv . accession_pattern , arxiv_id ) return match . group ( \"versionless_id\" ), match . group ( \"version\" ) def get_arxiv_csl_item ( arxiv_id : str ): \"\"\" Return csl_item item for an arXiv identifier. Chooses which arXiv API to use based on whether arxiv_id is versioned, since only one endpoint supports versioning. \"\"\" _ , version = split_arxiv_id_version ( arxiv_id ) if version : return get_arxiv_csl_item_export_api ( arxiv_id ) return get_arxiv_csl_item_oai ( arxiv_id ) def query_arxiv_api ( url , params ): headers = { \"User-Agent\" : get_manubot_user_agent ()} response = requests . get ( url , params , headers = headers ) response . raise_for_status () xml_tree = xml . etree . ElementTree . fromstring ( response . text ) return xml_tree def get_arxiv_csl_item_export_api ( arxiv_id ): \"\"\" Return csl_item item for an arXiv record. arxiv_id can be versioned, like `1512.00567v2`, or versionless, like `1512.00567`. If versionless, the arXiv API will return metadata for the latest version. Legacy IDs, such as `cond-mat/0703470v2`, are also supported. If arXiv has an associated DOI for the record, a warning is logged to alert the user that an alternative version of record exists. References: - https://arxiv.org/help/api/index - http://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html - https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" xml_tree = query_arxiv_api ( url = \"https://export.arxiv.org/api/query\" , params = { \"id_list\" : arxiv_id , \"max_results\" : 1 }, ) # XML namespace prefixes prefix = \"{http://www.w3.org/2005/Atom}\" alt_prefix = \"{http://arxiv.org/schemas/atom}\" # Parse XML ( entry ,) = xml_tree . findall ( prefix + \"entry\" ) # Create dictionary for CSL Item csl_item = CSL_Item_arXiv () # Extract versioned arXiv ID url = entry . findtext ( prefix + \"id\" ) pattern = re . compile ( r \"arxiv.org/abs/(.+)\" ) match = pattern . search ( url ) versioned_id = match . group ( 1 ) csl_item . set_identifier_fields ( versioned_id ) # Extrat CSL title field csl_item [ \"title\" ] = entry . findtext ( prefix + \"title\" ) # Extract CSL date field published = entry . findtext ( prefix + \"published\" ) csl_item . set_date ( published , variable = \"issued\" ) # Extract authors authors = list () for elem in entry . findall ( prefix + \"author\" ): name = elem . findtext ( prefix + \"name\" ) author = { \"literal\" : name } authors . append ( author ) csl_item [ \"author\" ] = authors csl_item . _set_invariant_fields () # Extract abstract abstract = entry . findtext ( prefix + \"summary\" ) . strip () if abstract : # remove newlines that were added to wrap abstract abstract = remove_newlines ( abstract ) csl_item [ \"abstract\" ] = abstract # Check if the article has been published with a DOI doi = entry . findtext ( f \" { alt_prefix } doi\" ) if doi : csl_item [ \"DOI\" ] = doi journal_ref = entry . findtext ( alt_prefix + \"journal_ref\" ) csl_item . log_journal_doi ( arxiv_id , journal_ref ) return csl_item def get_arxiv_csl_item_oai ( arxiv_id ): \"\"\" Generate a CSL Item for an unversioned arXiv identifier using arXiv's OAI_PMH v2.0 API <https://arxiv.org/help/oa>. This endpoint does not support versioned `arxiv_id`. \"\"\" # XML namespace prefixes ns_oai = \"{http://www.openarchives.org/OAI/2.0/}\" ns_arxiv = \"{http://arxiv.org/OAI/arXiv/}\" xml_tree = query_arxiv_api ( url = \"https://export.arxiv.org/oai2\" , params = { \"verb\" : \"GetRecord\" , \"metadataPrefix\" : \"arXiv\" , \"identifier\" : f \"oai:arXiv.org: { arxiv_id } \" , }, ) # Create dictionary for CSL Item csl_item = CSL_Item_arXiv () # Extract parent XML elements ( header_elem ,) = xml_tree . findall ( f \" { ns_oai } GetRecord/ { ns_oai } record/ { ns_oai } header\" ) ( metadata_elem ,) = xml_tree . findall ( f \" { ns_oai } GetRecord/ { ns_oai } record/ { ns_oai } metadata\" ) ( arxiv_elem ,) = metadata_elem . findall ( f \" { ns_arxiv } arXiv\" ) # Set identifier fields response_arxiv_id = arxiv_elem . findtext ( f \" { ns_arxiv } id\" ) if arxiv_id != response_arxiv_id : logging . warning ( \"arXiv oai2 query returned a different arxiv_id:\" f \" { arxiv_id } became { response_arxiv_id } \" ) csl_item . set_identifier_fields ( response_arxiv_id ) # Set title and date title = arxiv_elem . findtext ( f \" { ns_arxiv } title\" ) if title : csl_item [ \"title\" ] = \" \" . join ( title . split ()) datestamp = header_elem . findtext ( f \" { ns_oai } datestamp\" ) csl_item . set_date ( datestamp , \"issued\" ) # Extract authors author_elems = arxiv_elem . findall ( f \" { ns_arxiv } authors/ { ns_arxiv } author\" ) authors = list () for author_elem in author_elems : author = {} given = author_elem . findtext ( f \" { ns_arxiv } forenames\" ) family = author_elem . findtext ( f \" { ns_arxiv } keyname\" ) if given : author [ \"given\" ] = given if family : author [ \"family\" ] = family authors . append ( author ) csl_item [ \"author\" ] = authors csl_item . _set_invariant_fields () abstract = arxiv_elem . findtext ( f \" { ns_arxiv } abstract\" ) if abstract : csl_item [ \"abstract\" ] = remove_newlines ( abstract ) license = arxiv_elem . findtext ( f \" { ns_arxiv } license\" ) if license : csl_item . note_append_dict ({ \"license\" : license }) doi = arxiv_elem . findtext ( f \" { ns_arxiv } doi\" ) if doi : csl_item [ \"DOI\" ] = doi journal_ref = arxiv_elem . findtext ( f \" { ns_arxiv } journal-ref\" ) csl_item . log_journal_doi ( arxiv_id , journal_ref ) return csl_item def remove_newlines ( text ): return re . sub ( pattern = r \"\\n(?!\\s)\" , repl = \" \" , string = text ) def get_arxiv_csl_item_zotero ( arxiv_id ): \"\"\" Generate CSL JSON Data for an arXiv ID using Zotero's translation-server. \"\"\" from manubot.cite.zotero import get_csl_item return get_csl_item ( f \"arxiv: { arxiv_id } \" ) Functions get_arxiv_csl_item def get_arxiv_csl_item ( arxiv_id : str ) Return csl_item item for an arXiv identifier. Chooses which arXiv API to use based on whether arxiv_id is versioned, since only one endpoint supports versioning. View Source def get_arxiv_csl_item ( arxiv_id : str ): \"\"\" Return csl_item item for an arXiv identifier. Chooses which arXiv API to use based on whether arxiv_id is versioned, since only one endpoint supports versioning. \"\"\" _ , version = split_arxiv_id_version ( arxiv_id ) if version : return get_arxiv_csl_item_export_api ( arxiv_id ) return get_arxiv_csl_item_oai ( arxiv_id ) get_arxiv_csl_item_export_api def get_arxiv_csl_item_export_api ( arxiv_id ) Return csl_item item for an arXiv record. arxiv_id can be versioned, like 1512.00567v2 , or versionless, like 1512.00567 . If versionless, the arXiv API will return metadata for the latest version. Legacy IDs, such as cond-mat/0703470v2 , are also supported. If arXiv has an associated DOI for the record, a warning is logged to alert the user that an alternative version of record exists. References: - https://arxiv.org/help/api/index - http://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html - https://github.com/citation-style-language/schema/blob/master/csl-data.json View Source def get_arxiv_csl_item_export_api ( arxiv_id ): \"\"\" Return csl_item item for an arXiv record. arxiv_id can be versioned, like `1512.00567v2`, or versionless, like `1512.00567`. If versionless, the arXiv API will return metadata for the latest version. Legacy IDs, such as `cond-mat/0703470v2`, are also supported. If arXiv has an associated DOI for the record, a warning is logged to alert the user that an alternative version of record exists. References: - https://arxiv.org/help/api/index - http://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html - https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" xml_tree = query_arxiv_api ( url = \"https://export.arxiv.org/api/query\" , params = { \"id_list\" : arxiv_id , \"max_results\" : 1 }, ) # XML namespace prefixes prefix = \"{http://www.w3.org/2005/Atom}\" alt_prefix = \"{http://arxiv.org/schemas/atom}\" # Parse XML ( entry ,) = xml_tree . findall ( prefix + \"entry\" ) # Create dictionary for CSL Item csl_item = CSL_Item_arXiv () # Extract versioned arXiv ID url = entry . findtext ( prefix + \"id\" ) pattern = re . compile ( r \"arxiv.org/abs/(.+)\" ) match = pattern . search ( url ) versioned_id = match . group ( 1 ) csl_item . set_identifier_fields ( versioned_id ) # Extrat CSL title field csl_item [ \"title\" ] = entry . findtext ( prefix + \"title\" ) # Extract CSL date field published = entry . findtext ( prefix + \"published\" ) csl_item . set_date ( published , variable = \"issued\" ) # Extract authors authors = list () for elem in entry . findall ( prefix + \"author\" ): name = elem . findtext ( prefix + \"name\" ) author = { \"literal\" : name } authors . append ( author ) csl_item [ \"author\" ] = authors csl_item . _set_invariant_fields () # Extract abstract abstract = entry . findtext ( prefix + \"summary\" ) . strip () if abstract : # remove newlines that were added to wrap abstract abstract = remove_newlines ( abstract ) csl_item [ \"abstract\" ] = abstract # Check if the article has been published with a DOI doi = entry . findtext ( f \"{alt_prefix}doi\" ) if doi : csl_item [ \"DOI\" ] = doi journal_ref = entry . findtext ( alt_prefix + \"journal_ref\" ) csl_item . log_journal_doi ( arxiv_id , journal_ref ) return csl_item get_arxiv_csl_item_oai def get_arxiv_csl_item_oai ( arxiv_id ) Generate a CSL Item for an unversioned arXiv identifier using arXiv's OAI_PMH v2.0 API https://arxiv.org/help/oa . This endpoint does not support versioned arxiv_id . View Source def get_arxiv_csl_item_oai ( arxiv_id ) : \" \"\" Generate a CSL Item for an unversioned arXiv identifier using arXiv's OAI_PMH v2.0 API <https://arxiv.org/help/oa>. This endpoint does not support versioned `arxiv_id`. \"\" \" # XML namespace prefixes ns_oai = \"{http://www.openarchives.org/OAI/2.0/}\" ns_arxiv = \"{http://arxiv.org/OAI/arXiv/}\" xml_tree = query_arxiv_api ( url = \"https://export.arxiv.org/oai2\" , params = { \"verb\" : \"GetRecord\" , \"metadataPrefix\" : \"arXiv\" , \"identifier\" : f \"oai:arXiv.org:{arxiv_id}\" , } , ) # Create dictionary for CSL Item csl_item = CSL_Item_arXiv () # Extract parent XML elements ( header_elem ,) = xml_tree . findall ( f \"{ns_oai}GetRecord/{ns_oai}record/{ns_oai}header\" ) ( metadata_elem ,) = xml_tree . findall ( f \"{ns_oai}GetRecord/{ns_oai}record/{ns_oai}metadata\" ) ( arxiv_elem ,) = metadata_elem . findall ( f \"{ns_arxiv}arXiv\" ) # Set identifier fields response_arxiv_id = arxiv_elem . findtext ( f \"{ns_arxiv}id\" ) if arxiv_id != response_arxiv_id : logging . warning ( \"arXiv oai2 query returned a different arxiv_id:\" f \" {arxiv_id} became {response_arxiv_id}\" ) csl_item . set _identifier_fields ( response_arxiv_id ) # Set title and date title = arxiv_elem . findtext ( f \"{ns_arxiv}title\" ) if title : csl_item [ \"title\" ] = \" \" . join ( title . split ()) datestamp = header_elem . findtext ( f \"{ns_oai}datestamp\" ) csl_item . set _date ( datestamp , \"issued\" ) # Extract authors author_elems = arxiv_elem . findall ( f \"{ns_arxiv}authors/{ns_arxiv}author\" ) authors = list () for author_elem in author_elems : author = {} given = author_elem . findtext ( f \"{ns_arxiv}forenames\" ) family = author_elem . findtext ( f \"{ns_arxiv}keyname\" ) if given : author [ \"given\" ] = given if family : author [ \"family\" ] = family authors . append ( author ) csl_item [ \"author\" ] = authors csl_item . _set_invariant_fields () abstract = arxiv_elem . findtext ( f \"{ns_arxiv}abstract\" ) if abstract : csl_item [ \"abstract\" ] = remove_newlines ( abstract ) license = arxiv_elem . findtext ( f \"{ns_arxiv}license\" ) if license : csl_item . note_append_dict ( { \"license\" : license } ) doi = arxiv_elem . findtext ( f \"{ns_arxiv}doi\" ) if doi : csl_item [ \"DOI\" ] = doi journal_ref = arxiv_elem . findtext ( f \"{ns_arxiv}journal-ref\" ) csl_item . log_journal_doi ( arxiv_id , journal_ref ) return csl_item get_arxiv_csl_item_zotero def get_arxiv_csl_item_zotero ( arxiv_id ) Generate CSL JSON Data for an arXiv ID using Zotero's translation-server. View Source def get_arxiv_csl_item_zotero ( arxiv_id ): \"\"\" Generate CSL JSON Data for an arXiv ID using Zotero's translation-server. \"\"\" from manubot.cite.zotero import get_csl_item return get_csl_item ( f \"arxiv: { arxiv_id } \" ) query_arxiv_api def query_arxiv_api ( url , params ) View Source def query_arxiv_api ( url , params ) : headers = { \" User-Agent \" : get_manubot_user_agent () } response = requests . get ( url , params , headers = headers ) response . raise_for_status () xml_tree = xml . etree . ElementTree . fromstring ( response . text ) return xml_tree remove_newlines def remove_newlines ( text ) View Source def remove_newlines ( text ) : return re . sub ( pattern = r \" \\n (?!\\s) \" , repl = \" \" , string = text ) split_arxiv_id_version def split_arxiv_id_version ( arxiv_id : str ) Return (versionless_id, version) tuple. Version refers to the verion suffix like 'v2' or None. View Source def split_arxiv_id_version ( arxiv_id : str ) : \"\"\" Return ( versionless_id , version ) tuple . Version refers to the verion suffix like ' v2 ' or None . \"\"\" match = re . match ( Handler_arXiv . accession_pattern , arxiv_id ) return match . group ( \" versionless_id \" ) , match . group ( \" version \" ) Classes CSL_Item_arXiv class CSL_Item_arXiv ( dictionary = None , ** kwargs ) View Source class CSL_Item_arXiv ( CSL_Item ): def _set_invariant_fields ( self ): # Set journal/publisher to arXiv self [ \"container-title\" ] = \"arXiv\" self [ \"publisher\" ] = \"arXiv\" # Set CSL type to report for preprint self [ \"type\" ] = \"report\" return self def log_journal_doi ( self , arxiv_id , journal_ref = None ): if \"DOI\" not in self : return msg = f \"arXiv article {arxiv_id} published at https://doi.org/{self['DOI']}\" if journal_ref : msg += f \" \u2014 {journal_ref}\" logging . info ( msg ) def set_identifier_fields ( self , arxiv_id ): self . set_id ( f \"arxiv:{arxiv_id}\" ) self [ \"URL\" ] = f \"https://arxiv.org/abs/{arxiv_id}\" self [ \"number\" ] = arxiv_id _ , version = split_arxiv_id_version ( arxiv_id ) if version : self [ \"version\" ] = version Ancestors (in MRO) manubot.cite.csl_item.CSL_Item builtins.dict Class variables type_mapping Instance variables note Return the value of the \"note\" field as a string. If \"note\" key is not set, return empty string. note_dict Return a dictionary with key-value pairs encoded by this CSL Item's note. Extracts both forms (line-entry and braced-entry) of key-value pairs from the CSL JSON \"cheater syntax\" https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields Assigning to this dict will not update self[\"note\"] . Methods clean def clean ( self , prune : bool = True ) -> 'CSL_Item' Sanitize and touch-up a potentially dirty CSL_Item. The following steps are performed: - update incorrect values for \"type\" field when a correct variant is known - remove fields that violate the JSON Schema (if prune=True) - set default value for \"type\" if missing, since CSL JSON requires type - validate against the CSL JSON schema (if prune=True) to ensure output CSL_Item is clean View Source def clean ( self , prune : bool = True ) -> \"CSL_Item\" : \"\"\" Sanitize and touch-up a potentially dirty CSL_Item. The following steps are performed: - update incorrect values for \"type\" field when a correct variant is known - remove fields that violate the JSON Schema (if prune=True) - set default value for \"type\" if missing, since CSL JSON requires type - validate against the CSL JSON schema (if prune=True) to ensure output CSL_Item is clean \"\"\" logging . debug ( f \"Starting CSL_Item.clean with{'' if prune else 'out'}\" f \"CSL pruning for id: {self.get('id', 'id not specified')}\" ) self . correct_invalid_type () if prune : self . prune_against_schema () self . set_default_type () if prune : self . validate_against_schema () return self clear def clear ( ... ) D.clear() -> None. Remove all items from D. copy def copy ( ... ) D.copy() -> a shallow copy of D correct_invalid_type def correct_invalid_type ( self ) -> 'CSL_Item' Correct invalid CSL item type. Does nothing if type not present. For detail see https://github.com/CrossRef/rest-api-doc/issues/187 View Source def correct_invalid_type ( self ) -> \"CSL_Item\" : \" \"\" Correct invalid CSL item type. Does nothing if `type` not present. For detail see https://github.com/CrossRef/rest-api-doc/issues/187 \"\" \" if \"type\" in self : # Replace a type from in CSL_Item.type_mapping.keys(), # leave type intact in other cases. t = self [ \"type\" ] self [ \"type\" ] = self . type_mapping . get ( t , t ) return self fromkeys def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value. get def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default. get_date def get_date ( self , variable : str = 'issued' , fill : bool = False ) -> Optional [ str ] Return a CSL date-variable as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). variable: which CSL JSON date variable to retrieve fill: if True, set missing months to January and missing days to the first day of the month. View Source def get_date ( self , variable : str = \"issued\" , fill : bool = False ) -> Optional [ str ] : \"\"\" Return a CSL date-variable as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). variable: which CSL JSON date variable to retrieve fill: if True, set missing months to January and missing days to the first day of the month. \"\"\" try : date_parts = self [ variable ][ \"date-parts\" ][ 0 ] except ( IndexError , KeyError ) : return None return date_parts_to_string ( date_parts , fill = fill ) infer_id def infer_id ( self ) -> 'CSL_Item' Detect and set a non-null/empty value for \"id\" or else raise a ValueError. View Source def infer_id ( self ) -> \"CSL_Item\" : \"\"\" Detect and set a non-null/empty value for \" id \" or else raise a ValueError. \"\"\" if self . get ( \"standard_citation\" ) : # \"standard_citation\" field is set with a non - null / empty value return self . set_id ( self . pop ( \"standard_citation\" )) if self . note_dict . get ( \"standard_id\" ) : # \"standard_id\" note field is set with a non - null / empty value return self . set_id ( self . note_dict [ \"standard_id\" ]) if self . get ( \"id\" ) : # \"id\" field exists and is set with a non - null / empty value return self . set_id ( self [ \"id\" ]) raise ValueError ( \"infer_id could not detect a field with a citation / standard_citation. \" 'Consider setting the CSL Item \" id \" field.' ) items def items ( ... ) D.items() -> a set-like object providing a view on D's items keys def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys log_journal_doi def log_journal_doi ( self , arxiv_id , journal_ref = None ) View Source def log_journal_doi ( self , arxiv_id , journal_ref = None ) : if \" DOI \" not in self : return msg = f \" arXiv article {arxiv_id} published at https://doi.org/{self['DOI']} \" if journal_ref : msg += f \" \u2014 {journal_ref} \" logging . info ( msg ) note_append_dict def note_append_dict ( self , dictionary : dict ) -> None Append key-value pairs specified by dictionary to the note field of a CSL Item. Uses the the CSL JSON \"cheater syntax\" to encode additional values not defined by the CSL JSON schema. View Source def note_append_dict ( self , dictionary : dict ) -> None : \" \"\" Append key-value pairs specified by `dictionary` to the note field of a CSL Item. Uses the the [CSL JSON \" cheater syntax \"](https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields) to encode additional values not defined by the CSL JSON schema. \"\" \" for key , value in dictionary . items () : if not re . fullmatch ( r \"[A-Z]+|[-_a-z]+\" , key ) : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"it does not conform to the variable_name syntax as per https://git.io/fjTzW.\" ) continue if \" \\n \" in value : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"the value contains a newline: {value!r}\" ) continue self . note_append_text ( f \"{key}: {value}\" ) note_append_text def note_append_text ( self , text : str ) -> None Append text to the note field (as a new line) of a CSL Item. If a line already exists equal to text , do nothing. View Source def note_append_text ( self , text : str ) -> None : \" \"\" Append text to the note field (as a new line) of a CSL Item. If a line already exists equal to `text`, do nothing. \"\" \" if not text : return note = self . note if re . search ( f \"^{re.escape(text)}$\" , note , flags = re . MULTILINE ) : # do not accumulate duplicate lines of text # https://github.com/manubot/manubot/issues/258 return if note and not note . endswith ( \" \\n \" ) : note += \" \\n \" note += text self . note = note pop def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, default is returned if given, otherwise KeyError is raised popitem def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. prune_against_schema def prune_against_schema ( self ) -> 'CSL_Item' Remove fields that violate the CSL Item JSON Schema. View Source def prune_against_schema ( self ) -> \"CSL_Item\" : \"\"\" Remove fields that violate the CSL Item JSON Schema. \"\"\" from .citeproc import remove_jsonschema_errors ( csl_item ,) = remove_jsonschema_errors ([ self ], in_place = True ) assert csl_item is self return self set_date def set_date ( self , date : Union [ NoneType , str , datetime . date , datetime . datetime ], variable : str = 'issued' ) -> 'CSL_Item' date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). variable: which variable to assign the date to. View Source def set_date ( self , date : Union [ None, str, datetime.date, datetime.datetime ] , variable : str = \"issued\" , ) -> \"CSL_Item\" : \"\"\" date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). variable: which variable to assign the date to. \"\"\" date_parts = date_to_date_parts ( date ) if date_parts : self [ variable ] = { \"date-parts\" : [ date_parts ] } return self set_default_type def set_default_type ( self ) -> 'CSL_Item' Set type to 'entry', if type not specified. View Source def set_default_type ( self ) -> \"CSL_Item\" : \"\"\" Set type to ' entry ', if type not specified. \"\"\" self [ \"type\" ] = self . get ( \"type\" , \"entry\" ) return self set_id def set_id ( self , id_ ) -> 'CSL_Item' View Source def set_id ( self , id_ ) -> \"CSL_Item\" : self [ \"id\" ] = id_ return self set_identifier_fields def set_identifier_fields ( self , arxiv_id ) View Source def set_identifier_fields ( self , arxiv_id ) : self . set_id ( f \" arxiv:{arxiv_id} \" ) self [ \" URL \" ] = f \" https://arxiv.org/abs/{arxiv_id} \" self [ \" number \" ] = arxiv_id _ , version = split_arxiv_id_version ( arxiv_id ) if version : self [ \" version \" ] = version setdefault def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. standardize_id def standardize_id ( self ) -> 'CSL_Item' Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \"id\" field. The standard_id is extracted from a \"standard_citation\" field, the \"note\" field, or the \"id\" field. The extracted citation is checked for validity and standardized, after which it is the final \"standard_id\". Regarding csl_item modification, the csl_item \"id\" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id and original_id. Note that the Manubot software generally refers to the \"id\" of a CSL Item as a citekey. However, in this context, we use \"id\" rather than \"citekey\" for consistency with CSL's \"id\" field. View Source def standardize_id ( self ) -> \"CSL_Item\" : \"\"\" Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \" id \" field. The standard_id is extracted from a \" standard_citation \" field, the \" note \" field, or the \" id \" field. The extracted citation is checked for validity and standardized, after which it is the final \" standard_id \". Regarding csl_item modification, the csl_item \" id \" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id and original_id. Note that the Manubot software generally refers to the \" id \" of a CSL Item as a citekey. However, in this context, we use \" id \" rather than \" citekey \" for consistency with CSL' s \"id\" field . \"\"\" original_id = self.get(\" id \") self.infer_id() original_standard_id = self[\" id \"] citekey = CiteKey(original_standard_id) standard_id = citekey.standard_id add_to_note = {} note_dict = self.note_dict if original_id and original_id != standard_id: if original_id != note_dict.get(\" original_id \"): add_to_note[\" original_id \"] = original_id if original_standard_id and original_standard_id != standard_id: if original_standard_id != note_dict.get(\" original_standard_id \"): add_to_note[\" original_standard_id \"] = original_standard_id if standard_id != note_dict.get(\" standard_id \"): add_to_note[\" standard_id \"] = standard_id self.note_append_dict(dictionary=add_to_note) self.set_id(standard_id) return self update def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k] validate_against_schema def validate_against_schema ( self ) -> 'CSL_Item' Confirm that the CSL_Item validates. If not, raises a jsonschema.exceptions.ValidationError. View Source def validate_against_schema ( self ) -> \"CSL_Item\" : \"\"\" Confirm that the CSL_Item validates. If not, raises a jsonschema.exceptions.ValidationError. \"\"\" from .citeproc import get_jsonschema_csl_validator validator = get_jsonschema_csl_validator () validator . validate ([ self ]) return self values def values ( ... ) D.values() -> an object providing a view on D's values Handler_arXiv class Handler_arXiv ( prefix_lower : str ) View Source class Handler_arXiv ( Handler ): standard_prefix = \"arxiv\" prefixes = [ \"arxiv\" , ] accession_pattern = re . compile ( r \"(?P<versionless_id>[0-9]{4}\\.[0-9]{4,5}|[a-z\\-]+(\\.[A-Z]{2})?/[0-9]{7})(?P<version>v[0-9]+)?\" ) def inspect ( self , citekey ): # https://arxiv.org/help/arxiv_identifier if not self . _get_pattern (). fullmatch ( citekey . accession ): return \"arXiv identifiers must conform to syntax described at https://arxiv.org/help/arxiv_identifier.\" def get_csl_item ( self , citekey ): return get_arxiv_csl_item ( citekey . standard_accession ) Ancestors (in MRO) manubot.cite.handlers.Handler Class variables accession_pattern prefixes standard_prefix Methods get_csl_item def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ) : return get_arxiv_csl_item ( citekey . standard_accession ) inspect def inspect ( self , citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey ) : # https : // arxiv . org / help / arxiv_identifier if not self . _get_pattern () . fullmatch ( citekey . accession ) : return \" arXiv identifiers must conform to syntax described at https://arxiv.org/help/arxiv_identifier. \" standardize_prefix_accession def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ] Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ] : \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession","title":"Arxiv"},{"location":"reference/manubot/cite/arxiv/#module-manubotcitearxiv","text":"None None View Source import logging import re import xml.etree.ElementTree import requests from manubot.util import get_manubot_user_agent from .csl_item import CSL_Item from .handlers import Handler class Handler_arXiv ( Handler ): standard_prefix = \"arxiv\" prefixes = [ \"arxiv\" , ] accession_pattern = re . compile ( r \"(?P<versionless_id>[0-9] {4} \\.[0-9]{4,5}|[a-z\\-]+(\\.[A-Z] {2} )?/[0-9] {7} )(?P<version>v[0-9]+)?\" ) def inspect ( self , citekey ): # https://arxiv.org/help/arxiv_identifier if not self . _get_pattern () . fullmatch ( citekey . accession ): return \"arXiv identifiers must conform to syntax described at https://arxiv.org/help/arxiv_identifier.\" def get_csl_item ( self , citekey ): return get_arxiv_csl_item ( citekey . standard_accession ) class CSL_Item_arXiv ( CSL_Item ): def _set_invariant_fields ( self ): # Set journal/publisher to arXiv self [ \"container-title\" ] = \"arXiv\" self [ \"publisher\" ] = \"arXiv\" # Set CSL type to report for preprint self [ \"type\" ] = \"report\" return self def log_journal_doi ( self , arxiv_id , journal_ref = None ): if \"DOI\" not in self : return msg = f \"arXiv article { arxiv_id } published at https://doi.org/ { self [ 'DOI' ] } \" if journal_ref : msg += f \" \u2014 { journal_ref } \" logging . info ( msg ) def set_identifier_fields ( self , arxiv_id ): self . set_id ( f \"arxiv: { arxiv_id } \" ) self [ \"URL\" ] = f \"https://arxiv.org/abs/ { arxiv_id } \" self [ \"number\" ] = arxiv_id _ , version = split_arxiv_id_version ( arxiv_id ) if version : self [ \"version\" ] = version def split_arxiv_id_version ( arxiv_id : str ): \"\"\" Return (versionless_id, version) tuple. Version refers to the verion suffix like 'v2' or None. \"\"\" match = re . match ( Handler_arXiv . accession_pattern , arxiv_id ) return match . group ( \"versionless_id\" ), match . group ( \"version\" ) def get_arxiv_csl_item ( arxiv_id : str ): \"\"\" Return csl_item item for an arXiv identifier. Chooses which arXiv API to use based on whether arxiv_id is versioned, since only one endpoint supports versioning. \"\"\" _ , version = split_arxiv_id_version ( arxiv_id ) if version : return get_arxiv_csl_item_export_api ( arxiv_id ) return get_arxiv_csl_item_oai ( arxiv_id ) def query_arxiv_api ( url , params ): headers = { \"User-Agent\" : get_manubot_user_agent ()} response = requests . get ( url , params , headers = headers ) response . raise_for_status () xml_tree = xml . etree . ElementTree . fromstring ( response . text ) return xml_tree def get_arxiv_csl_item_export_api ( arxiv_id ): \"\"\" Return csl_item item for an arXiv record. arxiv_id can be versioned, like `1512.00567v2`, or versionless, like `1512.00567`. If versionless, the arXiv API will return metadata for the latest version. Legacy IDs, such as `cond-mat/0703470v2`, are also supported. If arXiv has an associated DOI for the record, a warning is logged to alert the user that an alternative version of record exists. References: - https://arxiv.org/help/api/index - http://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html - https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" xml_tree = query_arxiv_api ( url = \"https://export.arxiv.org/api/query\" , params = { \"id_list\" : arxiv_id , \"max_results\" : 1 }, ) # XML namespace prefixes prefix = \"{http://www.w3.org/2005/Atom}\" alt_prefix = \"{http://arxiv.org/schemas/atom}\" # Parse XML ( entry ,) = xml_tree . findall ( prefix + \"entry\" ) # Create dictionary for CSL Item csl_item = CSL_Item_arXiv () # Extract versioned arXiv ID url = entry . findtext ( prefix + \"id\" ) pattern = re . compile ( r \"arxiv.org/abs/(.+)\" ) match = pattern . search ( url ) versioned_id = match . group ( 1 ) csl_item . set_identifier_fields ( versioned_id ) # Extrat CSL title field csl_item [ \"title\" ] = entry . findtext ( prefix + \"title\" ) # Extract CSL date field published = entry . findtext ( prefix + \"published\" ) csl_item . set_date ( published , variable = \"issued\" ) # Extract authors authors = list () for elem in entry . findall ( prefix + \"author\" ): name = elem . findtext ( prefix + \"name\" ) author = { \"literal\" : name } authors . append ( author ) csl_item [ \"author\" ] = authors csl_item . _set_invariant_fields () # Extract abstract abstract = entry . findtext ( prefix + \"summary\" ) . strip () if abstract : # remove newlines that were added to wrap abstract abstract = remove_newlines ( abstract ) csl_item [ \"abstract\" ] = abstract # Check if the article has been published with a DOI doi = entry . findtext ( f \" { alt_prefix } doi\" ) if doi : csl_item [ \"DOI\" ] = doi journal_ref = entry . findtext ( alt_prefix + \"journal_ref\" ) csl_item . log_journal_doi ( arxiv_id , journal_ref ) return csl_item def get_arxiv_csl_item_oai ( arxiv_id ): \"\"\" Generate a CSL Item for an unversioned arXiv identifier using arXiv's OAI_PMH v2.0 API <https://arxiv.org/help/oa>. This endpoint does not support versioned `arxiv_id`. \"\"\" # XML namespace prefixes ns_oai = \"{http://www.openarchives.org/OAI/2.0/}\" ns_arxiv = \"{http://arxiv.org/OAI/arXiv/}\" xml_tree = query_arxiv_api ( url = \"https://export.arxiv.org/oai2\" , params = { \"verb\" : \"GetRecord\" , \"metadataPrefix\" : \"arXiv\" , \"identifier\" : f \"oai:arXiv.org: { arxiv_id } \" , }, ) # Create dictionary for CSL Item csl_item = CSL_Item_arXiv () # Extract parent XML elements ( header_elem ,) = xml_tree . findall ( f \" { ns_oai } GetRecord/ { ns_oai } record/ { ns_oai } header\" ) ( metadata_elem ,) = xml_tree . findall ( f \" { ns_oai } GetRecord/ { ns_oai } record/ { ns_oai } metadata\" ) ( arxiv_elem ,) = metadata_elem . findall ( f \" { ns_arxiv } arXiv\" ) # Set identifier fields response_arxiv_id = arxiv_elem . findtext ( f \" { ns_arxiv } id\" ) if arxiv_id != response_arxiv_id : logging . warning ( \"arXiv oai2 query returned a different arxiv_id:\" f \" { arxiv_id } became { response_arxiv_id } \" ) csl_item . set_identifier_fields ( response_arxiv_id ) # Set title and date title = arxiv_elem . findtext ( f \" { ns_arxiv } title\" ) if title : csl_item [ \"title\" ] = \" \" . join ( title . split ()) datestamp = header_elem . findtext ( f \" { ns_oai } datestamp\" ) csl_item . set_date ( datestamp , \"issued\" ) # Extract authors author_elems = arxiv_elem . findall ( f \" { ns_arxiv } authors/ { ns_arxiv } author\" ) authors = list () for author_elem in author_elems : author = {} given = author_elem . findtext ( f \" { ns_arxiv } forenames\" ) family = author_elem . findtext ( f \" { ns_arxiv } keyname\" ) if given : author [ \"given\" ] = given if family : author [ \"family\" ] = family authors . append ( author ) csl_item [ \"author\" ] = authors csl_item . _set_invariant_fields () abstract = arxiv_elem . findtext ( f \" { ns_arxiv } abstract\" ) if abstract : csl_item [ \"abstract\" ] = remove_newlines ( abstract ) license = arxiv_elem . findtext ( f \" { ns_arxiv } license\" ) if license : csl_item . note_append_dict ({ \"license\" : license }) doi = arxiv_elem . findtext ( f \" { ns_arxiv } doi\" ) if doi : csl_item [ \"DOI\" ] = doi journal_ref = arxiv_elem . findtext ( f \" { ns_arxiv } journal-ref\" ) csl_item . log_journal_doi ( arxiv_id , journal_ref ) return csl_item def remove_newlines ( text ): return re . sub ( pattern = r \"\\n(?!\\s)\" , repl = \" \" , string = text ) def get_arxiv_csl_item_zotero ( arxiv_id ): \"\"\" Generate CSL JSON Data for an arXiv ID using Zotero's translation-server. \"\"\" from manubot.cite.zotero import get_csl_item return get_csl_item ( f \"arxiv: { arxiv_id } \" )","title":"Module manubot.cite.arxiv"},{"location":"reference/manubot/cite/arxiv/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/arxiv/#get_arxiv_csl_item","text":"def get_arxiv_csl_item ( arxiv_id : str ) Return csl_item item for an arXiv identifier. Chooses which arXiv API to use based on whether arxiv_id is versioned, since only one endpoint supports versioning. View Source def get_arxiv_csl_item ( arxiv_id : str ): \"\"\" Return csl_item item for an arXiv identifier. Chooses which arXiv API to use based on whether arxiv_id is versioned, since only one endpoint supports versioning. \"\"\" _ , version = split_arxiv_id_version ( arxiv_id ) if version : return get_arxiv_csl_item_export_api ( arxiv_id ) return get_arxiv_csl_item_oai ( arxiv_id )","title":"get_arxiv_csl_item"},{"location":"reference/manubot/cite/arxiv/#get_arxiv_csl_item_export_api","text":"def get_arxiv_csl_item_export_api ( arxiv_id ) Return csl_item item for an arXiv record. arxiv_id can be versioned, like 1512.00567v2 , or versionless, like 1512.00567 . If versionless, the arXiv API will return metadata for the latest version. Legacy IDs, such as cond-mat/0703470v2 , are also supported. If arXiv has an associated DOI for the record, a warning is logged to alert the user that an alternative version of record exists. References: - https://arxiv.org/help/api/index - http://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html - https://github.com/citation-style-language/schema/blob/master/csl-data.json View Source def get_arxiv_csl_item_export_api ( arxiv_id ): \"\"\" Return csl_item item for an arXiv record. arxiv_id can be versioned, like `1512.00567v2`, or versionless, like `1512.00567`. If versionless, the arXiv API will return metadata for the latest version. Legacy IDs, such as `cond-mat/0703470v2`, are also supported. If arXiv has an associated DOI for the record, a warning is logged to alert the user that an alternative version of record exists. References: - https://arxiv.org/help/api/index - http://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html - https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" xml_tree = query_arxiv_api ( url = \"https://export.arxiv.org/api/query\" , params = { \"id_list\" : arxiv_id , \"max_results\" : 1 }, ) # XML namespace prefixes prefix = \"{http://www.w3.org/2005/Atom}\" alt_prefix = \"{http://arxiv.org/schemas/atom}\" # Parse XML ( entry ,) = xml_tree . findall ( prefix + \"entry\" ) # Create dictionary for CSL Item csl_item = CSL_Item_arXiv () # Extract versioned arXiv ID url = entry . findtext ( prefix + \"id\" ) pattern = re . compile ( r \"arxiv.org/abs/(.+)\" ) match = pattern . search ( url ) versioned_id = match . group ( 1 ) csl_item . set_identifier_fields ( versioned_id ) # Extrat CSL title field csl_item [ \"title\" ] = entry . findtext ( prefix + \"title\" ) # Extract CSL date field published = entry . findtext ( prefix + \"published\" ) csl_item . set_date ( published , variable = \"issued\" ) # Extract authors authors = list () for elem in entry . findall ( prefix + \"author\" ): name = elem . findtext ( prefix + \"name\" ) author = { \"literal\" : name } authors . append ( author ) csl_item [ \"author\" ] = authors csl_item . _set_invariant_fields () # Extract abstract abstract = entry . findtext ( prefix + \"summary\" ) . strip () if abstract : # remove newlines that were added to wrap abstract abstract = remove_newlines ( abstract ) csl_item [ \"abstract\" ] = abstract # Check if the article has been published with a DOI doi = entry . findtext ( f \"{alt_prefix}doi\" ) if doi : csl_item [ \"DOI\" ] = doi journal_ref = entry . findtext ( alt_prefix + \"journal_ref\" ) csl_item . log_journal_doi ( arxiv_id , journal_ref ) return csl_item","title":"get_arxiv_csl_item_export_api"},{"location":"reference/manubot/cite/arxiv/#get_arxiv_csl_item_oai","text":"def get_arxiv_csl_item_oai ( arxiv_id ) Generate a CSL Item for an unversioned arXiv identifier using arXiv's OAI_PMH v2.0 API https://arxiv.org/help/oa . This endpoint does not support versioned arxiv_id . View Source def get_arxiv_csl_item_oai ( arxiv_id ) : \" \"\" Generate a CSL Item for an unversioned arXiv identifier using arXiv's OAI_PMH v2.0 API <https://arxiv.org/help/oa>. This endpoint does not support versioned `arxiv_id`. \"\" \" # XML namespace prefixes ns_oai = \"{http://www.openarchives.org/OAI/2.0/}\" ns_arxiv = \"{http://arxiv.org/OAI/arXiv/}\" xml_tree = query_arxiv_api ( url = \"https://export.arxiv.org/oai2\" , params = { \"verb\" : \"GetRecord\" , \"metadataPrefix\" : \"arXiv\" , \"identifier\" : f \"oai:arXiv.org:{arxiv_id}\" , } , ) # Create dictionary for CSL Item csl_item = CSL_Item_arXiv () # Extract parent XML elements ( header_elem ,) = xml_tree . findall ( f \"{ns_oai}GetRecord/{ns_oai}record/{ns_oai}header\" ) ( metadata_elem ,) = xml_tree . findall ( f \"{ns_oai}GetRecord/{ns_oai}record/{ns_oai}metadata\" ) ( arxiv_elem ,) = metadata_elem . findall ( f \"{ns_arxiv}arXiv\" ) # Set identifier fields response_arxiv_id = arxiv_elem . findtext ( f \"{ns_arxiv}id\" ) if arxiv_id != response_arxiv_id : logging . warning ( \"arXiv oai2 query returned a different arxiv_id:\" f \" {arxiv_id} became {response_arxiv_id}\" ) csl_item . set _identifier_fields ( response_arxiv_id ) # Set title and date title = arxiv_elem . findtext ( f \"{ns_arxiv}title\" ) if title : csl_item [ \"title\" ] = \" \" . join ( title . split ()) datestamp = header_elem . findtext ( f \"{ns_oai}datestamp\" ) csl_item . set _date ( datestamp , \"issued\" ) # Extract authors author_elems = arxiv_elem . findall ( f \"{ns_arxiv}authors/{ns_arxiv}author\" ) authors = list () for author_elem in author_elems : author = {} given = author_elem . findtext ( f \"{ns_arxiv}forenames\" ) family = author_elem . findtext ( f \"{ns_arxiv}keyname\" ) if given : author [ \"given\" ] = given if family : author [ \"family\" ] = family authors . append ( author ) csl_item [ \"author\" ] = authors csl_item . _set_invariant_fields () abstract = arxiv_elem . findtext ( f \"{ns_arxiv}abstract\" ) if abstract : csl_item [ \"abstract\" ] = remove_newlines ( abstract ) license = arxiv_elem . findtext ( f \"{ns_arxiv}license\" ) if license : csl_item . note_append_dict ( { \"license\" : license } ) doi = arxiv_elem . findtext ( f \"{ns_arxiv}doi\" ) if doi : csl_item [ \"DOI\" ] = doi journal_ref = arxiv_elem . findtext ( f \"{ns_arxiv}journal-ref\" ) csl_item . log_journal_doi ( arxiv_id , journal_ref ) return csl_item","title":"get_arxiv_csl_item_oai"},{"location":"reference/manubot/cite/arxiv/#get_arxiv_csl_item_zotero","text":"def get_arxiv_csl_item_zotero ( arxiv_id ) Generate CSL JSON Data for an arXiv ID using Zotero's translation-server. View Source def get_arxiv_csl_item_zotero ( arxiv_id ): \"\"\" Generate CSL JSON Data for an arXiv ID using Zotero's translation-server. \"\"\" from manubot.cite.zotero import get_csl_item return get_csl_item ( f \"arxiv: { arxiv_id } \" )","title":"get_arxiv_csl_item_zotero"},{"location":"reference/manubot/cite/arxiv/#query_arxiv_api","text":"def query_arxiv_api ( url , params ) View Source def query_arxiv_api ( url , params ) : headers = { \" User-Agent \" : get_manubot_user_agent () } response = requests . get ( url , params , headers = headers ) response . raise_for_status () xml_tree = xml . etree . ElementTree . fromstring ( response . text ) return xml_tree","title":"query_arxiv_api"},{"location":"reference/manubot/cite/arxiv/#remove_newlines","text":"def remove_newlines ( text ) View Source def remove_newlines ( text ) : return re . sub ( pattern = r \" \\n (?!\\s) \" , repl = \" \" , string = text )","title":"remove_newlines"},{"location":"reference/manubot/cite/arxiv/#split_arxiv_id_version","text":"def split_arxiv_id_version ( arxiv_id : str ) Return (versionless_id, version) tuple. Version refers to the verion suffix like 'v2' or None. View Source def split_arxiv_id_version ( arxiv_id : str ) : \"\"\" Return ( versionless_id , version ) tuple . Version refers to the verion suffix like ' v2 ' or None . \"\"\" match = re . match ( Handler_arXiv . accession_pattern , arxiv_id ) return match . group ( \" versionless_id \" ) , match . group ( \" version \" )","title":"split_arxiv_id_version"},{"location":"reference/manubot/cite/arxiv/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/arxiv/#csl_item_arxiv","text":"class CSL_Item_arXiv ( dictionary = None , ** kwargs ) View Source class CSL_Item_arXiv ( CSL_Item ): def _set_invariant_fields ( self ): # Set journal/publisher to arXiv self [ \"container-title\" ] = \"arXiv\" self [ \"publisher\" ] = \"arXiv\" # Set CSL type to report for preprint self [ \"type\" ] = \"report\" return self def log_journal_doi ( self , arxiv_id , journal_ref = None ): if \"DOI\" not in self : return msg = f \"arXiv article {arxiv_id} published at https://doi.org/{self['DOI']}\" if journal_ref : msg += f \" \u2014 {journal_ref}\" logging . info ( msg ) def set_identifier_fields ( self , arxiv_id ): self . set_id ( f \"arxiv:{arxiv_id}\" ) self [ \"URL\" ] = f \"https://arxiv.org/abs/{arxiv_id}\" self [ \"number\" ] = arxiv_id _ , version = split_arxiv_id_version ( arxiv_id ) if version : self [ \"version\" ] = version","title":"CSL_Item_arXiv"},{"location":"reference/manubot/cite/arxiv/#ancestors-in-mro","text":"manubot.cite.csl_item.CSL_Item builtins.dict","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/arxiv/#class-variables","text":"type_mapping","title":"Class variables"},{"location":"reference/manubot/cite/arxiv/#instance-variables","text":"note Return the value of the \"note\" field as a string. If \"note\" key is not set, return empty string. note_dict Return a dictionary with key-value pairs encoded by this CSL Item's note. Extracts both forms (line-entry and braced-entry) of key-value pairs from the CSL JSON \"cheater syntax\" https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields Assigning to this dict will not update self[\"note\"] .","title":"Instance variables"},{"location":"reference/manubot/cite/arxiv/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/arxiv/#clean","text":"def clean ( self , prune : bool = True ) -> 'CSL_Item' Sanitize and touch-up a potentially dirty CSL_Item. The following steps are performed: - update incorrect values for \"type\" field when a correct variant is known - remove fields that violate the JSON Schema (if prune=True) - set default value for \"type\" if missing, since CSL JSON requires type - validate against the CSL JSON schema (if prune=True) to ensure output CSL_Item is clean View Source def clean ( self , prune : bool = True ) -> \"CSL_Item\" : \"\"\" Sanitize and touch-up a potentially dirty CSL_Item. The following steps are performed: - update incorrect values for \"type\" field when a correct variant is known - remove fields that violate the JSON Schema (if prune=True) - set default value for \"type\" if missing, since CSL JSON requires type - validate against the CSL JSON schema (if prune=True) to ensure output CSL_Item is clean \"\"\" logging . debug ( f \"Starting CSL_Item.clean with{'' if prune else 'out'}\" f \"CSL pruning for id: {self.get('id', 'id not specified')}\" ) self . correct_invalid_type () if prune : self . prune_against_schema () self . set_default_type () if prune : self . validate_against_schema () return self","title":"clean"},{"location":"reference/manubot/cite/arxiv/#clear","text":"def clear ( ... ) D.clear() -> None. Remove all items from D.","title":"clear"},{"location":"reference/manubot/cite/arxiv/#copy","text":"def copy ( ... ) D.copy() -> a shallow copy of D","title":"copy"},{"location":"reference/manubot/cite/arxiv/#correct_invalid_type","text":"def correct_invalid_type ( self ) -> 'CSL_Item' Correct invalid CSL item type. Does nothing if type not present. For detail see https://github.com/CrossRef/rest-api-doc/issues/187 View Source def correct_invalid_type ( self ) -> \"CSL_Item\" : \" \"\" Correct invalid CSL item type. Does nothing if `type` not present. For detail see https://github.com/CrossRef/rest-api-doc/issues/187 \"\" \" if \"type\" in self : # Replace a type from in CSL_Item.type_mapping.keys(), # leave type intact in other cases. t = self [ \"type\" ] self [ \"type\" ] = self . type_mapping . get ( t , t ) return self","title":"correct_invalid_type"},{"location":"reference/manubot/cite/arxiv/#fromkeys","text":"def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value.","title":"fromkeys"},{"location":"reference/manubot/cite/arxiv/#get","text":"def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default.","title":"get"},{"location":"reference/manubot/cite/arxiv/#get_date","text":"def get_date ( self , variable : str = 'issued' , fill : bool = False ) -> Optional [ str ] Return a CSL date-variable as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). variable: which CSL JSON date variable to retrieve fill: if True, set missing months to January and missing days to the first day of the month. View Source def get_date ( self , variable : str = \"issued\" , fill : bool = False ) -> Optional [ str ] : \"\"\" Return a CSL date-variable as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). variable: which CSL JSON date variable to retrieve fill: if True, set missing months to January and missing days to the first day of the month. \"\"\" try : date_parts = self [ variable ][ \"date-parts\" ][ 0 ] except ( IndexError , KeyError ) : return None return date_parts_to_string ( date_parts , fill = fill )","title":"get_date"},{"location":"reference/manubot/cite/arxiv/#infer_id","text":"def infer_id ( self ) -> 'CSL_Item' Detect and set a non-null/empty value for \"id\" or else raise a ValueError. View Source def infer_id ( self ) -> \"CSL_Item\" : \"\"\" Detect and set a non-null/empty value for \" id \" or else raise a ValueError. \"\"\" if self . get ( \"standard_citation\" ) : # \"standard_citation\" field is set with a non - null / empty value return self . set_id ( self . pop ( \"standard_citation\" )) if self . note_dict . get ( \"standard_id\" ) : # \"standard_id\" note field is set with a non - null / empty value return self . set_id ( self . note_dict [ \"standard_id\" ]) if self . get ( \"id\" ) : # \"id\" field exists and is set with a non - null / empty value return self . set_id ( self [ \"id\" ]) raise ValueError ( \"infer_id could not detect a field with a citation / standard_citation. \" 'Consider setting the CSL Item \" id \" field.' )","title":"infer_id"},{"location":"reference/manubot/cite/arxiv/#items","text":"def items ( ... ) D.items() -> a set-like object providing a view on D's items","title":"items"},{"location":"reference/manubot/cite/arxiv/#keys","text":"def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys","title":"keys"},{"location":"reference/manubot/cite/arxiv/#log_journal_doi","text":"def log_journal_doi ( self , arxiv_id , journal_ref = None ) View Source def log_journal_doi ( self , arxiv_id , journal_ref = None ) : if \" DOI \" not in self : return msg = f \" arXiv article {arxiv_id} published at https://doi.org/{self['DOI']} \" if journal_ref : msg += f \" \u2014 {journal_ref} \" logging . info ( msg )","title":"log_journal_doi"},{"location":"reference/manubot/cite/arxiv/#note_append_dict","text":"def note_append_dict ( self , dictionary : dict ) -> None Append key-value pairs specified by dictionary to the note field of a CSL Item. Uses the the CSL JSON \"cheater syntax\" to encode additional values not defined by the CSL JSON schema. View Source def note_append_dict ( self , dictionary : dict ) -> None : \" \"\" Append key-value pairs specified by `dictionary` to the note field of a CSL Item. Uses the the [CSL JSON \" cheater syntax \"](https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields) to encode additional values not defined by the CSL JSON schema. \"\" \" for key , value in dictionary . items () : if not re . fullmatch ( r \"[A-Z]+|[-_a-z]+\" , key ) : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"it does not conform to the variable_name syntax as per https://git.io/fjTzW.\" ) continue if \" \\n \" in value : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"the value contains a newline: {value!r}\" ) continue self . note_append_text ( f \"{key}: {value}\" )","title":"note_append_dict"},{"location":"reference/manubot/cite/arxiv/#note_append_text","text":"def note_append_text ( self , text : str ) -> None Append text to the note field (as a new line) of a CSL Item. If a line already exists equal to text , do nothing. View Source def note_append_text ( self , text : str ) -> None : \" \"\" Append text to the note field (as a new line) of a CSL Item. If a line already exists equal to `text`, do nothing. \"\" \" if not text : return note = self . note if re . search ( f \"^{re.escape(text)}$\" , note , flags = re . MULTILINE ) : # do not accumulate duplicate lines of text # https://github.com/manubot/manubot/issues/258 return if note and not note . endswith ( \" \\n \" ) : note += \" \\n \" note += text self . note = note","title":"note_append_text"},{"location":"reference/manubot/cite/arxiv/#pop","text":"def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, default is returned if given, otherwise KeyError is raised","title":"pop"},{"location":"reference/manubot/cite/arxiv/#popitem","text":"def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty.","title":"popitem"},{"location":"reference/manubot/cite/arxiv/#prune_against_schema","text":"def prune_against_schema ( self ) -> 'CSL_Item' Remove fields that violate the CSL Item JSON Schema. View Source def prune_against_schema ( self ) -> \"CSL_Item\" : \"\"\" Remove fields that violate the CSL Item JSON Schema. \"\"\" from .citeproc import remove_jsonschema_errors ( csl_item ,) = remove_jsonschema_errors ([ self ], in_place = True ) assert csl_item is self return self","title":"prune_against_schema"},{"location":"reference/manubot/cite/arxiv/#set_date","text":"def set_date ( self , date : Union [ NoneType , str , datetime . date , datetime . datetime ], variable : str = 'issued' ) -> 'CSL_Item' date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). variable: which variable to assign the date to. View Source def set_date ( self , date : Union [ None, str, datetime.date, datetime.datetime ] , variable : str = \"issued\" , ) -> \"CSL_Item\" : \"\"\" date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). variable: which variable to assign the date to. \"\"\" date_parts = date_to_date_parts ( date ) if date_parts : self [ variable ] = { \"date-parts\" : [ date_parts ] } return self","title":"set_date"},{"location":"reference/manubot/cite/arxiv/#set_default_type","text":"def set_default_type ( self ) -> 'CSL_Item' Set type to 'entry', if type not specified. View Source def set_default_type ( self ) -> \"CSL_Item\" : \"\"\" Set type to ' entry ', if type not specified. \"\"\" self [ \"type\" ] = self . get ( \"type\" , \"entry\" ) return self","title":"set_default_type"},{"location":"reference/manubot/cite/arxiv/#set_id","text":"def set_id ( self , id_ ) -> 'CSL_Item' View Source def set_id ( self , id_ ) -> \"CSL_Item\" : self [ \"id\" ] = id_ return self","title":"set_id"},{"location":"reference/manubot/cite/arxiv/#set_identifier_fields","text":"def set_identifier_fields ( self , arxiv_id ) View Source def set_identifier_fields ( self , arxiv_id ) : self . set_id ( f \" arxiv:{arxiv_id} \" ) self [ \" URL \" ] = f \" https://arxiv.org/abs/{arxiv_id} \" self [ \" number \" ] = arxiv_id _ , version = split_arxiv_id_version ( arxiv_id ) if version : self [ \" version \" ] = version","title":"set_identifier_fields"},{"location":"reference/manubot/cite/arxiv/#setdefault","text":"def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default.","title":"setdefault"},{"location":"reference/manubot/cite/arxiv/#standardize_id","text":"def standardize_id ( self ) -> 'CSL_Item' Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \"id\" field. The standard_id is extracted from a \"standard_citation\" field, the \"note\" field, or the \"id\" field. The extracted citation is checked for validity and standardized, after which it is the final \"standard_id\". Regarding csl_item modification, the csl_item \"id\" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id and original_id. Note that the Manubot software generally refers to the \"id\" of a CSL Item as a citekey. However, in this context, we use \"id\" rather than \"citekey\" for consistency with CSL's \"id\" field. View Source def standardize_id ( self ) -> \"CSL_Item\" : \"\"\" Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \" id \" field. The standard_id is extracted from a \" standard_citation \" field, the \" note \" field, or the \" id \" field. The extracted citation is checked for validity and standardized, after which it is the final \" standard_id \". Regarding csl_item modification, the csl_item \" id \" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id and original_id. Note that the Manubot software generally refers to the \" id \" of a CSL Item as a citekey. However, in this context, we use \" id \" rather than \" citekey \" for consistency with CSL' s \"id\" field . \"\"\" original_id = self.get(\" id \") self.infer_id() original_standard_id = self[\" id \"] citekey = CiteKey(original_standard_id) standard_id = citekey.standard_id add_to_note = {} note_dict = self.note_dict if original_id and original_id != standard_id: if original_id != note_dict.get(\" original_id \"): add_to_note[\" original_id \"] = original_id if original_standard_id and original_standard_id != standard_id: if original_standard_id != note_dict.get(\" original_standard_id \"): add_to_note[\" original_standard_id \"] = original_standard_id if standard_id != note_dict.get(\" standard_id \"): add_to_note[\" standard_id \"] = standard_id self.note_append_dict(dictionary=add_to_note) self.set_id(standard_id) return self","title":"standardize_id"},{"location":"reference/manubot/cite/arxiv/#update","text":"def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k]","title":"update"},{"location":"reference/manubot/cite/arxiv/#validate_against_schema","text":"def validate_against_schema ( self ) -> 'CSL_Item' Confirm that the CSL_Item validates. If not, raises a jsonschema.exceptions.ValidationError. View Source def validate_against_schema ( self ) -> \"CSL_Item\" : \"\"\" Confirm that the CSL_Item validates. If not, raises a jsonschema.exceptions.ValidationError. \"\"\" from .citeproc import get_jsonschema_csl_validator validator = get_jsonschema_csl_validator () validator . validate ([ self ]) return self","title":"validate_against_schema"},{"location":"reference/manubot/cite/arxiv/#values","text":"def values ( ... ) D.values() -> an object providing a view on D's values","title":"values"},{"location":"reference/manubot/cite/arxiv/#handler_arxiv","text":"class Handler_arXiv ( prefix_lower : str ) View Source class Handler_arXiv ( Handler ): standard_prefix = \"arxiv\" prefixes = [ \"arxiv\" , ] accession_pattern = re . compile ( r \"(?P<versionless_id>[0-9]{4}\\.[0-9]{4,5}|[a-z\\-]+(\\.[A-Z]{2})?/[0-9]{7})(?P<version>v[0-9]+)?\" ) def inspect ( self , citekey ): # https://arxiv.org/help/arxiv_identifier if not self . _get_pattern (). fullmatch ( citekey . accession ): return \"arXiv identifiers must conform to syntax described at https://arxiv.org/help/arxiv_identifier.\" def get_csl_item ( self , citekey ): return get_arxiv_csl_item ( citekey . standard_accession )","title":"Handler_arXiv"},{"location":"reference/manubot/cite/arxiv/#ancestors-in-mro_1","text":"manubot.cite.handlers.Handler","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/arxiv/#class-variables_1","text":"accession_pattern prefixes standard_prefix","title":"Class variables"},{"location":"reference/manubot/cite/arxiv/#methods_1","text":"","title":"Methods"},{"location":"reference/manubot/cite/arxiv/#get_csl_item","text":"def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ) : return get_arxiv_csl_item ( citekey . standard_accession )","title":"get_csl_item"},{"location":"reference/manubot/cite/arxiv/#inspect","text":"def inspect ( self , citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey ) : # https : // arxiv . org / help / arxiv_identifier if not self . _get_pattern () . fullmatch ( citekey . accession ) : return \" arXiv identifiers must conform to syntax described at https://arxiv.org/help/arxiv_identifier. \"","title":"inspect"},{"location":"reference/manubot/cite/arxiv/#standardize_prefix_accession","text":"def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ] Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ] : \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession","title":"standardize_prefix_accession"},{"location":"reference/manubot/cite/citations/","text":"Module manubot.cite.citations None None View Source import dataclasses import json import logging import os import pathlib import typing as tp from manubot.cite.citekey import CiteKey , citekey_to_csl_item @dataclasses . dataclass class Citations : \"\"\" Class for operating on a set of citations provided by their citekey input_ids. \"\"\" # Input citekey IDs as strings input_ids : list # Citation key aliases aliases : dict = dataclasses . field ( default_factory = dict ) # infer prefixes for citekeys (e.g. support DOIs without a \"doi:\" prefix) infer_citekey_prefixes : bool = True # manual references dictionary of standard_id to CSL_Item. manual_refs : dict = dataclasses . field ( default_factory = dict ) # level to log failures related to CSL Item generation csl_item_failure_log_level : tp . Union [ str , int ] = \"WARNING\" # whether to prune csl items according to the JSON Schema prune_csl_items : bool = True # whether to sort csl items by standard_id # `sort_csl_items=False` retains order of input_ids in get_csl_items. # (input_ids with the same standard_id will still be deduplicated). sort_csl_items : bool = True def __post_init__ ( self ): input_ids = list ( dict . fromkeys ( self . input_ids )) # deduplicate self . citekeys = [ CiteKey ( x , aliases = self . aliases , infer_prefix = self . infer_citekey_prefixes ) for x in input_ids ] def filter_pandoc_xnos ( self ) -> list : \"\"\" Filter self.citekeys to remove pandoc-xnos style citekeys. Return removed citekeys. \"\"\" keep , remove = [], [] for citekey in self . citekeys : remove_ = citekey . is_pandoc_xnos_prefix ( log_case_warning = True ) ( keep , remove )[ remove_ ] . append ( citekey ) self . citekeys = keep return remove def filter_unhandled ( self ) -> list : \"\"\" Filter self.citekeys to remove unhandled citekeys. Return removed citekeys. \"\"\" keep , remove = [], [] for citekey in self . citekeys : ( remove , keep )[ citekey . is_handled_prefix ] . append ( citekey ) self . citekeys = keep return remove def group_citekeys_by ( self , attribute : str = \"standard_id\" , sort : bool = True , ) -> tp . List [ tp . Tuple [ str , list ]]: \"\"\" Group `self.citekeys` by `attribute`. \"\"\" def get_key ( x ): return getattr ( x , attribute ) key_to_indices = dict () for i , citekey in enumerate ( self . citekeys ): key = get_key ( citekey ) key_to_indices . setdefault ( key , list ()) . append ( i ) items = list ( key_to_indices . items ()) if sort : items . sort ( key = lambda item : item [ 0 ]) return [( key , [ self . citekeys [ i ] for i in indices ]) for key , indices in items ] def unique_citekeys_by ( self , attribute : str = \"standard_id\" ) -> list : return [ citekeys [ 0 ] for key , citekeys in self . group_citekeys_by ( attribute )] def check_collisions ( self ): \"\"\" Check for short_id hash collisions \"\"\" for short_id , citekeys in self . group_citekeys_by ( \"short_id\" ): standard_ids = sorted ( set ( x . standard_id for x in citekeys )) if len ( standard_ids ) == 1 : continue logging . error ( \"Congratulations! Hash collision. Please report to https://git.io/JfuhH. \\n \" f \"Multiple standard_ids hashed to { short_id } : { standard_ids } \" ) def check_multiple_input_ids ( self ): \"\"\" Identify different input_ids referring to the same reference. \"\"\" for standard_id , citekeys in self . group_citekeys_by ( \"standard_id\" ): input_ids = [ x . input_id for x in citekeys ] if len ( input_ids ) < 2 : continue logging . warning ( f \"Multiple citekey input_ids refer to the same standard_id { standard_id } : \\n { input_ids } \" ) def inspect ( self , log_level = None ): \"\"\" If log_level is not None, log combined inspection report at this level. \"\"\" citekeys = self . unique_citekeys_by ( \"dealiased_id\" ) reports = [] for citekey in citekeys : report = citekey . inspect () if not report : continue reports . append ( f \" { citekey . dealiased_id } -- { report } \" ) report = \" \\n \" . join ( reports ) if reports and log_level is not None : log_level = logging . _checkLevel ( log_level ) msg = f \"Inspection of dealiased citekeys revealed potential problems: \\n { report } \" logging . log ( log_level , msg ) return report def load_manual_references ( self , * args , ** kwargs ): \"\"\" Load manual references \"\"\" from manubot.process.bibliography import load_manual_references manual_refs = load_manual_references ( * args , ** kwargs ) self . manual_refs . update ( manual_refs ) def get_csl_items ( self ) -> tp . List : \"\"\" Produce a list of CSL_Items. I.e. a references list / bibliography for `self.citekeys`. \"\"\" # dictionary of input_id to CSL_Item ID (i.e. short_id), # excludes standard_ids for which CSL Items could not be generated. self . input_to_csl_id = {} self . csl_items = [] for _standard_id , citekeys in self . group_citekeys_by ( \"standard_id\" , sort = self . sort_csl_items ): csl_item = citekey_to_csl_item ( citekey = citekeys [ 0 ], prune = self . prune_csl_items , log_level = self . csl_item_failure_log_level , manual_refs = self . manual_refs , ) if csl_item : for ck in citekeys : self . input_to_csl_id [ ck . input_id ] = csl_item [ \"id\" ] self . csl_items . append ( csl_item ) return self . csl_items @property def citekeys_tsv ( self ) -> str : import csv import io fields = [ \"input_id\" , \"dealiased_id\" , \"standard_id\" , \"short_id\" ] output = io . StringIO () writer = csv . DictWriter ( output , fieldnames = fields , delimiter = \" \\t \" ) writer . writeheader () for citekey in self . citekeys : row = { x : getattr ( citekey , x ) for x in fields } writer . writerow ( row ) return output . getvalue () @property def csl_json ( self ) -> str : assert hasattr ( self , \"csl_items\" ) json_str = json . dumps ( self . csl_items , indent = 2 , ensure_ascii = False ) json_str += \" \\n \" return json_str @property def csl_yaml ( self ) -> str : from manubot.util import get_configured_yaml yaml = get_configured_yaml () assert hasattr ( self , \"csl_items\" ) # dump rather than safe_dump is required for # pyyaml to use custom representers. return yaml . dump ( data = self . csl_items , default_flow_style = False , width = float ( \"inf\" ), allow_unicode = True , sort_keys = False , ) def write_csl_items ( self , path : tp . Union [ os . PathLike , str , None ]) -> None : \"\"\" Write CSL Items to a JSON or YAML file at `path`. If path ends with a .yml or .yaml extension, write as CSL YAML. Otherwise write CSL JSON. If `path` evaluates as False, do nothing. \"\"\" if not path : return path = pathlib . Path ( path ) text = self . csl_yaml if path . suffix in [ \".yaml\" , \".yml\" ] else self . csl_json path . write_text ( text , encoding = \"utf-8\" ) def write_citekeys_tsv ( self , path : tp . Union [ os . PathLike , str , None ]): \"\"\" Write `self.citekeys_tsv` to a file. If `path` evaluates as False, do nothing. \"\"\" if not path : return path = pathlib . Path ( path ) path . write_text ( self . citekeys_tsv , encoding = \"utf-8\" ) Classes Citations class Citations ( input_ids : list , aliases : dict = < factory > , infer_citekey_prefixes : bool = True , manual_refs : dict = < factory > , csl_item_failure_log_level : Union [ str , int ] = 'WARNING' , prune_csl_items : bool = True , sort_csl_items : bool = True ) View Source @ dataclasses . dataclass class Citations : \"\"\" Class for operating on a set of citations provided by their citekey input_ids. \"\"\" # Input citekey IDs as strings input_ids : list # Citation key aliases aliases : dict = dataclasses . field ( default_factory = dict ) # infer prefixes for citekeys (e.g. support DOIs without a \"doi:\" prefix) infer_citekey_prefixes : bool = True # manual references dictionary of standard_id to CSL_Item. manual_refs : dict = dataclasses . field ( default_factory = dict ) # level to log failures related to CSL Item generation csl_item_failure_log_level : tp . Union [ str , int ] = \"WARNING\" # whether to prune csl items according to the JSON Schema prune_csl_items : bool = True # whether to sort csl items by standard_id # `sort_csl_items=False` retains order of input_ids in get_csl_items. # (input_ids with the same standard_id will still be deduplicated). sort_csl_items : bool = True def __post_init__ ( self ): input_ids = list ( dict . fromkeys ( self . input_ids )) # deduplicate self . citekeys = [ CiteKey ( x , aliases = self . aliases , infer_prefix = self . infer_citekey_prefixes ) for x in input_ids ] def filter_pandoc_xnos ( self ) -> list : \"\"\" Filter self.citekeys to remove pandoc-xnos style citekeys. Return removed citekeys. \"\"\" keep , remove = [], [] for citekey in self . citekeys : remove_ = citekey . is_pandoc_xnos_prefix ( log_case_warning = True ) ( keep , remove )[ remove_ ] . append ( citekey ) self . citekeys = keep return remove def filter_unhandled ( self ) -> list : \"\"\" Filter self.citekeys to remove unhandled citekeys. Return removed citekeys. \"\"\" keep , remove = [], [] for citekey in self . citekeys : ( remove , keep )[ citekey . is_handled_prefix ] . append ( citekey ) self . citekeys = keep return remove def group_citekeys_by ( self , attribute : str = \"standard_id\" , sort : bool = True , ) -> tp . List [ tp . Tuple [ str , list ]]: \"\"\" Group `self.citekeys` by `attribute`. \"\"\" def get_key ( x ): return getattr ( x , attribute ) key_to_indices = dict () for i , citekey in enumerate ( self . citekeys ): key = get_key ( citekey ) key_to_indices . setdefault ( key , list ()) . append ( i ) items = list ( key_to_indices . items ()) if sort : items . sort ( key = lambda item : item [ 0 ]) return [( key , [ self . citekeys [ i ] for i in indices ]) for key , indices in items ] def unique_citekeys_by ( self , attribute : str = \"standard_id\" ) -> list : return [ citekeys [ 0 ] for key , citekeys in self . group_citekeys_by ( attribute )] def check_collisions ( self ): \"\"\" Check for short_id hash collisions \"\"\" for short_id , citekeys in self . group_citekeys_by ( \"short_id\" ): standard_ids = sorted ( set ( x . standard_id for x in citekeys )) if len ( standard_ids ) == 1 : continue logging . error ( \"Congratulations! Hash collision. Please report to https://git.io/JfuhH. \\n \" f \"Multiple standard_ids hashed to {short_id}: {standard_ids}\" ) def check_multiple_input_ids ( self ): \"\"\" Identify different input_ids referring to the same reference. \"\"\" for standard_id , citekeys in self . group_citekeys_by ( \"standard_id\" ): input_ids = [ x . input_id for x in citekeys ] if len ( input_ids ) < 2 : continue logging . warning ( f \"Multiple citekey input_ids refer to the same standard_id {standard_id}: \\n {input_ids}\" ) def inspect ( self , log_level = None ): \"\"\" If log_level is not None, log combined inspection report at this level. \"\"\" citekeys = self . unique_citekeys_by ( \"dealiased_id\" ) reports = [] for citekey in citekeys : report = citekey . inspect () if not report : continue reports . append ( f \"{citekey.dealiased_id} -- {report}\" ) report = \" \\n \" . join ( reports ) if reports and log_level is not None : log_level = logging . _checkLevel ( log_level ) msg = f \"Inspection of dealiased citekeys revealed potential problems: \\n {report}\" logging . log ( log_level , msg ) return report def load_manual_references ( self , * args , ** kwargs ): \"\"\" Load manual references \"\"\" from manubot . process . bibliography import load_manual_references manual_refs = load_manual_references ( * args , ** kwargs ) self . manual_refs . update ( manual_refs ) def get_csl_items ( self ) -> tp . List : \"\"\" Produce a list of CSL_Items. I.e. a references list / bibliography for `self.citekeys`. \"\"\" # dictionary of input_id to CSL_Item ID (i.e. short_id), # excludes standard_ids for which CSL Items could not be generated. self . input_to_csl_id = {} self . csl_items = [] for _standard_id , citekeys in self . group_citekeys_by ( \"standard_id\" , sort = self . sort_csl_items ): csl_item = citekey_to_csl_item ( citekey = citekeys [ 0 ], prune = self . prune_csl_items , log_level = self . csl_item_failure_log_level , manual_refs = self . manual_refs , ) if csl_item : for ck in citekeys : self . input_to_csl_id [ ck . input_id ] = csl_item [ \"id\" ] self . csl_items . append ( csl_item ) return self . csl_items @ property def citekeys_tsv ( self ) -> str : import csv import io fields = [ \"input_id\" , \"dealiased_id\" , \"standard_id\" , \"short_id\" ] output = io . StringIO () writer = csv . DictWriter ( output , fieldnames = fields , delimiter = \" \\t \" ) writer . writeheader () for citekey in self . citekeys : row = { x : getattr ( citekey , x ) for x in fields } writer . writerow ( row ) return output . getvalue () @ property def csl_json ( self ) -> str : assert hasattr ( self , \"csl_items\" ) json_str = json . dumps ( self . csl_items , indent = 2 , ensure_ascii = False ) json_str += \" \\n \" return json_str @ property def csl_yaml ( self ) -> str : from manubot . util import get_configured_yaml yaml = get_configured_yaml () assert hasattr ( self , \"csl_items\" ) # dump rather than safe_dump is required for # pyyaml to use custom representers. return yaml . dump ( data = self . csl_items , default_flow_style = False , width = float ( \"inf\" ), allow_unicode = True , sort_keys = False , ) def write_csl_items ( self , path : tp . Union [ os . PathLike , str , None ]) -> None : \"\"\" Write CSL Items to a JSON or YAML file at `path`. If path ends with a .yml or .yaml extension, write as CSL YAML. Otherwise write CSL JSON. If `path` evaluates as False, do nothing. \"\"\" if not path : return path = pathlib . Path ( path ) text = self . csl_yaml if path . suffix in [ \".yaml\" , \".yml\" ] else self . csl_json path . write_text ( text , encoding = \"utf-8\" ) def write_citekeys_tsv ( self , path : tp . Union [ os . PathLike , str , None ]): \"\"\" Write `self.citekeys_tsv` to a file. If `path` evaluates as False, do nothing. \"\"\" if not path : return path = pathlib . Path ( path ) path . write_text ( self . citekeys_tsv , encoding = \"utf-8\" ) Class variables csl_item_failure_log_level infer_citekey_prefixes prune_csl_items sort_csl_items Instance variables citekeys_tsv csl_json csl_yaml Methods check_collisions def check_collisions ( self ) Check for short_id hash collisions View Source def check_collisions ( self ) : \"\"\" Check for short_id hash collisions \"\"\" for short_id , citekeys in self . group_citekeys_by ( \" short_id \" ) : standard_ids = sorted ( set ( x . standard_id for x in citekeys )) if len ( standard_ids ) == 1 : continue logging . error ( \" Congratulations! Hash collision. Please report to https://git.io/JfuhH. \\n \" f \" Multiple standard_ids hashed to {short_id}: {standard_ids} \" ) check_multiple_input_ids def check_multiple_input_ids ( self ) Identify different input_ids referring to the same reference. View Source def check_multiple_input_ids ( self ) : \"\"\" Identify different input_ids referring to the same reference . \"\"\" for standard_id , citekeys in self . group_citekeys_by ( \" standard_id \" ) : input_ids = [ x . input_id for x in citekeys ] if len ( input_ids ) < 2 : continue logging . warning ( f \" Multiple citekey input_ids refer to the same standard_id {standard_id}: \\n {input_ids} \" ) filter_pandoc_xnos def filter_pandoc_xnos ( self ) -> list Filter self.citekeys to remove pandoc-xnos style citekeys. Return removed citekeys. View Source def filter_pandoc_xnos ( self ) -> list : \"\"\" Filter self.citekeys to remove pandoc-xnos style citekeys. Return removed citekeys. \"\"\" keep , remove = [] , [] for citekey in self . citekeys : remove_ = citekey . is_pandoc_xnos_prefix ( log_case_warning = True ) ( keep , remove ) [ remove_ ] . append ( citekey ) self . citekeys = keep return remove filter_unhandled def filter_unhandled ( self ) -> list Filter self.citekeys to remove unhandled citekeys. Return removed citekeys. View Source def filter_unhandled ( self ) -> list : \"\"\" Filter self.citekeys to remove unhandled citekeys. Return removed citekeys. \"\"\" keep , remove = [], [] for citekey in self . citekeys : ( remove , keep )[ citekey . is_handled_prefix ]. append ( citekey ) self . citekeys = keep return remove get_csl_items def get_csl_items ( self ) -> List Produce a list of CSL_Items. I.e. a references list / bibliography for self.citekeys . View Source def get_csl_items ( self ) -> tp . List : \"\"\" Produce a list of CSL_Items. I.e. a references list / bibliography for `self.citekeys`. \"\"\" # dictionary of input_id to CSL_Item ID ( i . e . short_id ), # excludes standard_ids for which CSL Items could not be generated . self . input_to_csl_id = {} self . csl_items = [] for _standard_id , citekeys in self . group_citekeys_by ( \"standard_id\" , sort = self . sort_csl_items ) : csl_item = citekey_to_csl_item ( citekey = citekeys [ 0 ], prune = self . prune_csl_items , log_level = self . csl_item_failure_log_level , manual_refs = self . manual_refs , ) if csl_item : for ck in citekeys : self . input_to_csl_id [ ck . input_id ] = csl_item [ \"id\" ] self . csl_items . append ( csl_item ) return self . csl_items group_citekeys_by def group_citekeys_by ( self , attribute : str = 'standard_id' , sort : bool = True ) -> List [ Tuple [ str , list ]] Group self.citekeys by attribute . View Source def group_citekeys_by ( self , attribute : str = \"standard_id\" , sort : bool = True , ) -> tp . List [ tp . Tuple [ str , list ]]: \"\"\" Group `self.citekeys` by `attribute`. \"\"\" def get_key ( x ): return getattr ( x , attribute ) key_to_indices = dict () for i , citekey in enumerate ( self . citekeys ): key = get_key ( citekey ) key_to_indices . setdefault ( key , list ()) . append ( i ) items = list ( key_to_indices . items ()) if sort : items . sort ( key = lambda item : item [ 0 ]) return [( key , [ self . citekeys [ i ] for i in indices ]) for key , indices in items ] inspect def inspect ( self , log_level = None ) If log_level is not None, log combined inspection report at this level. View Source def inspect ( self , log_level = None ) : \"\"\" If log_level is not None , log combined inspection report at this level . \"\"\" citekeys = self . unique_citekeys_by ( \" dealiased_id \" ) reports = [] for citekey in citekeys : report = citekey . inspect () if not report : continue reports . append ( f \" {citekey.dealiased_id} -- {report} \" ) report = \" \\n \" . join ( reports ) if reports and log_level is not None : log_level = logging . _checkLevel ( log_level ) msg = f \" Inspection of dealiased citekeys revealed potential problems: \\n {report} \" logging . log ( log_level , msg ) return report load_manual_references def load_manual_references ( self , * args , ** kwargs ) Load manual references View Source def load_manual_references ( self , * args , ** kwargs ): \"\"\" Load manual references \"\"\" from manubot.process.bibliography import load_manual_references manual_refs = load_manual_references ( * args , ** kwargs ) self . manual_refs . update ( manual_refs ) unique_citekeys_by def unique_citekeys_by ( self , attribute : str = 'standard_id' ) -> list View Source def unique_citekeys_by ( self , attribute : str = \"standard_id\" ) -> list : return [ citekeys [ 0 ] for key , citekeys in self . group_citekeys_by ( attribute )] write_citekeys_tsv def write_citekeys_tsv ( self , path : Union [ os . PathLike , str , NoneType ] ) Write self.citekeys_tsv to a file. If path evaluates as False, do nothing. View Source def write_citekeys_tsv ( self , path : tp . Union [ os . PathLike , str , None ] ) : \" \"\" Write `self.citekeys_tsv` to a file. If `path` evaluates as False, do nothing. \"\" \" if not path : return path = pathlib . Path ( path ) path . write_text ( self . citekeys_tsv , encoding = \"utf-8\" ) write_csl_items def write_csl_items ( self , path : Union [ os . PathLike , str , NoneType ] ) -> None Write CSL Items to a JSON or YAML file at path . If path ends with a .yml or .yaml extension, write as CSL YAML. Otherwise write CSL JSON. If path evaluates as False, do nothing. View Source def write_csl_items ( self , path : tp . Union [ os . PathLike , str , None ] ) -> None : \" \"\" Write CSL Items to a JSON or YAML file at `path`. If path ends with a .yml or .yaml extension, write as CSL YAML. Otherwise write CSL JSON. If `path` evaluates as False, do nothing. \"\" \" if not path : return path = pathlib . Path ( path ) text = self . csl_yaml if path . suffix in [ \".yaml\" , \".yml\" ] else self . csl_json path . write_text ( text , encoding = \"utf-8\" )","title":"Citations"},{"location":"reference/manubot/cite/citations/#module-manubotcitecitations","text":"None None View Source import dataclasses import json import logging import os import pathlib import typing as tp from manubot.cite.citekey import CiteKey , citekey_to_csl_item @dataclasses . dataclass class Citations : \"\"\" Class for operating on a set of citations provided by their citekey input_ids. \"\"\" # Input citekey IDs as strings input_ids : list # Citation key aliases aliases : dict = dataclasses . field ( default_factory = dict ) # infer prefixes for citekeys (e.g. support DOIs without a \"doi:\" prefix) infer_citekey_prefixes : bool = True # manual references dictionary of standard_id to CSL_Item. manual_refs : dict = dataclasses . field ( default_factory = dict ) # level to log failures related to CSL Item generation csl_item_failure_log_level : tp . Union [ str , int ] = \"WARNING\" # whether to prune csl items according to the JSON Schema prune_csl_items : bool = True # whether to sort csl items by standard_id # `sort_csl_items=False` retains order of input_ids in get_csl_items. # (input_ids with the same standard_id will still be deduplicated). sort_csl_items : bool = True def __post_init__ ( self ): input_ids = list ( dict . fromkeys ( self . input_ids )) # deduplicate self . citekeys = [ CiteKey ( x , aliases = self . aliases , infer_prefix = self . infer_citekey_prefixes ) for x in input_ids ] def filter_pandoc_xnos ( self ) -> list : \"\"\" Filter self.citekeys to remove pandoc-xnos style citekeys. Return removed citekeys. \"\"\" keep , remove = [], [] for citekey in self . citekeys : remove_ = citekey . is_pandoc_xnos_prefix ( log_case_warning = True ) ( keep , remove )[ remove_ ] . append ( citekey ) self . citekeys = keep return remove def filter_unhandled ( self ) -> list : \"\"\" Filter self.citekeys to remove unhandled citekeys. Return removed citekeys. \"\"\" keep , remove = [], [] for citekey in self . citekeys : ( remove , keep )[ citekey . is_handled_prefix ] . append ( citekey ) self . citekeys = keep return remove def group_citekeys_by ( self , attribute : str = \"standard_id\" , sort : bool = True , ) -> tp . List [ tp . Tuple [ str , list ]]: \"\"\" Group `self.citekeys` by `attribute`. \"\"\" def get_key ( x ): return getattr ( x , attribute ) key_to_indices = dict () for i , citekey in enumerate ( self . citekeys ): key = get_key ( citekey ) key_to_indices . setdefault ( key , list ()) . append ( i ) items = list ( key_to_indices . items ()) if sort : items . sort ( key = lambda item : item [ 0 ]) return [( key , [ self . citekeys [ i ] for i in indices ]) for key , indices in items ] def unique_citekeys_by ( self , attribute : str = \"standard_id\" ) -> list : return [ citekeys [ 0 ] for key , citekeys in self . group_citekeys_by ( attribute )] def check_collisions ( self ): \"\"\" Check for short_id hash collisions \"\"\" for short_id , citekeys in self . group_citekeys_by ( \"short_id\" ): standard_ids = sorted ( set ( x . standard_id for x in citekeys )) if len ( standard_ids ) == 1 : continue logging . error ( \"Congratulations! Hash collision. Please report to https://git.io/JfuhH. \\n \" f \"Multiple standard_ids hashed to { short_id } : { standard_ids } \" ) def check_multiple_input_ids ( self ): \"\"\" Identify different input_ids referring to the same reference. \"\"\" for standard_id , citekeys in self . group_citekeys_by ( \"standard_id\" ): input_ids = [ x . input_id for x in citekeys ] if len ( input_ids ) < 2 : continue logging . warning ( f \"Multiple citekey input_ids refer to the same standard_id { standard_id } : \\n { input_ids } \" ) def inspect ( self , log_level = None ): \"\"\" If log_level is not None, log combined inspection report at this level. \"\"\" citekeys = self . unique_citekeys_by ( \"dealiased_id\" ) reports = [] for citekey in citekeys : report = citekey . inspect () if not report : continue reports . append ( f \" { citekey . dealiased_id } -- { report } \" ) report = \" \\n \" . join ( reports ) if reports and log_level is not None : log_level = logging . _checkLevel ( log_level ) msg = f \"Inspection of dealiased citekeys revealed potential problems: \\n { report } \" logging . log ( log_level , msg ) return report def load_manual_references ( self , * args , ** kwargs ): \"\"\" Load manual references \"\"\" from manubot.process.bibliography import load_manual_references manual_refs = load_manual_references ( * args , ** kwargs ) self . manual_refs . update ( manual_refs ) def get_csl_items ( self ) -> tp . List : \"\"\" Produce a list of CSL_Items. I.e. a references list / bibliography for `self.citekeys`. \"\"\" # dictionary of input_id to CSL_Item ID (i.e. short_id), # excludes standard_ids for which CSL Items could not be generated. self . input_to_csl_id = {} self . csl_items = [] for _standard_id , citekeys in self . group_citekeys_by ( \"standard_id\" , sort = self . sort_csl_items ): csl_item = citekey_to_csl_item ( citekey = citekeys [ 0 ], prune = self . prune_csl_items , log_level = self . csl_item_failure_log_level , manual_refs = self . manual_refs , ) if csl_item : for ck in citekeys : self . input_to_csl_id [ ck . input_id ] = csl_item [ \"id\" ] self . csl_items . append ( csl_item ) return self . csl_items @property def citekeys_tsv ( self ) -> str : import csv import io fields = [ \"input_id\" , \"dealiased_id\" , \"standard_id\" , \"short_id\" ] output = io . StringIO () writer = csv . DictWriter ( output , fieldnames = fields , delimiter = \" \\t \" ) writer . writeheader () for citekey in self . citekeys : row = { x : getattr ( citekey , x ) for x in fields } writer . writerow ( row ) return output . getvalue () @property def csl_json ( self ) -> str : assert hasattr ( self , \"csl_items\" ) json_str = json . dumps ( self . csl_items , indent = 2 , ensure_ascii = False ) json_str += \" \\n \" return json_str @property def csl_yaml ( self ) -> str : from manubot.util import get_configured_yaml yaml = get_configured_yaml () assert hasattr ( self , \"csl_items\" ) # dump rather than safe_dump is required for # pyyaml to use custom representers. return yaml . dump ( data = self . csl_items , default_flow_style = False , width = float ( \"inf\" ), allow_unicode = True , sort_keys = False , ) def write_csl_items ( self , path : tp . Union [ os . PathLike , str , None ]) -> None : \"\"\" Write CSL Items to a JSON or YAML file at `path`. If path ends with a .yml or .yaml extension, write as CSL YAML. Otherwise write CSL JSON. If `path` evaluates as False, do nothing. \"\"\" if not path : return path = pathlib . Path ( path ) text = self . csl_yaml if path . suffix in [ \".yaml\" , \".yml\" ] else self . csl_json path . write_text ( text , encoding = \"utf-8\" ) def write_citekeys_tsv ( self , path : tp . Union [ os . PathLike , str , None ]): \"\"\" Write `self.citekeys_tsv` to a file. If `path` evaluates as False, do nothing. \"\"\" if not path : return path = pathlib . Path ( path ) path . write_text ( self . citekeys_tsv , encoding = \"utf-8\" )","title":"Module manubot.cite.citations"},{"location":"reference/manubot/cite/citations/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/citations/#citations","text":"class Citations ( input_ids : list , aliases : dict = < factory > , infer_citekey_prefixes : bool = True , manual_refs : dict = < factory > , csl_item_failure_log_level : Union [ str , int ] = 'WARNING' , prune_csl_items : bool = True , sort_csl_items : bool = True ) View Source @ dataclasses . dataclass class Citations : \"\"\" Class for operating on a set of citations provided by their citekey input_ids. \"\"\" # Input citekey IDs as strings input_ids : list # Citation key aliases aliases : dict = dataclasses . field ( default_factory = dict ) # infer prefixes for citekeys (e.g. support DOIs without a \"doi:\" prefix) infer_citekey_prefixes : bool = True # manual references dictionary of standard_id to CSL_Item. manual_refs : dict = dataclasses . field ( default_factory = dict ) # level to log failures related to CSL Item generation csl_item_failure_log_level : tp . Union [ str , int ] = \"WARNING\" # whether to prune csl items according to the JSON Schema prune_csl_items : bool = True # whether to sort csl items by standard_id # `sort_csl_items=False` retains order of input_ids in get_csl_items. # (input_ids with the same standard_id will still be deduplicated). sort_csl_items : bool = True def __post_init__ ( self ): input_ids = list ( dict . fromkeys ( self . input_ids )) # deduplicate self . citekeys = [ CiteKey ( x , aliases = self . aliases , infer_prefix = self . infer_citekey_prefixes ) for x in input_ids ] def filter_pandoc_xnos ( self ) -> list : \"\"\" Filter self.citekeys to remove pandoc-xnos style citekeys. Return removed citekeys. \"\"\" keep , remove = [], [] for citekey in self . citekeys : remove_ = citekey . is_pandoc_xnos_prefix ( log_case_warning = True ) ( keep , remove )[ remove_ ] . append ( citekey ) self . citekeys = keep return remove def filter_unhandled ( self ) -> list : \"\"\" Filter self.citekeys to remove unhandled citekeys. Return removed citekeys. \"\"\" keep , remove = [], [] for citekey in self . citekeys : ( remove , keep )[ citekey . is_handled_prefix ] . append ( citekey ) self . citekeys = keep return remove def group_citekeys_by ( self , attribute : str = \"standard_id\" , sort : bool = True , ) -> tp . List [ tp . Tuple [ str , list ]]: \"\"\" Group `self.citekeys` by `attribute`. \"\"\" def get_key ( x ): return getattr ( x , attribute ) key_to_indices = dict () for i , citekey in enumerate ( self . citekeys ): key = get_key ( citekey ) key_to_indices . setdefault ( key , list ()) . append ( i ) items = list ( key_to_indices . items ()) if sort : items . sort ( key = lambda item : item [ 0 ]) return [( key , [ self . citekeys [ i ] for i in indices ]) for key , indices in items ] def unique_citekeys_by ( self , attribute : str = \"standard_id\" ) -> list : return [ citekeys [ 0 ] for key , citekeys in self . group_citekeys_by ( attribute )] def check_collisions ( self ): \"\"\" Check for short_id hash collisions \"\"\" for short_id , citekeys in self . group_citekeys_by ( \"short_id\" ): standard_ids = sorted ( set ( x . standard_id for x in citekeys )) if len ( standard_ids ) == 1 : continue logging . error ( \"Congratulations! Hash collision. Please report to https://git.io/JfuhH. \\n \" f \"Multiple standard_ids hashed to {short_id}: {standard_ids}\" ) def check_multiple_input_ids ( self ): \"\"\" Identify different input_ids referring to the same reference. \"\"\" for standard_id , citekeys in self . group_citekeys_by ( \"standard_id\" ): input_ids = [ x . input_id for x in citekeys ] if len ( input_ids ) < 2 : continue logging . warning ( f \"Multiple citekey input_ids refer to the same standard_id {standard_id}: \\n {input_ids}\" ) def inspect ( self , log_level = None ): \"\"\" If log_level is not None, log combined inspection report at this level. \"\"\" citekeys = self . unique_citekeys_by ( \"dealiased_id\" ) reports = [] for citekey in citekeys : report = citekey . inspect () if not report : continue reports . append ( f \"{citekey.dealiased_id} -- {report}\" ) report = \" \\n \" . join ( reports ) if reports and log_level is not None : log_level = logging . _checkLevel ( log_level ) msg = f \"Inspection of dealiased citekeys revealed potential problems: \\n {report}\" logging . log ( log_level , msg ) return report def load_manual_references ( self , * args , ** kwargs ): \"\"\" Load manual references \"\"\" from manubot . process . bibliography import load_manual_references manual_refs = load_manual_references ( * args , ** kwargs ) self . manual_refs . update ( manual_refs ) def get_csl_items ( self ) -> tp . List : \"\"\" Produce a list of CSL_Items. I.e. a references list / bibliography for `self.citekeys`. \"\"\" # dictionary of input_id to CSL_Item ID (i.e. short_id), # excludes standard_ids for which CSL Items could not be generated. self . input_to_csl_id = {} self . csl_items = [] for _standard_id , citekeys in self . group_citekeys_by ( \"standard_id\" , sort = self . sort_csl_items ): csl_item = citekey_to_csl_item ( citekey = citekeys [ 0 ], prune = self . prune_csl_items , log_level = self . csl_item_failure_log_level , manual_refs = self . manual_refs , ) if csl_item : for ck in citekeys : self . input_to_csl_id [ ck . input_id ] = csl_item [ \"id\" ] self . csl_items . append ( csl_item ) return self . csl_items @ property def citekeys_tsv ( self ) -> str : import csv import io fields = [ \"input_id\" , \"dealiased_id\" , \"standard_id\" , \"short_id\" ] output = io . StringIO () writer = csv . DictWriter ( output , fieldnames = fields , delimiter = \" \\t \" ) writer . writeheader () for citekey in self . citekeys : row = { x : getattr ( citekey , x ) for x in fields } writer . writerow ( row ) return output . getvalue () @ property def csl_json ( self ) -> str : assert hasattr ( self , \"csl_items\" ) json_str = json . dumps ( self . csl_items , indent = 2 , ensure_ascii = False ) json_str += \" \\n \" return json_str @ property def csl_yaml ( self ) -> str : from manubot . util import get_configured_yaml yaml = get_configured_yaml () assert hasattr ( self , \"csl_items\" ) # dump rather than safe_dump is required for # pyyaml to use custom representers. return yaml . dump ( data = self . csl_items , default_flow_style = False , width = float ( \"inf\" ), allow_unicode = True , sort_keys = False , ) def write_csl_items ( self , path : tp . Union [ os . PathLike , str , None ]) -> None : \"\"\" Write CSL Items to a JSON or YAML file at `path`. If path ends with a .yml or .yaml extension, write as CSL YAML. Otherwise write CSL JSON. If `path` evaluates as False, do nothing. \"\"\" if not path : return path = pathlib . Path ( path ) text = self . csl_yaml if path . suffix in [ \".yaml\" , \".yml\" ] else self . csl_json path . write_text ( text , encoding = \"utf-8\" ) def write_citekeys_tsv ( self , path : tp . Union [ os . PathLike , str , None ]): \"\"\" Write `self.citekeys_tsv` to a file. If `path` evaluates as False, do nothing. \"\"\" if not path : return path = pathlib . Path ( path ) path . write_text ( self . citekeys_tsv , encoding = \"utf-8\" )","title":"Citations"},{"location":"reference/manubot/cite/citations/#class-variables","text":"csl_item_failure_log_level infer_citekey_prefixes prune_csl_items sort_csl_items","title":"Class variables"},{"location":"reference/manubot/cite/citations/#instance-variables","text":"citekeys_tsv csl_json csl_yaml","title":"Instance variables"},{"location":"reference/manubot/cite/citations/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/citations/#check_collisions","text":"def check_collisions ( self ) Check for short_id hash collisions View Source def check_collisions ( self ) : \"\"\" Check for short_id hash collisions \"\"\" for short_id , citekeys in self . group_citekeys_by ( \" short_id \" ) : standard_ids = sorted ( set ( x . standard_id for x in citekeys )) if len ( standard_ids ) == 1 : continue logging . error ( \" Congratulations! Hash collision. Please report to https://git.io/JfuhH. \\n \" f \" Multiple standard_ids hashed to {short_id}: {standard_ids} \" )","title":"check_collisions"},{"location":"reference/manubot/cite/citations/#check_multiple_input_ids","text":"def check_multiple_input_ids ( self ) Identify different input_ids referring to the same reference. View Source def check_multiple_input_ids ( self ) : \"\"\" Identify different input_ids referring to the same reference . \"\"\" for standard_id , citekeys in self . group_citekeys_by ( \" standard_id \" ) : input_ids = [ x . input_id for x in citekeys ] if len ( input_ids ) < 2 : continue logging . warning ( f \" Multiple citekey input_ids refer to the same standard_id {standard_id}: \\n {input_ids} \" )","title":"check_multiple_input_ids"},{"location":"reference/manubot/cite/citations/#filter_pandoc_xnos","text":"def filter_pandoc_xnos ( self ) -> list Filter self.citekeys to remove pandoc-xnos style citekeys. Return removed citekeys. View Source def filter_pandoc_xnos ( self ) -> list : \"\"\" Filter self.citekeys to remove pandoc-xnos style citekeys. Return removed citekeys. \"\"\" keep , remove = [] , [] for citekey in self . citekeys : remove_ = citekey . is_pandoc_xnos_prefix ( log_case_warning = True ) ( keep , remove ) [ remove_ ] . append ( citekey ) self . citekeys = keep return remove","title":"filter_pandoc_xnos"},{"location":"reference/manubot/cite/citations/#filter_unhandled","text":"def filter_unhandled ( self ) -> list Filter self.citekeys to remove unhandled citekeys. Return removed citekeys. View Source def filter_unhandled ( self ) -> list : \"\"\" Filter self.citekeys to remove unhandled citekeys. Return removed citekeys. \"\"\" keep , remove = [], [] for citekey in self . citekeys : ( remove , keep )[ citekey . is_handled_prefix ]. append ( citekey ) self . citekeys = keep return remove","title":"filter_unhandled"},{"location":"reference/manubot/cite/citations/#get_csl_items","text":"def get_csl_items ( self ) -> List Produce a list of CSL_Items. I.e. a references list / bibliography for self.citekeys . View Source def get_csl_items ( self ) -> tp . List : \"\"\" Produce a list of CSL_Items. I.e. a references list / bibliography for `self.citekeys`. \"\"\" # dictionary of input_id to CSL_Item ID ( i . e . short_id ), # excludes standard_ids for which CSL Items could not be generated . self . input_to_csl_id = {} self . csl_items = [] for _standard_id , citekeys in self . group_citekeys_by ( \"standard_id\" , sort = self . sort_csl_items ) : csl_item = citekey_to_csl_item ( citekey = citekeys [ 0 ], prune = self . prune_csl_items , log_level = self . csl_item_failure_log_level , manual_refs = self . manual_refs , ) if csl_item : for ck in citekeys : self . input_to_csl_id [ ck . input_id ] = csl_item [ \"id\" ] self . csl_items . append ( csl_item ) return self . csl_items","title":"get_csl_items"},{"location":"reference/manubot/cite/citations/#group_citekeys_by","text":"def group_citekeys_by ( self , attribute : str = 'standard_id' , sort : bool = True ) -> List [ Tuple [ str , list ]] Group self.citekeys by attribute . View Source def group_citekeys_by ( self , attribute : str = \"standard_id\" , sort : bool = True , ) -> tp . List [ tp . Tuple [ str , list ]]: \"\"\" Group `self.citekeys` by `attribute`. \"\"\" def get_key ( x ): return getattr ( x , attribute ) key_to_indices = dict () for i , citekey in enumerate ( self . citekeys ): key = get_key ( citekey ) key_to_indices . setdefault ( key , list ()) . append ( i ) items = list ( key_to_indices . items ()) if sort : items . sort ( key = lambda item : item [ 0 ]) return [( key , [ self . citekeys [ i ] for i in indices ]) for key , indices in items ]","title":"group_citekeys_by"},{"location":"reference/manubot/cite/citations/#inspect","text":"def inspect ( self , log_level = None ) If log_level is not None, log combined inspection report at this level. View Source def inspect ( self , log_level = None ) : \"\"\" If log_level is not None , log combined inspection report at this level . \"\"\" citekeys = self . unique_citekeys_by ( \" dealiased_id \" ) reports = [] for citekey in citekeys : report = citekey . inspect () if not report : continue reports . append ( f \" {citekey.dealiased_id} -- {report} \" ) report = \" \\n \" . join ( reports ) if reports and log_level is not None : log_level = logging . _checkLevel ( log_level ) msg = f \" Inspection of dealiased citekeys revealed potential problems: \\n {report} \" logging . log ( log_level , msg ) return report","title":"inspect"},{"location":"reference/manubot/cite/citations/#load_manual_references","text":"def load_manual_references ( self , * args , ** kwargs ) Load manual references View Source def load_manual_references ( self , * args , ** kwargs ): \"\"\" Load manual references \"\"\" from manubot.process.bibliography import load_manual_references manual_refs = load_manual_references ( * args , ** kwargs ) self . manual_refs . update ( manual_refs )","title":"load_manual_references"},{"location":"reference/manubot/cite/citations/#unique_citekeys_by","text":"def unique_citekeys_by ( self , attribute : str = 'standard_id' ) -> list View Source def unique_citekeys_by ( self , attribute : str = \"standard_id\" ) -> list : return [ citekeys [ 0 ] for key , citekeys in self . group_citekeys_by ( attribute )]","title":"unique_citekeys_by"},{"location":"reference/manubot/cite/citations/#write_citekeys_tsv","text":"def write_citekeys_tsv ( self , path : Union [ os . PathLike , str , NoneType ] ) Write self.citekeys_tsv to a file. If path evaluates as False, do nothing. View Source def write_citekeys_tsv ( self , path : tp . Union [ os . PathLike , str , None ] ) : \" \"\" Write `self.citekeys_tsv` to a file. If `path` evaluates as False, do nothing. \"\" \" if not path : return path = pathlib . Path ( path ) path . write_text ( self . citekeys_tsv , encoding = \"utf-8\" )","title":"write_citekeys_tsv"},{"location":"reference/manubot/cite/citations/#write_csl_items","text":"def write_csl_items ( self , path : Union [ os . PathLike , str , NoneType ] ) -> None Write CSL Items to a JSON or YAML file at path . If path ends with a .yml or .yaml extension, write as CSL YAML. Otherwise write CSL JSON. If path evaluates as False, do nothing. View Source def write_csl_items ( self , path : tp . Union [ os . PathLike , str , None ] ) -> None : \" \"\" Write CSL Items to a JSON or YAML file at `path`. If path ends with a .yml or .yaml extension, write as CSL YAML. Otherwise write CSL JSON. If `path` evaluates as False, do nothing. \"\" \" if not path : return path = pathlib . Path ( path ) text = self . csl_yaml if path . suffix in [ \".yaml\" , \".yml\" ] else self . csl_json path . write_text ( text , encoding = \"utf-8\" )","title":"write_csl_items"},{"location":"reference/manubot/cite/cite_command/","text":"Module manubot.cite.cite_command None None View Source import argparse import json import logging import pathlib import subprocess import sys from manubot.cite.citations import Citations from manubot.pandoc.util import get_pandoc_info from manubot.util import shlex_join # For manubot cite, infer --format from --output filename extensions extension_to_format = { \".json\" : \"csljson\" , \".yaml\" : \"cslyaml\" , \".yml\" : \"cslyaml\" , \".txt\" : \"plain\" , \".md\" : \"markdown\" , \".docx\" : \"docx\" , \".html\" : \"html\" , \".xml\" : \"jats\" , } def call_pandoc ( metadata , path , format = \"plain\" ): \"\"\" path is the path to write to. \"\"\" _exit_without_pandoc () info = get_pandoc_info () _check_pandoc_version ( info , metadata , format ) metadata_block = \"--- \\n {yaml} \\n ... \\n \" . format ( yaml = json . dumps ( metadata , ensure_ascii = False , indent = 2 ) ) args = [ \"pandoc\" , \"--citeproc\" if info [ \"pandoc version\" ] >= ( 2 , 11 ) else \"--filter=pandoc-citeproc\" , f \"--output= { path or '-' } \" , ] if format == \"markdown\" : args . extend ([ \"--to=markdown_strict-raw_html\" , \"--wrap=none\" ]) elif format == \"jats\" : args . extend ([ \"--to=jats\" , \"--standalone\" ]) elif format == \"docx\" : args . extend ([ \"--to=docx\" ]) elif format == \"html\" : args . extend ([ \"--to=html\" ]) elif format == \"plain\" : args . extend ([ \"--to=plain\" , \"--wrap=none\" ]) if info [ \"pandoc version\" ] >= ( 2 ,): # Do not use ALL_CAPS for bold & underscores for italics # https://github.com/jgm/pandoc/issues/4834#issuecomment-412972008 filter_path = ( pathlib . Path ( __file__ ) . joinpath ( \"..\" , \"plain-pandoc-filter.lua\" ) . resolve () ) assert filter_path . exists () args . append ( f \"--lua-filter= { filter_path } \" ) logging . info ( \"call_pandoc subprocess args: \\n \" + shlex_join ( args )) process = subprocess . run ( args = args , input = metadata_block . encode (), ) process . check_returncode () def _parse_cli_cite_args ( args : argparse . Namespace ): arg_dict = vars ( args ) # infer format from output extension if not args . format and args . output : arg_dict [ \"format\" ] = extension_to_format . get ( args . output . suffix ) # default format to csljson if not args . format : arg_dict [ \"format\" ] = \"csljson\" # whether to render references with Pandoc arg_dict [ \"render\" ] = args . format not in { \"csljson\" , \"cslyaml\" } logging . debug ( f \"_parse_cli_cite_args: { args } \" ) def cli_cite ( args : argparse . Namespace ): \"\"\" Main function for the manubot cite command-line interface. Does not allow user to directly specify Pandoc's --to argument, due to inconsistent citation rendering by output format. See https://github.com/jgm/pandoc/issues/4834 \"\"\" _parse_cli_cite_args ( args ) citations = Citations ( input_ids = args . citekeys , infer_citekey_prefixes = args . infer_prefix , prune_csl_items = args . prune_csl , sort_csl_items = False , ) citations . load_manual_references ( paths = args . bibliography ) citations . inspect ( log_level = \"WARNING\" ) csl_items = citations . get_csl_items () # output CSL data, if --render is False if not args . render : if args . format == \"csljson\" : text = citations . csl_json elif args . format == \"cslyaml\" : text = citations . csl_yaml else : raise ValueError ( \"format must be csljson or cslyaml\" ) write_file = args . output . open ( \"wb\" ) if args . output else sys . stdout . buffer with write_file : write_file . write ( text . encode ()) return # use Pandoc to render references pandoc_metadata = { \"nocite\" : \"@*\" , \"csl\" : args . csl , \"references\" : csl_items } call_pandoc ( metadata = pandoc_metadata , path = args . output , format = args . format ) def _exit_without_pandoc () -> None : \"\"\" Given info from get_pandoc_info, exit Python if Pandoc is not available. \"\"\" if get_pandoc_info ()[ \"pandoc\" ]: return logging . critical ( f \"pandoc command not found on system. Ensure that Pandoc is installed.\" ) raise SystemExit ( 1 ) def _check_pandoc_version ( info , metadata , format ): \"\"\" Given info from get_pandoc_info, check that Pandoc's version is sufficient to perform the citation rendering command specified by metadata and format. Please add additional minimum version information to this function, as its discovered. \"\"\" issues = list () if format == \"jats\" and info [ \"pandoc version\" ] < ( 2 ,): issues . append ( \"--jats requires pandoc >= v2.0.\" ) # --csl=URL did not work in https://travis-ci.org/greenelab/manubot/builds/417314743#L796, # but exact version where this fails unknown # if metadata.get('csl', '').startswith('http') and pandoc_version < (2,): # issues.append('--csl=URL requires pandoc >= v2.0.') issues = \" \\n \" . join ( issues ) if issues : logging . critical ( f \"issues with pandoc version detected: \\n { issues } \" ) Variables extension_to_format Functions call_pandoc def call_pandoc ( metadata , path , format = 'plain' ) path is the path to write to. View Source def call_pandoc ( metadata , path , format = \" plain \" ) : \"\"\" path is the path to write to . \"\"\" _exit_without_pandoc () info = get_pandoc_info () _check_pandoc_version ( info , metadata , format ) metadata_block = \" --- \\n {yaml} \\n ... \\n \" . format ( yaml = json . dumps ( metadata , ensure_ascii = False , indent = 2 ) ) args = [ \" pandoc \" , \" --citeproc \" if info [ \" pandoc version \" ] >= ( 2 , 11 ) else \" --filter=pandoc-citeproc \" , f \" --output={path or '-'} \" , ] if format == \" markdown \" : args . extend ( [ \" --to=markdown_strict-raw_html \" , \" --wrap=none \" ] ) elif format == \" jats \" : args . extend ( [ \" --to=jats \" , \" --standalone \" ] ) elif format == \" docx \" : args . extend ( [ \" --to=docx \" ] ) elif format == \" html \" : args . extend ( [ \" --to=html \" ] ) elif format == \" plain \" : args . extend ( [ \" --to=plain \" , \" --wrap=none \" ] ) if info [ \" pandoc version \" ] >= ( 2 , ) : # Do not use ALL_CAPS for bold & underscores for italics # https : // github . com / jgm / pandoc / issues / 4834 # issuecomment - 412972008 filter_path = ( pathlib . Path ( __file__ ) . joinpath ( \" .. \" , \" plain-pandoc-filter.lua \" ) . resolve () ) assert filter_path . exists () args . append ( f \" --lua-filter={filter_path} \" ) logging . info ( \" call_pandoc subprocess args: \\n \" + shlex_join ( args )) process = subprocess . run ( args = args , input = metadata_block . encode () , ) process . check_returncode () cli_cite def cli_cite ( args : argparse . Namespace ) Main function for the manubot cite command-line interface. Does not allow user to directly specify Pandoc's --to argument, due to inconsistent citation rendering by output format. See https://github.com/jgm/pandoc/issues/4834 View Source def cli_cite ( args : argparse . Namespace ): \"\"\" Main function for the manubot cite command-line interface. Does not allow user to directly specify Pandoc's --to argument, due to inconsistent citation rendering by output format. See https://github.com/jgm/pandoc/issues/4834 \"\"\" _parse_cli_cite_args ( args ) citations = Citations ( input_ids = args . citekeys , infer_citekey_prefixes = args . infer_prefix , prune_csl_items = args . prune_csl , sort_csl_items = False , ) citations . load_manual_references ( paths = args . bibliography ) citations . inspect ( log_level = \"WARNING\" ) csl_items = citations . get_csl_items () # output CSL data, if --render is False if not args . render : if args . format == \"csljson\" : text = citations . csl_json elif args . format == \"cslyaml\" : text = citations . csl_yaml else : raise ValueError ( \"format must be csljson or cslyaml\" ) write_file = args . output . open ( \"wb\" ) if args . output else sys . stdout . buffer with write_file : write_file . write ( text . encode ()) return # use Pandoc to render references pandoc_metadata = { \"nocite\" : \"@*\" , \"csl\" : args . csl , \"references\" : csl_items } call_pandoc ( metadata = pandoc_metadata , path = args . output , format = args . format )","title":"Cite Command"},{"location":"reference/manubot/cite/cite_command/#module-manubotcitecite_command","text":"None None View Source import argparse import json import logging import pathlib import subprocess import sys from manubot.cite.citations import Citations from manubot.pandoc.util import get_pandoc_info from manubot.util import shlex_join # For manubot cite, infer --format from --output filename extensions extension_to_format = { \".json\" : \"csljson\" , \".yaml\" : \"cslyaml\" , \".yml\" : \"cslyaml\" , \".txt\" : \"plain\" , \".md\" : \"markdown\" , \".docx\" : \"docx\" , \".html\" : \"html\" , \".xml\" : \"jats\" , } def call_pandoc ( metadata , path , format = \"plain\" ): \"\"\" path is the path to write to. \"\"\" _exit_without_pandoc () info = get_pandoc_info () _check_pandoc_version ( info , metadata , format ) metadata_block = \"--- \\n {yaml} \\n ... \\n \" . format ( yaml = json . dumps ( metadata , ensure_ascii = False , indent = 2 ) ) args = [ \"pandoc\" , \"--citeproc\" if info [ \"pandoc version\" ] >= ( 2 , 11 ) else \"--filter=pandoc-citeproc\" , f \"--output= { path or '-' } \" , ] if format == \"markdown\" : args . extend ([ \"--to=markdown_strict-raw_html\" , \"--wrap=none\" ]) elif format == \"jats\" : args . extend ([ \"--to=jats\" , \"--standalone\" ]) elif format == \"docx\" : args . extend ([ \"--to=docx\" ]) elif format == \"html\" : args . extend ([ \"--to=html\" ]) elif format == \"plain\" : args . extend ([ \"--to=plain\" , \"--wrap=none\" ]) if info [ \"pandoc version\" ] >= ( 2 ,): # Do not use ALL_CAPS for bold & underscores for italics # https://github.com/jgm/pandoc/issues/4834#issuecomment-412972008 filter_path = ( pathlib . Path ( __file__ ) . joinpath ( \"..\" , \"plain-pandoc-filter.lua\" ) . resolve () ) assert filter_path . exists () args . append ( f \"--lua-filter= { filter_path } \" ) logging . info ( \"call_pandoc subprocess args: \\n \" + shlex_join ( args )) process = subprocess . run ( args = args , input = metadata_block . encode (), ) process . check_returncode () def _parse_cli_cite_args ( args : argparse . Namespace ): arg_dict = vars ( args ) # infer format from output extension if not args . format and args . output : arg_dict [ \"format\" ] = extension_to_format . get ( args . output . suffix ) # default format to csljson if not args . format : arg_dict [ \"format\" ] = \"csljson\" # whether to render references with Pandoc arg_dict [ \"render\" ] = args . format not in { \"csljson\" , \"cslyaml\" } logging . debug ( f \"_parse_cli_cite_args: { args } \" ) def cli_cite ( args : argparse . Namespace ): \"\"\" Main function for the manubot cite command-line interface. Does not allow user to directly specify Pandoc's --to argument, due to inconsistent citation rendering by output format. See https://github.com/jgm/pandoc/issues/4834 \"\"\" _parse_cli_cite_args ( args ) citations = Citations ( input_ids = args . citekeys , infer_citekey_prefixes = args . infer_prefix , prune_csl_items = args . prune_csl , sort_csl_items = False , ) citations . load_manual_references ( paths = args . bibliography ) citations . inspect ( log_level = \"WARNING\" ) csl_items = citations . get_csl_items () # output CSL data, if --render is False if not args . render : if args . format == \"csljson\" : text = citations . csl_json elif args . format == \"cslyaml\" : text = citations . csl_yaml else : raise ValueError ( \"format must be csljson or cslyaml\" ) write_file = args . output . open ( \"wb\" ) if args . output else sys . stdout . buffer with write_file : write_file . write ( text . encode ()) return # use Pandoc to render references pandoc_metadata = { \"nocite\" : \"@*\" , \"csl\" : args . csl , \"references\" : csl_items } call_pandoc ( metadata = pandoc_metadata , path = args . output , format = args . format ) def _exit_without_pandoc () -> None : \"\"\" Given info from get_pandoc_info, exit Python if Pandoc is not available. \"\"\" if get_pandoc_info ()[ \"pandoc\" ]: return logging . critical ( f \"pandoc command not found on system. Ensure that Pandoc is installed.\" ) raise SystemExit ( 1 ) def _check_pandoc_version ( info , metadata , format ): \"\"\" Given info from get_pandoc_info, check that Pandoc's version is sufficient to perform the citation rendering command specified by metadata and format. Please add additional minimum version information to this function, as its discovered. \"\"\" issues = list () if format == \"jats\" and info [ \"pandoc version\" ] < ( 2 ,): issues . append ( \"--jats requires pandoc >= v2.0.\" ) # --csl=URL did not work in https://travis-ci.org/greenelab/manubot/builds/417314743#L796, # but exact version where this fails unknown # if metadata.get('csl', '').startswith('http') and pandoc_version < (2,): # issues.append('--csl=URL requires pandoc >= v2.0.') issues = \" \\n \" . join ( issues ) if issues : logging . critical ( f \"issues with pandoc version detected: \\n { issues } \" )","title":"Module manubot.cite.cite_command"},{"location":"reference/manubot/cite/cite_command/#variables","text":"extension_to_format","title":"Variables"},{"location":"reference/manubot/cite/cite_command/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/cite_command/#call_pandoc","text":"def call_pandoc ( metadata , path , format = 'plain' ) path is the path to write to. View Source def call_pandoc ( metadata , path , format = \" plain \" ) : \"\"\" path is the path to write to . \"\"\" _exit_without_pandoc () info = get_pandoc_info () _check_pandoc_version ( info , metadata , format ) metadata_block = \" --- \\n {yaml} \\n ... \\n \" . format ( yaml = json . dumps ( metadata , ensure_ascii = False , indent = 2 ) ) args = [ \" pandoc \" , \" --citeproc \" if info [ \" pandoc version \" ] >= ( 2 , 11 ) else \" --filter=pandoc-citeproc \" , f \" --output={path or '-'} \" , ] if format == \" markdown \" : args . extend ( [ \" --to=markdown_strict-raw_html \" , \" --wrap=none \" ] ) elif format == \" jats \" : args . extend ( [ \" --to=jats \" , \" --standalone \" ] ) elif format == \" docx \" : args . extend ( [ \" --to=docx \" ] ) elif format == \" html \" : args . extend ( [ \" --to=html \" ] ) elif format == \" plain \" : args . extend ( [ \" --to=plain \" , \" --wrap=none \" ] ) if info [ \" pandoc version \" ] >= ( 2 , ) : # Do not use ALL_CAPS for bold & underscores for italics # https : // github . com / jgm / pandoc / issues / 4834 # issuecomment - 412972008 filter_path = ( pathlib . Path ( __file__ ) . joinpath ( \" .. \" , \" plain-pandoc-filter.lua \" ) . resolve () ) assert filter_path . exists () args . append ( f \" --lua-filter={filter_path} \" ) logging . info ( \" call_pandoc subprocess args: \\n \" + shlex_join ( args )) process = subprocess . run ( args = args , input = metadata_block . encode () , ) process . check_returncode ()","title":"call_pandoc"},{"location":"reference/manubot/cite/cite_command/#cli_cite","text":"def cli_cite ( args : argparse . Namespace ) Main function for the manubot cite command-line interface. Does not allow user to directly specify Pandoc's --to argument, due to inconsistent citation rendering by output format. See https://github.com/jgm/pandoc/issues/4834 View Source def cli_cite ( args : argparse . Namespace ): \"\"\" Main function for the manubot cite command-line interface. Does not allow user to directly specify Pandoc's --to argument, due to inconsistent citation rendering by output format. See https://github.com/jgm/pandoc/issues/4834 \"\"\" _parse_cli_cite_args ( args ) citations = Citations ( input_ids = args . citekeys , infer_citekey_prefixes = args . infer_prefix , prune_csl_items = args . prune_csl , sort_csl_items = False , ) citations . load_manual_references ( paths = args . bibliography ) citations . inspect ( log_level = \"WARNING\" ) csl_items = citations . get_csl_items () # output CSL data, if --render is False if not args . render : if args . format == \"csljson\" : text = citations . csl_json elif args . format == \"cslyaml\" : text = citations . csl_yaml else : raise ValueError ( \"format must be csljson or cslyaml\" ) write_file = args . output . open ( \"wb\" ) if args . output else sys . stdout . buffer with write_file : write_file . write ( text . encode ()) return # use Pandoc to render references pandoc_metadata = { \"nocite\" : \"@*\" , \"csl\" : args . csl , \"references\" : csl_items } call_pandoc ( metadata = pandoc_metadata , path = args . output , format = args . format )","title":"cli_cite"},{"location":"reference/manubot/cite/citekey/","text":"Module manubot.cite.citekey Utilities for representing and processing citation keys. None View Source \"\"\" Utilities for representing and processing citation keys. \"\"\" import dataclasses import functools import logging import re import typing as tp try : from functools import cached_property except ImportError : from backports . cached_property import cached_property @dataclasses . dataclass class CiteKey : input_id: str \"\"\"Input identifier for the citekey\"\"\" aliases : dict = dataclasses . field ( default_factory = dict ) \"\"\"Mapping from input identifier to aliases\"\"\" infer_prefix: bool = True \"\"\"Whether to infer the citekey's prefix when a prefix is missing or unhandled\"\"\" def __ post_init__ ( self ) : self . check_input_id ( self . input_id ) @staticmethod def check_input_id ( input_id ) -> None : if not isinstance ( input_id , str ) : raise TypeError ( \"input_id should be type 'str' not \" f \"{type(input_id).__name__!r}: {input_id!r}\" ) if input_id . startswith ( \"@\" ) : raise ValueError ( f \"invalid citekey input_id: {input_id!r}\\nstarts with '@'\" ) @classmethod @functools . lru_cache ( maxsize = None ) def from_input_id ( cls , * args , **kwargs ) -> \"CiteKey\" : \"\"\"Cached constructor\"\"\" return cls ( * args , **kwargs ) @cached_property def dealiased_id ( self ) -> str : \"\"\" If `self.input_id` is in `self.aliases`, the value specified by `self.aliases`. Otherwise, `self.input_id`. \"\"\" return self . aliases . get ( self . input_id , self . input_id ) def _ set_prefix_accession ( self ) -> None : self . _ prefix = None self . _ accession = None split_id = self . dealiased_id . split ( \":\" , 1 ) if len ( split_id ) == 2 : self . _ prefix , self . _ accession = split_id if self . infer_prefix and not self . is_known_prefix: self . _ infer_prefix () def _ infer_prefix ( self ) -> None : \"\"\" Treat `self.dealiased_id` as missing a prefix. If the prefix can be inferred, set `self._prefix` and `self._accession`. Only call this function from _set_prefix_accession, since it is not safe after instance attributes or properties have been cached. \"\"\" from . handlers import infer_prefix prefix = infer_prefix ( self . dealiased_id ) if not prefix : return self . _ prefix = prefix self . _ accession = self . dealiased_id @property def prefix ( self ) -> tp . Optional [ str ] : \"\"\" If `self.input_id` contains a colon, the substring up to the first colon. Otherwise, None. \"\"\" if not hasattr ( self , \"_prefix\" ) : self . _ set_prefix_accession () return self . _ prefix @property def prefix_lower ( self ) -> tp . Optional [ str ] : \"\"\" A lowercase version of `self.prefix` or None. \"\"\" if self . prefix is None : return None return self . prefix . lower () @property def accession ( self ) -> tp . Optional [ str ] : \"\"\" If `self.prefix`, the remainder of `self.input_id` following the first colon. \"\"\" if not hasattr ( self , \"_accession\" ) : self . _ set_prefix_accession () return self . _ accession @property def standard_prefix ( self ) -> tp . Optional [ str ] : \"\"\" If the citekey is handled, the standard prefix specified by the handler. Otherwise, None. \"\"\" if not hasattr ( self , \"_standard_prefix\" ) : self . _ standardize () return self . _ standard_prefix @property def standard_accession ( self ) -> tp . Optional [ str ] : \"\"\" If the citekey is handled, the standard accession specified by the handler. Otherwise, None. \"\"\" if not hasattr ( self , \"_standard_accession\" ) : self . _ standardize () return self . _ standard_accession @cached_property def handler ( self ) : from . handlers import Handler , get_handler if self . is_handled_prefix: return get_handler ( self . prefix_lower ) return Handler ( self . prefix_lower ) @property def is_handled_prefix ( self ) -> bool : from . handlers import prefix_to_handler return self . prefix_lower in prefix_to_handler @property def is_known_prefix ( self ) -> bool : return self . is_handled_prefix or self . is_pandoc_xnos_prefix () def inspect ( self ) -> tp . Optional [ str ] : \"\"\" Inspect citekey for potential problems. If no problems are found, return None. Otherwise, returns a string describing the problem. \"\"\" return self . handler . inspect ( self ) def _ standardize ( self ) -> None : \"\"\" Set `self._standard_prefix`, `self._standard_accession`, and `self._standard_id`. For citekeys without a prefix or with an unhandled prefix, _standard_prefix and _standard_accession are set to None. \"\"\" if not self . is_handled_prefix: self . _ standard_prefix = None self . _ standard_accession = None self . _ standard_id = self . dealiased_id return fxn = self . handler . standardize_prefix_accession self . _ standard_prefix , self . _ standard_accession = fxn ( self . accession ) self . _ standard_id = f \"{self._standard_prefix}:{self._standard_accession}\" @property def standard_id ( self ) -> str : \"\"\" If the citekey is handled, the standard_id specified by the handler. Otherwise, `self.dealiased_id`. \"\"\" if not hasattr ( self , \"_standard_id\" ) : self . _ standardize () return self . _ standard_id @cached_property def short_id ( self ) -> str : \"\"\" A hashed version of standard_id whose characters are within the ranges 0-9, a-z and A-Z. \"\"\" return shorten_citekey ( self . standard_id ) @cached_property def all_ids ( self ) -> tp . List [ str ] : ids = [ self . input_id , self . dealiased_id , self . standard_id , self . short_id ] ids = [ x for x in ids if x ] # remove None ids = list ( dict . fromkeys ( ids )) # deduplicate return ids def __ hash__ ( self ) : return hash (( self . input_id , self . dealiased_id )) def __ repr__ ( self ) : return \" --> \" . join ( f \"{getattr(self, key)} ({key})\" for key in ( \"input_id\" , \"dealiased_id\" , \"prefix_lower\" , \"accession\" , \"standard_id\" , \"short_id\" , ) ) @cached_property def csl_item ( self ) : from . csl_item import CSL_Item csl_item = self . handler . get_csl_item ( self ) if not isinstance ( csl_item , CSL_Item ) : csl_item = CSL_Item ( csl_item ) csl_item . set_id ( self . standard_id ) return csl_item def is_pandoc_xnos_prefix ( self , log_case_warning: bool = False ) -> bool : from . handlers import _ pandoc_xnos_prefixes if self . prefix in _ pandoc_xnos_prefixes: return True if log_case_warning and self . prefix_lower in _ pandoc_xnos_prefixes: logging . warning ( \"pandoc-xnos prefixes should be all lowercase.\\n\" f'Should {self.input_id!r} use {self.prefix_lower!r} rather than \"{self.prefix!r}\"?' ) return False def shorten_citekey ( standard_citekey: str ) -> str : \"\"\" Return a shortened citekey derived from the input citekey. The input citekey should be standardized prior to this function, since differences in the input citekey will result in different shortened citekeys. Short citekeys are generated by converting the input citekey to a 6 byte hash, and then converting this digest to a base62 ASCII str. Shortened citekeys consist of characters in the following ranges: 0-9, a-z and A-Z. \"\"\" import hashlib import base62 assert not standard_citekey . startswith ( \"@\" ) as_bytes = standard_citekey . encode () blake_hash = hashlib . blake2b ( as_bytes , digest_size = 6 ) digest = blake_hash . digest () short_citekey = base62 . encodebytes ( digest ) return short_citekey def citekey_to_csl_item ( citekey , prune = True , manual_refs= {}, log_level: tp . Union [ str , int ] = \"WARNING\" ) : \"\"\" Generate a CSL_Item for the input citekey. \"\"\" from manubot import __ version__ as manubot_version # https : // stackoverflow . com / a / 35704430 / 4651668 log_level = logging . _ checkLevel ( log_level ) if not isinstance ( citekey , CiteKey ) : citekey = CiteKey ( citekey ) if citekey . standard_id in manual_refs: return manual_refs [ citekey . standard_id ] try : csl_item = citekey . csl_item except Exception as error : logging . log ( log_level , f \"Generating csl_item for {citekey.standard_id!r} failed \" f \"due to a {error.__class__.__name__}:\\n{error}\" , ) logging . info ( error , exc_info = True ) return None # update csl_item with manubot generated metadata note_text = f \"This CSL Item was generated by Manubot v{manubot_version} from its persistent identifier (standard_id).\" note_dict = { \"standard_id\" : citekey . standard_id } csl_item . note_append_text ( note_text ) csl_item . note_append_dict ( note_dict ) csl_item . set_id ( citekey . short_id ) csl_item . clean ( prune = prune ) return csl_item def url_to_citekey ( url : str ) -> str : \"\"\" Convert a HTTP(s) URL into a citekey. For supported sources, convert from url citekey to an alternative source like doi. If citekeys fail inspection, revert alternative sources to URLs. \"\"\" from urllib . parse import unquote , urlparse citekey = None parsed_url = urlparse ( url ) domain_levels = parsed_url . hostname . split ( \".\" ) if domain_levels [ - 2 : ] == [ \"doi\" , \"org\" ] : # DOI URLs doi = unquote ( parsed_url . path . lstrip ( \"/\" )) citekey = f \"doi:{doi}\" if domain_levels [ - 2 ] == \"sci-hub\" : # Sci - Hub domains doi = parsed_url . path . lstrip ( \"/\" ) citekey = f \"doi:{doi}\" if domain_levels [ - 2 : ] == [ \"biorxiv\" , \"org\" ] : # bioRxiv URL to DOI . See https : // git . io / Je9Hq match = re . search ( r \" / ( ? P < biorxiv_id> ([ 0 - 9 ]{ 4 } \\. [ 0 - 9 ]{ 2 } \\. [ 0 - 9 ]{ 2 } \\. ) ? [ 0 - 9 ]{ 6 ,}) \", parsed_url.path, ) if match: citekey = f\" doi : 10.1101 / { match . group ( 'biorxiv_id' )} \" is_ncbi_url = parsed_url.hostname.endswith(\" ncbi . nlm . nih . gov \") if is_ncbi_url and parsed_url.path.startswith(\" / pubmed/ \"): # PubMed URLs try: pmid = parsed_url.path.split(\" / \")[2] citekey = f\" pmid :{ pmid } \" except IndexError: pass if is_ncbi_url and parsed_url.path.startswith(\" / pmc/ \"): # PubMed Central URLs try: pmcid = parsed_url.path.split(\" / \")[3] citekey = f\" pmcid :{ pmcid } \" except IndexError: pass if domain_levels[-2:] == [\" wikidata \", \" org \"] and parsed_url.path.startswith( \" / wiki/ \" ): # Wikidata URLs try: wikidata_id = parsed_url.path.split(\" / \")[2] citekey = f\" wikidata :{ wikidata_id } \" except IndexError: pass if domain_levels[-2:] == [\" arxiv \", \" org \"]: # arXiv identifiers. See https://arxiv.org/help/arxiv_identifier try: arxiv_id = parsed_url.path.split(\" / \", maxsplit=2)[2] if arxiv_id.endswith(\" . pdf \"): arxiv_id = arxiv_id[:-4] citekey = f\" arxiv :{ arxiv_id } \" except IndexError: pass if citekey is None or CiteKey(citekey).inspect() is not None: citekey = f\" url :{ url } \" return citekey Functions citekey_to_csl_item def citekey_to_csl_item ( citekey , prune = True , manual_refs = {}, log_level : Union [ str , int ] = 'WARNING' ) Generate a CSL_Item for the input citekey. View Source def citekey_to_csl_item ( citekey , prune = True , manual_refs = {}, log_level : tp . Union [ str , int ] = \"WARNING\" ): \"\"\" Generate a CSL_Item for the input citekey. \"\"\" from manubot import __version__ as manubot_version # https://stackoverflow.com/a/35704430/4651668 log_level = logging . _checkLevel ( log_level ) if not isinstance ( citekey , CiteKey ): citekey = CiteKey ( citekey ) if citekey . standard_id in manual_refs : return manual_refs [ citekey . standard_id ] try : csl_item = citekey . csl_item except Exception as error : logging . log ( log_level , f \"Generating csl_item for { citekey . standard_id !r} failed \" f \"due to a { error . __class__ . __name__ } : \\n { error } \" , ) logging . info ( error , exc_info = True ) return None # update csl_item with manubot generated metadata note_text = f \"This CSL Item was generated by Manubot v { manubot_version } from its persistent identifier (standard_id).\" note_dict = { \"standard_id\" : citekey . standard_id } csl_item . note_append_text ( note_text ) csl_item . note_append_dict ( note_dict ) csl_item . set_id ( citekey . short_id ) csl_item . clean ( prune = prune ) return csl_item shorten_citekey def shorten_citekey ( standard_citekey : str ) -> str Return a shortened citekey derived from the input citekey. The input citekey should be standardized prior to this function, since differences in the input citekey will result in different shortened citekeys. Short citekeys are generated by converting the input citekey to a 6 byte hash, and then converting this digest to a base62 ASCII str. Shortened citekeys consist of characters in the following ranges: 0-9, a-z and A-Z. View Source def shorten_citekey ( standard_citekey : str ) -> str : \"\"\" Return a shortened citekey derived from the input citekey. The input citekey should be standardized prior to this function, since differences in the input citekey will result in different shortened citekeys. Short citekeys are generated by converting the input citekey to a 6 byte hash, and then converting this digest to a base62 ASCII str. Shortened citekeys consist of characters in the following ranges: 0-9, a-z and A-Z. \"\"\" import hashlib import base62 assert not standard_citekey . startswith ( \"@\" ) as_bytes = standard_citekey . encode () blake_hash = hashlib . blake2b ( as_bytes , digest_size = 6 ) digest = blake_hash . digest () short_citekey = base62 . encodebytes ( digest ) return short_citekey url_to_citekey def url_to_citekey ( url : str ) -> str Convert a HTTP(s) URL into a citekey. For supported sources, convert from url citekey to an alternative source like doi. If citekeys fail inspection, revert alternative sources to URLs. View Source def url_to_citekey ( url : str ) -> str : \"\"\" Convert a HTTP(s) URL into a citekey. For supported sources, convert from url citekey to an alternative source like doi. If citekeys fail inspection, revert alternative sources to URLs. \"\"\" from urllib . parse import unquote , urlparse citekey = None parsed_url = urlparse ( url ) domain_levels = parsed_url . hostname . split ( \".\" ) if domain_levels [ - 2 : ] == [ \"doi\" , \"org\" ] : # DOI URLs doi = unquote ( parsed_url . path . lstrip ( \"/\" )) citekey = f \"doi:{doi}\" if domain_levels [ - 2 ] == \"sci-hub\" : # Sci - Hub domains doi = parsed_url . path . lstrip ( \"/\" ) citekey = f \"doi:{doi}\" if domain_levels [ - 2 : ] == [ \"biorxiv\" , \"org\" ] : # bioRxiv URL to DOI . See https : // git . io / Je9Hq match = re . search ( r \" / ( ? P < biorxiv_id> ([ 0 - 9 ]{ 4 } \\. [ 0 - 9 ]{ 2 } \\. [ 0 - 9 ]{ 2 } \\. ) ? [ 0 - 9 ]{ 6 ,}) \", parsed_url.path, ) if match: citekey = f\" doi : 10.1101 / { match . group ( 'biorxiv_id' )} \" is_ncbi_url = parsed_url.hostname.endswith(\" ncbi . nlm . nih . gov \") if is_ncbi_url and parsed_url.path.startswith(\" / pubmed/ \"): # PubMed URLs try: pmid = parsed_url.path.split(\" / \")[2] citekey = f\" pmid :{ pmid } \" except IndexError: pass if is_ncbi_url and parsed_url.path.startswith(\" / pmc/ \"): # PubMed Central URLs try: pmcid = parsed_url.path.split(\" / \")[3] citekey = f\" pmcid :{ pmcid } \" except IndexError: pass if domain_levels[-2:] == [\" wikidata \", \" org \"] and parsed_url.path.startswith( \" / wiki/ \" ): # Wikidata URLs try: wikidata_id = parsed_url.path.split(\" / \")[2] citekey = f\" wikidata :{ wikidata_id } \" except IndexError: pass if domain_levels[-2:] == [\" arxiv \", \" org \"]: # arXiv identifiers. See https://arxiv.org/help/arxiv_identifier try: arxiv_id = parsed_url.path.split(\" / \", maxsplit=2)[2] if arxiv_id.endswith(\" . pdf \"): arxiv_id = arxiv_id[:-4] citekey = f\" arxiv :{ arxiv_id } \" except IndexError: pass if citekey is None or CiteKey(citekey).inspect() is not None: citekey = f\" url :{ url } \" return citekey Classes CiteKey class CiteKey ( input_id : str , aliases : dict = < factory > , infer_prefix : bool = True ) View Source @ dataclasses . dataclass class CiteKey : input_id : str \"\"\"Input identifier for the citekey\"\"\" aliases : dict = dataclasses . field ( default_factory = dict ) \"\"\"Mapping from input identifier to aliases\"\"\" infer_prefix : bool = True \"\"\"Whether to infer the citekey's prefix when a prefix is missing or unhandled\"\"\" def __post_init__ ( self ) : self . check_input_id ( self . input_id ) @ staticmethod def check_input_id ( input_id ) -> None : if not isinstance ( input_id , str ) : raise TypeError ( \"input_id should be type 'str' not \" f \"{type(input_id).__name__!r}: {input_id!r}\" ) if input_id . startswith ( \"@\" ) : raise ValueError ( f \"invalid citekey input_id: {input_id!r} \\n starts with '@'\" ) @ classmethod @ functools . lru_cache ( maxsize = None ) def from_input_id ( cls , * args , ** kwargs ) -> \"CiteKey\" : \"\"\"Cached constructor\"\"\" return cls ( * args , ** kwargs ) @ cached_property def dealiased_id ( self ) -> str : \"\"\" If ` self . input_id ` is in ` self . aliases ` , the value specified by ` self . aliases ` . Otherwise , ` self . input_id ` . \"\"\" return self . aliases . get ( self . input_id , self . input_id ) def _set_prefix_accession ( self ) -> None : self . _prefix = None self . _accession = None split_id = self . dealiased_id . split ( \":\" , 1 ) if len ( split_id ) == 2 : self . _prefix , self . _accession = split_id if self . infer_prefix and not self . is_known_prefix : self . _infer_prefix () def _infer_prefix ( self ) -> None : \"\"\" Treat ` self . dealiased_id ` as missing a prefix . If the prefix can be inferred , set ` self . _prefix ` and ` self . _accession ` . Only call this function from _set_prefix_accession , since it is not safe after instance attributes or properties have been cached . \"\"\" from . handlers import infer_prefix prefix = infer_prefix ( self . dealiased_id ) if not prefix : return self . _prefix = prefix self . _accession = self . dealiased_id @property def prefix ( self ) -> tp . Optional [ str ] : \"\"\" If ` self . input_id ` contains a colon , the substring up to the first colon . Otherwise , None . \"\"\" if not hasattr ( self , \"_prefix\" ) : self . _set_prefix_accession () return self . _prefix @property def prefix_lower ( self ) -> tp . Optional [ str ] : \"\"\" A lowercase version of ` self . prefix ` or None . \"\"\" if self . prefix is None : return None return self . prefix . lower () @property def accession ( self ) -> tp . Optional [ str ] : \"\"\" If ` self . prefix ` , the remainder of ` self . input_id ` following the first colon . \"\"\" if not hasattr ( self , \"_accession\" ) : self . _set_prefix_accession () return self . _accession @property def standard_prefix ( self ) -> tp . Optional [ str ] : \"\"\" If the citekey is handled , the standard prefix specified by the handler . Otherwise , None . \"\"\" if not hasattr ( self , \"_standard_prefix\" ) : self . _standardize () return self . _standard_prefix @property def standard_accession ( self ) -> tp . Optional [ str ] : \"\"\" If the citekey is handled , the standard accession specified by the handler . Otherwise , None . \"\"\" if not hasattr ( self , \"_standard_accession\" ) : self . _standardize () return self . _standard_accession @ cached_property def handler ( self ) : from . handlers import Handler , get_handler if self . is_handled_prefix : return get_handler ( self . prefix_lower ) return Handler ( self . prefix_lower ) @property def is_handled_prefix ( self ) -> bool : from . handlers import prefix_to_handler return self . prefix_lower in prefix_to_handler @property def is_known_prefix ( self ) -> bool : return self . is_handled_prefix or self . is_pandoc_xnos_prefix () def inspect ( self ) -> tp . Optional [ str ] : \"\"\" Inspect citekey for potential problems . If no problems are found , return None . Otherwise , returns a string describing the problem . \"\"\" return self . handler . inspect ( self ) def _standardize ( self ) -> None : \"\"\" Set ` self . _standard_prefix ` , ` self . _standard_accession ` , and ` self . _standard_id ` . For citekeys without a prefix or with an unhandled prefix , _standard_prefix and _standard_accession are set to None . \"\"\" if not self . is_handled_prefix : self . _standard_prefix = None self . _standard_accession = None self . _standard_id = self . dealiased_id return fxn = self . handler . standardize_prefix_accession self . _standard_prefix , self . _standard_accession = fxn ( self . accession ) self . _standard_id = f \" { self . _standard_prefix } : { self . _standard_accession } \" @property def standard_id ( self ) -> str : \"\"\" If the citekey is handled , the standard_id specified by the handler . Otherwise , ` self . dealiased_id ` . \"\"\" if not hasattr ( self , \"_standard_id\" ) : self . _standardize () return self . _standard_id @ cached_property def short_id ( self ) -> str : \"\"\" A hashed version of standard_id whose characters are within the ranges 0-9 , a - z and A - Z . \"\"\" return shorten_citekey ( self . standard_id ) @ cached_property def all_ids ( self ) -> tp . List [ str ] : ids = [ self . input_id , self . dealiased_id , self . standard_id , self . short_id ] ids = [ x for x in ids if x ] # remove None ids = list ( dict . fromkeys ( ids )) # deduplicate return ids def __hash__ ( self ) : return hash (( self . input_id , self . dealiased_id )) def __repr__ ( self ) : return \" --> \" . join ( f \" { getattr ( self , key )} ({ key }) \" for key in ( \"input_id\" , \"dealiased_id\" , \"prefix_lower\" , \"accession\" , \"standard_id\" , \"short_id\" , ) ) @ cached_property def csl_item ( self ) : from . csl_item import CSL_Item csl_item = self . handler . get_csl_item ( self ) if not isinstance ( csl_item , CSL_Item ) : csl_item = CSL_Item ( csl_item ) csl_item . set_id ( self . standard_id ) return csl_item def is_pandoc_xnos_prefix ( self , log_case_warning : bool = False ) -> bool : from . handlers import _pandoc_xnos_prefixes if self . prefix in _pandoc_xnos_prefixes : return True if log_case_warning and self . prefix_lower in _pandoc_xnos_prefixes : logging . warning ( \"pandoc-xnos prefixes should be all lowercase. \\n \" f ' Should { self . input_id ! r } use { self . prefix_lower ! r } rather than \"{self.prefix!r}\" ? ' ) return False Class variables infer_prefix Static methods check_input_id def check_input_id ( input_id ) -> None View Source @ staticmethod def check_input_id ( input_id ) -> None : if not isinstance ( input_id , str ) : raise TypeError ( \"input_id should be type 'str' not \" f \"{type(input_id).__name__!r}: {input_id!r}\" ) if input_id . startswith ( \"@\" ) : raise ValueError ( f \"invalid citekey input_id: {input_id!r} \\n starts with '@'\" ) from_input_id def from_input_id ( * args , ** kwargs ) -> 'CiteKey' Cached constructor View Source @ classmethod @ functools . lru_cache ( maxsize = None ) def from_input_id ( cls , * args , ** kwargs ) -> \"CiteKey\" : \"\"\"Cached constructor\"\"\" return cls ( * args , ** kwargs ) Instance variables accession If self.prefix , the remainder of self.input_id following the first colon. is_handled_prefix is_known_prefix prefix If self.input_id contains a colon, the substring up to the first colon. Otherwise, None. prefix_lower A lowercase version of self.prefix or None. standard_accession If the citekey is handled, the standard accession specified by the handler. Otherwise, None. standard_id If the citekey is handled, the standard_id specified by the handler. Otherwise, self.dealiased_id . standard_prefix If the citekey is handled, the standard prefix specified by the handler. Otherwise, None. Methods all_ids def all_ids ( ... ) csl_item def csl_item ( ... ) dealiased_id def dealiased_id ( ... ) If self.input_id is in self.aliases , the value specified by self.aliases . Otherwise, self.input_id . handler def handler ( ... ) inspect def inspect ( self ) -> Optional [ str ] Inspect citekey for potential problems. If no problems are found, return None. Otherwise, returns a string describing the problem. View Source def inspect ( self ) -> tp . Optional [ str ] : \"\"\" Inspect citekey for potential problems. If no problems are found, return None. Otherwise, returns a string describing the problem. \"\"\" return self . handler . inspect ( self ) is_pandoc_xnos_prefix def is_pandoc_xnos_prefix ( self , log_case_warning : bool = False ) -> bool View Source def is_pandoc_xnos_prefix ( self , log_case_warning : bool = False ) -> bool : from .handlers import _pandoc_xnos_prefixes if self . prefix in _pandoc_xnos_prefixes : return True if log_case_warning and self . prefix_lower in _pandoc_xnos_prefixes : logging . warning ( \"pandoc-xnos prefixes should be all lowercase. \\n \" f 'Should { self . input_id !r} use { self . prefix_lower !r} rather than \" { self . prefix !r} \"?' ) return False short_id def short_id ( ... ) A hashed version of standard_id whose characters are within the ranges 0-9, a-z and A-Z.","title":"Citekey"},{"location":"reference/manubot/cite/citekey/#module-manubotcitecitekey","text":"Utilities for representing and processing citation keys. None View Source \"\"\" Utilities for representing and processing citation keys. \"\"\" import dataclasses import functools import logging import re import typing as tp try : from functools import cached_property except ImportError : from backports . cached_property import cached_property @dataclasses . dataclass class CiteKey : input_id: str \"\"\"Input identifier for the citekey\"\"\" aliases : dict = dataclasses . field ( default_factory = dict ) \"\"\"Mapping from input identifier to aliases\"\"\" infer_prefix: bool = True \"\"\"Whether to infer the citekey's prefix when a prefix is missing or unhandled\"\"\" def __ post_init__ ( self ) : self . check_input_id ( self . input_id ) @staticmethod def check_input_id ( input_id ) -> None : if not isinstance ( input_id , str ) : raise TypeError ( \"input_id should be type 'str' not \" f \"{type(input_id).__name__!r}: {input_id!r}\" ) if input_id . startswith ( \"@\" ) : raise ValueError ( f \"invalid citekey input_id: {input_id!r}\\nstarts with '@'\" ) @classmethod @functools . lru_cache ( maxsize = None ) def from_input_id ( cls , * args , **kwargs ) -> \"CiteKey\" : \"\"\"Cached constructor\"\"\" return cls ( * args , **kwargs ) @cached_property def dealiased_id ( self ) -> str : \"\"\" If `self.input_id` is in `self.aliases`, the value specified by `self.aliases`. Otherwise, `self.input_id`. \"\"\" return self . aliases . get ( self . input_id , self . input_id ) def _ set_prefix_accession ( self ) -> None : self . _ prefix = None self . _ accession = None split_id = self . dealiased_id . split ( \":\" , 1 ) if len ( split_id ) == 2 : self . _ prefix , self . _ accession = split_id if self . infer_prefix and not self . is_known_prefix: self . _ infer_prefix () def _ infer_prefix ( self ) -> None : \"\"\" Treat `self.dealiased_id` as missing a prefix. If the prefix can be inferred, set `self._prefix` and `self._accession`. Only call this function from _set_prefix_accession, since it is not safe after instance attributes or properties have been cached. \"\"\" from . handlers import infer_prefix prefix = infer_prefix ( self . dealiased_id ) if not prefix : return self . _ prefix = prefix self . _ accession = self . dealiased_id @property def prefix ( self ) -> tp . Optional [ str ] : \"\"\" If `self.input_id` contains a colon, the substring up to the first colon. Otherwise, None. \"\"\" if not hasattr ( self , \"_prefix\" ) : self . _ set_prefix_accession () return self . _ prefix @property def prefix_lower ( self ) -> tp . Optional [ str ] : \"\"\" A lowercase version of `self.prefix` or None. \"\"\" if self . prefix is None : return None return self . prefix . lower () @property def accession ( self ) -> tp . Optional [ str ] : \"\"\" If `self.prefix`, the remainder of `self.input_id` following the first colon. \"\"\" if not hasattr ( self , \"_accession\" ) : self . _ set_prefix_accession () return self . _ accession @property def standard_prefix ( self ) -> tp . Optional [ str ] : \"\"\" If the citekey is handled, the standard prefix specified by the handler. Otherwise, None. \"\"\" if not hasattr ( self , \"_standard_prefix\" ) : self . _ standardize () return self . _ standard_prefix @property def standard_accession ( self ) -> tp . Optional [ str ] : \"\"\" If the citekey is handled, the standard accession specified by the handler. Otherwise, None. \"\"\" if not hasattr ( self , \"_standard_accession\" ) : self . _ standardize () return self . _ standard_accession @cached_property def handler ( self ) : from . handlers import Handler , get_handler if self . is_handled_prefix: return get_handler ( self . prefix_lower ) return Handler ( self . prefix_lower ) @property def is_handled_prefix ( self ) -> bool : from . handlers import prefix_to_handler return self . prefix_lower in prefix_to_handler @property def is_known_prefix ( self ) -> bool : return self . is_handled_prefix or self . is_pandoc_xnos_prefix () def inspect ( self ) -> tp . Optional [ str ] : \"\"\" Inspect citekey for potential problems. If no problems are found, return None. Otherwise, returns a string describing the problem. \"\"\" return self . handler . inspect ( self ) def _ standardize ( self ) -> None : \"\"\" Set `self._standard_prefix`, `self._standard_accession`, and `self._standard_id`. For citekeys without a prefix or with an unhandled prefix, _standard_prefix and _standard_accession are set to None. \"\"\" if not self . is_handled_prefix: self . _ standard_prefix = None self . _ standard_accession = None self . _ standard_id = self . dealiased_id return fxn = self . handler . standardize_prefix_accession self . _ standard_prefix , self . _ standard_accession = fxn ( self . accession ) self . _ standard_id = f \"{self._standard_prefix}:{self._standard_accession}\" @property def standard_id ( self ) -> str : \"\"\" If the citekey is handled, the standard_id specified by the handler. Otherwise, `self.dealiased_id`. \"\"\" if not hasattr ( self , \"_standard_id\" ) : self . _ standardize () return self . _ standard_id @cached_property def short_id ( self ) -> str : \"\"\" A hashed version of standard_id whose characters are within the ranges 0-9, a-z and A-Z. \"\"\" return shorten_citekey ( self . standard_id ) @cached_property def all_ids ( self ) -> tp . List [ str ] : ids = [ self . input_id , self . dealiased_id , self . standard_id , self . short_id ] ids = [ x for x in ids if x ] # remove None ids = list ( dict . fromkeys ( ids )) # deduplicate return ids def __ hash__ ( self ) : return hash (( self . input_id , self . dealiased_id )) def __ repr__ ( self ) : return \" --> \" . join ( f \"{getattr(self, key)} ({key})\" for key in ( \"input_id\" , \"dealiased_id\" , \"prefix_lower\" , \"accession\" , \"standard_id\" , \"short_id\" , ) ) @cached_property def csl_item ( self ) : from . csl_item import CSL_Item csl_item = self . handler . get_csl_item ( self ) if not isinstance ( csl_item , CSL_Item ) : csl_item = CSL_Item ( csl_item ) csl_item . set_id ( self . standard_id ) return csl_item def is_pandoc_xnos_prefix ( self , log_case_warning: bool = False ) -> bool : from . handlers import _ pandoc_xnos_prefixes if self . prefix in _ pandoc_xnos_prefixes: return True if log_case_warning and self . prefix_lower in _ pandoc_xnos_prefixes: logging . warning ( \"pandoc-xnos prefixes should be all lowercase.\\n\" f'Should {self.input_id!r} use {self.prefix_lower!r} rather than \"{self.prefix!r}\"?' ) return False def shorten_citekey ( standard_citekey: str ) -> str : \"\"\" Return a shortened citekey derived from the input citekey. The input citekey should be standardized prior to this function, since differences in the input citekey will result in different shortened citekeys. Short citekeys are generated by converting the input citekey to a 6 byte hash, and then converting this digest to a base62 ASCII str. Shortened citekeys consist of characters in the following ranges: 0-9, a-z and A-Z. \"\"\" import hashlib import base62 assert not standard_citekey . startswith ( \"@\" ) as_bytes = standard_citekey . encode () blake_hash = hashlib . blake2b ( as_bytes , digest_size = 6 ) digest = blake_hash . digest () short_citekey = base62 . encodebytes ( digest ) return short_citekey def citekey_to_csl_item ( citekey , prune = True , manual_refs= {}, log_level: tp . Union [ str , int ] = \"WARNING\" ) : \"\"\" Generate a CSL_Item for the input citekey. \"\"\" from manubot import __ version__ as manubot_version # https : // stackoverflow . com / a / 35704430 / 4651668 log_level = logging . _ checkLevel ( log_level ) if not isinstance ( citekey , CiteKey ) : citekey = CiteKey ( citekey ) if citekey . standard_id in manual_refs: return manual_refs [ citekey . standard_id ] try : csl_item = citekey . csl_item except Exception as error : logging . log ( log_level , f \"Generating csl_item for {citekey.standard_id!r} failed \" f \"due to a {error.__class__.__name__}:\\n{error}\" , ) logging . info ( error , exc_info = True ) return None # update csl_item with manubot generated metadata note_text = f \"This CSL Item was generated by Manubot v{manubot_version} from its persistent identifier (standard_id).\" note_dict = { \"standard_id\" : citekey . standard_id } csl_item . note_append_text ( note_text ) csl_item . note_append_dict ( note_dict ) csl_item . set_id ( citekey . short_id ) csl_item . clean ( prune = prune ) return csl_item def url_to_citekey ( url : str ) -> str : \"\"\" Convert a HTTP(s) URL into a citekey. For supported sources, convert from url citekey to an alternative source like doi. If citekeys fail inspection, revert alternative sources to URLs. \"\"\" from urllib . parse import unquote , urlparse citekey = None parsed_url = urlparse ( url ) domain_levels = parsed_url . hostname . split ( \".\" ) if domain_levels [ - 2 : ] == [ \"doi\" , \"org\" ] : # DOI URLs doi = unquote ( parsed_url . path . lstrip ( \"/\" )) citekey = f \"doi:{doi}\" if domain_levels [ - 2 ] == \"sci-hub\" : # Sci - Hub domains doi = parsed_url . path . lstrip ( \"/\" ) citekey = f \"doi:{doi}\" if domain_levels [ - 2 : ] == [ \"biorxiv\" , \"org\" ] : # bioRxiv URL to DOI . See https : // git . io / Je9Hq match = re . search ( r \" / ( ? P < biorxiv_id> ([ 0 - 9 ]{ 4 } \\. [ 0 - 9 ]{ 2 } \\. [ 0 - 9 ]{ 2 } \\. ) ? [ 0 - 9 ]{ 6 ,}) \", parsed_url.path, ) if match: citekey = f\" doi : 10.1101 / { match . group ( 'biorxiv_id' )} \" is_ncbi_url = parsed_url.hostname.endswith(\" ncbi . nlm . nih . gov \") if is_ncbi_url and parsed_url.path.startswith(\" / pubmed/ \"): # PubMed URLs try: pmid = parsed_url.path.split(\" / \")[2] citekey = f\" pmid :{ pmid } \" except IndexError: pass if is_ncbi_url and parsed_url.path.startswith(\" / pmc/ \"): # PubMed Central URLs try: pmcid = parsed_url.path.split(\" / \")[3] citekey = f\" pmcid :{ pmcid } \" except IndexError: pass if domain_levels[-2:] == [\" wikidata \", \" org \"] and parsed_url.path.startswith( \" / wiki/ \" ): # Wikidata URLs try: wikidata_id = parsed_url.path.split(\" / \")[2] citekey = f\" wikidata :{ wikidata_id } \" except IndexError: pass if domain_levels[-2:] == [\" arxiv \", \" org \"]: # arXiv identifiers. See https://arxiv.org/help/arxiv_identifier try: arxiv_id = parsed_url.path.split(\" / \", maxsplit=2)[2] if arxiv_id.endswith(\" . pdf \"): arxiv_id = arxiv_id[:-4] citekey = f\" arxiv :{ arxiv_id } \" except IndexError: pass if citekey is None or CiteKey(citekey).inspect() is not None: citekey = f\" url :{ url } \" return citekey","title":"Module manubot.cite.citekey"},{"location":"reference/manubot/cite/citekey/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/citekey/#citekey_to_csl_item","text":"def citekey_to_csl_item ( citekey , prune = True , manual_refs = {}, log_level : Union [ str , int ] = 'WARNING' ) Generate a CSL_Item for the input citekey. View Source def citekey_to_csl_item ( citekey , prune = True , manual_refs = {}, log_level : tp . Union [ str , int ] = \"WARNING\" ): \"\"\" Generate a CSL_Item for the input citekey. \"\"\" from manubot import __version__ as manubot_version # https://stackoverflow.com/a/35704430/4651668 log_level = logging . _checkLevel ( log_level ) if not isinstance ( citekey , CiteKey ): citekey = CiteKey ( citekey ) if citekey . standard_id in manual_refs : return manual_refs [ citekey . standard_id ] try : csl_item = citekey . csl_item except Exception as error : logging . log ( log_level , f \"Generating csl_item for { citekey . standard_id !r} failed \" f \"due to a { error . __class__ . __name__ } : \\n { error } \" , ) logging . info ( error , exc_info = True ) return None # update csl_item with manubot generated metadata note_text = f \"This CSL Item was generated by Manubot v { manubot_version } from its persistent identifier (standard_id).\" note_dict = { \"standard_id\" : citekey . standard_id } csl_item . note_append_text ( note_text ) csl_item . note_append_dict ( note_dict ) csl_item . set_id ( citekey . short_id ) csl_item . clean ( prune = prune ) return csl_item","title":"citekey_to_csl_item"},{"location":"reference/manubot/cite/citekey/#shorten_citekey","text":"def shorten_citekey ( standard_citekey : str ) -> str Return a shortened citekey derived from the input citekey. The input citekey should be standardized prior to this function, since differences in the input citekey will result in different shortened citekeys. Short citekeys are generated by converting the input citekey to a 6 byte hash, and then converting this digest to a base62 ASCII str. Shortened citekeys consist of characters in the following ranges: 0-9, a-z and A-Z. View Source def shorten_citekey ( standard_citekey : str ) -> str : \"\"\" Return a shortened citekey derived from the input citekey. The input citekey should be standardized prior to this function, since differences in the input citekey will result in different shortened citekeys. Short citekeys are generated by converting the input citekey to a 6 byte hash, and then converting this digest to a base62 ASCII str. Shortened citekeys consist of characters in the following ranges: 0-9, a-z and A-Z. \"\"\" import hashlib import base62 assert not standard_citekey . startswith ( \"@\" ) as_bytes = standard_citekey . encode () blake_hash = hashlib . blake2b ( as_bytes , digest_size = 6 ) digest = blake_hash . digest () short_citekey = base62 . encodebytes ( digest ) return short_citekey","title":"shorten_citekey"},{"location":"reference/manubot/cite/citekey/#url_to_citekey","text":"def url_to_citekey ( url : str ) -> str Convert a HTTP(s) URL into a citekey. For supported sources, convert from url citekey to an alternative source like doi. If citekeys fail inspection, revert alternative sources to URLs. View Source def url_to_citekey ( url : str ) -> str : \"\"\" Convert a HTTP(s) URL into a citekey. For supported sources, convert from url citekey to an alternative source like doi. If citekeys fail inspection, revert alternative sources to URLs. \"\"\" from urllib . parse import unquote , urlparse citekey = None parsed_url = urlparse ( url ) domain_levels = parsed_url . hostname . split ( \".\" ) if domain_levels [ - 2 : ] == [ \"doi\" , \"org\" ] : # DOI URLs doi = unquote ( parsed_url . path . lstrip ( \"/\" )) citekey = f \"doi:{doi}\" if domain_levels [ - 2 ] == \"sci-hub\" : # Sci - Hub domains doi = parsed_url . path . lstrip ( \"/\" ) citekey = f \"doi:{doi}\" if domain_levels [ - 2 : ] == [ \"biorxiv\" , \"org\" ] : # bioRxiv URL to DOI . See https : // git . io / Je9Hq match = re . search ( r \" / ( ? P < biorxiv_id> ([ 0 - 9 ]{ 4 } \\. [ 0 - 9 ]{ 2 } \\. [ 0 - 9 ]{ 2 } \\. ) ? [ 0 - 9 ]{ 6 ,}) \", parsed_url.path, ) if match: citekey = f\" doi : 10.1101 / { match . group ( 'biorxiv_id' )} \" is_ncbi_url = parsed_url.hostname.endswith(\" ncbi . nlm . nih . gov \") if is_ncbi_url and parsed_url.path.startswith(\" / pubmed/ \"): # PubMed URLs try: pmid = parsed_url.path.split(\" / \")[2] citekey = f\" pmid :{ pmid } \" except IndexError: pass if is_ncbi_url and parsed_url.path.startswith(\" / pmc/ \"): # PubMed Central URLs try: pmcid = parsed_url.path.split(\" / \")[3] citekey = f\" pmcid :{ pmcid } \" except IndexError: pass if domain_levels[-2:] == [\" wikidata \", \" org \"] and parsed_url.path.startswith( \" / wiki/ \" ): # Wikidata URLs try: wikidata_id = parsed_url.path.split(\" / \")[2] citekey = f\" wikidata :{ wikidata_id } \" except IndexError: pass if domain_levels[-2:] == [\" arxiv \", \" org \"]: # arXiv identifiers. See https://arxiv.org/help/arxiv_identifier try: arxiv_id = parsed_url.path.split(\" / \", maxsplit=2)[2] if arxiv_id.endswith(\" . pdf \"): arxiv_id = arxiv_id[:-4] citekey = f\" arxiv :{ arxiv_id } \" except IndexError: pass if citekey is None or CiteKey(citekey).inspect() is not None: citekey = f\" url :{ url } \" return citekey","title":"url_to_citekey"},{"location":"reference/manubot/cite/citekey/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/citekey/#citekey","text":"class CiteKey ( input_id : str , aliases : dict = < factory > , infer_prefix : bool = True ) View Source @ dataclasses . dataclass class CiteKey : input_id : str \"\"\"Input identifier for the citekey\"\"\" aliases : dict = dataclasses . field ( default_factory = dict ) \"\"\"Mapping from input identifier to aliases\"\"\" infer_prefix : bool = True \"\"\"Whether to infer the citekey's prefix when a prefix is missing or unhandled\"\"\" def __post_init__ ( self ) : self . check_input_id ( self . input_id ) @ staticmethod def check_input_id ( input_id ) -> None : if not isinstance ( input_id , str ) : raise TypeError ( \"input_id should be type 'str' not \" f \"{type(input_id).__name__!r}: {input_id!r}\" ) if input_id . startswith ( \"@\" ) : raise ValueError ( f \"invalid citekey input_id: {input_id!r} \\n starts with '@'\" ) @ classmethod @ functools . lru_cache ( maxsize = None ) def from_input_id ( cls , * args , ** kwargs ) -> \"CiteKey\" : \"\"\"Cached constructor\"\"\" return cls ( * args , ** kwargs ) @ cached_property def dealiased_id ( self ) -> str : \"\"\" If ` self . input_id ` is in ` self . aliases ` , the value specified by ` self . aliases ` . Otherwise , ` self . input_id ` . \"\"\" return self . aliases . get ( self . input_id , self . input_id ) def _set_prefix_accession ( self ) -> None : self . _prefix = None self . _accession = None split_id = self . dealiased_id . split ( \":\" , 1 ) if len ( split_id ) == 2 : self . _prefix , self . _accession = split_id if self . infer_prefix and not self . is_known_prefix : self . _infer_prefix () def _infer_prefix ( self ) -> None : \"\"\" Treat ` self . dealiased_id ` as missing a prefix . If the prefix can be inferred , set ` self . _prefix ` and ` self . _accession ` . Only call this function from _set_prefix_accession , since it is not safe after instance attributes or properties have been cached . \"\"\" from . handlers import infer_prefix prefix = infer_prefix ( self . dealiased_id ) if not prefix : return self . _prefix = prefix self . _accession = self . dealiased_id @property def prefix ( self ) -> tp . Optional [ str ] : \"\"\" If ` self . input_id ` contains a colon , the substring up to the first colon . Otherwise , None . \"\"\" if not hasattr ( self , \"_prefix\" ) : self . _set_prefix_accession () return self . _prefix @property def prefix_lower ( self ) -> tp . Optional [ str ] : \"\"\" A lowercase version of ` self . prefix ` or None . \"\"\" if self . prefix is None : return None return self . prefix . lower () @property def accession ( self ) -> tp . Optional [ str ] : \"\"\" If ` self . prefix ` , the remainder of ` self . input_id ` following the first colon . \"\"\" if not hasattr ( self , \"_accession\" ) : self . _set_prefix_accession () return self . _accession @property def standard_prefix ( self ) -> tp . Optional [ str ] : \"\"\" If the citekey is handled , the standard prefix specified by the handler . Otherwise , None . \"\"\" if not hasattr ( self , \"_standard_prefix\" ) : self . _standardize () return self . _standard_prefix @property def standard_accession ( self ) -> tp . Optional [ str ] : \"\"\" If the citekey is handled , the standard accession specified by the handler . Otherwise , None . \"\"\" if not hasattr ( self , \"_standard_accession\" ) : self . _standardize () return self . _standard_accession @ cached_property def handler ( self ) : from . handlers import Handler , get_handler if self . is_handled_prefix : return get_handler ( self . prefix_lower ) return Handler ( self . prefix_lower ) @property def is_handled_prefix ( self ) -> bool : from . handlers import prefix_to_handler return self . prefix_lower in prefix_to_handler @property def is_known_prefix ( self ) -> bool : return self . is_handled_prefix or self . is_pandoc_xnos_prefix () def inspect ( self ) -> tp . Optional [ str ] : \"\"\" Inspect citekey for potential problems . If no problems are found , return None . Otherwise , returns a string describing the problem . \"\"\" return self . handler . inspect ( self ) def _standardize ( self ) -> None : \"\"\" Set ` self . _standard_prefix ` , ` self . _standard_accession ` , and ` self . _standard_id ` . For citekeys without a prefix or with an unhandled prefix , _standard_prefix and _standard_accession are set to None . \"\"\" if not self . is_handled_prefix : self . _standard_prefix = None self . _standard_accession = None self . _standard_id = self . dealiased_id return fxn = self . handler . standardize_prefix_accession self . _standard_prefix , self . _standard_accession = fxn ( self . accession ) self . _standard_id = f \" { self . _standard_prefix } : { self . _standard_accession } \" @property def standard_id ( self ) -> str : \"\"\" If the citekey is handled , the standard_id specified by the handler . Otherwise , ` self . dealiased_id ` . \"\"\" if not hasattr ( self , \"_standard_id\" ) : self . _standardize () return self . _standard_id @ cached_property def short_id ( self ) -> str : \"\"\" A hashed version of standard_id whose characters are within the ranges 0-9 , a - z and A - Z . \"\"\" return shorten_citekey ( self . standard_id ) @ cached_property def all_ids ( self ) -> tp . List [ str ] : ids = [ self . input_id , self . dealiased_id , self . standard_id , self . short_id ] ids = [ x for x in ids if x ] # remove None ids = list ( dict . fromkeys ( ids )) # deduplicate return ids def __hash__ ( self ) : return hash (( self . input_id , self . dealiased_id )) def __repr__ ( self ) : return \" --> \" . join ( f \" { getattr ( self , key )} ({ key }) \" for key in ( \"input_id\" , \"dealiased_id\" , \"prefix_lower\" , \"accession\" , \"standard_id\" , \"short_id\" , ) ) @ cached_property def csl_item ( self ) : from . csl_item import CSL_Item csl_item = self . handler . get_csl_item ( self ) if not isinstance ( csl_item , CSL_Item ) : csl_item = CSL_Item ( csl_item ) csl_item . set_id ( self . standard_id ) return csl_item def is_pandoc_xnos_prefix ( self , log_case_warning : bool = False ) -> bool : from . handlers import _pandoc_xnos_prefixes if self . prefix in _pandoc_xnos_prefixes : return True if log_case_warning and self . prefix_lower in _pandoc_xnos_prefixes : logging . warning ( \"pandoc-xnos prefixes should be all lowercase. \\n \" f ' Should { self . input_id ! r } use { self . prefix_lower ! r } rather than \"{self.prefix!r}\" ? ' ) return False","title":"CiteKey"},{"location":"reference/manubot/cite/citekey/#class-variables","text":"infer_prefix","title":"Class variables"},{"location":"reference/manubot/cite/citekey/#static-methods","text":"","title":"Static methods"},{"location":"reference/manubot/cite/citekey/#check_input_id","text":"def check_input_id ( input_id ) -> None View Source @ staticmethod def check_input_id ( input_id ) -> None : if not isinstance ( input_id , str ) : raise TypeError ( \"input_id should be type 'str' not \" f \"{type(input_id).__name__!r}: {input_id!r}\" ) if input_id . startswith ( \"@\" ) : raise ValueError ( f \"invalid citekey input_id: {input_id!r} \\n starts with '@'\" )","title":"check_input_id"},{"location":"reference/manubot/cite/citekey/#from_input_id","text":"def from_input_id ( * args , ** kwargs ) -> 'CiteKey' Cached constructor View Source @ classmethod @ functools . lru_cache ( maxsize = None ) def from_input_id ( cls , * args , ** kwargs ) -> \"CiteKey\" : \"\"\"Cached constructor\"\"\" return cls ( * args , ** kwargs )","title":"from_input_id"},{"location":"reference/manubot/cite/citekey/#instance-variables","text":"accession If self.prefix , the remainder of self.input_id following the first colon. is_handled_prefix is_known_prefix prefix If self.input_id contains a colon, the substring up to the first colon. Otherwise, None. prefix_lower A lowercase version of self.prefix or None. standard_accession If the citekey is handled, the standard accession specified by the handler. Otherwise, None. standard_id If the citekey is handled, the standard_id specified by the handler. Otherwise, self.dealiased_id . standard_prefix If the citekey is handled, the standard prefix specified by the handler. Otherwise, None.","title":"Instance variables"},{"location":"reference/manubot/cite/citekey/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/citekey/#all_ids","text":"def all_ids ( ... )","title":"all_ids"},{"location":"reference/manubot/cite/citekey/#csl_item","text":"def csl_item ( ... )","title":"csl_item"},{"location":"reference/manubot/cite/citekey/#dealiased_id","text":"def dealiased_id ( ... ) If self.input_id is in self.aliases , the value specified by self.aliases . Otherwise, self.input_id .","title":"dealiased_id"},{"location":"reference/manubot/cite/citekey/#handler","text":"def handler ( ... )","title":"handler"},{"location":"reference/manubot/cite/citekey/#inspect","text":"def inspect ( self ) -> Optional [ str ] Inspect citekey for potential problems. If no problems are found, return None. Otherwise, returns a string describing the problem. View Source def inspect ( self ) -> tp . Optional [ str ] : \"\"\" Inspect citekey for potential problems. If no problems are found, return None. Otherwise, returns a string describing the problem. \"\"\" return self . handler . inspect ( self )","title":"inspect"},{"location":"reference/manubot/cite/citekey/#is_pandoc_xnos_prefix","text":"def is_pandoc_xnos_prefix ( self , log_case_warning : bool = False ) -> bool View Source def is_pandoc_xnos_prefix ( self , log_case_warning : bool = False ) -> bool : from .handlers import _pandoc_xnos_prefixes if self . prefix in _pandoc_xnos_prefixes : return True if log_case_warning and self . prefix_lower in _pandoc_xnos_prefixes : logging . warning ( \"pandoc-xnos prefixes should be all lowercase. \\n \" f 'Should { self . input_id !r} use { self . prefix_lower !r} rather than \" { self . prefix !r} \"?' ) return False","title":"is_pandoc_xnos_prefix"},{"location":"reference/manubot/cite/citekey/#short_id","text":"def short_id ( ... ) A hashed version of standard_id whose characters are within the ranges 0-9, a-z and A-Z.","title":"short_id"},{"location":"reference/manubot/cite/citeproc/","text":"Module manubot.cite.citeproc Correct or validate CSL item schema. Module naming: citeproc is the generic name for programs that produce formatted bibliographies and citations based on the metadata of the cited objects and the formatting instructions provided by Citation Style Language (CSL) styles. -- https://en.wikipedia.org/wiki/CiteProc View Source \"\"\"Correct or validate CSL item schema. Module naming: citeproc is the generic name for programs that produce formatted bibliographies and citations based on the metadata of the cited objects and the formatting instructions provided by Citation Style Language (CSL) styles. -- https://en.wikipedia.org/wiki/CiteProc \"\"\" import copy import functools import logging from manubot.util import read_serialized_data @functools . lru_cache () def get_jsonschema_csl_validator (): \"\"\" Return a jsonschema validator for the CSL Item JSON Schema \"\"\" import jsonschema url = \"https://github.com/dhimmel/csl-schema/raw/manubot/csl-data.json\" schema = read_serialized_data ( url ) Validator = jsonschema . validators . validator_for ( schema ) Validator . check_schema ( schema ) return Validator ( schema ) def remove_jsonschema_errors ( instance , recurse_depth = 5 , in_place = False ): \"\"\" Remove fields in CSL Items that produce JSON Schema errors. Should errors be removed, but the JSON instance still fails to validate, recursively call remove_jsonschema_errors until the instance validates or the recursion depth limit is reached. Note that this method may not be work for all types of JSON Schema errors and users looking to adapt it for other applications should write task-specific tests to provide empirical evaluate that it works as intended. The default in_place=False creates a deepcopy of instance before pruning it, such that a new dictionary is returned and instance is not edited. Set in_place=True to edit instance in-place. The inital implementation of remove_jsonschema_errors always deepcopied instance, and it is possible deepcopying is important to prevent malfunction when encountering certain edge cases. Please report if you observe any in_place dependent behaviors. See also: https://github.com/Julian/jsonschema/issues/448 https://stackoverflow.com/questions/44694835 \"\"\" validator = get_jsonschema_csl_validator () errors = list ( validator . iter_errors ( instance )) if not in_place : instance = copy . deepcopy ( instance ) errors = sorted ( errors , key = lambda e : e . path , reverse = True ) for error in errors : _remove_error ( instance , error ) if validator . is_valid ( instance ) or recurse_depth < 1 : return instance return remove_jsonschema_errors ( instance , recurse_depth - 1 , in_place = in_place ) def _delete_elem ( instance , path , absolute_path = None , message = \"\" ): \"\"\" Helper function for remove_jsonschema_errors that deletes an element in the JSON-like input instance at the specified path. absolute_path is relative to the original validated instance for logging purposes. Defaults to path, if not specified. message is an optional string with additional error information to log. \"\"\" if absolute_path is None : absolute_path = path logging . debug ( ( f \" { message } \\n \" if message else message ) + \"_delete_elem deleting CSL element at: \" + \"/\" . join ( map ( str , absolute_path )) ) * head , tail = path try : del _deep_get ( instance , head )[ tail ] except KeyError : pass def _deep_get ( instance , path ): \"\"\" Descend path to return a deep element in the JSON object instance. \"\"\" for key in path : instance = instance [ key ] return instance def _remove_error ( instance , error ): \"\"\" Remove a jsonschema ValidationError from the JSON-like instance. See ValidationError documentation at https://python-jsonschema.readthedocs.io/en/latest/errors/#jsonschema.exceptions.ValidationError \"\"\" sub_errors = error . context if sub_errors : # already_removed_additional was neccessary to workaround # https://github.com/citation-style-language/schema/issues/154 already_removed_additional = False for sub_error in sub_errors : if sub_error . validator == \"additionalProperties\" : if already_removed_additional : continue already_removed_additional = True sub_instance = _deep_get ( instance , error . path ) _remove_error ( sub_instance , sub_error ) elif error . validator == \"additionalProperties\" : extras = set ( error . instance ) - set ( error . schema [ \"properties\" ]) logging . debug ( error . message + f \" \\n Will now remove these { len ( extras ) } additional properties.\" ) for key in extras : _delete_elem ( instance = instance , path = list ( error . path ) + [ key ], absolute_path = list ( error . absolute_path ) + [ key ], ) elif error . validator in { \"enum\" , \"type\" , \"minItems\" , \"maxItems\" }: _delete_elem ( instance , error . path , error . absolute_path , error . message ) elif error . validator == \"required\" : logging . warning ( ( f \" { error . message } \\n \" if error . message else error . message ) + \"required element missing at: \" + \"/\" . join ( map ( str , error . absolute_path )) ) else : raise NotImplementedError ( f \" { error . validator } is not yet supported\" ) Functions get_jsonschema_csl_validator def get_jsonschema_csl_validator ( ) Return a jsonschema validator for the CSL Item JSON Schema View Source @functools . lru_cache () def get_jsonschema_csl_validator (): \"\"\" Return a jsonschema validator for the CSL Item JSON Schema \"\"\" import jsonschema url = \"https://github.com/dhimmel/csl-schema/raw/manubot/csl-data.json\" schema = read_serialized_data ( url ) Validator = jsonschema . validators . validator_for ( schema ) Validator . check_schema ( schema ) return Validator ( schema ) remove_jsonschema_errors def remove_jsonschema_errors ( instance , recurse_depth = 5 , in_place = False ) Remove fields in CSL Items that produce JSON Schema errors. Should errors be removed, but the JSON instance still fails to validate, recursively call remove_jsonschema_errors until the instance validates or the recursion depth limit is reached. Note that this method may not be work for all types of JSON Schema errors and users looking to adapt it for other applications should write task-specific tests to provide empirical evaluate that it works as intended. The default in_place=False creates a deepcopy of instance before pruning it, such that a new dictionary is returned and instance is not edited. Set in_place=True to edit instance in-place. The inital implementation of remove_jsonschema_errors always deepcopied instance, and it is possible deepcopying is important to prevent malfunction when encountering certain edge cases. Please report if you observe any in_place dependent behaviors. See also: https://github.com/Julian/jsonschema/issues/448 https://stackoverflow.com/questions/44694835 View Source def remove_jsonschema_errors ( instance , recurse_depth = 5 , in_place = False ) : \"\"\" Remove fields in CSL Items that produce JSON Schema errors . Should errors be removed , but the JSON instance still fails to validate , recursively call remove_jsonschema_errors until the instance validates or the recursion depth limit is reached . Note that this method may not be work for all types of JSON Schema errors and users looking to adapt it for other applications should write task - specific tests to provide empirical evaluate that it works as intended . The default in_place = False creates a deepcopy of instance before pruning it , such that a new dictionary is returned and instance is not edited . Set in_place = True to edit instance in - place . The inital implementation of remove_jsonschema_errors always deepcopied instance , and it is possible deepcopying is important to prevent malfunction when encountering certain edge cases . Please report if you observe any in_place dependent behaviors . See also : https : // github . com / Julian / jsonschema / issues / 448 https : // stackoverflow . com / questions / 44694835 \"\"\" validator = get_jsonschema_csl_validator () errors = list ( validator . iter_errors ( instance )) if not in_place : instance = copy . deepcopy ( instance ) errors = sorted ( errors , key = lambda e : e . path , reverse = True ) for error in errors : _remove_error ( instance , error ) if validator . is_valid ( instance ) or recurse_depth < 1 : return instance return remove_jsonschema_errors ( instance , recurse_depth - 1 , in_place = in_place )","title":"Citeproc"},{"location":"reference/manubot/cite/citeproc/#module-manubotciteciteproc","text":"Correct or validate CSL item schema. Module naming: citeproc is the generic name for programs that produce formatted bibliographies and citations based on the metadata of the cited objects and the formatting instructions provided by Citation Style Language (CSL) styles. -- https://en.wikipedia.org/wiki/CiteProc View Source \"\"\"Correct or validate CSL item schema. Module naming: citeproc is the generic name for programs that produce formatted bibliographies and citations based on the metadata of the cited objects and the formatting instructions provided by Citation Style Language (CSL) styles. -- https://en.wikipedia.org/wiki/CiteProc \"\"\" import copy import functools import logging from manubot.util import read_serialized_data @functools . lru_cache () def get_jsonschema_csl_validator (): \"\"\" Return a jsonschema validator for the CSL Item JSON Schema \"\"\" import jsonschema url = \"https://github.com/dhimmel/csl-schema/raw/manubot/csl-data.json\" schema = read_serialized_data ( url ) Validator = jsonschema . validators . validator_for ( schema ) Validator . check_schema ( schema ) return Validator ( schema ) def remove_jsonschema_errors ( instance , recurse_depth = 5 , in_place = False ): \"\"\" Remove fields in CSL Items that produce JSON Schema errors. Should errors be removed, but the JSON instance still fails to validate, recursively call remove_jsonschema_errors until the instance validates or the recursion depth limit is reached. Note that this method may not be work for all types of JSON Schema errors and users looking to adapt it for other applications should write task-specific tests to provide empirical evaluate that it works as intended. The default in_place=False creates a deepcopy of instance before pruning it, such that a new dictionary is returned and instance is not edited. Set in_place=True to edit instance in-place. The inital implementation of remove_jsonschema_errors always deepcopied instance, and it is possible deepcopying is important to prevent malfunction when encountering certain edge cases. Please report if you observe any in_place dependent behaviors. See also: https://github.com/Julian/jsonschema/issues/448 https://stackoverflow.com/questions/44694835 \"\"\" validator = get_jsonschema_csl_validator () errors = list ( validator . iter_errors ( instance )) if not in_place : instance = copy . deepcopy ( instance ) errors = sorted ( errors , key = lambda e : e . path , reverse = True ) for error in errors : _remove_error ( instance , error ) if validator . is_valid ( instance ) or recurse_depth < 1 : return instance return remove_jsonschema_errors ( instance , recurse_depth - 1 , in_place = in_place ) def _delete_elem ( instance , path , absolute_path = None , message = \"\" ): \"\"\" Helper function for remove_jsonschema_errors that deletes an element in the JSON-like input instance at the specified path. absolute_path is relative to the original validated instance for logging purposes. Defaults to path, if not specified. message is an optional string with additional error information to log. \"\"\" if absolute_path is None : absolute_path = path logging . debug ( ( f \" { message } \\n \" if message else message ) + \"_delete_elem deleting CSL element at: \" + \"/\" . join ( map ( str , absolute_path )) ) * head , tail = path try : del _deep_get ( instance , head )[ tail ] except KeyError : pass def _deep_get ( instance , path ): \"\"\" Descend path to return a deep element in the JSON object instance. \"\"\" for key in path : instance = instance [ key ] return instance def _remove_error ( instance , error ): \"\"\" Remove a jsonschema ValidationError from the JSON-like instance. See ValidationError documentation at https://python-jsonschema.readthedocs.io/en/latest/errors/#jsonschema.exceptions.ValidationError \"\"\" sub_errors = error . context if sub_errors : # already_removed_additional was neccessary to workaround # https://github.com/citation-style-language/schema/issues/154 already_removed_additional = False for sub_error in sub_errors : if sub_error . validator == \"additionalProperties\" : if already_removed_additional : continue already_removed_additional = True sub_instance = _deep_get ( instance , error . path ) _remove_error ( sub_instance , sub_error ) elif error . validator == \"additionalProperties\" : extras = set ( error . instance ) - set ( error . schema [ \"properties\" ]) logging . debug ( error . message + f \" \\n Will now remove these { len ( extras ) } additional properties.\" ) for key in extras : _delete_elem ( instance = instance , path = list ( error . path ) + [ key ], absolute_path = list ( error . absolute_path ) + [ key ], ) elif error . validator in { \"enum\" , \"type\" , \"minItems\" , \"maxItems\" }: _delete_elem ( instance , error . path , error . absolute_path , error . message ) elif error . validator == \"required\" : logging . warning ( ( f \" { error . message } \\n \" if error . message else error . message ) + \"required element missing at: \" + \"/\" . join ( map ( str , error . absolute_path )) ) else : raise NotImplementedError ( f \" { error . validator } is not yet supported\" )","title":"Module manubot.cite.citeproc"},{"location":"reference/manubot/cite/citeproc/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/citeproc/#get_jsonschema_csl_validator","text":"def get_jsonschema_csl_validator ( ) Return a jsonschema validator for the CSL Item JSON Schema View Source @functools . lru_cache () def get_jsonschema_csl_validator (): \"\"\" Return a jsonschema validator for the CSL Item JSON Schema \"\"\" import jsonschema url = \"https://github.com/dhimmel/csl-schema/raw/manubot/csl-data.json\" schema = read_serialized_data ( url ) Validator = jsonschema . validators . validator_for ( schema ) Validator . check_schema ( schema ) return Validator ( schema )","title":"get_jsonschema_csl_validator"},{"location":"reference/manubot/cite/citeproc/#remove_jsonschema_errors","text":"def remove_jsonschema_errors ( instance , recurse_depth = 5 , in_place = False ) Remove fields in CSL Items that produce JSON Schema errors. Should errors be removed, but the JSON instance still fails to validate, recursively call remove_jsonschema_errors until the instance validates or the recursion depth limit is reached. Note that this method may not be work for all types of JSON Schema errors and users looking to adapt it for other applications should write task-specific tests to provide empirical evaluate that it works as intended. The default in_place=False creates a deepcopy of instance before pruning it, such that a new dictionary is returned and instance is not edited. Set in_place=True to edit instance in-place. The inital implementation of remove_jsonschema_errors always deepcopied instance, and it is possible deepcopying is important to prevent malfunction when encountering certain edge cases. Please report if you observe any in_place dependent behaviors. See also: https://github.com/Julian/jsonschema/issues/448 https://stackoverflow.com/questions/44694835 View Source def remove_jsonschema_errors ( instance , recurse_depth = 5 , in_place = False ) : \"\"\" Remove fields in CSL Items that produce JSON Schema errors . Should errors be removed , but the JSON instance still fails to validate , recursively call remove_jsonschema_errors until the instance validates or the recursion depth limit is reached . Note that this method may not be work for all types of JSON Schema errors and users looking to adapt it for other applications should write task - specific tests to provide empirical evaluate that it works as intended . The default in_place = False creates a deepcopy of instance before pruning it , such that a new dictionary is returned and instance is not edited . Set in_place = True to edit instance in - place . The inital implementation of remove_jsonschema_errors always deepcopied instance , and it is possible deepcopying is important to prevent malfunction when encountering certain edge cases . Please report if you observe any in_place dependent behaviors . See also : https : // github . com / Julian / jsonschema / issues / 448 https : // stackoverflow . com / questions / 44694835 \"\"\" validator = get_jsonschema_csl_validator () errors = list ( validator . iter_errors ( instance )) if not in_place : instance = copy . deepcopy ( instance ) errors = sorted ( errors , key = lambda e : e . path , reverse = True ) for error in errors : _remove_error ( instance , error ) if validator . is_valid ( instance ) or recurse_depth < 1 : return instance return remove_jsonschema_errors ( instance , recurse_depth - 1 , in_place = in_place )","title":"remove_jsonschema_errors"},{"location":"reference/manubot/cite/csl_item/","text":"Module manubot.cite.csl_item Represent bibliographic information for a single publication. From the CSL docs: Next up are the bibliographic details of the items you wish to cite : the item metadata . For example , the bibliographic entry for a journal article may show the names of the authors , the year in which the article was published , the article title , the journal title , the volume and issue in which the article appeared , the page numbers of the article , and the article \u2019 s Digital Object Identifier ( DOI ) . All these details help the reader identify and find the referenced work . Reference managers make it easy to create a library of items . While many reference managers have their own way of storing item metadata , most support common bibliographic exchange formats such as BibTeX and RIS . The citeproc - js CSL processor introduced a JSON - based format for storing item metadata in a way citeproc - js could understand . Several other CSL processors have since adopted this \u201c CSL JSON \u201d format ( also known as \u201c citeproc JSON \u201d ) . -- https://github.com/citation-style-language/documentation/blob/master/primer.txt The terminology we've adopted is csl_data for a list of csl_item dicts, and csl_json for csl_data that is JSON-serialized. View Source \"\"\"Represent bibliographic information for a single publication. From the CSL docs: Next up are the bibliographic details of the items you wish to cite: the item metadata. For example, the bibliographic entry for a journal article may show the names of the authors, the year in which the article was published, the article title, the journal title, the volume and issue in which the article appeared, the page numbers of the article, and the article\u2019s Digital Object Identifier (DOI). All these details help the reader identify and find the referenced work. Reference managers make it easy to create a library of items. While many reference managers have their own way of storing item metadata, most support common bibliographic exchange formats such as BibTeX and RIS. The citeproc-js CSL processor introduced a JSON-based format for storing item metadata in a way citeproc-js could understand. Several other CSL processors have since adopted this \u201cCSL JSON\u201d format (also known as \u201cciteproc JSON\u201d). -- https://github.com/citation-style-language/documentation/blob/master/primer.txt The terminology we've adopted is csl_data for a list of csl_item dicts, and csl_json for csl_data that is JSON-serialized. \"\"\" import copy import datetime import logging import re from typing import Dict , List , Optional , Union from manubot . cite . citekey import CiteKey class CSL_Item ( dict ) : \"\"\" CSL_Item represents bibliographic information for a single citeable work. On a technical side CSL_Item is a Python dictionary with extra methods that help cleaning and manipulating it. These methods relate to: - adding an `id` key and value for CSL item - correcting bibliographic information and its structure - adding and reading a custom note to CSL item More information on CSL JSON (a list of CSL_Items) is available at: - https://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html - http://docs.citationstyles.org/en/1.0.1/specification.html#standard-variables - https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" # The ideas for CSL_Item methods come from the following parts of code : # - [ ] citekey_to_csl_item ( citekey , prune = True ) # The methods in CSL_Item class provide primitives to reconstruct # functions above . type_mapping = { \"journal-article\" : \"article-journal\" , \"book-chapter\" : \"chapter\" , \"posted-content\" : \"manuscript\" , \"proceedings-article\" : \"paper-conference\" , \"standard\" : \"entry\" , \"reference-entry\" : \"entry\" , } def __init__ ( self , dictionary = None , ** kwargs ) -> None : \"\"\" Can use both a dictionary or keywords to create a CSL_Item object: CSL_Item(title='The Book') CSL_Item({'title': 'The Book'}) csl_dict = {'title': 'The Book', 'ISBN': '321-321'} CSL_Item(csl_dict, type='entry') CSL_Item(title='The Book', ISBN='321-321', type='entry') CSL_Item object is usually provided by bibliographic information API, but constructing small CSL_Item objects is useful for testing. \"\"\" if dictionary is None : dictionary = dict () super (). __init__ ( copy . deepcopy ( dictionary )) self . update ( copy . deepcopy ( kwargs )) def set_id ( self , id_ ) -> \"CSL_Item\" : self [ \"id\" ] = id_ return self def correct_invalid_type ( self ) -> \"CSL_Item\" : \"\"\" Correct invalid CSL item type. Does nothing if `type` not present. For detail see https://github.com/CrossRef/rest-api-doc/issues/187 \"\"\" if \"type\" in self : # Replace a type from in CSL_Item . type_mapping . keys (), # leave type intact in other cases . t = self [ \"type\" ] self [ \"type\" ] = self . type_mapping . get ( t , t ) return self def set_default_type ( self ) -> \"CSL_Item\" : \"\"\" Set type to 'entry', if type not specified. \"\"\" self [ \"type\" ] = self . get ( \"type\" , \"entry\" ) return self def prune_against_schema ( self ) -> \"CSL_Item\" : \"\"\" Remove fields that violate the CSL Item JSON Schema. \"\"\" from . citeproc import remove_jsonschema_errors ( csl_item ,) = remove_jsonschema_errors ( [ self ] , in_place = True ) assert csl_item is self return self def validate_against_schema ( self ) -> \"CSL_Item\" : \"\"\" Confirm that the CSL_Item validates. If not, raises a jsonschema.exceptions.ValidationError. \"\"\" from . citeproc import get_jsonschema_csl_validator validator = get_jsonschema_csl_validator () validator . validate ( [ self ] ) return self def clean ( self , prune : bool = True ) -> \"CSL_Item\" : \"\"\" Sanitize and touch-up a potentially dirty CSL_Item. The following steps are performed: - update incorrect values for \" type \" field when a correct variant is known - remove fields that violate the JSON Schema (if prune=True) - set default value for \" type \" if missing, since CSL JSON requires type - validate against the CSL JSON schema (if prune=True) to ensure output CSL_Item is clean \"\"\" logging . debug ( f \"Starting CSL_Item.clean with{'' if prune else 'out'}\" f \"CSL pruning for id: {self.get('id', 'id not specified')}\" ) self . correct_invalid_type () if prune : self . prune_against_schema () self . set_default_type () if prune : self . validate_against_schema () return self def set_date ( self , date : Union [ None, str, datetime.date, datetime.datetime ] , variable : str = \"issued\" , ) -> \"CSL_Item\" : \"\"\" date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). variable: which variable to assign the date to. \"\"\" date_parts = date_to_date_parts ( date ) if date_parts : self [ variable ] = { \"date-parts\" : [ date_parts ] } return self def get_date ( self , variable : str = \"issued\" , fill : bool = False ) -> Optional [ str ] : \"\"\" Return a CSL date-variable as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). variable: which CSL JSON date variable to retrieve fill: if True, set missing months to January and missing days to the first day of the month. \"\"\" try : date_parts = self [ variable ][ \"date-parts\" ][ 0 ] except ( IndexError , KeyError ) : return None return date_parts_to_string ( date_parts , fill = fill ) @property def note ( self ) -> str : \"\"\" Return the value of the \" note \" field as a string. If \" note \" key is not set, return empty string. \"\"\" return str ( self . get ( \"note\" ) or \"\" ) @note . setter def note ( self , text : str ) -> None : if text : self [ \"note\" ] = text else : # if text is None or an empty string , remove the \"note\" field self . pop ( \"note\" , None ) @property def note_dict ( self ) -> Dict [ str, str ] : \"\"\" Return a dictionary with key-value pairs encoded by this CSL Item's note. Extracts both forms (line-entry and braced-entry) of key-value pairs from the CSL JSON \" cheater syntax \" https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields Assigning to this dict will not update `self[\" note \"]`. \"\"\" note = self . note line_matches = re . findall ( r \"^(?P<key>[A-Z]+|[-_a-z]+): *(?P<value>.+?) *$\" , note , re . MULTILINE ) braced_matches = re . findall ( r \"{:(?P<key>[A-Z]+|[-_a-z]+): *(?P<value>.+?) *}\" , note ) return dict ( line_matches + braced_matches ) def note_append_text ( self , text : str ) -> None : \"\"\" Append text to the note field (as a new line) of a CSL Item. If a line already exists equal to `text`, do nothing. \"\"\" if not text : return note = self . note if re . search ( f \"^{re.escape(text)}$\" , note , flags = re . MULTILINE ) : # do not accumulate duplicate lines of text # https : // github . com / manubot / manubot / issues / 258 return if note and not note . endswith ( \"\\n\" ) : note += \"\\n\" note += text self . note = note def note_append_dict ( self , dictionary : dict ) -> None : \"\"\" Append key-value pairs specified by `dictionary` to the note field of a CSL Item. Uses the the [CSL JSON \" cheater syntax \"](https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields) to encode additional values not defined by the CSL JSON schema. \"\"\" for key , value in dictionary . items () : if not re . fullmatch ( r \"[A-Z]+|[-_a-z]+\" , key ) : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"it does not conform to the variable_name syntax as per https://git.io/fjTzW.\" ) continue if \"\\n\" in value : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"the value contains a newline: {value!r}\" ) continue self . note_append_text ( f \"{key}: {value}\" ) def infer_id ( self ) -> \"CSL_Item\" : \"\"\" Detect and set a non-null/empty value for \" id \" or else raise a ValueError. \"\"\" if self . get ( \"standard_citation\" ) : # \"standard_citation\" field is set with a non - null / empty value return self . set_id ( self . pop ( \"standard_citation\" )) if self . note_dict . get ( \"standard_id\" ) : # \"standard_id\" note field is set with a non - null / empty value return self . set_id ( self . note_dict [ \"standard_id\" ] ) if self . get ( \"id\" ) : # \"id\" field exists and is set with a non - null / empty value return self . set_id ( self [ \"id\" ] ) raise ValueError ( \"infer_id could not detect a field with a citation / standard_citation. \" 'Consider setting the CSL Item \"id\" field.' ) def standardize_id ( self ) -> \"CSL_Item\" : \"\"\" Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \" id \" field. The standard_id is extracted from a \" standard_citation \" field, the \" note \" field, or the \" id \" field. The extracted citation is checked for validity and standardized, after which it is the final \" standard_id \". Regarding csl_item modification, the csl_item \" id \" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id and original_id. Note that the Manubot software generally refers to the \" id \" of a CSL Item as a citekey. However, in this context, we use \" id \" rather than \" citekey \" for consistency with CSL's \" id \" field. \"\"\" original_id = self . get ( \"id\" ) self . infer_id () original_standard_id = self [ \"id\" ] citekey = CiteKey ( original_standard_id ) standard_id = citekey . standard_id add_to_note = {} note_dict = self . note_dict if original_id and original_id != standard_id : if original_id != note_dict . get ( \"original_id\" ) : add_to_note [ \"original_id\" ] = original_id if original_standard_id and original_standard_id != standard_id : if original_standard_id != note_dict . get ( \"original_standard_id\" ) : add_to_note [ \"original_standard_id\" ] = original_standard_id if standard_id != note_dict . get ( \"standard_id\" ) : add_to_note [ \"standard_id\" ] = standard_id self . note_append_dict ( dictionary = add_to_note ) self . set_id ( standard_id ) return self def assert_csl_item_type ( x ) -> None : if not isinstance ( x , CSL_Item ) : raise TypeError ( f \"Expected CSL_Item object, got {type(x)}\" ) def date_to_date_parts ( date : Union [ None, str, datetime.date, datetime.datetime ] ) -> Optional [ List[int ] ]: \"\"\" Convert a date string or object to a date parts list. date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). \"\"\" if date is None : return None if isinstance ( date , ( datetime . date , datetime . datetime )) : date = date . isoformat () if not isinstance ( date , str ) : raise ValueError ( f \"date_to_date_parts: unsupported type for {date}\" ) date = date . strip () re_year = r \"(?P<year>[0-9]{4})\" re_month = r \"(?P<month>1[0-2]|0[1-9])\" re_day = r \"(?P<day>[0-3][0-9])\" patterns = [ f\"{re_year}-{re_month}-{re_day}\", f\"{re_year}-{re_month}\", f\"{re_year}\", f\".*\", # regex to match anything ] for pattern in patterns : match = re . match ( pattern , date ) if match : break date_parts = [] for part in \"year\" , \"month\" , \"day\" : try : value = match . group ( part ) except IndexError : break if not value : break date_parts . append ( int ( value )) if date_parts : return date_parts return None def date_parts_to_string ( date_parts , fill : bool = False ) -> Optional [ str ] : \"\"\" Return a CSL date-parts list as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). date_parts: list or tuple like [year, month, day] as integers. Also supports [year, month] and [year] for situations where the day or month-and-day are missing. fill: if True, set missing months to January and missing days to the first day of the month. \"\"\" if not date_parts : return None if not isinstance ( date_parts , ( tuple , list )) : raise ValueError ( f \"date_parts must be a tuple or list\" ) while fill and 1 <= len ( date_parts ) < 3 : date_parts . append ( 1 ) widths = 4 , 2 , 2 str_parts = [] for i , part in enumerate ( date_parts [ :3 ] ) : width = widths [ i ] if isinstance ( part , int ) : part = str ( part ) if not isinstance ( part , str ) : break part = part . zfill ( width ) if len ( part ) != width or not part . isdigit () : break str_parts . append ( part ) if not str_parts : return None iso_str = \"-\" . join ( str_parts ) return iso_str Functions assert_csl_item_type def assert_csl_item_type ( x ) -> None View Source def assert_csl_item_type ( x ) -> None : if not isinstance ( x , CSL_Item ) : raise TypeError ( f \"Expected CSL_Item object, got {type(x)}\" ) date_parts_to_string def date_parts_to_string ( date_parts , fill : bool = False ) -> Optional [ str ] Return a CSL date-parts list as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). date_parts: list or tuple like [year, month, day] as integers. Also supports [year, month] and [year] for situations where the day or month-and-day are missing. fill: if True, set missing months to January and missing days to the first day of the month. View Source def date_parts_to_string ( date_parts , fill : bool = False ) -> Optional [ str ] : \"\"\" Return a CSL date-parts list as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). date_parts: list or tuple like [year, month, day] as integers. Also supports [year, month] and [year] for situations where the day or month-and-day are missing. fill: if True, set missing months to January and missing days to the first day of the month. \"\"\" if not date_parts : return None if not isinstance ( date_parts , ( tuple , list )) : raise ValueError ( f \"date_parts must be a tuple or list\" ) while fill and 1 <= len ( date_parts ) < 3 : date_parts . append ( 1 ) widths = 4 , 2 , 2 str_parts = [] for i , part in enumerate ( date_parts [ :3 ] ) : width = widths [ i ] if isinstance ( part , int ) : part = str ( part ) if not isinstance ( part , str ) : break part = part . zfill ( width ) if len ( part ) != width or not part . isdigit () : break str_parts . append ( part ) if not str_parts : return None iso_str = \"-\" . join ( str_parts ) return iso_str date_to_date_parts def date_to_date_parts ( date : Union [ NoneType , str , datetime . date , datetime . datetime ] ) -> Optional [ List [ int ]] Convert a date string or object to a date parts list. date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). View Source def date_to_date_parts ( date : Union [ None, str, datetime.date, datetime.datetime ] ) -> Optional [ List[int ] ]: \"\"\" Convert a date string or object to a date parts list. date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). \"\"\" if date is None : return None if isinstance ( date , ( datetime . date , datetime . datetime )) : date = date . isoformat () if not isinstance ( date , str ) : raise ValueError ( f \"date_to_date_parts: unsupported type for {date}\" ) date = date . strip () re_year = r \"(?P<year>[0-9]{4})\" re_month = r \"(?P<month>1[0-2]|0[1-9])\" re_day = r \"(?P<day>[0-3][0-9])\" patterns = [ f\"{re_year}-{re_month}-{re_day}\", f\"{re_year}-{re_month}\", f\"{re_year}\", f\".*\", # regex to match anything ] for pattern in patterns : match = re . match ( pattern , date ) if match : break date_parts = [] for part in \"year\" , \"month\" , \"day\" : try : value = match . group ( part ) except IndexError : break if not value : break date_parts . append ( int ( value )) if date_parts : return date_parts return None Classes CSL_Item class CSL_Item ( dictionary = None , ** kwargs ) View Source class CSL_Item ( dict ) : \"\"\" CSL_Item represents bibliographic information for a single citeable work. On a technical side CSL_Item is a Python dictionary with extra methods that help cleaning and manipulating it. These methods relate to: - adding an `id` key and value for CSL item - correcting bibliographic information and its structure - adding and reading a custom note to CSL item More information on CSL JSON (a list of CSL_Items) is available at: - https://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html - http://docs.citationstyles.org/en/1.0.1/specification.html#standard-variables - https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" # The ideas for CSL_Item methods come from the following parts of code : # - [ ] citekey_to_csl_item ( citekey , prune = True ) # The methods in CSL_Item class provide primitives to reconstruct # functions above . type_mapping = { \"journal-article\" : \"article-journal\" , \"book-chapter\" : \"chapter\" , \"posted-content\" : \"manuscript\" , \"proceedings-article\" : \"paper-conference\" , \"standard\" : \"entry\" , \"reference-entry\" : \"entry\" , } def __init__ ( self , dictionary = None , ** kwargs ) -> None : \"\"\" Can use both a dictionary or keywords to create a CSL_Item object: CSL_Item(title='The Book') CSL_Item({'title': 'The Book'}) csl_dict = {'title': 'The Book', 'ISBN': '321-321'} CSL_Item(csl_dict, type='entry') CSL_Item(title='The Book', ISBN='321-321', type='entry') CSL_Item object is usually provided by bibliographic information API, but constructing small CSL_Item objects is useful for testing. \"\"\" if dictionary is None : dictionary = dict () super (). __init__ ( copy . deepcopy ( dictionary )) self . update ( copy . deepcopy ( kwargs )) def set_id ( self , id_ ) -> \"CSL_Item\" : self [ \"id\" ] = id_ return self def correct_invalid_type ( self ) -> \"CSL_Item\" : \"\"\" Correct invalid CSL item type. Does nothing if `type` not present. For detail see https://github.com/CrossRef/rest-api-doc/issues/187 \"\"\" if \"type\" in self : # Replace a type from in CSL_Item . type_mapping . keys (), # leave type intact in other cases . t = self [ \"type\" ] self [ \"type\" ] = self . type_mapping . get ( t , t ) return self def set_default_type ( self ) -> \"CSL_Item\" : \"\"\" Set type to 'entry', if type not specified. \"\"\" self [ \"type\" ] = self . get ( \"type\" , \"entry\" ) return self def prune_against_schema ( self ) -> \"CSL_Item\" : \"\"\" Remove fields that violate the CSL Item JSON Schema. \"\"\" from . citeproc import remove_jsonschema_errors ( csl_item ,) = remove_jsonschema_errors ( [ self ] , in_place = True ) assert csl_item is self return self def validate_against_schema ( self ) -> \"CSL_Item\" : \"\"\" Confirm that the CSL_Item validates. If not, raises a jsonschema.exceptions.ValidationError. \"\"\" from . citeproc import get_jsonschema_csl_validator validator = get_jsonschema_csl_validator () validator . validate ( [ self ] ) return self def clean ( self , prune : bool = True ) -> \"CSL_Item\" : \"\"\" Sanitize and touch-up a potentially dirty CSL_Item. The following steps are performed: - update incorrect values for \" type \" field when a correct variant is known - remove fields that violate the JSON Schema (if prune=True) - set default value for \" type \" if missing, since CSL JSON requires type - validate against the CSL JSON schema (if prune=True) to ensure output CSL_Item is clean \"\"\" logging . debug ( f \"Starting CSL_Item.clean with{'' if prune else 'out'}\" f \"CSL pruning for id: {self.get('id', 'id not specified')}\" ) self . correct_invalid_type () if prune : self . prune_against_schema () self . set_default_type () if prune : self . validate_against_schema () return self def set_date ( self , date : Union [ None, str, datetime.date, datetime.datetime ] , variable : str = \"issued\" , ) -> \"CSL_Item\" : \"\"\" date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). variable: which variable to assign the date to. \"\"\" date_parts = date_to_date_parts ( date ) if date_parts : self [ variable ] = { \"date-parts\" : [ date_parts ] } return self def get_date ( self , variable : str = \"issued\" , fill : bool = False ) -> Optional [ str ] : \"\"\" Return a CSL date-variable as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). variable: which CSL JSON date variable to retrieve fill: if True, set missing months to January and missing days to the first day of the month. \"\"\" try : date_parts = self [ variable ][ \"date-parts\" ][ 0 ] except ( IndexError , KeyError ) : return None return date_parts_to_string ( date_parts , fill = fill ) @property def note ( self ) -> str : \"\"\" Return the value of the \" note \" field as a string. If \" note \" key is not set, return empty string. \"\"\" return str ( self . get ( \"note\" ) or \"\" ) @note . setter def note ( self , text : str ) -> None : if text : self [ \"note\" ] = text else : # if text is None or an empty string , remove the \"note\" field self . pop ( \"note\" , None ) @property def note_dict ( self ) -> Dict [ str, str ] : \"\"\" Return a dictionary with key-value pairs encoded by this CSL Item's note. Extracts both forms (line-entry and braced-entry) of key-value pairs from the CSL JSON \" cheater syntax \" https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields Assigning to this dict will not update `self[\" note \"]`. \"\"\" note = self . note line_matches = re . findall ( r \"^(?P<key>[A-Z]+|[-_a-z]+): *(?P<value>.+?) *$\" , note , re . MULTILINE ) braced_matches = re . findall ( r \"{:(?P<key>[A-Z]+|[-_a-z]+): *(?P<value>.+?) *}\" , note ) return dict ( line_matches + braced_matches ) def note_append_text ( self , text : str ) -> None : \"\"\" Append text to the note field (as a new line) of a CSL Item. If a line already exists equal to `text`, do nothing. \"\"\" if not text : return note = self . note if re . search ( f \"^{re.escape(text)}$\" , note , flags = re . MULTILINE ) : # do not accumulate duplicate lines of text # https : // github . com / manubot / manubot / issues / 258 return if note and not note . endswith ( \"\\n\" ) : note += \"\\n\" note += text self . note = note def note_append_dict ( self , dictionary : dict ) -> None : \"\"\" Append key-value pairs specified by `dictionary` to the note field of a CSL Item. Uses the the [CSL JSON \" cheater syntax \"](https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields) to encode additional values not defined by the CSL JSON schema. \"\"\" for key , value in dictionary . items () : if not re . fullmatch ( r \"[A-Z]+|[-_a-z]+\" , key ) : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"it does not conform to the variable_name syntax as per https://git.io/fjTzW.\" ) continue if \"\\n\" in value : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"the value contains a newline: {value!r}\" ) continue self . note_append_text ( f \"{key}: {value}\" ) def infer_id ( self ) -> \"CSL_Item\" : \"\"\" Detect and set a non-null/empty value for \" id \" or else raise a ValueError. \"\"\" if self . get ( \"standard_citation\" ) : # \"standard_citation\" field is set with a non - null / empty value return self . set_id ( self . pop ( \"standard_citation\" )) if self . note_dict . get ( \"standard_id\" ) : # \"standard_id\" note field is set with a non - null / empty value return self . set_id ( self . note_dict [ \"standard_id\" ] ) if self . get ( \"id\" ) : # \"id\" field exists and is set with a non - null / empty value return self . set_id ( self [ \"id\" ] ) raise ValueError ( \"infer_id could not detect a field with a citation / standard_citation. \" 'Consider setting the CSL Item \"id\" field.' ) def standardize_id ( self ) -> \"CSL_Item\" : \"\"\" Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \" id \" field. The standard_id is extracted from a \" standard_citation \" field, the \" note \" field, or the \" id \" field. The extracted citation is checked for validity and standardized, after which it is the final \" standard_id \". Regarding csl_item modification, the csl_item \" id \" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id and original_id. Note that the Manubot software generally refers to the \" id \" of a CSL Item as a citekey. However, in this context, we use \" id \" rather than \" citekey \" for consistency with CSL's \" id \" field. \"\"\" original_id = self . get ( \"id\" ) self . infer_id () original_standard_id = self [ \"id\" ] citekey = CiteKey ( original_standard_id ) standard_id = citekey . standard_id add_to_note = {} note_dict = self . note_dict if original_id and original_id != standard_id : if original_id != note_dict . get ( \"original_id\" ) : add_to_note [ \"original_id\" ] = original_id if original_standard_id and original_standard_id != standard_id : if original_standard_id != note_dict . get ( \"original_standard_id\" ) : add_to_note [ \"original_standard_id\" ] = original_standard_id if standard_id != note_dict . get ( \"standard_id\" ) : add_to_note [ \"standard_id\" ] = standard_id self . note_append_dict ( dictionary = add_to_note ) self . set_id ( standard_id ) return self Ancestors (in MRO) builtins.dict Descendants manubot.cite.arxiv.CSL_Item_arXiv Class variables type_mapping Instance variables note Return the value of the \"note\" field as a string. If \"note\" key is not set, return empty string. note_dict Return a dictionary with key-value pairs encoded by this CSL Item's note. Extracts both forms (line-entry and braced-entry) of key-value pairs from the CSL JSON \"cheater syntax\" https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields Assigning to this dict will not update self[\"note\"] . Methods clean def clean ( self , prune : bool = True ) -> 'CSL_Item' Sanitize and touch-up a potentially dirty CSL_Item. The following steps are performed: - update incorrect values for \"type\" field when a correct variant is known - remove fields that violate the JSON Schema (if prune=True) - set default value for \"type\" if missing, since CSL JSON requires type - validate against the CSL JSON schema (if prune=True) to ensure output CSL_Item is clean View Source def clean ( self , prune : bool = True ) -> \"CSL_Item\" : \"\"\" Sanitize and touch-up a potentially dirty CSL_Item. The following steps are performed: - update incorrect values for \"type\" field when a correct variant is known - remove fields that violate the JSON Schema (if prune=True) - set default value for \"type\" if missing, since CSL JSON requires type - validate against the CSL JSON schema (if prune=True) to ensure output CSL_Item is clean \"\"\" logging . debug ( f \"Starting CSL_Item.clean with{'' if prune else 'out'}\" f \"CSL pruning for id: {self.get('id', 'id not specified')}\" ) self . correct_invalid_type () if prune : self . prune_against_schema () self . set_default_type () if prune : self . validate_against_schema () return self clear def clear ( ... ) D.clear() -> None. Remove all items from D. copy def copy ( ... ) D.copy() -> a shallow copy of D correct_invalid_type def correct_invalid_type ( self ) -> 'CSL_Item' Correct invalid CSL item type. Does nothing if type not present. For detail see https://github.com/CrossRef/rest-api-doc/issues/187 View Source def correct_invalid_type ( self ) -> \"CSL_Item\" : \" \"\" Correct invalid CSL item type. Does nothing if `type` not present. For detail see https://github.com/CrossRef/rest-api-doc/issues/187 \"\" \" if \"type\" in self : # Replace a type from in CSL_Item.type_mapping.keys(), # leave type intact in other cases. t = self [ \"type\" ] self [ \"type\" ] = self . type_mapping . get ( t , t ) return self fromkeys def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value. get def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default. get_date def get_date ( self , variable : str = 'issued' , fill : bool = False ) -> Optional [ str ] Return a CSL date-variable as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). variable: which CSL JSON date variable to retrieve fill: if True, set missing months to January and missing days to the first day of the month. View Source def get_date ( self , variable : str = \"issued\" , fill : bool = False ) -> Optional [ str ] : \"\"\" Return a CSL date-variable as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). variable: which CSL JSON date variable to retrieve fill: if True, set missing months to January and missing days to the first day of the month. \"\"\" try : date_parts = self [ variable ][ \"date-parts\" ][ 0 ] except ( IndexError , KeyError ) : return None return date_parts_to_string ( date_parts , fill = fill ) infer_id def infer_id ( self ) -> 'CSL_Item' Detect and set a non-null/empty value for \"id\" or else raise a ValueError. View Source def infer_id ( self ) -> \"CSL_Item\" : \"\"\" Detect and set a non-null/empty value for \" id \" or else raise a ValueError. \"\"\" if self . get ( \"standard_citation\" ) : # \"standard_citation\" field is set with a non - null / empty value return self . set_id ( self . pop ( \"standard_citation\" )) if self . note_dict . get ( \"standard_id\" ) : # \"standard_id\" note field is set with a non - null / empty value return self . set_id ( self . note_dict [ \"standard_id\" ]) if self . get ( \"id\" ) : # \"id\" field exists and is set with a non - null / empty value return self . set_id ( self [ \"id\" ]) raise ValueError ( \"infer_id could not detect a field with a citation / standard_citation. \" 'Consider setting the CSL Item \" id \" field.' ) items def items ( ... ) D.items() -> a set-like object providing a view on D's items keys def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys note_append_dict def note_append_dict ( self , dictionary : dict ) -> None Append key-value pairs specified by dictionary to the note field of a CSL Item. Uses the the CSL JSON \"cheater syntax\" to encode additional values not defined by the CSL JSON schema. View Source def note_append_dict ( self , dictionary : dict ) -> None : \" \"\" Append key-value pairs specified by `dictionary` to the note field of a CSL Item. Uses the the [CSL JSON \" cheater syntax \"](https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields) to encode additional values not defined by the CSL JSON schema. \"\" \" for key , value in dictionary . items () : if not re . fullmatch ( r \"[A-Z]+|[-_a-z]+\" , key ) : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"it does not conform to the variable_name syntax as per https://git.io/fjTzW.\" ) continue if \" \\n \" in value : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"the value contains a newline: {value!r}\" ) continue self . note_append_text ( f \"{key}: {value}\" ) note_append_text def note_append_text ( self , text : str ) -> None Append text to the note field (as a new line) of a CSL Item. If a line already exists equal to text , do nothing. View Source def note_append_text ( self , text : str ) -> None : \" \"\" Append text to the note field (as a new line) of a CSL Item. If a line already exists equal to `text`, do nothing. \"\" \" if not text : return note = self . note if re . search ( f \"^{re.escape(text)}$\" , note , flags = re . MULTILINE ) : # do not accumulate duplicate lines of text # https://github.com/manubot/manubot/issues/258 return if note and not note . endswith ( \" \\n \" ) : note += \" \\n \" note += text self . note = note pop def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, default is returned if given, otherwise KeyError is raised popitem def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. prune_against_schema def prune_against_schema ( self ) -> 'CSL_Item' Remove fields that violate the CSL Item JSON Schema. View Source def prune_against_schema ( self ) -> \"CSL_Item\" : \"\"\" Remove fields that violate the CSL Item JSON Schema. \"\"\" from .citeproc import remove_jsonschema_errors ( csl_item ,) = remove_jsonschema_errors ([ self ], in_place = True ) assert csl_item is self return self set_date def set_date ( self , date : Union [ NoneType , str , datetime . date , datetime . datetime ], variable : str = 'issued' ) -> 'CSL_Item' date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). variable: which variable to assign the date to. View Source def set_date ( self , date : Union [ None, str, datetime.date, datetime.datetime ] , variable : str = \"issued\" , ) -> \"CSL_Item\" : \"\"\" date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). variable: which variable to assign the date to. \"\"\" date_parts = date_to_date_parts ( date ) if date_parts : self [ variable ] = { \"date-parts\" : [ date_parts ] } return self set_default_type def set_default_type ( self ) -> 'CSL_Item' Set type to 'entry', if type not specified. View Source def set_default_type ( self ) -> \"CSL_Item\" : \"\"\" Set type to ' entry ', if type not specified. \"\"\" self [ \"type\" ] = self . get ( \"type\" , \"entry\" ) return self set_id def set_id ( self , id_ ) -> 'CSL_Item' View Source def set_id ( self , id_ ) -> \"CSL_Item\" : self [ \"id\" ] = id_ return self setdefault def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. standardize_id def standardize_id ( self ) -> 'CSL_Item' Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \"id\" field. The standard_id is extracted from a \"standard_citation\" field, the \"note\" field, or the \"id\" field. The extracted citation is checked for validity and standardized, after which it is the final \"standard_id\". Regarding csl_item modification, the csl_item \"id\" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id and original_id. Note that the Manubot software generally refers to the \"id\" of a CSL Item as a citekey. However, in this context, we use \"id\" rather than \"citekey\" for consistency with CSL's \"id\" field. View Source def standardize_id ( self ) -> \"CSL_Item\" : \"\"\" Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \" id \" field. The standard_id is extracted from a \" standard_citation \" field, the \" note \" field, or the \" id \" field. The extracted citation is checked for validity and standardized, after which it is the final \" standard_id \". Regarding csl_item modification, the csl_item \" id \" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id and original_id. Note that the Manubot software generally refers to the \" id \" of a CSL Item as a citekey. However, in this context, we use \" id \" rather than \" citekey \" for consistency with CSL' s \"id\" field . \"\"\" original_id = self.get(\" id \") self.infer_id() original_standard_id = self[\" id \"] citekey = CiteKey(original_standard_id) standard_id = citekey.standard_id add_to_note = {} note_dict = self.note_dict if original_id and original_id != standard_id: if original_id != note_dict.get(\" original_id \"): add_to_note[\" original_id \"] = original_id if original_standard_id and original_standard_id != standard_id: if original_standard_id != note_dict.get(\" original_standard_id \"): add_to_note[\" original_standard_id \"] = original_standard_id if standard_id != note_dict.get(\" standard_id \"): add_to_note[\" standard_id \"] = standard_id self.note_append_dict(dictionary=add_to_note) self.set_id(standard_id) return self update def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k] validate_against_schema def validate_against_schema ( self ) -> 'CSL_Item' Confirm that the CSL_Item validates. If not, raises a jsonschema.exceptions.ValidationError. View Source def validate_against_schema ( self ) -> \"CSL_Item\" : \"\"\" Confirm that the CSL_Item validates. If not, raises a jsonschema.exceptions.ValidationError. \"\"\" from .citeproc import get_jsonschema_csl_validator validator = get_jsonschema_csl_validator () validator . validate ([ self ]) return self values def values ( ... ) D.values() -> an object providing a view on D's values","title":"Csl Item"},{"location":"reference/manubot/cite/csl_item/#module-manubotcitecsl_item","text":"Represent bibliographic information for a single publication. From the CSL docs: Next up are the bibliographic details of the items you wish to cite : the item metadata . For example , the bibliographic entry for a journal article may show the names of the authors , the year in which the article was published , the article title , the journal title , the volume and issue in which the article appeared , the page numbers of the article , and the article \u2019 s Digital Object Identifier ( DOI ) . All these details help the reader identify and find the referenced work . Reference managers make it easy to create a library of items . While many reference managers have their own way of storing item metadata , most support common bibliographic exchange formats such as BibTeX and RIS . The citeproc - js CSL processor introduced a JSON - based format for storing item metadata in a way citeproc - js could understand . Several other CSL processors have since adopted this \u201c CSL JSON \u201d format ( also known as \u201c citeproc JSON \u201d ) . -- https://github.com/citation-style-language/documentation/blob/master/primer.txt The terminology we've adopted is csl_data for a list of csl_item dicts, and csl_json for csl_data that is JSON-serialized. View Source \"\"\"Represent bibliographic information for a single publication. From the CSL docs: Next up are the bibliographic details of the items you wish to cite: the item metadata. For example, the bibliographic entry for a journal article may show the names of the authors, the year in which the article was published, the article title, the journal title, the volume and issue in which the article appeared, the page numbers of the article, and the article\u2019s Digital Object Identifier (DOI). All these details help the reader identify and find the referenced work. Reference managers make it easy to create a library of items. While many reference managers have their own way of storing item metadata, most support common bibliographic exchange formats such as BibTeX and RIS. The citeproc-js CSL processor introduced a JSON-based format for storing item metadata in a way citeproc-js could understand. Several other CSL processors have since adopted this \u201cCSL JSON\u201d format (also known as \u201cciteproc JSON\u201d). -- https://github.com/citation-style-language/documentation/blob/master/primer.txt The terminology we've adopted is csl_data for a list of csl_item dicts, and csl_json for csl_data that is JSON-serialized. \"\"\" import copy import datetime import logging import re from typing import Dict , List , Optional , Union from manubot . cite . citekey import CiteKey class CSL_Item ( dict ) : \"\"\" CSL_Item represents bibliographic information for a single citeable work. On a technical side CSL_Item is a Python dictionary with extra methods that help cleaning and manipulating it. These methods relate to: - adding an `id` key and value for CSL item - correcting bibliographic information and its structure - adding and reading a custom note to CSL item More information on CSL JSON (a list of CSL_Items) is available at: - https://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html - http://docs.citationstyles.org/en/1.0.1/specification.html#standard-variables - https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" # The ideas for CSL_Item methods come from the following parts of code : # - [ ] citekey_to_csl_item ( citekey , prune = True ) # The methods in CSL_Item class provide primitives to reconstruct # functions above . type_mapping = { \"journal-article\" : \"article-journal\" , \"book-chapter\" : \"chapter\" , \"posted-content\" : \"manuscript\" , \"proceedings-article\" : \"paper-conference\" , \"standard\" : \"entry\" , \"reference-entry\" : \"entry\" , } def __init__ ( self , dictionary = None , ** kwargs ) -> None : \"\"\" Can use both a dictionary or keywords to create a CSL_Item object: CSL_Item(title='The Book') CSL_Item({'title': 'The Book'}) csl_dict = {'title': 'The Book', 'ISBN': '321-321'} CSL_Item(csl_dict, type='entry') CSL_Item(title='The Book', ISBN='321-321', type='entry') CSL_Item object is usually provided by bibliographic information API, but constructing small CSL_Item objects is useful for testing. \"\"\" if dictionary is None : dictionary = dict () super (). __init__ ( copy . deepcopy ( dictionary )) self . update ( copy . deepcopy ( kwargs )) def set_id ( self , id_ ) -> \"CSL_Item\" : self [ \"id\" ] = id_ return self def correct_invalid_type ( self ) -> \"CSL_Item\" : \"\"\" Correct invalid CSL item type. Does nothing if `type` not present. For detail see https://github.com/CrossRef/rest-api-doc/issues/187 \"\"\" if \"type\" in self : # Replace a type from in CSL_Item . type_mapping . keys (), # leave type intact in other cases . t = self [ \"type\" ] self [ \"type\" ] = self . type_mapping . get ( t , t ) return self def set_default_type ( self ) -> \"CSL_Item\" : \"\"\" Set type to 'entry', if type not specified. \"\"\" self [ \"type\" ] = self . get ( \"type\" , \"entry\" ) return self def prune_against_schema ( self ) -> \"CSL_Item\" : \"\"\" Remove fields that violate the CSL Item JSON Schema. \"\"\" from . citeproc import remove_jsonschema_errors ( csl_item ,) = remove_jsonschema_errors ( [ self ] , in_place = True ) assert csl_item is self return self def validate_against_schema ( self ) -> \"CSL_Item\" : \"\"\" Confirm that the CSL_Item validates. If not, raises a jsonschema.exceptions.ValidationError. \"\"\" from . citeproc import get_jsonschema_csl_validator validator = get_jsonschema_csl_validator () validator . validate ( [ self ] ) return self def clean ( self , prune : bool = True ) -> \"CSL_Item\" : \"\"\" Sanitize and touch-up a potentially dirty CSL_Item. The following steps are performed: - update incorrect values for \" type \" field when a correct variant is known - remove fields that violate the JSON Schema (if prune=True) - set default value for \" type \" if missing, since CSL JSON requires type - validate against the CSL JSON schema (if prune=True) to ensure output CSL_Item is clean \"\"\" logging . debug ( f \"Starting CSL_Item.clean with{'' if prune else 'out'}\" f \"CSL pruning for id: {self.get('id', 'id not specified')}\" ) self . correct_invalid_type () if prune : self . prune_against_schema () self . set_default_type () if prune : self . validate_against_schema () return self def set_date ( self , date : Union [ None, str, datetime.date, datetime.datetime ] , variable : str = \"issued\" , ) -> \"CSL_Item\" : \"\"\" date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). variable: which variable to assign the date to. \"\"\" date_parts = date_to_date_parts ( date ) if date_parts : self [ variable ] = { \"date-parts\" : [ date_parts ] } return self def get_date ( self , variable : str = \"issued\" , fill : bool = False ) -> Optional [ str ] : \"\"\" Return a CSL date-variable as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). variable: which CSL JSON date variable to retrieve fill: if True, set missing months to January and missing days to the first day of the month. \"\"\" try : date_parts = self [ variable ][ \"date-parts\" ][ 0 ] except ( IndexError , KeyError ) : return None return date_parts_to_string ( date_parts , fill = fill ) @property def note ( self ) -> str : \"\"\" Return the value of the \" note \" field as a string. If \" note \" key is not set, return empty string. \"\"\" return str ( self . get ( \"note\" ) or \"\" ) @note . setter def note ( self , text : str ) -> None : if text : self [ \"note\" ] = text else : # if text is None or an empty string , remove the \"note\" field self . pop ( \"note\" , None ) @property def note_dict ( self ) -> Dict [ str, str ] : \"\"\" Return a dictionary with key-value pairs encoded by this CSL Item's note. Extracts both forms (line-entry and braced-entry) of key-value pairs from the CSL JSON \" cheater syntax \" https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields Assigning to this dict will not update `self[\" note \"]`. \"\"\" note = self . note line_matches = re . findall ( r \"^(?P<key>[A-Z]+|[-_a-z]+): *(?P<value>.+?) *$\" , note , re . MULTILINE ) braced_matches = re . findall ( r \"{:(?P<key>[A-Z]+|[-_a-z]+): *(?P<value>.+?) *}\" , note ) return dict ( line_matches + braced_matches ) def note_append_text ( self , text : str ) -> None : \"\"\" Append text to the note field (as a new line) of a CSL Item. If a line already exists equal to `text`, do nothing. \"\"\" if not text : return note = self . note if re . search ( f \"^{re.escape(text)}$\" , note , flags = re . MULTILINE ) : # do not accumulate duplicate lines of text # https : // github . com / manubot / manubot / issues / 258 return if note and not note . endswith ( \"\\n\" ) : note += \"\\n\" note += text self . note = note def note_append_dict ( self , dictionary : dict ) -> None : \"\"\" Append key-value pairs specified by `dictionary` to the note field of a CSL Item. Uses the the [CSL JSON \" cheater syntax \"](https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields) to encode additional values not defined by the CSL JSON schema. \"\"\" for key , value in dictionary . items () : if not re . fullmatch ( r \"[A-Z]+|[-_a-z]+\" , key ) : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"it does not conform to the variable_name syntax as per https://git.io/fjTzW.\" ) continue if \"\\n\" in value : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"the value contains a newline: {value!r}\" ) continue self . note_append_text ( f \"{key}: {value}\" ) def infer_id ( self ) -> \"CSL_Item\" : \"\"\" Detect and set a non-null/empty value for \" id \" or else raise a ValueError. \"\"\" if self . get ( \"standard_citation\" ) : # \"standard_citation\" field is set with a non - null / empty value return self . set_id ( self . pop ( \"standard_citation\" )) if self . note_dict . get ( \"standard_id\" ) : # \"standard_id\" note field is set with a non - null / empty value return self . set_id ( self . note_dict [ \"standard_id\" ] ) if self . get ( \"id\" ) : # \"id\" field exists and is set with a non - null / empty value return self . set_id ( self [ \"id\" ] ) raise ValueError ( \"infer_id could not detect a field with a citation / standard_citation. \" 'Consider setting the CSL Item \"id\" field.' ) def standardize_id ( self ) -> \"CSL_Item\" : \"\"\" Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \" id \" field. The standard_id is extracted from a \" standard_citation \" field, the \" note \" field, or the \" id \" field. The extracted citation is checked for validity and standardized, after which it is the final \" standard_id \". Regarding csl_item modification, the csl_item \" id \" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id and original_id. Note that the Manubot software generally refers to the \" id \" of a CSL Item as a citekey. However, in this context, we use \" id \" rather than \" citekey \" for consistency with CSL's \" id \" field. \"\"\" original_id = self . get ( \"id\" ) self . infer_id () original_standard_id = self [ \"id\" ] citekey = CiteKey ( original_standard_id ) standard_id = citekey . standard_id add_to_note = {} note_dict = self . note_dict if original_id and original_id != standard_id : if original_id != note_dict . get ( \"original_id\" ) : add_to_note [ \"original_id\" ] = original_id if original_standard_id and original_standard_id != standard_id : if original_standard_id != note_dict . get ( \"original_standard_id\" ) : add_to_note [ \"original_standard_id\" ] = original_standard_id if standard_id != note_dict . get ( \"standard_id\" ) : add_to_note [ \"standard_id\" ] = standard_id self . note_append_dict ( dictionary = add_to_note ) self . set_id ( standard_id ) return self def assert_csl_item_type ( x ) -> None : if not isinstance ( x , CSL_Item ) : raise TypeError ( f \"Expected CSL_Item object, got {type(x)}\" ) def date_to_date_parts ( date : Union [ None, str, datetime.date, datetime.datetime ] ) -> Optional [ List[int ] ]: \"\"\" Convert a date string or object to a date parts list. date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). \"\"\" if date is None : return None if isinstance ( date , ( datetime . date , datetime . datetime )) : date = date . isoformat () if not isinstance ( date , str ) : raise ValueError ( f \"date_to_date_parts: unsupported type for {date}\" ) date = date . strip () re_year = r \"(?P<year>[0-9]{4})\" re_month = r \"(?P<month>1[0-2]|0[1-9])\" re_day = r \"(?P<day>[0-3][0-9])\" patterns = [ f\"{re_year}-{re_month}-{re_day}\", f\"{re_year}-{re_month}\", f\"{re_year}\", f\".*\", # regex to match anything ] for pattern in patterns : match = re . match ( pattern , date ) if match : break date_parts = [] for part in \"year\" , \"month\" , \"day\" : try : value = match . group ( part ) except IndexError : break if not value : break date_parts . append ( int ( value )) if date_parts : return date_parts return None def date_parts_to_string ( date_parts , fill : bool = False ) -> Optional [ str ] : \"\"\" Return a CSL date-parts list as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). date_parts: list or tuple like [year, month, day] as integers. Also supports [year, month] and [year] for situations where the day or month-and-day are missing. fill: if True, set missing months to January and missing days to the first day of the month. \"\"\" if not date_parts : return None if not isinstance ( date_parts , ( tuple , list )) : raise ValueError ( f \"date_parts must be a tuple or list\" ) while fill and 1 <= len ( date_parts ) < 3 : date_parts . append ( 1 ) widths = 4 , 2 , 2 str_parts = [] for i , part in enumerate ( date_parts [ :3 ] ) : width = widths [ i ] if isinstance ( part , int ) : part = str ( part ) if not isinstance ( part , str ) : break part = part . zfill ( width ) if len ( part ) != width or not part . isdigit () : break str_parts . append ( part ) if not str_parts : return None iso_str = \"-\" . join ( str_parts ) return iso_str","title":"Module manubot.cite.csl_item"},{"location":"reference/manubot/cite/csl_item/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/csl_item/#assert_csl_item_type","text":"def assert_csl_item_type ( x ) -> None View Source def assert_csl_item_type ( x ) -> None : if not isinstance ( x , CSL_Item ) : raise TypeError ( f \"Expected CSL_Item object, got {type(x)}\" )","title":"assert_csl_item_type"},{"location":"reference/manubot/cite/csl_item/#date_parts_to_string","text":"def date_parts_to_string ( date_parts , fill : bool = False ) -> Optional [ str ] Return a CSL date-parts list as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). date_parts: list or tuple like [year, month, day] as integers. Also supports [year, month] and [year] for situations where the day or month-and-day are missing. fill: if True, set missing months to January and missing days to the first day of the month. View Source def date_parts_to_string ( date_parts , fill : bool = False ) -> Optional [ str ] : \"\"\" Return a CSL date-parts list as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). date_parts: list or tuple like [year, month, day] as integers. Also supports [year, month] and [year] for situations where the day or month-and-day are missing. fill: if True, set missing months to January and missing days to the first day of the month. \"\"\" if not date_parts : return None if not isinstance ( date_parts , ( tuple , list )) : raise ValueError ( f \"date_parts must be a tuple or list\" ) while fill and 1 <= len ( date_parts ) < 3 : date_parts . append ( 1 ) widths = 4 , 2 , 2 str_parts = [] for i , part in enumerate ( date_parts [ :3 ] ) : width = widths [ i ] if isinstance ( part , int ) : part = str ( part ) if not isinstance ( part , str ) : break part = part . zfill ( width ) if len ( part ) != width or not part . isdigit () : break str_parts . append ( part ) if not str_parts : return None iso_str = \"-\" . join ( str_parts ) return iso_str","title":"date_parts_to_string"},{"location":"reference/manubot/cite/csl_item/#date_to_date_parts","text":"def date_to_date_parts ( date : Union [ NoneType , str , datetime . date , datetime . datetime ] ) -> Optional [ List [ int ]] Convert a date string or object to a date parts list. date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). View Source def date_to_date_parts ( date : Union [ None, str, datetime.date, datetime.datetime ] ) -> Optional [ List[int ] ]: \"\"\" Convert a date string or object to a date parts list. date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). \"\"\" if date is None : return None if isinstance ( date , ( datetime . date , datetime . datetime )) : date = date . isoformat () if not isinstance ( date , str ) : raise ValueError ( f \"date_to_date_parts: unsupported type for {date}\" ) date = date . strip () re_year = r \"(?P<year>[0-9]{4})\" re_month = r \"(?P<month>1[0-2]|0[1-9])\" re_day = r \"(?P<day>[0-3][0-9])\" patterns = [ f\"{re_year}-{re_month}-{re_day}\", f\"{re_year}-{re_month}\", f\"{re_year}\", f\".*\", # regex to match anything ] for pattern in patterns : match = re . match ( pattern , date ) if match : break date_parts = [] for part in \"year\" , \"month\" , \"day\" : try : value = match . group ( part ) except IndexError : break if not value : break date_parts . append ( int ( value )) if date_parts : return date_parts return None","title":"date_to_date_parts"},{"location":"reference/manubot/cite/csl_item/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/csl_item/#csl_item","text":"class CSL_Item ( dictionary = None , ** kwargs ) View Source class CSL_Item ( dict ) : \"\"\" CSL_Item represents bibliographic information for a single citeable work. On a technical side CSL_Item is a Python dictionary with extra methods that help cleaning and manipulating it. These methods relate to: - adding an `id` key and value for CSL item - correcting bibliographic information and its structure - adding and reading a custom note to CSL item More information on CSL JSON (a list of CSL_Items) is available at: - https://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html - http://docs.citationstyles.org/en/1.0.1/specification.html#standard-variables - https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" # The ideas for CSL_Item methods come from the following parts of code : # - [ ] citekey_to_csl_item ( citekey , prune = True ) # The methods in CSL_Item class provide primitives to reconstruct # functions above . type_mapping = { \"journal-article\" : \"article-journal\" , \"book-chapter\" : \"chapter\" , \"posted-content\" : \"manuscript\" , \"proceedings-article\" : \"paper-conference\" , \"standard\" : \"entry\" , \"reference-entry\" : \"entry\" , } def __init__ ( self , dictionary = None , ** kwargs ) -> None : \"\"\" Can use both a dictionary or keywords to create a CSL_Item object: CSL_Item(title='The Book') CSL_Item({'title': 'The Book'}) csl_dict = {'title': 'The Book', 'ISBN': '321-321'} CSL_Item(csl_dict, type='entry') CSL_Item(title='The Book', ISBN='321-321', type='entry') CSL_Item object is usually provided by bibliographic information API, but constructing small CSL_Item objects is useful for testing. \"\"\" if dictionary is None : dictionary = dict () super (). __init__ ( copy . deepcopy ( dictionary )) self . update ( copy . deepcopy ( kwargs )) def set_id ( self , id_ ) -> \"CSL_Item\" : self [ \"id\" ] = id_ return self def correct_invalid_type ( self ) -> \"CSL_Item\" : \"\"\" Correct invalid CSL item type. Does nothing if `type` not present. For detail see https://github.com/CrossRef/rest-api-doc/issues/187 \"\"\" if \"type\" in self : # Replace a type from in CSL_Item . type_mapping . keys (), # leave type intact in other cases . t = self [ \"type\" ] self [ \"type\" ] = self . type_mapping . get ( t , t ) return self def set_default_type ( self ) -> \"CSL_Item\" : \"\"\" Set type to 'entry', if type not specified. \"\"\" self [ \"type\" ] = self . get ( \"type\" , \"entry\" ) return self def prune_against_schema ( self ) -> \"CSL_Item\" : \"\"\" Remove fields that violate the CSL Item JSON Schema. \"\"\" from . citeproc import remove_jsonschema_errors ( csl_item ,) = remove_jsonschema_errors ( [ self ] , in_place = True ) assert csl_item is self return self def validate_against_schema ( self ) -> \"CSL_Item\" : \"\"\" Confirm that the CSL_Item validates. If not, raises a jsonschema.exceptions.ValidationError. \"\"\" from . citeproc import get_jsonschema_csl_validator validator = get_jsonschema_csl_validator () validator . validate ( [ self ] ) return self def clean ( self , prune : bool = True ) -> \"CSL_Item\" : \"\"\" Sanitize and touch-up a potentially dirty CSL_Item. The following steps are performed: - update incorrect values for \" type \" field when a correct variant is known - remove fields that violate the JSON Schema (if prune=True) - set default value for \" type \" if missing, since CSL JSON requires type - validate against the CSL JSON schema (if prune=True) to ensure output CSL_Item is clean \"\"\" logging . debug ( f \"Starting CSL_Item.clean with{'' if prune else 'out'}\" f \"CSL pruning for id: {self.get('id', 'id not specified')}\" ) self . correct_invalid_type () if prune : self . prune_against_schema () self . set_default_type () if prune : self . validate_against_schema () return self def set_date ( self , date : Union [ None, str, datetime.date, datetime.datetime ] , variable : str = \"issued\" , ) -> \"CSL_Item\" : \"\"\" date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). variable: which variable to assign the date to. \"\"\" date_parts = date_to_date_parts ( date ) if date_parts : self [ variable ] = { \"date-parts\" : [ date_parts ] } return self def get_date ( self , variable : str = \"issued\" , fill : bool = False ) -> Optional [ str ] : \"\"\" Return a CSL date-variable as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). variable: which CSL JSON date variable to retrieve fill: if True, set missing months to January and missing days to the first day of the month. \"\"\" try : date_parts = self [ variable ][ \"date-parts\" ][ 0 ] except ( IndexError , KeyError ) : return None return date_parts_to_string ( date_parts , fill = fill ) @property def note ( self ) -> str : \"\"\" Return the value of the \" note \" field as a string. If \" note \" key is not set, return empty string. \"\"\" return str ( self . get ( \"note\" ) or \"\" ) @note . setter def note ( self , text : str ) -> None : if text : self [ \"note\" ] = text else : # if text is None or an empty string , remove the \"note\" field self . pop ( \"note\" , None ) @property def note_dict ( self ) -> Dict [ str, str ] : \"\"\" Return a dictionary with key-value pairs encoded by this CSL Item's note. Extracts both forms (line-entry and braced-entry) of key-value pairs from the CSL JSON \" cheater syntax \" https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields Assigning to this dict will not update `self[\" note \"]`. \"\"\" note = self . note line_matches = re . findall ( r \"^(?P<key>[A-Z]+|[-_a-z]+): *(?P<value>.+?) *$\" , note , re . MULTILINE ) braced_matches = re . findall ( r \"{:(?P<key>[A-Z]+|[-_a-z]+): *(?P<value>.+?) *}\" , note ) return dict ( line_matches + braced_matches ) def note_append_text ( self , text : str ) -> None : \"\"\" Append text to the note field (as a new line) of a CSL Item. If a line already exists equal to `text`, do nothing. \"\"\" if not text : return note = self . note if re . search ( f \"^{re.escape(text)}$\" , note , flags = re . MULTILINE ) : # do not accumulate duplicate lines of text # https : // github . com / manubot / manubot / issues / 258 return if note and not note . endswith ( \"\\n\" ) : note += \"\\n\" note += text self . note = note def note_append_dict ( self , dictionary : dict ) -> None : \"\"\" Append key-value pairs specified by `dictionary` to the note field of a CSL Item. Uses the the [CSL JSON \" cheater syntax \"](https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields) to encode additional values not defined by the CSL JSON schema. \"\"\" for key , value in dictionary . items () : if not re . fullmatch ( r \"[A-Z]+|[-_a-z]+\" , key ) : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"it does not conform to the variable_name syntax as per https://git.io/fjTzW.\" ) continue if \"\\n\" in value : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"the value contains a newline: {value!r}\" ) continue self . note_append_text ( f \"{key}: {value}\" ) def infer_id ( self ) -> \"CSL_Item\" : \"\"\" Detect and set a non-null/empty value for \" id \" or else raise a ValueError. \"\"\" if self . get ( \"standard_citation\" ) : # \"standard_citation\" field is set with a non - null / empty value return self . set_id ( self . pop ( \"standard_citation\" )) if self . note_dict . get ( \"standard_id\" ) : # \"standard_id\" note field is set with a non - null / empty value return self . set_id ( self . note_dict [ \"standard_id\" ] ) if self . get ( \"id\" ) : # \"id\" field exists and is set with a non - null / empty value return self . set_id ( self [ \"id\" ] ) raise ValueError ( \"infer_id could not detect a field with a citation / standard_citation. \" 'Consider setting the CSL Item \"id\" field.' ) def standardize_id ( self ) -> \"CSL_Item\" : \"\"\" Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \" id \" field. The standard_id is extracted from a \" standard_citation \" field, the \" note \" field, or the \" id \" field. The extracted citation is checked for validity and standardized, after which it is the final \" standard_id \". Regarding csl_item modification, the csl_item \" id \" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id and original_id. Note that the Manubot software generally refers to the \" id \" of a CSL Item as a citekey. However, in this context, we use \" id \" rather than \" citekey \" for consistency with CSL's \" id \" field. \"\"\" original_id = self . get ( \"id\" ) self . infer_id () original_standard_id = self [ \"id\" ] citekey = CiteKey ( original_standard_id ) standard_id = citekey . standard_id add_to_note = {} note_dict = self . note_dict if original_id and original_id != standard_id : if original_id != note_dict . get ( \"original_id\" ) : add_to_note [ \"original_id\" ] = original_id if original_standard_id and original_standard_id != standard_id : if original_standard_id != note_dict . get ( \"original_standard_id\" ) : add_to_note [ \"original_standard_id\" ] = original_standard_id if standard_id != note_dict . get ( \"standard_id\" ) : add_to_note [ \"standard_id\" ] = standard_id self . note_append_dict ( dictionary = add_to_note ) self . set_id ( standard_id ) return self","title":"CSL_Item"},{"location":"reference/manubot/cite/csl_item/#ancestors-in-mro","text":"builtins.dict","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/csl_item/#descendants","text":"manubot.cite.arxiv.CSL_Item_arXiv","title":"Descendants"},{"location":"reference/manubot/cite/csl_item/#class-variables","text":"type_mapping","title":"Class variables"},{"location":"reference/manubot/cite/csl_item/#instance-variables","text":"note Return the value of the \"note\" field as a string. If \"note\" key is not set, return empty string. note_dict Return a dictionary with key-value pairs encoded by this CSL Item's note. Extracts both forms (line-entry and braced-entry) of key-value pairs from the CSL JSON \"cheater syntax\" https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields Assigning to this dict will not update self[\"note\"] .","title":"Instance variables"},{"location":"reference/manubot/cite/csl_item/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/csl_item/#clean","text":"def clean ( self , prune : bool = True ) -> 'CSL_Item' Sanitize and touch-up a potentially dirty CSL_Item. The following steps are performed: - update incorrect values for \"type\" field when a correct variant is known - remove fields that violate the JSON Schema (if prune=True) - set default value for \"type\" if missing, since CSL JSON requires type - validate against the CSL JSON schema (if prune=True) to ensure output CSL_Item is clean View Source def clean ( self , prune : bool = True ) -> \"CSL_Item\" : \"\"\" Sanitize and touch-up a potentially dirty CSL_Item. The following steps are performed: - update incorrect values for \"type\" field when a correct variant is known - remove fields that violate the JSON Schema (if prune=True) - set default value for \"type\" if missing, since CSL JSON requires type - validate against the CSL JSON schema (if prune=True) to ensure output CSL_Item is clean \"\"\" logging . debug ( f \"Starting CSL_Item.clean with{'' if prune else 'out'}\" f \"CSL pruning for id: {self.get('id', 'id not specified')}\" ) self . correct_invalid_type () if prune : self . prune_against_schema () self . set_default_type () if prune : self . validate_against_schema () return self","title":"clean"},{"location":"reference/manubot/cite/csl_item/#clear","text":"def clear ( ... ) D.clear() -> None. Remove all items from D.","title":"clear"},{"location":"reference/manubot/cite/csl_item/#copy","text":"def copy ( ... ) D.copy() -> a shallow copy of D","title":"copy"},{"location":"reference/manubot/cite/csl_item/#correct_invalid_type","text":"def correct_invalid_type ( self ) -> 'CSL_Item' Correct invalid CSL item type. Does nothing if type not present. For detail see https://github.com/CrossRef/rest-api-doc/issues/187 View Source def correct_invalid_type ( self ) -> \"CSL_Item\" : \" \"\" Correct invalid CSL item type. Does nothing if `type` not present. For detail see https://github.com/CrossRef/rest-api-doc/issues/187 \"\" \" if \"type\" in self : # Replace a type from in CSL_Item.type_mapping.keys(), # leave type intact in other cases. t = self [ \"type\" ] self [ \"type\" ] = self . type_mapping . get ( t , t ) return self","title":"correct_invalid_type"},{"location":"reference/manubot/cite/csl_item/#fromkeys","text":"def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value.","title":"fromkeys"},{"location":"reference/manubot/cite/csl_item/#get","text":"def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default.","title":"get"},{"location":"reference/manubot/cite/csl_item/#get_date","text":"def get_date ( self , variable : str = 'issued' , fill : bool = False ) -> Optional [ str ] Return a CSL date-variable as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). variable: which CSL JSON date variable to retrieve fill: if True, set missing months to January and missing days to the first day of the month. View Source def get_date ( self , variable : str = \"issued\" , fill : bool = False ) -> Optional [ str ] : \"\"\" Return a CSL date-variable as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). variable: which CSL JSON date variable to retrieve fill: if True, set missing months to January and missing days to the first day of the month. \"\"\" try : date_parts = self [ variable ][ \"date-parts\" ][ 0 ] except ( IndexError , KeyError ) : return None return date_parts_to_string ( date_parts , fill = fill )","title":"get_date"},{"location":"reference/manubot/cite/csl_item/#infer_id","text":"def infer_id ( self ) -> 'CSL_Item' Detect and set a non-null/empty value for \"id\" or else raise a ValueError. View Source def infer_id ( self ) -> \"CSL_Item\" : \"\"\" Detect and set a non-null/empty value for \" id \" or else raise a ValueError. \"\"\" if self . get ( \"standard_citation\" ) : # \"standard_citation\" field is set with a non - null / empty value return self . set_id ( self . pop ( \"standard_citation\" )) if self . note_dict . get ( \"standard_id\" ) : # \"standard_id\" note field is set with a non - null / empty value return self . set_id ( self . note_dict [ \"standard_id\" ]) if self . get ( \"id\" ) : # \"id\" field exists and is set with a non - null / empty value return self . set_id ( self [ \"id\" ]) raise ValueError ( \"infer_id could not detect a field with a citation / standard_citation. \" 'Consider setting the CSL Item \" id \" field.' )","title":"infer_id"},{"location":"reference/manubot/cite/csl_item/#items","text":"def items ( ... ) D.items() -> a set-like object providing a view on D's items","title":"items"},{"location":"reference/manubot/cite/csl_item/#keys","text":"def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys","title":"keys"},{"location":"reference/manubot/cite/csl_item/#note_append_dict","text":"def note_append_dict ( self , dictionary : dict ) -> None Append key-value pairs specified by dictionary to the note field of a CSL Item. Uses the the CSL JSON \"cheater syntax\" to encode additional values not defined by the CSL JSON schema. View Source def note_append_dict ( self , dictionary : dict ) -> None : \" \"\" Append key-value pairs specified by `dictionary` to the note field of a CSL Item. Uses the the [CSL JSON \" cheater syntax \"](https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields) to encode additional values not defined by the CSL JSON schema. \"\" \" for key , value in dictionary . items () : if not re . fullmatch ( r \"[A-Z]+|[-_a-z]+\" , key ) : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"it does not conform to the variable_name syntax as per https://git.io/fjTzW.\" ) continue if \" \\n \" in value : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"the value contains a newline: {value!r}\" ) continue self . note_append_text ( f \"{key}: {value}\" )","title":"note_append_dict"},{"location":"reference/manubot/cite/csl_item/#note_append_text","text":"def note_append_text ( self , text : str ) -> None Append text to the note field (as a new line) of a CSL Item. If a line already exists equal to text , do nothing. View Source def note_append_text ( self , text : str ) -> None : \" \"\" Append text to the note field (as a new line) of a CSL Item. If a line already exists equal to `text`, do nothing. \"\" \" if not text : return note = self . note if re . search ( f \"^{re.escape(text)}$\" , note , flags = re . MULTILINE ) : # do not accumulate duplicate lines of text # https://github.com/manubot/manubot/issues/258 return if note and not note . endswith ( \" \\n \" ) : note += \" \\n \" note += text self . note = note","title":"note_append_text"},{"location":"reference/manubot/cite/csl_item/#pop","text":"def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, default is returned if given, otherwise KeyError is raised","title":"pop"},{"location":"reference/manubot/cite/csl_item/#popitem","text":"def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty.","title":"popitem"},{"location":"reference/manubot/cite/csl_item/#prune_against_schema","text":"def prune_against_schema ( self ) -> 'CSL_Item' Remove fields that violate the CSL Item JSON Schema. View Source def prune_against_schema ( self ) -> \"CSL_Item\" : \"\"\" Remove fields that violate the CSL Item JSON Schema. \"\"\" from .citeproc import remove_jsonschema_errors ( csl_item ,) = remove_jsonschema_errors ([ self ], in_place = True ) assert csl_item is self return self","title":"prune_against_schema"},{"location":"reference/manubot/cite/csl_item/#set_date","text":"def set_date ( self , date : Union [ NoneType , str , datetime . date , datetime . datetime ], variable : str = 'issued' ) -> 'CSL_Item' date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). variable: which variable to assign the date to. View Source def set_date ( self , date : Union [ None, str, datetime.date, datetime.datetime ] , variable : str = \"issued\" , ) -> \"CSL_Item\" : \"\"\" date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). variable: which variable to assign the date to. \"\"\" date_parts = date_to_date_parts ( date ) if date_parts : self [ variable ] = { \"date-parts\" : [ date_parts ] } return self","title":"set_date"},{"location":"reference/manubot/cite/csl_item/#set_default_type","text":"def set_default_type ( self ) -> 'CSL_Item' Set type to 'entry', if type not specified. View Source def set_default_type ( self ) -> \"CSL_Item\" : \"\"\" Set type to ' entry ', if type not specified. \"\"\" self [ \"type\" ] = self . get ( \"type\" , \"entry\" ) return self","title":"set_default_type"},{"location":"reference/manubot/cite/csl_item/#set_id","text":"def set_id ( self , id_ ) -> 'CSL_Item' View Source def set_id ( self , id_ ) -> \"CSL_Item\" : self [ \"id\" ] = id_ return self","title":"set_id"},{"location":"reference/manubot/cite/csl_item/#setdefault","text":"def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default.","title":"setdefault"},{"location":"reference/manubot/cite/csl_item/#standardize_id","text":"def standardize_id ( self ) -> 'CSL_Item' Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \"id\" field. The standard_id is extracted from a \"standard_citation\" field, the \"note\" field, or the \"id\" field. The extracted citation is checked for validity and standardized, after which it is the final \"standard_id\". Regarding csl_item modification, the csl_item \"id\" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id and original_id. Note that the Manubot software generally refers to the \"id\" of a CSL Item as a citekey. However, in this context, we use \"id\" rather than \"citekey\" for consistency with CSL's \"id\" field. View Source def standardize_id ( self ) -> \"CSL_Item\" : \"\"\" Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \" id \" field. The standard_id is extracted from a \" standard_citation \" field, the \" note \" field, or the \" id \" field. The extracted citation is checked for validity and standardized, after which it is the final \" standard_id \". Regarding csl_item modification, the csl_item \" id \" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id and original_id. Note that the Manubot software generally refers to the \" id \" of a CSL Item as a citekey. However, in this context, we use \" id \" rather than \" citekey \" for consistency with CSL' s \"id\" field . \"\"\" original_id = self.get(\" id \") self.infer_id() original_standard_id = self[\" id \"] citekey = CiteKey(original_standard_id) standard_id = citekey.standard_id add_to_note = {} note_dict = self.note_dict if original_id and original_id != standard_id: if original_id != note_dict.get(\" original_id \"): add_to_note[\" original_id \"] = original_id if original_standard_id and original_standard_id != standard_id: if original_standard_id != note_dict.get(\" original_standard_id \"): add_to_note[\" original_standard_id \"] = original_standard_id if standard_id != note_dict.get(\" standard_id \"): add_to_note[\" standard_id \"] = standard_id self.note_append_dict(dictionary=add_to_note) self.set_id(standard_id) return self","title":"standardize_id"},{"location":"reference/manubot/cite/csl_item/#update","text":"def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k]","title":"update"},{"location":"reference/manubot/cite/csl_item/#validate_against_schema","text":"def validate_against_schema ( self ) -> 'CSL_Item' Confirm that the CSL_Item validates. If not, raises a jsonschema.exceptions.ValidationError. View Source def validate_against_schema ( self ) -> \"CSL_Item\" : \"\"\" Confirm that the CSL_Item validates. If not, raises a jsonschema.exceptions.ValidationError. \"\"\" from .citeproc import get_jsonschema_csl_validator validator = get_jsonschema_csl_validator () validator . validate ([ self ]) return self","title":"validate_against_schema"},{"location":"reference/manubot/cite/csl_item/#values","text":"def values ( ... ) D.values() -> an object providing a view on D's values","title":"values"},{"location":"reference/manubot/cite/curie/","text":"Module manubot.cite.curie Compact Uniform Resource Identifiers Manubot keeps a local versions of the identifier.org registry. Repository developers can run the following commands to update the Manubot version. # regenerate manubot/cite/curie/namespaces.json python manubot/cite/curie/__init__.py # if namespaces.json has changed, the following test will likely fail: pytest manubot/cite/tests/test_handlers.py::test_prefix_to_handler # copy captured stdout from failed test_prefix_to_handler to # manubot.cite.handlers.prefix_to_handler. Pre-commit hook will reformat file. References: https://en.wikipedia.org/wiki/CURIE https://identifiers.org/ https://github.com/manubot/manubot/issues/218 https://docs.identifiers.org/articles/api.html https://n2t.net/e/compact_ids.html https://n2t.net/e/cdl_ebi_prefixes.yaml https://en.wikipedia.org/wiki/MIRIAM_Registry Identifiers.org and MIRIAM Registry: community resources to provide persistent identification On the road to robust data citation Uniform Resolution of Compact Identifiers for Biomedical Data View Source \"\"\" Compact Uniform Resource Identifiers Manubot keeps a local versions of the identifier.org registry. Repository developers can run the following commands to update the Manubot version. ```shell # regenerate manubot/cite/curie/namespaces.json python manubot/cite/curie/__init__.py # if namespaces.json has changed, the following test will likely fail: pytest manubot/cite/tests/test_handlers.py::test_prefix_to_handler # copy captured stdout from failed test_prefix_to_handler to # manubot.cite.handlers.prefix_to_handler. Pre-commit hook will reformat file. ``` References: - https://en.wikipedia.org/wiki/CURIE - https://identifiers.org/ - https://github.com/manubot/manubot/issues/218 - https://docs.identifiers.org/articles/api.html - https://n2t.net/e/compact_ids.html - https://n2t.net/e/cdl_ebi_prefixes.yaml - https://en.wikipedia.org/wiki/MIRIAM_Registry - [Identifiers.org and MIRIAM Registry: community resources to provide persistent identification](https://doi.org/10.1093/nar/gkr1097) - [On the road to robust data citation](https://doi.org/10.1038/sdata.2018.95) - [Uniform Resolution of Compact Identifiers for Biomedical Data](https://doi.org/10.1038/sdata.2018.29) \"\"\" import dataclasses import functools import json import logging import pathlib import re import typing from manubot . cite . handlers import Handler _keep_namespace_fields = { \"prefix\" , \"mirId\" , # MIRIAM Registry identifier \"name\" , \"pattern\" , # regex pattern \"description\" , \"sampleId\" , # example identifier \"namespaceEmbeddedInLui\" , # whether prefix is included in the local unique identifier \"curiePrefix\" , # a computed field for the actual prefix used by CURIEs including required capitalization } namespace_path = pathlib . Path ( __file__ ) . parent . joinpath ( \"namespaces.json\" ) @ dataclasses . dataclass class Handler_CURIE ( Handler ): def __post_init__ ( self ): prefix_to_namespace = get_prefix_to_namespace () self . namespace = prefix_to_namespace [ self . prefix_lower ] self . standard_prefix = self . namespace [ \"prefix\" ] self . prefixes = sorted ( { self . namespace [ \"prefix\" ], self . namespace [ \"curiePrefix\" ] . lower ()} ) self . accession_pattern = self . namespace [ \"pattern\" ] def get_csl_item ( self , citekey ): from .. url import get_url_csl_item url = curie_to_url ( citekey . standard_id ) return get_url_csl_item ( url ) def _get_lui ( self , citekey ) -> str : \"\"\" When namespaceEmbeddedInLui is true, some identifiers.org namespace metadata, such as pattern, include curiePrefix. This function returns the local unique identifier (lui / accession) based on this caveat. \"\"\" lui = citekey . accession if self . namespace . get ( \"namespaceEmbeddedInLui\" , False ): lui = f \"{self.namespace['curiePrefix']}:{lui}\" return lui def inspect ( self , citekey ): pattern = self . _get_pattern ( \"accession_pattern\" ) lui = self . _get_lui ( citekey ) if not pattern . fullmatch ( lui ): return f \"{lui} does not match regex {pattern.pattern}\" def get_curie_handlers (): \"\"\"Get all possible CURIE handlers\"\"\" namespaces = get_namespaces ( compile_patterns = True ) handlers = [ Handler_CURIE ( ns [ \"prefix\" ]) for ns in namespaces ] return handlers def _download_namespaces (): \"\"\" Download all namespaces from the Identifiers.org Central Registry. Example of a single namespace JSON data at <https://registry.api.identifiers.org/restApi/namespaces/230> \"\"\" import requests params = dict ( size = 5000 , sort = \"prefix\" ) url = \"https://registry.api.identifiers.org/restApi/namespaces\" response = requests . get ( url , params ) response . raise_for_status () results = response . json () if results [ \"page\" ][ \"totalPages\" ] > 1 : logging . warning ( \"_download_curie_registry does not support multi-page results \\n \" f \"{response.url} \\n {json.dumps(results['page'])}\" ) namespaces = results [ \"_embedded\" ][ \"namespaces\" ] # filter namespace fields to reduce diskspace for namespace in namespaces : namespace [ \"curiePrefix\" ] = get_curie_prefix ( namespace ) for field in set ( namespace ) - _keep_namespace_fields : del namespace [ field ] json_text = json . dumps ( namespaces , indent = 2 , ensure_ascii = False ) namespace_path . write_text ( json_text + \" \\n \" , encoding = \"utf-8\" ) def get_curie_prefix ( namespace ): \"\"\" The prefix portion of a CURIE is not always the same as the identifiers.org namespace prefix. This occurs when namespaceEmbeddedInLui is true. When namespaceEmbeddedInLui, CURIEs require a specific prefix capitalization. The actual prefix and capitalization is reverse engineered from the regex pattern. References: https://github.com/identifiers-org/identifiers-org.github.io/issues/100#issuecomment-614679142 \"\"\" if not namespace [ \"namespaceEmbeddedInLui\" ]: return namespace [ \"prefix\" ] import exrex example_curie = exrex . getone ( namespace [ \"pattern\" ]) curie_prefix , _ = example_curie . split ( \":\" , 1 ) return curie_prefix def get_namespaces ( compile_patterns = False ) -> typing . List [ dict ]: with namespace_path . open ( encoding = \"utf-8-sig\" ) as read_file : namespaces = json . load ( read_file ) if compile_patterns : for namespace in namespaces : namespace [ \"compiled_pattern\" ] = re . compile ( namespace [ \"pattern\" ]) return namespaces @ functools . lru_cache () def get_prefix_to_namespace () -> typing . Dict [ str , typing . Dict ]: prefix_to_namespace = dict () for ns in get_namespaces (): for key in \"prefix\" , \"curiePrefix\" : prefix_to_namespace [ ns [ key ] . lower ()] = ns return prefix_to_namespace def standardize_curie ( curie ): \"\"\" Return CURIE with identifiers.org expected capitalization. `curie` should be in `prefix:accession` format. If `curie` is malformed or uses an unrecognized prefix, raise ValueError. \"\"\" if not isinstance ( curie , str ): raise TypeError ( f \"curie parameter should be string. Received {curie.__class__.__name__} instead for {curie}\" ) try : prefix , accession = curie . split ( \":\" , 1 ) except ValueError : raise ValueError ( f \"curie must be splittable by `:` and formatted like `prefix:accession`. Received {curie}\" ) # do not yet understand capitalization # https://github.com/identifiers-org/identifiers-org.github.io/issues/100 prefix_lower = prefix . lower () prefix_to_namespaces = get_prefix_to_namespace () try : namespace = prefix_to_namespaces [ prefix_lower ] except KeyError : raise ValueError ( f \"prefix {prefix_lower} for {curie} is not a recognized prefix\" ) return f \"{namespace['curiePrefix']}:{accession}\" def curie_to_url ( curie ): \"\"\" `curie` should be in `prefix:accession` format \"\"\" curie = standardize_curie ( curie ) resolver_url = \"https://identifiers.org\" return f \"{resolver_url}/{curie}\" if __name__ == \"__main__\" : _download_namespaces () namespaces = get_namespaces () Variables namespace_path Functions curie_to_url def curie_to_url ( curie ) curie should be in prefix:accession format View Source def curie_to_url ( curie ) : \" \"\" `curie` should be in `prefix:accession` format \"\" \" curie = standardize_curie ( curie ) resolver_url = \"https://identifiers.org\" return f \"{resolver_url}/{curie}\" get_curie_handlers def get_curie_handlers ( ) Get all possible CURIE handlers View Source def get_curie_handlers () : \"\"\" Get all possible CURIE handlers \"\"\" namespaces = get_namespaces ( compile_patterns = True ) handlers = [ Handler_CURIE ( ns [ \" prefix \" ] ) for ns in namespaces ] return handlers get_curie_prefix def get_curie_prefix ( namespace ) The prefix portion of a CURIE is not always the same as the identifiers.org namespace prefix. This occurs when namespaceEmbeddedInLui is true. When namespaceEmbeddedInLui, CURIEs require a specific prefix capitalization. The actual prefix and capitalization is reverse engineered from the regex pattern. References: https://github.com/identifiers-org/identifiers-org.github.io/issues/100#issuecomment-614679142 View Source def get_curie_prefix ( namespace ): \"\"\" The prefix portion of a CURIE is not always the same as the identifiers.org namespace prefix. This occurs when namespaceEmbeddedInLui is true. When namespaceEmbeddedInLui, CURIEs require a specific prefix capitalization. The actual prefix and capitalization is reverse engineered from the regex pattern. References: https://github.com/identifiers-org/identifiers-org.github.io/issues/100#issuecomment-614679142 \"\"\" if not namespace [ \"namespaceEmbeddedInLui\" ]: return namespace [ \"prefix\" ] import exrex example_curie = exrex . getone ( namespace [ \"pattern\" ]) curie_prefix , _ = example_curie . split ( \":\" , 1 ) return curie_prefix get_namespaces def get_namespaces ( compile_patterns = False ) -> List [ dict ] View Source def get_namespaces ( compile_patterns = False ) -> typing . List [ dict ] : with namespace_path . open ( encoding = \"utf-8-sig\" ) as read_file : namespaces = json . load ( read_file ) if compile_patterns : for namespace in namespaces : namespace [ \"compiled_pattern\" ] = re . compile ( namespace [ \"pattern\" ] ) return namespaces get_prefix_to_namespace def get_prefix_to_namespace ( ) -> Dict [ str , Dict ] View Source @functools . lru_cache () def get_prefix_to_namespace () -> typing . Dict [ str, typing.Dict ] : prefix_to_namespace = dict () for ns in get_namespaces () : for key in \"prefix\" , \"curiePrefix\" : prefix_to_namespace [ ns[key ] . lower () ] = ns return prefix_to_namespace standardize_curie def standardize_curie ( curie ) Return CURIE with identifiers.org expected capitalization. curie should be in prefix:accession format. If curie is malformed or uses an unrecognized prefix, raise ValueError. View Source def standardize_curie ( curie ) : \" \"\" Return CURIE with identifiers.org expected capitalization. `curie` should be in `prefix:accession` format. If `curie` is malformed or uses an unrecognized prefix, raise ValueError. \"\" \" if not isinstance ( curie , str ) : raise TypeError ( f \"curie parameter should be string. Received {curie.__class__.__name__} instead for {curie}\" ) try : prefix , accession = curie . split ( \":\" , 1 ) except ValueError : raise ValueError ( f \"curie must be splittable by `:` and formatted like `prefix:accession`. Received {curie}\" ) # do not yet understand capitalization # https://github.com/identifiers-org/identifiers-org.github.io/issues/100 prefix_lower = prefix . lower () prefix_to_namespaces = get_prefix_to_namespace () try : namespace = prefix_to_namespaces [ prefix_lower ] except KeyError : raise ValueError ( f \"prefix {prefix_lower} for {curie} is not a recognized prefix\" ) return f \"{namespace['curiePrefix']}:{accession}\" Classes Handler_CURIE class Handler_CURIE ( prefix_lower : str ) View Source @dataclasses . dataclass class Handler_CURIE ( Handler ): def __post_init__ ( self ): prefix_to_namespace = get_prefix_to_namespace () self . namespace = prefix_to_namespace [ self . prefix_lower ] self . standard_prefix = self . namespace [ \"prefix\" ] self . prefixes = sorted ( { self . namespace [ \"prefix\" ], self . namespace [ \"curiePrefix\" ] . lower ()} ) self . accession_pattern = self . namespace [ \"pattern\" ] def get_csl_item ( self , citekey ): from ..url import get_url_csl_item url = curie_to_url ( citekey . standard_id ) return get_url_csl_item ( url ) def _get_lui ( self , citekey ) -> str : \"\"\" When namespaceEmbeddedInLui is true, some identifiers.org namespace metadata, such as pattern, include curiePrefix. This function returns the local unique identifier (lui / accession) based on this caveat. \"\"\" lui = citekey . accession if self . namespace . get ( \"namespaceEmbeddedInLui\" , False ): lui = f \" { self . namespace [ 'curiePrefix' ] } : { lui } \" return lui def inspect ( self , citekey ): pattern = self . _get_pattern ( \"accession_pattern\" ) lui = self . _get_lui ( citekey ) if not pattern . fullmatch ( lui ): return f \" { lui } does not match regex { pattern . pattern } \" Ancestors (in MRO) manubot.cite.handlers.Handler Class variables prefixes Methods get_csl_item def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ): from ..url import get_url_csl_item url = curie_to_url ( citekey . standard_id ) return get_url_csl_item ( url ) inspect def inspect ( self , citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey ) : pattern = self . _get_pattern ( \"accession_pattern\" ) lui = self . _get_lui ( citekey ) if not pattern . fullmatch ( lui ) : return f \"{lui} does not match regex {pattern.pattern}\" standardize_prefix_accession def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ] Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ] : \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession","title":"Curie"},{"location":"reference/manubot/cite/curie/#module-manubotcitecurie","text":"Compact Uniform Resource Identifiers Manubot keeps a local versions of the identifier.org registry. Repository developers can run the following commands to update the Manubot version. # regenerate manubot/cite/curie/namespaces.json python manubot/cite/curie/__init__.py # if namespaces.json has changed, the following test will likely fail: pytest manubot/cite/tests/test_handlers.py::test_prefix_to_handler # copy captured stdout from failed test_prefix_to_handler to # manubot.cite.handlers.prefix_to_handler. Pre-commit hook will reformat file. References: https://en.wikipedia.org/wiki/CURIE https://identifiers.org/ https://github.com/manubot/manubot/issues/218 https://docs.identifiers.org/articles/api.html https://n2t.net/e/compact_ids.html https://n2t.net/e/cdl_ebi_prefixes.yaml https://en.wikipedia.org/wiki/MIRIAM_Registry Identifiers.org and MIRIAM Registry: community resources to provide persistent identification On the road to robust data citation Uniform Resolution of Compact Identifiers for Biomedical Data View Source \"\"\" Compact Uniform Resource Identifiers Manubot keeps a local versions of the identifier.org registry. Repository developers can run the following commands to update the Manubot version. ```shell # regenerate manubot/cite/curie/namespaces.json python manubot/cite/curie/__init__.py # if namespaces.json has changed, the following test will likely fail: pytest manubot/cite/tests/test_handlers.py::test_prefix_to_handler # copy captured stdout from failed test_prefix_to_handler to # manubot.cite.handlers.prefix_to_handler. Pre-commit hook will reformat file. ``` References: - https://en.wikipedia.org/wiki/CURIE - https://identifiers.org/ - https://github.com/manubot/manubot/issues/218 - https://docs.identifiers.org/articles/api.html - https://n2t.net/e/compact_ids.html - https://n2t.net/e/cdl_ebi_prefixes.yaml - https://en.wikipedia.org/wiki/MIRIAM_Registry - [Identifiers.org and MIRIAM Registry: community resources to provide persistent identification](https://doi.org/10.1093/nar/gkr1097) - [On the road to robust data citation](https://doi.org/10.1038/sdata.2018.95) - [Uniform Resolution of Compact Identifiers for Biomedical Data](https://doi.org/10.1038/sdata.2018.29) \"\"\" import dataclasses import functools import json import logging import pathlib import re import typing from manubot . cite . handlers import Handler _keep_namespace_fields = { \"prefix\" , \"mirId\" , # MIRIAM Registry identifier \"name\" , \"pattern\" , # regex pattern \"description\" , \"sampleId\" , # example identifier \"namespaceEmbeddedInLui\" , # whether prefix is included in the local unique identifier \"curiePrefix\" , # a computed field for the actual prefix used by CURIEs including required capitalization } namespace_path = pathlib . Path ( __file__ ) . parent . joinpath ( \"namespaces.json\" ) @ dataclasses . dataclass class Handler_CURIE ( Handler ): def __post_init__ ( self ): prefix_to_namespace = get_prefix_to_namespace () self . namespace = prefix_to_namespace [ self . prefix_lower ] self . standard_prefix = self . namespace [ \"prefix\" ] self . prefixes = sorted ( { self . namespace [ \"prefix\" ], self . namespace [ \"curiePrefix\" ] . lower ()} ) self . accession_pattern = self . namespace [ \"pattern\" ] def get_csl_item ( self , citekey ): from .. url import get_url_csl_item url = curie_to_url ( citekey . standard_id ) return get_url_csl_item ( url ) def _get_lui ( self , citekey ) -> str : \"\"\" When namespaceEmbeddedInLui is true, some identifiers.org namespace metadata, such as pattern, include curiePrefix. This function returns the local unique identifier (lui / accession) based on this caveat. \"\"\" lui = citekey . accession if self . namespace . get ( \"namespaceEmbeddedInLui\" , False ): lui = f \"{self.namespace['curiePrefix']}:{lui}\" return lui def inspect ( self , citekey ): pattern = self . _get_pattern ( \"accession_pattern\" ) lui = self . _get_lui ( citekey ) if not pattern . fullmatch ( lui ): return f \"{lui} does not match regex {pattern.pattern}\" def get_curie_handlers (): \"\"\"Get all possible CURIE handlers\"\"\" namespaces = get_namespaces ( compile_patterns = True ) handlers = [ Handler_CURIE ( ns [ \"prefix\" ]) for ns in namespaces ] return handlers def _download_namespaces (): \"\"\" Download all namespaces from the Identifiers.org Central Registry. Example of a single namespace JSON data at <https://registry.api.identifiers.org/restApi/namespaces/230> \"\"\" import requests params = dict ( size = 5000 , sort = \"prefix\" ) url = \"https://registry.api.identifiers.org/restApi/namespaces\" response = requests . get ( url , params ) response . raise_for_status () results = response . json () if results [ \"page\" ][ \"totalPages\" ] > 1 : logging . warning ( \"_download_curie_registry does not support multi-page results \\n \" f \"{response.url} \\n {json.dumps(results['page'])}\" ) namespaces = results [ \"_embedded\" ][ \"namespaces\" ] # filter namespace fields to reduce diskspace for namespace in namespaces : namespace [ \"curiePrefix\" ] = get_curie_prefix ( namespace ) for field in set ( namespace ) - _keep_namespace_fields : del namespace [ field ] json_text = json . dumps ( namespaces , indent = 2 , ensure_ascii = False ) namespace_path . write_text ( json_text + \" \\n \" , encoding = \"utf-8\" ) def get_curie_prefix ( namespace ): \"\"\" The prefix portion of a CURIE is not always the same as the identifiers.org namespace prefix. This occurs when namespaceEmbeddedInLui is true. When namespaceEmbeddedInLui, CURIEs require a specific prefix capitalization. The actual prefix and capitalization is reverse engineered from the regex pattern. References: https://github.com/identifiers-org/identifiers-org.github.io/issues/100#issuecomment-614679142 \"\"\" if not namespace [ \"namespaceEmbeddedInLui\" ]: return namespace [ \"prefix\" ] import exrex example_curie = exrex . getone ( namespace [ \"pattern\" ]) curie_prefix , _ = example_curie . split ( \":\" , 1 ) return curie_prefix def get_namespaces ( compile_patterns = False ) -> typing . List [ dict ]: with namespace_path . open ( encoding = \"utf-8-sig\" ) as read_file : namespaces = json . load ( read_file ) if compile_patterns : for namespace in namespaces : namespace [ \"compiled_pattern\" ] = re . compile ( namespace [ \"pattern\" ]) return namespaces @ functools . lru_cache () def get_prefix_to_namespace () -> typing . Dict [ str , typing . Dict ]: prefix_to_namespace = dict () for ns in get_namespaces (): for key in \"prefix\" , \"curiePrefix\" : prefix_to_namespace [ ns [ key ] . lower ()] = ns return prefix_to_namespace def standardize_curie ( curie ): \"\"\" Return CURIE with identifiers.org expected capitalization. `curie` should be in `prefix:accession` format. If `curie` is malformed or uses an unrecognized prefix, raise ValueError. \"\"\" if not isinstance ( curie , str ): raise TypeError ( f \"curie parameter should be string. Received {curie.__class__.__name__} instead for {curie}\" ) try : prefix , accession = curie . split ( \":\" , 1 ) except ValueError : raise ValueError ( f \"curie must be splittable by `:` and formatted like `prefix:accession`. Received {curie}\" ) # do not yet understand capitalization # https://github.com/identifiers-org/identifiers-org.github.io/issues/100 prefix_lower = prefix . lower () prefix_to_namespaces = get_prefix_to_namespace () try : namespace = prefix_to_namespaces [ prefix_lower ] except KeyError : raise ValueError ( f \"prefix {prefix_lower} for {curie} is not a recognized prefix\" ) return f \"{namespace['curiePrefix']}:{accession}\" def curie_to_url ( curie ): \"\"\" `curie` should be in `prefix:accession` format \"\"\" curie = standardize_curie ( curie ) resolver_url = \"https://identifiers.org\" return f \"{resolver_url}/{curie}\" if __name__ == \"__main__\" : _download_namespaces () namespaces = get_namespaces ()","title":"Module manubot.cite.curie"},{"location":"reference/manubot/cite/curie/#variables","text":"namespace_path","title":"Variables"},{"location":"reference/manubot/cite/curie/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/curie/#curie_to_url","text":"def curie_to_url ( curie ) curie should be in prefix:accession format View Source def curie_to_url ( curie ) : \" \"\" `curie` should be in `prefix:accession` format \"\" \" curie = standardize_curie ( curie ) resolver_url = \"https://identifiers.org\" return f \"{resolver_url}/{curie}\"","title":"curie_to_url"},{"location":"reference/manubot/cite/curie/#get_curie_handlers","text":"def get_curie_handlers ( ) Get all possible CURIE handlers View Source def get_curie_handlers () : \"\"\" Get all possible CURIE handlers \"\"\" namespaces = get_namespaces ( compile_patterns = True ) handlers = [ Handler_CURIE ( ns [ \" prefix \" ] ) for ns in namespaces ] return handlers","title":"get_curie_handlers"},{"location":"reference/manubot/cite/curie/#get_curie_prefix","text":"def get_curie_prefix ( namespace ) The prefix portion of a CURIE is not always the same as the identifiers.org namespace prefix. This occurs when namespaceEmbeddedInLui is true. When namespaceEmbeddedInLui, CURIEs require a specific prefix capitalization. The actual prefix and capitalization is reverse engineered from the regex pattern. References: https://github.com/identifiers-org/identifiers-org.github.io/issues/100#issuecomment-614679142 View Source def get_curie_prefix ( namespace ): \"\"\" The prefix portion of a CURIE is not always the same as the identifiers.org namespace prefix. This occurs when namespaceEmbeddedInLui is true. When namespaceEmbeddedInLui, CURIEs require a specific prefix capitalization. The actual prefix and capitalization is reverse engineered from the regex pattern. References: https://github.com/identifiers-org/identifiers-org.github.io/issues/100#issuecomment-614679142 \"\"\" if not namespace [ \"namespaceEmbeddedInLui\" ]: return namespace [ \"prefix\" ] import exrex example_curie = exrex . getone ( namespace [ \"pattern\" ]) curie_prefix , _ = example_curie . split ( \":\" , 1 ) return curie_prefix","title":"get_curie_prefix"},{"location":"reference/manubot/cite/curie/#get_namespaces","text":"def get_namespaces ( compile_patterns = False ) -> List [ dict ] View Source def get_namespaces ( compile_patterns = False ) -> typing . List [ dict ] : with namespace_path . open ( encoding = \"utf-8-sig\" ) as read_file : namespaces = json . load ( read_file ) if compile_patterns : for namespace in namespaces : namespace [ \"compiled_pattern\" ] = re . compile ( namespace [ \"pattern\" ] ) return namespaces","title":"get_namespaces"},{"location":"reference/manubot/cite/curie/#get_prefix_to_namespace","text":"def get_prefix_to_namespace ( ) -> Dict [ str , Dict ] View Source @functools . lru_cache () def get_prefix_to_namespace () -> typing . Dict [ str, typing.Dict ] : prefix_to_namespace = dict () for ns in get_namespaces () : for key in \"prefix\" , \"curiePrefix\" : prefix_to_namespace [ ns[key ] . lower () ] = ns return prefix_to_namespace","title":"get_prefix_to_namespace"},{"location":"reference/manubot/cite/curie/#standardize_curie","text":"def standardize_curie ( curie ) Return CURIE with identifiers.org expected capitalization. curie should be in prefix:accession format. If curie is malformed or uses an unrecognized prefix, raise ValueError. View Source def standardize_curie ( curie ) : \" \"\" Return CURIE with identifiers.org expected capitalization. `curie` should be in `prefix:accession` format. If `curie` is malformed or uses an unrecognized prefix, raise ValueError. \"\" \" if not isinstance ( curie , str ) : raise TypeError ( f \"curie parameter should be string. Received {curie.__class__.__name__} instead for {curie}\" ) try : prefix , accession = curie . split ( \":\" , 1 ) except ValueError : raise ValueError ( f \"curie must be splittable by `:` and formatted like `prefix:accession`. Received {curie}\" ) # do not yet understand capitalization # https://github.com/identifiers-org/identifiers-org.github.io/issues/100 prefix_lower = prefix . lower () prefix_to_namespaces = get_prefix_to_namespace () try : namespace = prefix_to_namespaces [ prefix_lower ] except KeyError : raise ValueError ( f \"prefix {prefix_lower} for {curie} is not a recognized prefix\" ) return f \"{namespace['curiePrefix']}:{accession}\"","title":"standardize_curie"},{"location":"reference/manubot/cite/curie/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/curie/#handler_curie","text":"class Handler_CURIE ( prefix_lower : str ) View Source @dataclasses . dataclass class Handler_CURIE ( Handler ): def __post_init__ ( self ): prefix_to_namespace = get_prefix_to_namespace () self . namespace = prefix_to_namespace [ self . prefix_lower ] self . standard_prefix = self . namespace [ \"prefix\" ] self . prefixes = sorted ( { self . namespace [ \"prefix\" ], self . namespace [ \"curiePrefix\" ] . lower ()} ) self . accession_pattern = self . namespace [ \"pattern\" ] def get_csl_item ( self , citekey ): from ..url import get_url_csl_item url = curie_to_url ( citekey . standard_id ) return get_url_csl_item ( url ) def _get_lui ( self , citekey ) -> str : \"\"\" When namespaceEmbeddedInLui is true, some identifiers.org namespace metadata, such as pattern, include curiePrefix. This function returns the local unique identifier (lui / accession) based on this caveat. \"\"\" lui = citekey . accession if self . namespace . get ( \"namespaceEmbeddedInLui\" , False ): lui = f \" { self . namespace [ 'curiePrefix' ] } : { lui } \" return lui def inspect ( self , citekey ): pattern = self . _get_pattern ( \"accession_pattern\" ) lui = self . _get_lui ( citekey ) if not pattern . fullmatch ( lui ): return f \" { lui } does not match regex { pattern . pattern } \"","title":"Handler_CURIE"},{"location":"reference/manubot/cite/curie/#ancestors-in-mro","text":"manubot.cite.handlers.Handler","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/curie/#class-variables","text":"prefixes","title":"Class variables"},{"location":"reference/manubot/cite/curie/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/curie/#get_csl_item","text":"def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ): from ..url import get_url_csl_item url = curie_to_url ( citekey . standard_id ) return get_url_csl_item ( url )","title":"get_csl_item"},{"location":"reference/manubot/cite/curie/#inspect","text":"def inspect ( self , citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey ) : pattern = self . _get_pattern ( \"accession_pattern\" ) lui = self . _get_lui ( citekey ) if not pattern . fullmatch ( lui ) : return f \"{lui} does not match regex {pattern.pattern}\"","title":"inspect"},{"location":"reference/manubot/cite/curie/#standardize_prefix_accession","text":"def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ] Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ] : \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession","title":"standardize_prefix_accession"},{"location":"reference/manubot/cite/doi/","text":"Module manubot.cite.doi None None View Source import json import logging import urllib.parse import urllib.request from typing import Any , Callable , Optional import requests from manubot.util import get_manubot_user_agent from .handlers import Handler from .pubmed import get_pubmed_ids_for_doi class Handler_DOI ( Handler ): standard_prefix = \"doi\" prefixes = [ \"doi\" , \"shortdoi\" , ] accession_pattern = r \"10\\.[0-9]{4,9}/\\S+\" shortdoi_pattern = r \"10/[a-zA-Z0-9]+\" def inspect ( self , citekey ): identifier = citekey . accession if identifier . startswith ( \"10.\" ): # https://www.crossref.org/blog/dois-and-matching-regular-expressions/ if not self . _get_pattern () . fullmatch ( identifier ): return ( \"Identifier does not conform to the DOI regex. \" \"Double check the DOI.\" ) elif identifier . startswith ( \"10/\" ): # shortDOI, see http://shortdoi.org if not self . _get_pattern ( \"shortdoi_pattern\" ) . fullmatch ( identifier ): return ( \"Identifier does not conform to the shortDOI regex. \" \"Double check the shortDOI.\" ) else : return \"DOIs must start with '10.' (or '10/' for shortDOIs).\" def standardize_prefix_accession ( self , accession ): if accession . startswith ( \"10/\" ): try : accession = expand_short_doi ( accession ) except Exception as error : # If DOI shortening fails, return the unshortened DOI. # DOI metadata lookup will eventually fail somewhere with # appropriate error handling, as opposed to here. logging . error ( f \"Error in expand_short_doi for { accession } \" f \"due to a { error . __class__ . __name__ } : \\n { error } \" ) logging . info ( error , exc_info = True ) accession = accession . lower () return self . standard_prefix , accession def get_csl_item ( self , citekey ): return get_doi_csl_item ( citekey . standard_accession ) def expand_short_doi ( short_doi : str ) -> str : \"\"\" Convert a shortDOI to a regular DOI. \"\"\" if not short_doi . startswith ( \"10/\" ): raise ValueError ( f \"shortDOIs start with `10/`, but expand_short_doi received: { short_doi } \" ) url = f \"https://doi.org/api/handles/ { short_doi . lower () } \" params = { \"type\" : \"HS_ALIAS\" } response = requests . get ( url , params = params ) # response documentation at https://www.handle.net/proxy_servlet.html results = response . json () response_code = results . get ( \"responseCode\" ) # Handle protocol response code if response_code == 100 : raise ValueError ( f \"Handle not found. Double check short_doi: { short_doi } \" ) if response_code == 200 : raise ValueError ( f \"HS_ALIAS values not found. Double check short_doi: { short_doi } \" ) if response_code != 1 : raise ValueError ( f \"Error response code of { response_code } returned by { response . url } \" ) values = results . get ( \"values\" , []) for value in values : if value . get ( \"type\" ) == \"HS_ALIAS\" : doi = value [ \"data\" ][ \"value\" ] return doi . lower () raise RuntimeError ( f 'HS_ALIAS value not found by expand_short_doi(\" { short_doi } \") \\n ' f \"The following JSON was retrieved from { response . url } : \\n \" + json . dumps ( results , indent = 2 ) ) def get_short_doi_url ( doi : str ) -> Optional [ str ]: \"\"\" Get the shortDOI URL for a DOI. \"\"\" quoted_doi = urllib . request . quote ( doi ) url = \"http://shortdoi.org/ {} ?format=json\" . format ( quoted_doi ) headers = { \"User-Agent\" : get_manubot_user_agent ()} try : response = requests . get ( url , headers = headers ) . json () short_doi = response [ \"ShortDOI\" ] short_url = \"https://doi.org/\" + short_doi [ 3 :] # Remove \"10/\" prefix return short_url except Exception : logging . warning ( f \"shortDOI lookup failed for { doi } \" , exc_info = True ) return None \"\"\" Base URL to use for content negotiation. Options include: 1. <https://data.crosscite.org> documented at <https://support.datacite.org/docs/datacite-content-resolver> 2. <https://doi.org/> documented at <https://citation.crosscite.org/docs.html> \"\"\" content_negotiation_url : str = \"https://data.crosscite.org\" def get_doi_csl_item_crosscite ( doi : str ): \"\"\" Use Content Negotioation to retrieve the CSL Item metadata for a DOI. \"\"\" url = urllib . parse . urljoin ( content_negotiation_url , urllib . request . quote ( doi )) header = { \"Accept\" : \"application/vnd.citationstyles.csl+json\" , \"User-Agent\" : get_manubot_user_agent (), } response = requests . get ( url , headers = header ) try : return response . json () except Exception as error : logging . error ( f \"Error fetching metadata for doi: { doi } . \\n \" f \"Invalid response from { response . url } : \\n { response . text } \" ) raise error def get_doi_csl_item_zotero ( doi : str ): \"\"\" Generate CSL JSON Data for a DOI using Zotero's translation-server. \"\"\" from manubot.cite.zotero import get_csl_item return get_csl_item ( f \"doi: { doi } \" ) def augment_get_doi_csl_item ( function : Callable [ ... , Any ]): \"\"\" Decorator providing edits to the csl_item returned by a get_doi_csl_item_* function. \"\"\" def wrapper ( doi : str ): doi = doi . lower () csl_item = function ( doi ) csl_item [ \"DOI\" ] = doi csl_item [ \"URL\" ] = f \"https://doi.org/ { doi } \" short_doi_url = get_short_doi_url ( doi ) if short_doi_url : csl_item [ \"URL\" ] = short_doi_url try : csl_item . update ( get_pubmed_ids_for_doi ( doi )) except Exception : logging . warning ( f \"Error calling get_pubmed_ids_for_doi for { doi } \" , exc_info = True ) return csl_item return wrapper @augment_get_doi_csl_item def get_doi_csl_item ( doi : str ): \"\"\" Generate CSL JSON Data for an DOI. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `doi_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\"\" # FIXME: this function is repetitive with other get_*_csl_item functions. for retriever in doi_retrievers : try : return retriever ( doi ) except Exception as error : logging . warning ( f \"Error in { retriever . __name__ } for { doi } \" f \"due to a { error . __class__ . __name__ } : \\n { error } \" ) logging . info ( error , exc_info = True ) raise Exception ( f \"all get_doi_csl_item methods failed for { doi } \" ) doi_retrievers = [ get_doi_csl_item_crosscite , get_doi_csl_item_zotero ] Variables content_negotiation_url doi_retrievers Functions augment_get_doi_csl_item def augment_get_doi_csl_item ( function : Callable [ ... , Any ] ) Decorator providing edits to the csl_item returned by a get_doi_csl_item_* function. View Source def augment_get_doi_csl_item ( function : Callable [..., Any ] ) : \"\"\" Decorator providing edits to the csl_item returned by a get_doi_csl_item_ * function . \"\"\" def wrapper ( doi : str ) : doi = doi . lower () csl_item = function ( doi ) csl_item [ \" DOI \" ] = doi csl_item [ \" URL \" ] = f \" https://doi.org/{doi} \" short_doi_url = get_short_doi_url ( doi ) if short_doi_url : csl_item [ \" URL \" ] = short_doi_url try : csl_item . update ( get_pubmed_ids_for_doi ( doi )) except Exception : logging . warning ( f \" Error calling get_pubmed_ids_for_doi for {doi} \" , exc_info = True ) return csl_item return wrapper expand_short_doi def expand_short_doi ( short_doi : str ) -> str Convert a shortDOI to a regular DOI. View Source def expand_short_doi ( short_doi: str ) -> str: \"\"\" Convert a shortDOI to a regular DOI . \"\"\" if not short_doi . startswith ( \"10/\" ) : raise ValueError ( f \"shortDOIs start with `10/`, but expand_short_doi received: {short_doi}\" ) url = f \"https://doi.org/api/handles/{short_doi.lower()}\" params = { \"type\" : \"HS_ALIAS\" } response = requests . get ( url , params = params ) # response documentation at https: //www.handle.net/proxy_servlet.html results = response . json () response_code = results . get ( \"responseCode\" ) # Handle protocol response code if response_code == 100 : raise ValueError ( f \"Handle not found. Double check short_doi: {short_doi}\" ) if response_code == 200 : raise ValueError ( f \"HS_ALIAS values not found. Double check short_doi: {short_doi}\" ) if response_code != 1 : raise ValueError ( f \"Error response code of {response_code} returned by {response.url}\" ) values = results . get ( \"values\" , []) for value in values: if value . get ( \"type\" ) == \"HS_ALIAS\" : doi = value [ \"data\" ][ \"value\" ] return doi . lower () raise RuntimeError ( f ' HS_ALIAS value not found by expand_short_doi ( \"{short_doi}\" ) \\n' f \"The following JSON was retrieved from {response.url}: \\n \" + json . dumps ( results , indent = 2 ) ) get_doi_csl_item def get_doi_csl_item ( doi : str ) View Source def wrapper ( doi : str ) : doi = doi . lower () csl_item = function ( doi ) csl_item [ \" DOI \" ] = doi csl_item [ \" URL \" ] = f \" https://doi.org/{doi} \" short_doi_url = get_short_doi_url ( doi ) if short_doi_url : csl_item [ \" URL \" ] = short_doi_url try : csl_item . update ( get_pubmed_ids_for_doi ( doi )) except Exception : logging . warning ( f \" Error calling get_pubmed_ids_for_doi for {doi} \" , exc_info = True ) return csl_item get_doi_csl_item_crosscite def get_doi_csl_item_crosscite ( doi : str ) Use Content Negotioation to retrieve the CSL Item metadata for a DOI. View Source def get_doi_csl_item_crosscite ( doi : str ) : \"\"\" Use Content Negotioation to retrieve the CSL Item metadata for a DOI . \"\"\" url = urllib . parse . urljoin ( content_negotiation_url , urllib . request . quote ( doi )) header = { \" Accept \" : \" application/vnd.citationstyles.csl+json \" , \" User-Agent \" : get_manubot_user_agent () , } response = requests . get ( url , headers = header ) try : return response . json () except Exception as error : logging . error ( f \" Error fetching metadata for doi:{doi}. \\n \" f \" Invalid response from {response.url}: \\n {response.text} \" ) raise error get_doi_csl_item_zotero def get_doi_csl_item_zotero ( doi : str ) Generate CSL JSON Data for a DOI using Zotero's translation-server. View Source def get_doi_csl_item_zotero ( doi : str ): \"\"\" Generate CSL JSON Data for a DOI using Zotero's translation-server. \"\"\" from manubot.cite.zotero import get_csl_item return get_csl_item ( f \"doi: { doi } \" ) get_short_doi_url def get_short_doi_url ( doi : str ) -> Optional [ str ] Get the shortDOI URL for a DOI. View Source def get_short_doi_url ( doi : str ) -> Optional [ str ] : \"\"\" Get the shortDOI URL for a DOI. \"\"\" quoted_doi = urllib . request . quote ( doi ) url = \"http://shortdoi.org/{}?format=json\" . format ( quoted_doi ) headers = { \"User-Agent\" : get_manubot_user_agent () } try : response = requests . get ( url , headers = headers ). json () short_doi = response [ \"ShortDOI\" ] short_url = \"https://doi.org/\" + short_doi [ 3: ] # Remove \"10/\" prefix return short_url except Exception : logging . warning ( f \"shortDOI lookup failed for {doi}\" , exc_info = True ) return None Classes Handler_DOI class Handler_DOI ( prefix_lower : str ) View Source class Handler_DOI ( Handler ) : standard_prefix = \"doi\" prefixes = [ \"doi\" , \"shortdoi\" , ] accession_pattern = r \"10\\.[0-9]{4,9}/\\S+\" shortdoi_pattern = r \"10/[a-zA-Z0-9]+\" def inspect ( self , citekey ) : identifier = citekey . accession if identifier . startswith ( \"10.\" ) : # https: //www.crossref.org/blog/dois-and-matching-regular-expressions/ if not self . _get_pattern (). fullmatch ( identifier ) : return ( \"Identifier does not conform to the DOI regex. \" \"Double check the DOI.\" ) elif identifier . startswith ( \"10/\" ) : # shortDOI , see http: //shortdoi.org if not self . _get_pattern ( \"shortdoi_pattern\" ). fullmatch ( identifier ) : return ( \"Identifier does not conform to the shortDOI regex. \" \"Double check the shortDOI.\" ) else : return \"DOIs must start with '10.' (or '10/' for shortDOIs).\" def standardize_prefix_accession ( self , accession ) : if accession . startswith ( \"10/\" ) : try: accession = expand_short_doi ( accession ) except Exception as error: # If DOI shortening fails , return the unshortened DOI . # DOI metadata lookup will eventually fail somewhere with # appropriate error handling , as opposed to here . logging . error ( f \"Error in expand_short_doi for {accession} \" f \"due to a {error.__class__.__name__}: \\n {error}\" ) logging . info ( error , exc_info = True ) accession = accession . lower () return self . standard_prefix , accession def get_csl_item ( self , citekey ) : return get_doi_csl_item ( citekey . standard_accession ) Ancestors (in MRO) manubot.cite.handlers.Handler Class variables accession_pattern prefixes shortdoi_pattern standard_prefix Methods get_csl_item def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ) : return get_doi_csl_item ( citekey . standard_accession ) inspect def inspect ( self , citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey ) : identifier = citekey . accession if identifier . startswith ( \"10.\" ) : # https: //www.crossref.org/blog/dois-and-matching-regular-expressions/ if not self . _get_pattern (). fullmatch ( identifier ) : return ( \"Identifier does not conform to the DOI regex. \" \"Double check the DOI.\" ) elif identifier . startswith ( \"10/\" ) : # shortDOI , see http: //shortdoi.org if not self . _get_pattern ( \"shortdoi_pattern\" ). fullmatch ( identifier ) : return ( \"Identifier does not conform to the shortDOI regex. \" \"Double check the shortDOI.\" ) else : return \"DOIs must start with '10.' (or '10/' for shortDOIs).\" standardize_prefix_accession def standardize_prefix_accession ( self , accession ) Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession ) : if accession . startswith ( \" 10/ \" ) : try : accession = expand_short_doi ( accession ) except Exception as error : # If DOI shortening fails , return the unshortened DOI . # DOI metadata lookup will eventually fail somewhere with # appropriate error handling , as opposed to here . logging . error ( f \" Error in expand_short_doi for {accession} \" f \" due to a {error.__class__.__name__}: \\n {error} \" ) logging . info ( error , exc_info = True ) accession = accession . lower () return self . standard_prefix , accession","title":"Doi"},{"location":"reference/manubot/cite/doi/#module-manubotcitedoi","text":"None None View Source import json import logging import urllib.parse import urllib.request from typing import Any , Callable , Optional import requests from manubot.util import get_manubot_user_agent from .handlers import Handler from .pubmed import get_pubmed_ids_for_doi class Handler_DOI ( Handler ): standard_prefix = \"doi\" prefixes = [ \"doi\" , \"shortdoi\" , ] accession_pattern = r \"10\\.[0-9]{4,9}/\\S+\" shortdoi_pattern = r \"10/[a-zA-Z0-9]+\" def inspect ( self , citekey ): identifier = citekey . accession if identifier . startswith ( \"10.\" ): # https://www.crossref.org/blog/dois-and-matching-regular-expressions/ if not self . _get_pattern () . fullmatch ( identifier ): return ( \"Identifier does not conform to the DOI regex. \" \"Double check the DOI.\" ) elif identifier . startswith ( \"10/\" ): # shortDOI, see http://shortdoi.org if not self . _get_pattern ( \"shortdoi_pattern\" ) . fullmatch ( identifier ): return ( \"Identifier does not conform to the shortDOI regex. \" \"Double check the shortDOI.\" ) else : return \"DOIs must start with '10.' (or '10/' for shortDOIs).\" def standardize_prefix_accession ( self , accession ): if accession . startswith ( \"10/\" ): try : accession = expand_short_doi ( accession ) except Exception as error : # If DOI shortening fails, return the unshortened DOI. # DOI metadata lookup will eventually fail somewhere with # appropriate error handling, as opposed to here. logging . error ( f \"Error in expand_short_doi for { accession } \" f \"due to a { error . __class__ . __name__ } : \\n { error } \" ) logging . info ( error , exc_info = True ) accession = accession . lower () return self . standard_prefix , accession def get_csl_item ( self , citekey ): return get_doi_csl_item ( citekey . standard_accession ) def expand_short_doi ( short_doi : str ) -> str : \"\"\" Convert a shortDOI to a regular DOI. \"\"\" if not short_doi . startswith ( \"10/\" ): raise ValueError ( f \"shortDOIs start with `10/`, but expand_short_doi received: { short_doi } \" ) url = f \"https://doi.org/api/handles/ { short_doi . lower () } \" params = { \"type\" : \"HS_ALIAS\" } response = requests . get ( url , params = params ) # response documentation at https://www.handle.net/proxy_servlet.html results = response . json () response_code = results . get ( \"responseCode\" ) # Handle protocol response code if response_code == 100 : raise ValueError ( f \"Handle not found. Double check short_doi: { short_doi } \" ) if response_code == 200 : raise ValueError ( f \"HS_ALIAS values not found. Double check short_doi: { short_doi } \" ) if response_code != 1 : raise ValueError ( f \"Error response code of { response_code } returned by { response . url } \" ) values = results . get ( \"values\" , []) for value in values : if value . get ( \"type\" ) == \"HS_ALIAS\" : doi = value [ \"data\" ][ \"value\" ] return doi . lower () raise RuntimeError ( f 'HS_ALIAS value not found by expand_short_doi(\" { short_doi } \") \\n ' f \"The following JSON was retrieved from { response . url } : \\n \" + json . dumps ( results , indent = 2 ) ) def get_short_doi_url ( doi : str ) -> Optional [ str ]: \"\"\" Get the shortDOI URL for a DOI. \"\"\" quoted_doi = urllib . request . quote ( doi ) url = \"http://shortdoi.org/ {} ?format=json\" . format ( quoted_doi ) headers = { \"User-Agent\" : get_manubot_user_agent ()} try : response = requests . get ( url , headers = headers ) . json () short_doi = response [ \"ShortDOI\" ] short_url = \"https://doi.org/\" + short_doi [ 3 :] # Remove \"10/\" prefix return short_url except Exception : logging . warning ( f \"shortDOI lookup failed for { doi } \" , exc_info = True ) return None \"\"\" Base URL to use for content negotiation. Options include: 1. <https://data.crosscite.org> documented at <https://support.datacite.org/docs/datacite-content-resolver> 2. <https://doi.org/> documented at <https://citation.crosscite.org/docs.html> \"\"\" content_negotiation_url : str = \"https://data.crosscite.org\" def get_doi_csl_item_crosscite ( doi : str ): \"\"\" Use Content Negotioation to retrieve the CSL Item metadata for a DOI. \"\"\" url = urllib . parse . urljoin ( content_negotiation_url , urllib . request . quote ( doi )) header = { \"Accept\" : \"application/vnd.citationstyles.csl+json\" , \"User-Agent\" : get_manubot_user_agent (), } response = requests . get ( url , headers = header ) try : return response . json () except Exception as error : logging . error ( f \"Error fetching metadata for doi: { doi } . \\n \" f \"Invalid response from { response . url } : \\n { response . text } \" ) raise error def get_doi_csl_item_zotero ( doi : str ): \"\"\" Generate CSL JSON Data for a DOI using Zotero's translation-server. \"\"\" from manubot.cite.zotero import get_csl_item return get_csl_item ( f \"doi: { doi } \" ) def augment_get_doi_csl_item ( function : Callable [ ... , Any ]): \"\"\" Decorator providing edits to the csl_item returned by a get_doi_csl_item_* function. \"\"\" def wrapper ( doi : str ): doi = doi . lower () csl_item = function ( doi ) csl_item [ \"DOI\" ] = doi csl_item [ \"URL\" ] = f \"https://doi.org/ { doi } \" short_doi_url = get_short_doi_url ( doi ) if short_doi_url : csl_item [ \"URL\" ] = short_doi_url try : csl_item . update ( get_pubmed_ids_for_doi ( doi )) except Exception : logging . warning ( f \"Error calling get_pubmed_ids_for_doi for { doi } \" , exc_info = True ) return csl_item return wrapper @augment_get_doi_csl_item def get_doi_csl_item ( doi : str ): \"\"\" Generate CSL JSON Data for an DOI. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `doi_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\"\" # FIXME: this function is repetitive with other get_*_csl_item functions. for retriever in doi_retrievers : try : return retriever ( doi ) except Exception as error : logging . warning ( f \"Error in { retriever . __name__ } for { doi } \" f \"due to a { error . __class__ . __name__ } : \\n { error } \" ) logging . info ( error , exc_info = True ) raise Exception ( f \"all get_doi_csl_item methods failed for { doi } \" ) doi_retrievers = [ get_doi_csl_item_crosscite , get_doi_csl_item_zotero ]","title":"Module manubot.cite.doi"},{"location":"reference/manubot/cite/doi/#variables","text":"content_negotiation_url doi_retrievers","title":"Variables"},{"location":"reference/manubot/cite/doi/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/doi/#augment_get_doi_csl_item","text":"def augment_get_doi_csl_item ( function : Callable [ ... , Any ] ) Decorator providing edits to the csl_item returned by a get_doi_csl_item_* function. View Source def augment_get_doi_csl_item ( function : Callable [..., Any ] ) : \"\"\" Decorator providing edits to the csl_item returned by a get_doi_csl_item_ * function . \"\"\" def wrapper ( doi : str ) : doi = doi . lower () csl_item = function ( doi ) csl_item [ \" DOI \" ] = doi csl_item [ \" URL \" ] = f \" https://doi.org/{doi} \" short_doi_url = get_short_doi_url ( doi ) if short_doi_url : csl_item [ \" URL \" ] = short_doi_url try : csl_item . update ( get_pubmed_ids_for_doi ( doi )) except Exception : logging . warning ( f \" Error calling get_pubmed_ids_for_doi for {doi} \" , exc_info = True ) return csl_item return wrapper","title":"augment_get_doi_csl_item"},{"location":"reference/manubot/cite/doi/#expand_short_doi","text":"def expand_short_doi ( short_doi : str ) -> str Convert a shortDOI to a regular DOI. View Source def expand_short_doi ( short_doi: str ) -> str: \"\"\" Convert a shortDOI to a regular DOI . \"\"\" if not short_doi . startswith ( \"10/\" ) : raise ValueError ( f \"shortDOIs start with `10/`, but expand_short_doi received: {short_doi}\" ) url = f \"https://doi.org/api/handles/{short_doi.lower()}\" params = { \"type\" : \"HS_ALIAS\" } response = requests . get ( url , params = params ) # response documentation at https: //www.handle.net/proxy_servlet.html results = response . json () response_code = results . get ( \"responseCode\" ) # Handle protocol response code if response_code == 100 : raise ValueError ( f \"Handle not found. Double check short_doi: {short_doi}\" ) if response_code == 200 : raise ValueError ( f \"HS_ALIAS values not found. Double check short_doi: {short_doi}\" ) if response_code != 1 : raise ValueError ( f \"Error response code of {response_code} returned by {response.url}\" ) values = results . get ( \"values\" , []) for value in values: if value . get ( \"type\" ) == \"HS_ALIAS\" : doi = value [ \"data\" ][ \"value\" ] return doi . lower () raise RuntimeError ( f ' HS_ALIAS value not found by expand_short_doi ( \"{short_doi}\" ) \\n' f \"The following JSON was retrieved from {response.url}: \\n \" + json . dumps ( results , indent = 2 ) )","title":"expand_short_doi"},{"location":"reference/manubot/cite/doi/#get_doi_csl_item","text":"def get_doi_csl_item ( doi : str ) View Source def wrapper ( doi : str ) : doi = doi . lower () csl_item = function ( doi ) csl_item [ \" DOI \" ] = doi csl_item [ \" URL \" ] = f \" https://doi.org/{doi} \" short_doi_url = get_short_doi_url ( doi ) if short_doi_url : csl_item [ \" URL \" ] = short_doi_url try : csl_item . update ( get_pubmed_ids_for_doi ( doi )) except Exception : logging . warning ( f \" Error calling get_pubmed_ids_for_doi for {doi} \" , exc_info = True ) return csl_item","title":"get_doi_csl_item"},{"location":"reference/manubot/cite/doi/#get_doi_csl_item_crosscite","text":"def get_doi_csl_item_crosscite ( doi : str ) Use Content Negotioation to retrieve the CSL Item metadata for a DOI. View Source def get_doi_csl_item_crosscite ( doi : str ) : \"\"\" Use Content Negotioation to retrieve the CSL Item metadata for a DOI . \"\"\" url = urllib . parse . urljoin ( content_negotiation_url , urllib . request . quote ( doi )) header = { \" Accept \" : \" application/vnd.citationstyles.csl+json \" , \" User-Agent \" : get_manubot_user_agent () , } response = requests . get ( url , headers = header ) try : return response . json () except Exception as error : logging . error ( f \" Error fetching metadata for doi:{doi}. \\n \" f \" Invalid response from {response.url}: \\n {response.text} \" ) raise error","title":"get_doi_csl_item_crosscite"},{"location":"reference/manubot/cite/doi/#get_doi_csl_item_zotero","text":"def get_doi_csl_item_zotero ( doi : str ) Generate CSL JSON Data for a DOI using Zotero's translation-server. View Source def get_doi_csl_item_zotero ( doi : str ): \"\"\" Generate CSL JSON Data for a DOI using Zotero's translation-server. \"\"\" from manubot.cite.zotero import get_csl_item return get_csl_item ( f \"doi: { doi } \" )","title":"get_doi_csl_item_zotero"},{"location":"reference/manubot/cite/doi/#get_short_doi_url","text":"def get_short_doi_url ( doi : str ) -> Optional [ str ] Get the shortDOI URL for a DOI. View Source def get_short_doi_url ( doi : str ) -> Optional [ str ] : \"\"\" Get the shortDOI URL for a DOI. \"\"\" quoted_doi = urllib . request . quote ( doi ) url = \"http://shortdoi.org/{}?format=json\" . format ( quoted_doi ) headers = { \"User-Agent\" : get_manubot_user_agent () } try : response = requests . get ( url , headers = headers ). json () short_doi = response [ \"ShortDOI\" ] short_url = \"https://doi.org/\" + short_doi [ 3: ] # Remove \"10/\" prefix return short_url except Exception : logging . warning ( f \"shortDOI lookup failed for {doi}\" , exc_info = True ) return None","title":"get_short_doi_url"},{"location":"reference/manubot/cite/doi/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/doi/#handler_doi","text":"class Handler_DOI ( prefix_lower : str ) View Source class Handler_DOI ( Handler ) : standard_prefix = \"doi\" prefixes = [ \"doi\" , \"shortdoi\" , ] accession_pattern = r \"10\\.[0-9]{4,9}/\\S+\" shortdoi_pattern = r \"10/[a-zA-Z0-9]+\" def inspect ( self , citekey ) : identifier = citekey . accession if identifier . startswith ( \"10.\" ) : # https: //www.crossref.org/blog/dois-and-matching-regular-expressions/ if not self . _get_pattern (). fullmatch ( identifier ) : return ( \"Identifier does not conform to the DOI regex. \" \"Double check the DOI.\" ) elif identifier . startswith ( \"10/\" ) : # shortDOI , see http: //shortdoi.org if not self . _get_pattern ( \"shortdoi_pattern\" ). fullmatch ( identifier ) : return ( \"Identifier does not conform to the shortDOI regex. \" \"Double check the shortDOI.\" ) else : return \"DOIs must start with '10.' (or '10/' for shortDOIs).\" def standardize_prefix_accession ( self , accession ) : if accession . startswith ( \"10/\" ) : try: accession = expand_short_doi ( accession ) except Exception as error: # If DOI shortening fails , return the unshortened DOI . # DOI metadata lookup will eventually fail somewhere with # appropriate error handling , as opposed to here . logging . error ( f \"Error in expand_short_doi for {accession} \" f \"due to a {error.__class__.__name__}: \\n {error}\" ) logging . info ( error , exc_info = True ) accession = accession . lower () return self . standard_prefix , accession def get_csl_item ( self , citekey ) : return get_doi_csl_item ( citekey . standard_accession )","title":"Handler_DOI"},{"location":"reference/manubot/cite/doi/#ancestors-in-mro","text":"manubot.cite.handlers.Handler","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/doi/#class-variables","text":"accession_pattern prefixes shortdoi_pattern standard_prefix","title":"Class variables"},{"location":"reference/manubot/cite/doi/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/doi/#get_csl_item","text":"def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ) : return get_doi_csl_item ( citekey . standard_accession )","title":"get_csl_item"},{"location":"reference/manubot/cite/doi/#inspect","text":"def inspect ( self , citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey ) : identifier = citekey . accession if identifier . startswith ( \"10.\" ) : # https: //www.crossref.org/blog/dois-and-matching-regular-expressions/ if not self . _get_pattern (). fullmatch ( identifier ) : return ( \"Identifier does not conform to the DOI regex. \" \"Double check the DOI.\" ) elif identifier . startswith ( \"10/\" ) : # shortDOI , see http: //shortdoi.org if not self . _get_pattern ( \"shortdoi_pattern\" ). fullmatch ( identifier ) : return ( \"Identifier does not conform to the shortDOI regex. \" \"Double check the shortDOI.\" ) else : return \"DOIs must start with '10.' (or '10/' for shortDOIs).\"","title":"inspect"},{"location":"reference/manubot/cite/doi/#standardize_prefix_accession","text":"def standardize_prefix_accession ( self , accession ) Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession ) : if accession . startswith ( \" 10/ \" ) : try : accession = expand_short_doi ( accession ) except Exception as error : # If DOI shortening fails , return the unshortened DOI . # DOI metadata lookup will eventually fail somewhere with # appropriate error handling , as opposed to here . logging . error ( f \" Error in expand_short_doi for {accession} \" f \" due to a {error.__class__.__name__}: \\n {error} \" ) logging . info ( error , exc_info = True ) accession = accession . lower () return self . standard_prefix , accession","title":"standardize_prefix_accession"},{"location":"reference/manubot/cite/handlers/","text":"Module manubot.cite.handlers None None View Source import abc import dataclasses import functools import re from typing import Any , Dict , List , Optional , Pattern , Set , Tuple from manubot.util import import_function from .citekey import CiteKey \"\"\" Non-citation prefixes used by the pandoc-xnos suite of Pandoc filters, including pandoc-fignos, pandoc-tablenos, and pandoc-eqnos. \"\"\" _pandoc_xnos_prefixes : Set [ str ] = { \"fig\" , \"tbl\" , \"eq\" } _local_handlers : List [ str ] = [ \"manubot.cite.arxiv.Handler_arXiv\" , \"manubot.cite.doi.Handler_DOI\" , \"manubot.cite.isbn.Handler_ISBN\" , \"manubot.cite.pubmed.Handler_PMC\" , \"manubot.cite.pubmed.Handler_PubMed\" , \"manubot.cite.url.Handler_URL\" , \"manubot.cite.wikidata.Handler_Wikidata\" , ] _infer_prefix_patterns : List [ Tuple [ str , str ]] = [ ( \"doi\" , \"accession_pattern\" ), ( \"doi\" , \"shortdoi_pattern\" ), ( \"pmc\" , \"accession_pattern\" ), ( \"pubmed\" , \"accession_pattern\" ), ( \"wikidata\" , \"accession_pattern\" ), ( \"arxiv\" , \"accession_pattern\" ), ] \"\"\" Each list element is a tuple of (handler, pattern attribute). Handler is the handler prefix (lowercase). Pattern attribute refers to the Handler attribute containing a regex pattern. A citekey prefix is inferred from the first matching pattern in this list. \"\"\" def infer_prefix ( dealiased_id : str ) -> Optional [ str ]: \"\"\" Infer the prefix for citekey by matching it against a sequence of regexes. If a match is found, return the coressponding standard prefix. Otherwise, return None. \"\"\" for prefix , pattern_attrib in _infer_prefix_patterns : handler = get_handler ( prefix ) pattern = handler . _get_pattern ( attribute = pattern_attrib ) if pattern . fullmatch ( dealiased_id ): return handler . standard_prefix return None @functools . lru_cache ( maxsize = 10_000 ) def get_handler ( prefix_lower : str ) -> \"Handler\" : if not isinstance ( prefix_lower , str ): raise TypeError ( f \"prefix_lower should be a str, instead received { prefix_lower . __class__ . __name__ } \" ) assert prefix_lower == prefix_lower . lower () handler = prefix_to_handler [ prefix_lower ] handler = import_function ( handler )( prefix_lower ) return handler def _generate_prefix_to_handler () -> Dict [ str , str ]: \"\"\" Generate complete dictionary for prefix_to_handler. \"\"\" import inspect from .curie import get_curie_handlers curie_handlers = get_curie_handlers () pth = {} for handler in curie_handlers + _local_handlers : if isinstance ( handler , str ): handler = import_function ( handler )( \"dummy_prefix_lower\" ) for prefix in handler . prefixes : pth [ prefix ] = f \" { inspect . getmodule ( handler ) . __name__ } . { handler . __class__ . __name__ } \" pth = dict ( sorted ( pth . items ())) # sort for clean diffs of serialized dict return pth @dataclasses . dataclass class Handler : \"\"\" A Handler is a class that provides support for a certain type of citekey. For example, a Handler subclass could provide support for DOI citekeys. Subclasses enable custom logic for different citekey prefixes, including how to standardize the citekey and how to retrieve CSL Item metadata. \"\"\" prefix_lower : str prefixes = [] def _get_pattern ( self , attribute = \"accession_pattern\" ) -> Pattern : \"\"\" Return a compiled regex pattern stored by `attribute`. By default, return `self.accession_pattern`, which Handler subclasses can set to provide the expected pattern for `self.accession`. \"\"\" # todo: consider caching compilation pattern = getattr ( self , attribute , None ) if not pattern : return None if not isinstance ( pattern , Pattern ): pattern = re . compile ( pattern ) return pattern def inspect ( self , citekey : CiteKey ) -> Optional [ str ]: \"\"\" Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. \"\"\" pattern = self . _get_pattern ( \"accession_pattern\" ) if not pattern : return None if not pattern . fullmatch ( citekey . accession ): return f \" { citekey . accession } does not match regex { pattern . pattern } \" def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ]: \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession @abc . abstractmethod def get_csl_item ( self , citekey ) -> Dict [ str , Any ]: \"\"\" Return a CSL_Item with bibliographic details for citekey. \"\"\" raise NotImplementedError ( f \"Manubot does not know how to generate a csl_item for { citekey . standard_id !r} \" ) \"\"\" Mapping from lowercase prefix to Handler class function location as string. This output is automatically generated using `_generate_prefix_to_handler`. Hardcoding this mapping reduces startup time and helps keep imports to a minimum, allowing installations without the full dependencies to function. \"\"\" prefix_to_handler : Dict [ str , str ] = { \"3dmet\" : \"manubot.cite.curie.Handler_CURIE\" , \"abs\" : \"manubot.cite.curie.Handler_CURIE\" , \"aceview.worm\" : \"manubot.cite.curie.Handler_CURIE\" , \"addgene\" : \"manubot.cite.curie.Handler_CURIE\" , \"adw\" : \"manubot.cite.curie.Handler_CURIE\" , \"affy.probeset\" : \"manubot.cite.curie.Handler_CURIE\" , \"aftol.taxonomy\" : \"manubot.cite.curie.Handler_CURIE\" , \"agricola\" : \"manubot.cite.curie.Handler_CURIE\" , \"allergome\" : \"manubot.cite.curie.Handler_CURIE\" , \"amoebadb\" : \"manubot.cite.curie.Handler_CURIE\" , \"antibodyregistry\" : \"manubot.cite.curie.Handler_CURIE\" , \"antweb\" : \"manubot.cite.curie.Handler_CURIE\" , \"aop\" : \"manubot.cite.curie.Handler_CURIE\" , \"aop.events\" : \"manubot.cite.curie.Handler_CURIE\" , \"aop.relationships\" : \"manubot.cite.curie.Handler_CURIE\" , \"aop.stressor\" : \"manubot.cite.curie.Handler_CURIE\" , \"apd\" : \"manubot.cite.curie.Handler_CURIE\" , \"aphidbase.transcript\" : \"manubot.cite.curie.Handler_CURIE\" , \"apid.interactions\" : \"manubot.cite.curie.Handler_CURIE\" , \"arachnoserver\" : \"manubot.cite.curie.Handler_CURIE\" , \"ardb\" : \"manubot.cite.curie.Handler_CURIE\" , \"ark\" : \"manubot.cite.curie.Handler_CURIE\" , \"arrayexpress\" : \"manubot.cite.curie.Handler_CURIE\" , \"arrayexpress.platform\" : \"manubot.cite.curie.Handler_CURIE\" , \"arraymap\" : \"manubot.cite.curie.Handler_CURIE\" , \"arxiv\" : \"manubot.cite.arxiv.Handler_arXiv\" , \"asap\" : \"manubot.cite.curie.Handler_CURIE\" , \"ascl\" : \"manubot.cite.curie.Handler_CURIE\" , \"asin\" : \"manubot.cite.curie.Handler_CURIE\" , \"aspgd.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"aspgd.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"atc\" : \"manubot.cite.curie.Handler_CURIE\" , \"atcc\" : \"manubot.cite.curie.Handler_CURIE\" , \"atcvet\" : \"manubot.cite.curie.Handler_CURIE\" , \"atfdb.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"autdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"bacdive\" : \"manubot.cite.curie.Handler_CURIE\" , \"bacmap.biog\" : \"manubot.cite.curie.Handler_CURIE\" , \"bacmap.map\" : \"manubot.cite.curie.Handler_CURIE\" , \"bao\" : \"manubot.cite.curie.Handler_CURIE\" , \"bdgp.est\" : \"manubot.cite.curie.Handler_CURIE\" , \"bdgp.insertion\" : \"manubot.cite.curie.Handler_CURIE\" , \"bdsc\" : \"manubot.cite.curie.Handler_CURIE\" , \"beetlebase\" : \"manubot.cite.curie.Handler_CURIE\" , \"begdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"bgee.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"bgee.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"bgee.organ\" : \"manubot.cite.curie.Handler_CURIE\" , \"bgee.stage\" : \"manubot.cite.curie.Handler_CURIE\" , \"bigg.compartment\" : \"manubot.cite.curie.Handler_CURIE\" , \"bigg.metabolite\" : \"manubot.cite.curie.Handler_CURIE\" , \"bigg.model\" : \"manubot.cite.curie.Handler_CURIE\" , \"bigg.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"bindingdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"biocarta.pathway\" : \"manubot.cite.curie.Handler_CURIE\" , \"biocatalogue.service\" : \"manubot.cite.curie.Handler_CURIE\" , \"biocyc\" : \"manubot.cite.curie.Handler_CURIE\" , \"biogrid\" : \"manubot.cite.curie.Handler_CURIE\" , \"biolink\" : \"manubot.cite.curie.Handler_CURIE\" , \"biominder\" : \"manubot.cite.curie.Handler_CURIE\" , \"biomodels.db\" : \"manubot.cite.curie.Handler_CURIE\" , \"biomodels.kisao\" : \"manubot.cite.curie.Handler_CURIE\" , \"biomodels.teddy\" : \"manubot.cite.curie.Handler_CURIE\" , \"biomodels.vocabulary\" : \"manubot.cite.curie.Handler_CURIE\" , \"bionumbers\" : \"manubot.cite.curie.Handler_CURIE\" , \"bioportal\" : \"manubot.cite.curie.Handler_CURIE\" , \"bioproject\" : \"manubot.cite.curie.Handler_CURIE\" , \"biosample\" : \"manubot.cite.curie.Handler_CURIE\" , \"biosimulators\" : \"manubot.cite.curie.Handler_CURIE\" , \"biostudies\" : \"manubot.cite.curie.Handler_CURIE\" , \"biosystems\" : \"manubot.cite.curie.Handler_CURIE\" , \"biotools\" : \"manubot.cite.curie.Handler_CURIE\" , \"bitterdb.cpd\" : \"manubot.cite.curie.Handler_CURIE\" , \"bitterdb.rec\" : \"manubot.cite.curie.Handler_CURIE\" , \"bmrb\" : \"manubot.cite.curie.Handler_CURIE\" , \"bold.taxonomy\" : \"manubot.cite.curie.Handler_CURIE\" , \"brenda\" : \"manubot.cite.curie.Handler_CURIE\" , \"broad\" : \"manubot.cite.curie.Handler_CURIE\" , \"bto\" : \"manubot.cite.curie.Handler_CURIE\" , \"bugbase.expt\" : \"manubot.cite.curie.Handler_CURIE\" , \"bugbase.protocol\" : \"manubot.cite.curie.Handler_CURIE\" , \"bykdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"cabri\" : \"manubot.cite.curie.Handler_CURIE\" , \"cadsr\" : \"manubot.cite.curie.Handler_CURIE\" , \"cameo\" : \"manubot.cite.curie.Handler_CURIE\" , \"caps\" : \"manubot.cite.curie.Handler_CURIE\" , \"cas\" : \"manubot.cite.curie.Handler_CURIE\" , \"cath\" : \"manubot.cite.curie.Handler_CURIE\" , \"cath.domain\" : \"manubot.cite.curie.Handler_CURIE\" , \"cath.superfamily\" : \"manubot.cite.curie.Handler_CURIE\" , \"cattleqtldb\" : \"manubot.cite.curie.Handler_CURIE\" , \"cazy\" : \"manubot.cite.curie.Handler_CURIE\" , \"cbioportal\" : \"manubot.cite.curie.Handler_CURIE\" , \"ccds\" : \"manubot.cite.curie.Handler_CURIE\" , \"cco\" : \"manubot.cite.curie.Handler_CURIE\" , \"cdd\" : \"manubot.cite.curie.Handler_CURIE\" , \"cdpd\" : \"manubot.cite.curie.Handler_CURIE\" , \"cellimage\" : \"manubot.cite.curie.Handler_CURIE\" , \"cellosaurus\" : \"manubot.cite.curie.Handler_CURIE\" , \"cgd\" : \"manubot.cite.curie.Handler_CURIE\" , \"cgsc\" : \"manubot.cite.curie.Handler_CURIE\" , \"charprot\" : \"manubot.cite.curie.Handler_CURIE\" , \"chebi\" : \"manubot.cite.curie.Handler_CURIE\" , \"chembl.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"chembl.target\" : \"manubot.cite.curie.Handler_CURIE\" , \"chemdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"chemidplus\" : \"manubot.cite.curie.Handler_CURIE\" , \"chemspider\" : \"manubot.cite.curie.Handler_CURIE\" , \"chickenqtldb\" : \"manubot.cite.curie.Handler_CURIE\" , \"cl\" : \"manubot.cite.curie.Handler_CURIE\" , \"classyfire\" : \"manubot.cite.curie.Handler_CURIE\" , \"cldb\" : \"manubot.cite.curie.Handler_CURIE\" , \"clinicaltrials\" : \"manubot.cite.curie.Handler_CURIE\" , \"clinvar\" : \"manubot.cite.curie.Handler_CURIE\" , \"clinvar.record\" : \"manubot.cite.curie.Handler_CURIE\" , \"clinvar.submission\" : \"manubot.cite.curie.Handler_CURIE\" , \"combine.specifications\" : \"manubot.cite.curie.Handler_CURIE\" , \"complexportal\" : \"manubot.cite.curie.Handler_CURIE\" , \"comptox\" : \"manubot.cite.curie.Handler_CURIE\" , \"compulyeast\" : \"manubot.cite.curie.Handler_CURIE\" , \"conoserver\" : \"manubot.cite.curie.Handler_CURIE\" , \"coriell\" : \"manubot.cite.curie.Handler_CURIE\" , \"corum\" : \"manubot.cite.curie.Handler_CURIE\" , \"cosmic\" : \"manubot.cite.curie.Handler_CURIE\" , \"cpc\" : \"manubot.cite.curie.Handler_CURIE\" , \"crisprdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"cryptodb\" : \"manubot.cite.curie.Handler_CURIE\" , \"csa\" : \"manubot.cite.curie.Handler_CURIE\" , \"cst\" : \"manubot.cite.curie.Handler_CURIE\" , \"cst.ab\" : \"manubot.cite.curie.Handler_CURIE\" , \"ctd.chemical\" : \"manubot.cite.curie.Handler_CURIE\" , \"ctd.disease\" : \"manubot.cite.curie.Handler_CURIE\" , \"ctd.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"cubedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"d1id\" : \"manubot.cite.curie.Handler_CURIE\" , \"dailymed\" : \"manubot.cite.curie.Handler_CURIE\" , \"dandi\" : \"manubot.cite.curie.Handler_CURIE\" , \"darc\" : \"manubot.cite.curie.Handler_CURIE\" , \"dashr\" : \"manubot.cite.curie.Handler_CURIE\" , \"dashr.expression\" : \"manubot.cite.curie.Handler_CURIE\" , \"datanator.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"datanator.metabolite\" : \"manubot.cite.curie.Handler_CURIE\" , \"datf\" : \"manubot.cite.curie.Handler_CURIE\" , \"dbd\" : \"manubot.cite.curie.Handler_CURIE\" , \"dbest\" : \"manubot.cite.curie.Handler_CURIE\" , \"dbg2introns\" : \"manubot.cite.curie.Handler_CURIE\" , \"dbgap\" : \"manubot.cite.curie.Handler_CURIE\" , \"dbprobe\" : \"manubot.cite.curie.Handler_CURIE\" , \"dbsnp\" : \"manubot.cite.curie.Handler_CURIE\" , \"degradome\" : \"manubot.cite.curie.Handler_CURIE\" , \"depod\" : \"manubot.cite.curie.Handler_CURIE\" , \"dev.ga4ghdos\" : \"manubot.cite.curie.Handler_CURIE\" , \"dictybase.est\" : \"manubot.cite.curie.Handler_CURIE\" , \"dictybase.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"did\" : \"manubot.cite.curie.Handler_CURIE\" , \"dip\" : \"manubot.cite.curie.Handler_CURIE\" , \"disprot\" : \"manubot.cite.curie.Handler_CURIE\" , \"disprot.region\" : \"manubot.cite.curie.Handler_CURIE\" , \"dlxb\" : \"manubot.cite.curie.Handler_CURIE\" , \"dlxc\" : \"manubot.cite.curie.Handler_CURIE\" , \"doi\" : \"manubot.cite.doi.Handler_DOI\" , \"doid\" : \"manubot.cite.curie.Handler_CURIE\" , \"dommino\" : \"manubot.cite.curie.Handler_CURIE\" , \"door\" : \"manubot.cite.curie.Handler_CURIE\" , \"doqcs.model\" : \"manubot.cite.curie.Handler_CURIE\" , \"doqcs.pathway\" : \"manubot.cite.curie.Handler_CURIE\" , \"dpv\" : \"manubot.cite.curie.Handler_CURIE\" , \"dragondb.allele\" : \"manubot.cite.curie.Handler_CURIE\" , \"dragondb.dna\" : \"manubot.cite.curie.Handler_CURIE\" , \"dragondb.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"dragondb.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"drsc\" : \"manubot.cite.curie.Handler_CURIE\" , \"drugbank\" : \"manubot.cite.curie.Handler_CURIE\" , \"drugbankv4.target\" : \"manubot.cite.curie.Handler_CURIE\" , \"ec-code\" : \"manubot.cite.curie.Handler_CURIE\" , \"echobase\" : \"manubot.cite.curie.Handler_CURIE\" , \"eco\" : \"manubot.cite.curie.Handler_CURIE\" , \"ecogene\" : \"manubot.cite.curie.Handler_CURIE\" , \"ecoliwiki\" : \"manubot.cite.curie.Handler_CURIE\" , \"ecyano.entity\" : \"manubot.cite.curie.Handler_CURIE\" , \"ecyano.experiment\" : \"manubot.cite.curie.Handler_CURIE\" , \"ecyano.model\" : \"manubot.cite.curie.Handler_CURIE\" , \"ecyano.rule\" : \"manubot.cite.curie.Handler_CURIE\" , \"edam\" : \"manubot.cite.curie.Handler_CURIE\" , \"efo\" : \"manubot.cite.curie.Handler_CURIE\" , \"ega.dataset\" : \"manubot.cite.curie.Handler_CURIE\" , \"ega.study\" : \"manubot.cite.curie.Handler_CURIE\" , \"eggnog\" : \"manubot.cite.curie.Handler_CURIE\" , \"elm\" : \"manubot.cite.curie.Handler_CURIE\" , \"emdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"ena.embl\" : \"manubot.cite.curie.Handler_CURIE\" , \"encode\" : \"manubot.cite.curie.Handler_CURIE\" , \"ensembl\" : \"manubot.cite.curie.Handler_CURIE\" , \"ensembl.bacteria\" : \"manubot.cite.curie.Handler_CURIE\" , \"ensembl.fungi\" : \"manubot.cite.curie.Handler_CURIE\" , \"ensembl.metazoa\" : \"manubot.cite.curie.Handler_CURIE\" , \"ensembl.plant\" : \"manubot.cite.curie.Handler_CURIE\" , \"ensembl.protist\" : \"manubot.cite.curie.Handler_CURIE\" , \"envipath\" : \"manubot.cite.curie.Handler_CURIE\" , \"envo\" : \"manubot.cite.curie.Handler_CURIE\" , \"eo\" : \"manubot.cite.curie.Handler_CURIE\" , \"epd\" : \"manubot.cite.curie.Handler_CURIE\" , \"erm\" : \"manubot.cite.curie.Handler_CURIE\" , \"erv\" : \"manubot.cite.curie.Handler_CURIE\" , \"eu89h\" : \"manubot.cite.curie.Handler_CURIE\" , \"euclinicaltrials\" : \"manubot.cite.curie.Handler_CURIE\" , \"exac.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"exac.transcript\" : \"manubot.cite.curie.Handler_CURIE\" , \"exac.variant\" : \"manubot.cite.curie.Handler_CURIE\" , \"facebase\" : \"manubot.cite.curie.Handler_CURIE\" , \"fairsharing\" : \"manubot.cite.curie.Handler_CURIE\" , \"fb\" : \"manubot.cite.curie.Handler_CURIE\" , \"fbol\" : \"manubot.cite.curie.Handler_CURIE\" , \"flowrepository\" : \"manubot.cite.curie.Handler_CURIE\" , \"fma\" : \"manubot.cite.curie.Handler_CURIE\" , \"foodb.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"foodon\" : \"manubot.cite.curie.Handler_CURIE\" , \"fplx\" : \"manubot.cite.curie.Handler_CURIE\" , \"fsnp\" : \"manubot.cite.curie.Handler_CURIE\" , \"funcbase.fly\" : \"manubot.cite.curie.Handler_CURIE\" , \"funcbase.human\" : \"manubot.cite.curie.Handler_CURIE\" , \"funcbase.mouse\" : \"manubot.cite.curie.Handler_CURIE\" , \"funcbase.yeast\" : \"manubot.cite.curie.Handler_CURIE\" , \"fungidb\" : \"manubot.cite.curie.Handler_CURIE\" , \"ga4ghdos\" : \"manubot.cite.curie.Handler_CURIE\" , \"gabi\" : \"manubot.cite.curie.Handler_CURIE\" , \"gcst\" : \"manubot.cite.curie.Handler_CURIE\" , \"gdc\" : \"manubot.cite.curie.Handler_CURIE\" , \"gdsc\" : \"manubot.cite.curie.Handler_CURIE\" , \"genatlas\" : \"manubot.cite.curie.Handler_CURIE\" , \"genecards\" : \"manubot.cite.curie.Handler_CURIE\" , \"genedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"genefarm\" : \"manubot.cite.curie.Handler_CURIE\" , \"genetree\" : \"manubot.cite.curie.Handler_CURIE\" , \"genewiki\" : \"manubot.cite.curie.Handler_CURIE\" , \"genpept\" : \"manubot.cite.curie.Handler_CURIE\" , \"genprop\" : \"manubot.cite.curie.Handler_CURIE\" , \"geo\" : \"manubot.cite.curie.Handler_CURIE\" , \"giardiadb\" : \"manubot.cite.curie.Handler_CURIE\" , \"glida.gpcr\" : \"manubot.cite.curie.Handler_CURIE\" , \"glida.ligand\" : \"manubot.cite.curie.Handler_CURIE\" , \"glycoepitope\" : \"manubot.cite.curie.Handler_CURIE\" , \"glycomedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"glycopost\" : \"manubot.cite.curie.Handler_CURIE\" , \"glytoucan\" : \"manubot.cite.curie.Handler_CURIE\" , \"gmd\" : \"manubot.cite.curie.Handler_CURIE\" , \"gmd.analyte\" : \"manubot.cite.curie.Handler_CURIE\" , \"gmd.gcms\" : \"manubot.cite.curie.Handler_CURIE\" , \"gmd.profile\" : \"manubot.cite.curie.Handler_CURIE\" , \"gmd.ref\" : \"manubot.cite.curie.Handler_CURIE\" , \"gnpis\" : \"manubot.cite.curie.Handler_CURIE\" , \"go\" : \"manubot.cite.curie.Handler_CURIE\" , \"go_ref\" : \"manubot.cite.curie.Handler_CURIE\" , \"goa\" : \"manubot.cite.curie.Handler_CURIE\" , \"gold\" : \"manubot.cite.curie.Handler_CURIE\" , \"gold.genome\" : \"manubot.cite.curie.Handler_CURIE\" , \"gold.meta\" : \"manubot.cite.curie.Handler_CURIE\" , \"google.patent\" : \"manubot.cite.curie.Handler_CURIE\" , \"gpcrdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"gpmdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"gramene.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"gramene.growthstage\" : \"manubot.cite.curie.Handler_CURIE\" , \"gramene.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"gramene.qtl\" : \"manubot.cite.curie.Handler_CURIE\" , \"gramene.taxonomy\" : \"manubot.cite.curie.Handler_CURIE\" , \"greengenes\" : \"manubot.cite.curie.Handler_CURIE\" , \"grid\" : \"manubot.cite.curie.Handler_CURIE\" , \"grin.taxonomy\" : \"manubot.cite.curie.Handler_CURIE\" , \"gro\" : \"manubot.cite.curie.Handler_CURIE\" , \"grsdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"gsso\" : \"manubot.cite.curie.Handler_CURIE\" , \"gtex\" : \"manubot.cite.curie.Handler_CURIE\" , \"gudmap\" : \"manubot.cite.curie.Handler_CURIE\" , \"gwascentral.marker\" : \"manubot.cite.curie.Handler_CURIE\" , \"gwascentral.phenotype\" : \"manubot.cite.curie.Handler_CURIE\" , \"gwascentral.study\" : \"manubot.cite.curie.Handler_CURIE\" , \"gxa.expt\" : \"manubot.cite.curie.Handler_CURIE\" , \"gxa.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"hamap\" : \"manubot.cite.curie.Handler_CURIE\" , \"hcvdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"hdr\" : \"manubot.cite.curie.Handler_CURIE\" , \"hgmd\" : \"manubot.cite.curie.Handler_CURIE\" , \"hgnc\" : \"manubot.cite.curie.Handler_CURIE\" , \"hgnc.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"hgnc.genefamily\" : \"manubot.cite.curie.Handler_CURIE\" , \"hgnc.genegroup\" : \"manubot.cite.curie.Handler_CURIE\" , \"hgnc.symbol\" : \"manubot.cite.curie.Handler_CURIE\" , \"hinv.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"hinv.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"hinv.transcript\" : \"manubot.cite.curie.Handler_CURIE\" , \"hmdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"hogenom\" : \"manubot.cite.curie.Handler_CURIE\" , \"homd.seq\" : \"manubot.cite.curie.Handler_CURIE\" , \"homd.taxon\" : \"manubot.cite.curie.Handler_CURIE\" , \"homologene\" : \"manubot.cite.curie.Handler_CURIE\" , \"hovergen\" : \"manubot.cite.curie.Handler_CURIE\" , \"hp\" : \"manubot.cite.curie.Handler_CURIE\" , \"hpa\" : \"manubot.cite.curie.Handler_CURIE\" , \"hpm.peptide\" : \"manubot.cite.curie.Handler_CURIE\" , \"hpm.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"hprd\" : \"manubot.cite.curie.Handler_CURIE\" , \"hpscreg\" : \"manubot.cite.curie.Handler_CURIE\" , \"hssp\" : \"manubot.cite.curie.Handler_CURIE\" , \"http\" : \"manubot.cite.url.Handler_URL\" , \"https\" : \"manubot.cite.url.Handler_URL\" , \"huge\" : \"manubot.cite.curie.Handler_CURIE\" , \"iao\" : \"manubot.cite.curie.Handler_CURIE\" , \"icd\" : \"manubot.cite.curie.Handler_CURIE\" , \"iceberg.element\" : \"manubot.cite.curie.Handler_CURIE\" , \"iceberg.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"ideal\" : \"manubot.cite.curie.Handler_CURIE\" , \"ido\" : \"manubot.cite.curie.Handler_CURIE\" , \"idoo\" : \"manubot.cite.curie.Handler_CURIE\" , \"idot\" : \"manubot.cite.curie.Handler_CURIE\" , \"idr\" : \"manubot.cite.curie.Handler_CURIE\" , \"imex\" : \"manubot.cite.curie.Handler_CURIE\" , \"img.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"img.taxon\" : \"manubot.cite.curie.Handler_CURIE\" , \"imgt.hla\" : \"manubot.cite.curie.Handler_CURIE\" , \"imgt.ligm\" : \"manubot.cite.curie.Handler_CURIE\" , \"inchi\" : \"manubot.cite.curie.Handler_CURIE\" , \"inchikey\" : \"manubot.cite.curie.Handler_CURIE\" , \"insdc\" : \"manubot.cite.curie.Handler_CURIE\" , \"insdc.cds\" : \"manubot.cite.curie.Handler_CURIE\" , \"insdc.gca\" : \"manubot.cite.curie.Handler_CURIE\" , \"insdc.sra\" : \"manubot.cite.curie.Handler_CURIE\" , \"intact\" : \"manubot.cite.curie.Handler_CURIE\" , \"intact.molecule\" : \"manubot.cite.curie.Handler_CURIE\" , \"interpro\" : \"manubot.cite.curie.Handler_CURIE\" , \"ird.segment\" : \"manubot.cite.curie.Handler_CURIE\" , \"irefweb\" : \"manubot.cite.curie.Handler_CURIE\" , \"isbn\" : \"manubot.cite.isbn.Handler_ISBN\" , \"isfinder\" : \"manubot.cite.curie.Handler_CURIE\" , \"isni\" : \"manubot.cite.curie.Handler_CURIE\" , \"issn\" : \"manubot.cite.curie.Handler_CURIE\" , \"iuphar.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"iuphar.ligand\" : \"manubot.cite.curie.Handler_CURIE\" , \"iuphar.receptor\" : \"manubot.cite.curie.Handler_CURIE\" , \"jaxmice\" : \"manubot.cite.curie.Handler_CURIE\" , \"jcggdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"jcm\" : \"manubot.cite.curie.Handler_CURIE\" , \"jcsd\" : \"manubot.cite.curie.Handler_CURIE\" , \"jstor\" : \"manubot.cite.curie.Handler_CURIE\" , \"jws\" : \"manubot.cite.curie.Handler_CURIE\" , \"kaggle\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.disease\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.drug\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.environ\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.genes\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.genome\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.glycan\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.metagenome\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.module\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.orthology\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.pathway\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"knapsack\" : \"manubot.cite.curie.Handler_CURIE\" , \"lei\" : \"manubot.cite.curie.Handler_CURIE\" , \"lgic\" : \"manubot.cite.curie.Handler_CURIE\" , \"licebase\" : \"manubot.cite.curie.Handler_CURIE\" , \"ligandbook\" : \"manubot.cite.curie.Handler_CURIE\" , \"ligandbox\" : \"manubot.cite.curie.Handler_CURIE\" , \"ligandexpo\" : \"manubot.cite.curie.Handler_CURIE\" , \"lincs.cell\" : \"manubot.cite.curie.Handler_CURIE\" , \"lincs.data\" : \"manubot.cite.curie.Handler_CURIE\" , \"lincs.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"lincs.smallmolecule\" : \"manubot.cite.curie.Handler_CURIE\" , \"lipidbank\" : \"manubot.cite.curie.Handler_CURIE\" , \"lipidmaps\" : \"manubot.cite.curie.Handler_CURIE\" , \"lrg\" : \"manubot.cite.curie.Handler_CURIE\" , \"ma\" : \"manubot.cite.curie.Handler_CURIE\" , \"macie\" : \"manubot.cite.curie.Handler_CURIE\" , \"maizegdb.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"mamo\" : \"manubot.cite.curie.Handler_CURIE\" , \"massbank\" : \"manubot.cite.curie.Handler_CURIE\" , \"massive\" : \"manubot.cite.curie.Handler_CURIE\" , \"matrixdb.association\" : \"manubot.cite.curie.Handler_CURIE\" , \"mdm\" : \"manubot.cite.curie.Handler_CURIE\" , \"meddra\" : \"manubot.cite.curie.Handler_CURIE\" , \"medgen\" : \"manubot.cite.curie.Handler_CURIE\" , \"medlineplus\" : \"manubot.cite.curie.Handler_CURIE\" , \"merops\" : \"manubot.cite.curie.Handler_CURIE\" , \"merops.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"merops.inhibitor\" : \"manubot.cite.curie.Handler_CURIE\" , \"mesh\" : \"manubot.cite.curie.Handler_CURIE\" , \"mesh.2012\" : \"manubot.cite.curie.Handler_CURIE\" , \"mesh.2013\" : \"manubot.cite.curie.Handler_CURIE\" , \"metabolights\" : \"manubot.cite.curie.Handler_CURIE\" , \"metacyc.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"metacyc.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"metanetx.chemical\" : \"manubot.cite.curie.Handler_CURIE\" , \"metanetx.compartment\" : \"manubot.cite.curie.Handler_CURIE\" , \"metanetx.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"metlin\" : \"manubot.cite.curie.Handler_CURIE\" , \"mex\" : \"manubot.cite.curie.Handler_CURIE\" , \"mge\" : \"manubot.cite.curie.Handler_CURIE\" , \"mgi\" : \"manubot.cite.curie.Handler_CURIE\" , \"mgnify.proj\" : \"manubot.cite.curie.Handler_CURIE\" , \"mgnify.samp\" : \"manubot.cite.curie.Handler_CURIE\" , \"mi\" : \"manubot.cite.curie.Handler_CURIE\" , \"microscope\" : \"manubot.cite.curie.Handler_CURIE\" , \"microsporidia\" : \"manubot.cite.curie.Handler_CURIE\" , \"mim\" : \"manubot.cite.curie.Handler_CURIE\" , \"mimodb\" : \"manubot.cite.curie.Handler_CURIE\" , \"minid\" : \"manubot.cite.curie.Handler_CURIE\" , \"minid.test\" : \"manubot.cite.curie.Handler_CURIE\" , \"mint\" : \"manubot.cite.curie.Handler_CURIE\" , \"mipmod\" : \"manubot.cite.curie.Handler_CURIE\" , \"mir\" : \"manubot.cite.curie.Handler_CURIE\" , \"mirbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"mirbase.mature\" : \"manubot.cite.curie.Handler_CURIE\" , \"mirex\" : \"manubot.cite.curie.Handler_CURIE\" , \"miriam.collection\" : \"manubot.cite.curie.Handler_CURIE\" , \"miriam.resource\" : \"manubot.cite.curie.Handler_CURIE\" , \"mirnest\" : \"manubot.cite.curie.Handler_CURIE\" , \"mirtarbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmmp:biomaps\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmp.cat\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmp.db\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmp.fun\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmp.ref\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmrrc\" : \"manubot.cite.curie.Handler_CURIE\" , \"mo\" : \"manubot.cite.curie.Handler_CURIE\" , \"mobidb\" : \"manubot.cite.curie.Handler_CURIE\" , \"mod\" : \"manubot.cite.curie.Handler_CURIE\" , \"modeldb\" : \"manubot.cite.curie.Handler_CURIE\" , \"molbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"molmedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"morpheus\" : \"manubot.cite.curie.Handler_CURIE\" , \"mp\" : \"manubot.cite.curie.Handler_CURIE\" , \"mpid\" : \"manubot.cite.curie.Handler_CURIE\" , \"ms\" : \"manubot.cite.curie.Handler_CURIE\" , \"multicellds.cell_line\" : \"manubot.cite.curie.Handler_CURIE\" , \"multicellds.collection\" : \"manubot.cite.curie.Handler_CURIE\" , \"multicellds.snapshot\" : \"manubot.cite.curie.Handler_CURIE\" , \"mw.project\" : \"manubot.cite.curie.Handler_CURIE\" , \"mw.study\" : \"manubot.cite.curie.Handler_CURIE\" , \"myco.lepra\" : \"manubot.cite.curie.Handler_CURIE\" , \"myco.marinum\" : \"manubot.cite.curie.Handler_CURIE\" , \"myco.smeg\" : \"manubot.cite.curie.Handler_CURIE\" , \"myco.tuber\" : \"manubot.cite.curie.Handler_CURIE\" , \"mycobank\" : \"manubot.cite.curie.Handler_CURIE\" , \"mzspec\" : \"manubot.cite.curie.Handler_CURIE\" , \"napdi\" : \"manubot.cite.curie.Handler_CURIE\" , \"napp\" : \"manubot.cite.curie.Handler_CURIE\" , \"narcis\" : \"manubot.cite.curie.Handler_CURIE\" , \"nasc\" : \"manubot.cite.curie.Handler_CURIE\" , \"nbn\" : \"manubot.cite.curie.Handler_CURIE\" , \"nbrc\" : \"manubot.cite.curie.Handler_CURIE\" , \"ncbigene\" : \"manubot.cite.curie.Handler_CURIE\" , \"ncbiprotein\" : \"manubot.cite.curie.Handler_CURIE\" , \"ncim\" : \"manubot.cite.curie.Handler_CURIE\" , \"ncit\" : \"manubot.cite.curie.Handler_CURIE\" , \"ndc\" : \"manubot.cite.curie.Handler_CURIE\" , \"nemo\" : \"manubot.cite.curie.Handler_CURIE\" , \"neurolex\" : \"manubot.cite.curie.Handler_CURIE\" , \"neuromorpho\" : \"manubot.cite.curie.Handler_CURIE\" , \"neurondb\" : \"manubot.cite.curie.Handler_CURIE\" , \"neurovault.collection\" : \"manubot.cite.curie.Handler_CURIE\" , \"neurovault.image\" : \"manubot.cite.curie.Handler_CURIE\" , \"nextdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"nextprot\" : \"manubot.cite.curie.Handler_CURIE\" , \"ngl\" : \"manubot.cite.curie.Handler_CURIE\" , \"niaest\" : \"manubot.cite.curie.Handler_CURIE\" , \"nmr\" : \"manubot.cite.curie.Handler_CURIE\" , \"nmrshiftdb2\" : \"manubot.cite.curie.Handler_CURIE\" , \"noncodev3\" : \"manubot.cite.curie.Handler_CURIE\" , \"noncodev4.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"noncodev4.rna\" : \"manubot.cite.curie.Handler_CURIE\" , \"norine\" : \"manubot.cite.curie.Handler_CURIE\" , \"nuclearbd\" : \"manubot.cite.curie.Handler_CURIE\" , \"obi\" : \"manubot.cite.curie.Handler_CURIE\" , \"occ\" : \"manubot.cite.curie.Handler_CURIE\" , \"oci\" : \"manubot.cite.curie.Handler_CURIE\" , \"ocid\" : \"manubot.cite.curie.Handler_CURIE\" , \"oclc\" : \"manubot.cite.curie.Handler_CURIE\" , \"odam\" : \"manubot.cite.curie.Handler_CURIE\" , \"odor\" : \"manubot.cite.curie.Handler_CURIE\" , \"oid\" : \"manubot.cite.curie.Handler_CURIE\" , \"oma.grp\" : \"manubot.cite.curie.Handler_CURIE\" , \"oma.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"omia\" : \"manubot.cite.curie.Handler_CURIE\" , \"omit\" : \"manubot.cite.curie.Handler_CURIE\" , \"opb\" : \"manubot.cite.curie.Handler_CURIE\" , \"opm\" : \"manubot.cite.curie.Handler_CURIE\" , \"orcid\" : \"manubot.cite.curie.Handler_CURIE\" , \"ordb\" : \"manubot.cite.curie.Handler_CURIE\" , \"oridb.sacch\" : \"manubot.cite.curie.Handler_CURIE\" , \"oridb.schizo\" : \"manubot.cite.curie.Handler_CURIE\" , \"orphanet\" : \"manubot.cite.curie.Handler_CURIE\" , \"orphanet.ordo\" : \"manubot.cite.curie.Handler_CURIE\" , \"orthodb\" : \"manubot.cite.curie.Handler_CURIE\" , \"oryzabase.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"oryzabase.mutant\" : \"manubot.cite.curie.Handler_CURIE\" , \"oryzabase.reference\" : \"manubot.cite.curie.Handler_CURIE\" , \"oryzabase.stage\" : \"manubot.cite.curie.Handler_CURIE\" , \"oryzabase.strain\" : \"manubot.cite.curie.Handler_CURIE\" , \"otl\" : \"manubot.cite.curie.Handler_CURIE\" , \"p3db.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"p3db.site\" : \"manubot.cite.curie.Handler_CURIE\" , \"paleodb\" : \"manubot.cite.curie.Handler_CURIE\" , \"panther.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"panther.node\" : \"manubot.cite.curie.Handler_CURIE\" , \"panther.pathway\" : \"manubot.cite.curie.Handler_CURIE\" , \"panther.pthcmp\" : \"manubot.cite.curie.Handler_CURIE\" , \"pass2\" : \"manubot.cite.curie.Handler_CURIE\" , \"pathwaycommons\" : \"manubot.cite.curie.Handler_CURIE\" , \"pato\" : \"manubot.cite.curie.Handler_CURIE\" , \"paxdb.organism\" : \"manubot.cite.curie.Handler_CURIE\" , \"paxdb.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"pazar\" : \"manubot.cite.curie.Handler_CURIE\" , \"pdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pdb-ccd\" : \"manubot.cite.curie.Handler_CURIE\" , \"pdb.ligand\" : \"manubot.cite.curie.Handler_CURIE\" , \"peptideatlas\" : \"manubot.cite.curie.Handler_CURIE\" , \"peptideatlas.dataset\" : \"manubot.cite.curie.Handler_CURIE\" , \"peroxibase\" : \"manubot.cite.curie.Handler_CURIE\" , \"pfam\" : \"manubot.cite.curie.Handler_CURIE\" , \"pgs\" : \"manubot.cite.curie.Handler_CURIE\" , \"pgx\" : \"manubot.cite.curie.Handler_CURIE\" , \"pharmgkb.disease\" : \"manubot.cite.curie.Handler_CURIE\" , \"pharmgkb.drug\" : \"manubot.cite.curie.Handler_CURIE\" , \"pharmgkb.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"pharmgkb.pathways\" : \"manubot.cite.curie.Handler_CURIE\" , \"phenolexplorer\" : \"manubot.cite.curie.Handler_CURIE\" , \"phosphopoint.kinase\" : \"manubot.cite.curie.Handler_CURIE\" , \"phosphopoint.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"phosphosite.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"phosphosite.residue\" : \"manubot.cite.curie.Handler_CURIE\" , \"phylomedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"phytozome.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"pid.pathway\" : \"manubot.cite.curie.Handler_CURIE\" , \"pigqtldb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pina\" : \"manubot.cite.curie.Handler_CURIE\" , \"piroplasma\" : \"manubot.cite.curie.Handler_CURIE\" , \"pirsf\" : \"manubot.cite.curie.Handler_CURIE\" , \"pkdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"planttfdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"plasmodb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pmap.cutdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pmap.substratedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pmc\" : \"manubot.cite.pubmed.Handler_PMC\" , \"pmcid\" : \"manubot.cite.pubmed.Handler_PMC\" , \"pmdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pmid\" : \"manubot.cite.pubmed.Handler_PubMed\" , \"pmp\" : \"manubot.cite.curie.Handler_CURIE\" , \"po\" : \"manubot.cite.curie.Handler_CURIE\" , \"pocketome\" : \"manubot.cite.curie.Handler_CURIE\" , \"polbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"pombase\" : \"manubot.cite.curie.Handler_CURIE\" , \"pr\" : \"manubot.cite.curie.Handler_CURIE\" , \"pride\" : \"manubot.cite.curie.Handler_CURIE\" , \"pride.project\" : \"manubot.cite.curie.Handler_CURIE\" , \"prints\" : \"manubot.cite.curie.Handler_CURIE\" , \"probonto\" : \"manubot.cite.curie.Handler_CURIE\" , \"prodom\" : \"manubot.cite.curie.Handler_CURIE\" , \"proglyc\" : \"manubot.cite.curie.Handler_CURIE\" , \"prosite\" : \"manubot.cite.curie.Handler_CURIE\" , \"protclustdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"proteomicsdb.peptide\" : \"manubot.cite.curie.Handler_CURIE\" , \"proteomicsdb.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"protonet.cluster\" : \"manubot.cite.curie.Handler_CURIE\" , \"protonet.proteincard\" : \"manubot.cite.curie.Handler_CURIE\" , \"pscdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pseudomonas\" : \"manubot.cite.curie.Handler_CURIE\" , \"psipar\" : \"manubot.cite.curie.Handler_CURIE\" , \"pubchem.bioassay\" : \"manubot.cite.curie.Handler_CURIE\" , \"pubchem.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"pubchem.substance\" : \"manubot.cite.curie.Handler_CURIE\" , \"pubmed\" : \"manubot.cite.pubmed.Handler_PubMed\" , \"pw\" : \"manubot.cite.curie.Handler_CURIE\" , \"px\" : \"manubot.cite.curie.Handler_CURIE\" , \"rapdb.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"rapdb.transcript\" : \"manubot.cite.curie.Handler_CURIE\" , \"rbk\" : \"manubot.cite.curie.Handler_CURIE\" , \"reactome\" : \"manubot.cite.curie.Handler_CURIE\" , \"rebase\" : \"manubot.cite.curie.Handler_CURIE\" , \"refseq\" : \"manubot.cite.curie.Handler_CURIE\" , \"resid\" : \"manubot.cite.curie.Handler_CURIE\" , \"rfam\" : \"manubot.cite.curie.Handler_CURIE\" , \"rgd\" : \"manubot.cite.curie.Handler_CURIE\" , \"rgd.qtl\" : \"manubot.cite.curie.Handler_CURIE\" , \"rgd.strain\" : \"manubot.cite.curie.Handler_CURIE\" , \"rhea\" : \"manubot.cite.curie.Handler_CURIE\" , \"ricegap\" : \"manubot.cite.curie.Handler_CURIE\" , \"ricenetdb.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"ricenetdb.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"ricenetdb.mirna\" : \"manubot.cite.curie.Handler_CURIE\" , \"ricenetdb.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"ricenetdb.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"rnacentral\" : \"manubot.cite.curie.Handler_CURIE\" , \"rnamods\" : \"manubot.cite.curie.Handler_CURIE\" , \"ro\" : \"manubot.cite.curie.Handler_CURIE\" , \"rouge\" : \"manubot.cite.curie.Handler_CURIE\" , \"rrid\" : \"manubot.cite.curie.Handler_CURIE\" , \"sabiork.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"sabiork.ec\" : \"manubot.cite.curie.Handler_CURIE\" , \"sabiork.kineticrecord\" : \"manubot.cite.curie.Handler_CURIE\" , \"sabiork.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"sasbdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"sbo\" : \"manubot.cite.curie.Handler_CURIE\" , \"scop\" : \"manubot.cite.curie.Handler_CURIE\" , \"scretf\" : \"manubot.cite.curie.Handler_CURIE\" , \"sdbs\" : \"manubot.cite.curie.Handler_CURIE\" , \"seed\" : \"manubot.cite.curie.Handler_CURIE\" , \"seed.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"seed.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"sgd\" : \"manubot.cite.curie.Handler_CURIE\" , \"sgd.pathways\" : \"manubot.cite.curie.Handler_CURIE\" , \"sgn\" : \"manubot.cite.curie.Handler_CURIE\" , \"sheepqtldb\" : \"manubot.cite.curie.Handler_CURIE\" , \"shortdoi\" : \"manubot.cite.doi.Handler_DOI\" , \"sider.drug\" : \"manubot.cite.curie.Handler_CURIE\" , \"sider.effect\" : \"manubot.cite.curie.Handler_CURIE\" , \"signaling-gateway\" : \"manubot.cite.curie.Handler_CURIE\" , \"sisu\" : \"manubot.cite.curie.Handler_CURIE\" , \"sitex\" : \"manubot.cite.curie.Handler_CURIE\" , \"slm\" : \"manubot.cite.curie.Handler_CURIE\" , \"smart\" : \"manubot.cite.curie.Handler_CURIE\" , \"smpdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"snomedct\" : \"manubot.cite.curie.Handler_CURIE\" , \"snp2tfbs\" : \"manubot.cite.curie.Handler_CURIE\" , \"so\" : \"manubot.cite.curie.Handler_CURIE\" , \"soybase\" : \"manubot.cite.curie.Handler_CURIE\" , \"spdx\" : \"manubot.cite.curie.Handler_CURIE\" , \"spike.map\" : \"manubot.cite.curie.Handler_CURIE\" , \"splash\" : \"manubot.cite.curie.Handler_CURIE\" , \"spp\" : \"manubot.cite.curie.Handler_CURIE\" , \"stap\" : \"manubot.cite.curie.Handler_CURIE\" , \"stitch\" : \"manubot.cite.curie.Handler_CURIE\" , \"storedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"string\" : \"manubot.cite.curie.Handler_CURIE\" , \"subtilist\" : \"manubot.cite.curie.Handler_CURIE\" , \"subtiwiki\" : \"manubot.cite.curie.Handler_CURIE\" , \"sugarbind\" : \"manubot.cite.curie.Handler_CURIE\" , \"supfam\" : \"manubot.cite.curie.Handler_CURIE\" , \"swh\" : \"manubot.cite.curie.Handler_CURIE\" , \"swiss-model\" : \"manubot.cite.curie.Handler_CURIE\" , \"swissregulon\" : \"manubot.cite.curie.Handler_CURIE\" , \"t3db\" : \"manubot.cite.curie.Handler_CURIE\" , \"tair.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"tair.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"tair.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"tarbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"taxonomy\" : \"manubot.cite.curie.Handler_CURIE\" , \"tcdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"tgd\" : \"manubot.cite.curie.Handler_CURIE\" , \"tigrfam\" : \"manubot.cite.curie.Handler_CURIE\" , \"tissuelist\" : \"manubot.cite.curie.Handler_CURIE\" , \"tol\" : \"manubot.cite.curie.Handler_CURIE\" , \"topdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"topfind\" : \"manubot.cite.curie.Handler_CURIE\" , \"toxoplasma\" : \"manubot.cite.curie.Handler_CURIE\" , \"transyt\" : \"manubot.cite.curie.Handler_CURIE\" , \"treebase\" : \"manubot.cite.curie.Handler_CURIE\" , \"treefam\" : \"manubot.cite.curie.Handler_CURIE\" , \"trichdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"tritrypdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"ttd.drug\" : \"manubot.cite.curie.Handler_CURIE\" , \"ttd.target\" : \"manubot.cite.curie.Handler_CURIE\" , \"uberon\" : \"manubot.cite.curie.Handler_CURIE\" , \"ubio.namebank\" : \"manubot.cite.curie.Handler_CURIE\" , \"umbbd.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"umbbd.enzyme\" : \"manubot.cite.curie.Handler_CURIE\" , \"umbbd.pathway\" : \"manubot.cite.curie.Handler_CURIE\" , \"umbbd.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"umbbd.rule\" : \"manubot.cite.curie.Handler_CURIE\" , \"umls\" : \"manubot.cite.curie.Handler_CURIE\" , \"unigene\" : \"manubot.cite.curie.Handler_CURIE\" , \"unii\" : \"manubot.cite.curie.Handler_CURIE\" , \"unimod\" : \"manubot.cite.curie.Handler_CURIE\" , \"uniparc\" : \"manubot.cite.curie.Handler_CURIE\" , \"unipathway.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"unipathway.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"uniprot\" : \"manubot.cite.curie.Handler_CURIE\" , \"uniprot.chain\" : \"manubot.cite.curie.Handler_CURIE\" , \"uniprot.isoform\" : \"manubot.cite.curie.Handler_CURIE\" , \"unists\" : \"manubot.cite.curie.Handler_CURIE\" , \"unite\" : \"manubot.cite.curie.Handler_CURIE\" , \"uo\" : \"manubot.cite.curie.Handler_CURIE\" , \"url\" : \"manubot.cite.url.Handler_URL\" , \"uspto\" : \"manubot.cite.curie.Handler_CURIE\" , \"validatordb\" : \"manubot.cite.curie.Handler_CURIE\" , \"vario\" : \"manubot.cite.curie.Handler_CURIE\" , \"vbase2\" : \"manubot.cite.curie.Handler_CURIE\" , \"vbrc\" : \"manubot.cite.curie.Handler_CURIE\" , \"vectorbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"vegbank\" : \"manubot.cite.curie.Handler_CURIE\" , \"vfb\" : \"manubot.cite.curie.Handler_CURIE\" , \"vfdb.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"vfdb.genus\" : \"manubot.cite.curie.Handler_CURIE\" , \"vgnc\" : \"manubot.cite.curie.Handler_CURIE\" , \"viaf\" : \"manubot.cite.curie.Handler_CURIE\" , \"vipr\" : \"manubot.cite.curie.Handler_CURIE\" , \"viralzone\" : \"manubot.cite.curie.Handler_CURIE\" , \"virsirna\" : \"manubot.cite.curie.Handler_CURIE\" , \"vmhmetabolite\" : \"manubot.cite.curie.Handler_CURIE\" , \"vmhreaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"wb\" : \"manubot.cite.curie.Handler_CURIE\" , \"wb.rnai\" : \"manubot.cite.curie.Handler_CURIE\" , \"wikidata\" : \"manubot.cite.wikidata.Handler_Wikidata\" , \"wikigenes\" : \"manubot.cite.curie.Handler_CURIE\" , \"wikipathways\" : \"manubot.cite.curie.Handler_CURIE\" , \"wikipedia.en\" : \"manubot.cite.curie.Handler_CURIE\" , \"worfdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"wormpep\" : \"manubot.cite.curie.Handler_CURIE\" , \"worms\" : \"manubot.cite.curie.Handler_CURIE\" , \"xenbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"ydpm\" : \"manubot.cite.curie.Handler_CURIE\" , \"yeastintron\" : \"manubot.cite.curie.Handler_CURIE\" , \"yetfasco\" : \"manubot.cite.curie.Handler_CURIE\" , \"yid\" : \"manubot.cite.curie.Handler_CURIE\" , \"yrcpdr\" : \"manubot.cite.curie.Handler_CURIE\" , \"zfin\" : \"manubot.cite.curie.Handler_CURIE\" , \"zinc\" : \"manubot.cite.curie.Handler_CURIE\" , } Variables prefix_to_handler Functions get_handler def get_handler ( prefix_lower : str ) -> 'Handler' View Source @functools . lru_cache ( maxsize = 10 _000 ) def get_handler ( prefix_lower : str ) -> \"Handler\" : if not isinstance ( prefix_lower , str ) : raise TypeError ( f \"prefix_lower should be a str, instead received {prefix_lower.__class__.__name__}\" ) assert prefix_lower == prefix_lower . lower () handler = prefix_to_handler [ prefix_lower ] handler = import_function ( handler )( prefix_lower ) return handler infer_prefix def infer_prefix ( dealiased_id : str ) -> Optional [ str ] Infer the prefix for citekey by matching it against a sequence of regexes. If a match is found, return the coressponding standard prefix. Otherwise, return None. View Source def infer_prefix ( dealiased_id : str ) -> Optional [ str ] : \"\"\" Infer the prefix for citekey by matching it against a sequence of regexes. If a match is found, return the coressponding standard prefix. Otherwise, return None. \"\"\" for prefix , pattern_attrib in _infer_prefix_patterns : handler = get_handler ( prefix ) pattern = handler . _get_pattern ( attribute = pattern_attrib ) if pattern . fullmatch ( dealiased_id ) : return handler . standard_prefix return None Classes Handler class Handler ( prefix_lower : str ) View Source @dataclasses . dataclass class Handler : \"\"\" A Handler is a class that provides support for a certain type of citekey. For example, a Handler subclass could provide support for DOI citekeys. Subclasses enable custom logic for different citekey prefixes, including how to standardize the citekey and how to retrieve CSL Item metadata. \"\"\" prefix_lower : str prefixes = [] def _get_pattern ( self , attribute = \"accession_pattern\" ) -> Pattern : \"\"\" Return a compiled regex pattern stored by `attribute`. By default, return `self.accession_pattern`, which Handler subclasses can set to provide the expected pattern for `self.accession`. \"\"\" # todo : consider caching compilation pattern = getattr ( self , attribute , None ) if not pattern : return None if not isinstance ( pattern , Pattern ) : pattern = re . compile ( pattern ) return pattern def inspect ( self , citekey : CiteKey ) -> Optional [ str ] : \"\"\" Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. \"\"\" pattern = self . _get_pattern ( \"accession_pattern\" ) if not pattern : return None if not pattern . fullmatch ( citekey . accession ) : return f \"{citekey.accession} does not match regex {pattern.pattern}\" def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str, str ] : \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession @abc . abstractmethod def get_csl_item ( self , citekey ) -> Dict [ str, Any ] : \"\"\" Return a CSL_Item with bibliographic details for citekey. \"\"\" raise NotImplementedError ( f \"Manubot does not know how to generate a csl_item for {citekey.standard_id!r}\" ) Descendants manubot.cite.arxiv.Handler_arXiv manubot.cite.curie.Handler_CURIE manubot.cite.pubmed.Handler_PubMed manubot.cite.pubmed.Handler_PMC manubot.cite.doi.Handler_DOI manubot.cite.isbn.Handler_ISBN manubot.cite.url.Handler_URL manubot.cite.wikidata.Handler_Wikidata Class variables prefixes Methods get_csl_item def get_csl_item ( self , citekey ) -> Dict [ str , Any ] Return a CSL_Item with bibliographic details for citekey. View Source @abc . abstractmethod def get_csl_item ( self , citekey ) -> Dict [ str, Any ] : \"\"\" Return a CSL_Item with bibliographic details for citekey. \"\"\" raise NotImplementedError ( f \"Manubot does not know how to generate a csl_item for {citekey.standard_id!r}\" ) inspect def inspect ( self , citekey : manubot . cite . citekey . CiteKey ) -> Optional [ str ] Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey : CiteKey ) -> Optional [ str ] : \"\"\" Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. \"\"\" pattern = self . _get_pattern ( \"accession_pattern\" ) if not pattern : return None if not pattern . fullmatch ( citekey . accession ) : return f \"{citekey.accession} does not match regex {pattern.pattern}\" standardize_prefix_accession def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ] Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ] : \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession","title":"Handlers"},{"location":"reference/manubot/cite/handlers/#module-manubotcitehandlers","text":"None None View Source import abc import dataclasses import functools import re from typing import Any , Dict , List , Optional , Pattern , Set , Tuple from manubot.util import import_function from .citekey import CiteKey \"\"\" Non-citation prefixes used by the pandoc-xnos suite of Pandoc filters, including pandoc-fignos, pandoc-tablenos, and pandoc-eqnos. \"\"\" _pandoc_xnos_prefixes : Set [ str ] = { \"fig\" , \"tbl\" , \"eq\" } _local_handlers : List [ str ] = [ \"manubot.cite.arxiv.Handler_arXiv\" , \"manubot.cite.doi.Handler_DOI\" , \"manubot.cite.isbn.Handler_ISBN\" , \"manubot.cite.pubmed.Handler_PMC\" , \"manubot.cite.pubmed.Handler_PubMed\" , \"manubot.cite.url.Handler_URL\" , \"manubot.cite.wikidata.Handler_Wikidata\" , ] _infer_prefix_patterns : List [ Tuple [ str , str ]] = [ ( \"doi\" , \"accession_pattern\" ), ( \"doi\" , \"shortdoi_pattern\" ), ( \"pmc\" , \"accession_pattern\" ), ( \"pubmed\" , \"accession_pattern\" ), ( \"wikidata\" , \"accession_pattern\" ), ( \"arxiv\" , \"accession_pattern\" ), ] \"\"\" Each list element is a tuple of (handler, pattern attribute). Handler is the handler prefix (lowercase). Pattern attribute refers to the Handler attribute containing a regex pattern. A citekey prefix is inferred from the first matching pattern in this list. \"\"\" def infer_prefix ( dealiased_id : str ) -> Optional [ str ]: \"\"\" Infer the prefix for citekey by matching it against a sequence of regexes. If a match is found, return the coressponding standard prefix. Otherwise, return None. \"\"\" for prefix , pattern_attrib in _infer_prefix_patterns : handler = get_handler ( prefix ) pattern = handler . _get_pattern ( attribute = pattern_attrib ) if pattern . fullmatch ( dealiased_id ): return handler . standard_prefix return None @functools . lru_cache ( maxsize = 10_000 ) def get_handler ( prefix_lower : str ) -> \"Handler\" : if not isinstance ( prefix_lower , str ): raise TypeError ( f \"prefix_lower should be a str, instead received { prefix_lower . __class__ . __name__ } \" ) assert prefix_lower == prefix_lower . lower () handler = prefix_to_handler [ prefix_lower ] handler = import_function ( handler )( prefix_lower ) return handler def _generate_prefix_to_handler () -> Dict [ str , str ]: \"\"\" Generate complete dictionary for prefix_to_handler. \"\"\" import inspect from .curie import get_curie_handlers curie_handlers = get_curie_handlers () pth = {} for handler in curie_handlers + _local_handlers : if isinstance ( handler , str ): handler = import_function ( handler )( \"dummy_prefix_lower\" ) for prefix in handler . prefixes : pth [ prefix ] = f \" { inspect . getmodule ( handler ) . __name__ } . { handler . __class__ . __name__ } \" pth = dict ( sorted ( pth . items ())) # sort for clean diffs of serialized dict return pth @dataclasses . dataclass class Handler : \"\"\" A Handler is a class that provides support for a certain type of citekey. For example, a Handler subclass could provide support for DOI citekeys. Subclasses enable custom logic for different citekey prefixes, including how to standardize the citekey and how to retrieve CSL Item metadata. \"\"\" prefix_lower : str prefixes = [] def _get_pattern ( self , attribute = \"accession_pattern\" ) -> Pattern : \"\"\" Return a compiled regex pattern stored by `attribute`. By default, return `self.accession_pattern`, which Handler subclasses can set to provide the expected pattern for `self.accession`. \"\"\" # todo: consider caching compilation pattern = getattr ( self , attribute , None ) if not pattern : return None if not isinstance ( pattern , Pattern ): pattern = re . compile ( pattern ) return pattern def inspect ( self , citekey : CiteKey ) -> Optional [ str ]: \"\"\" Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. \"\"\" pattern = self . _get_pattern ( \"accession_pattern\" ) if not pattern : return None if not pattern . fullmatch ( citekey . accession ): return f \" { citekey . accession } does not match regex { pattern . pattern } \" def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ]: \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession @abc . abstractmethod def get_csl_item ( self , citekey ) -> Dict [ str , Any ]: \"\"\" Return a CSL_Item with bibliographic details for citekey. \"\"\" raise NotImplementedError ( f \"Manubot does not know how to generate a csl_item for { citekey . standard_id !r} \" ) \"\"\" Mapping from lowercase prefix to Handler class function location as string. This output is automatically generated using `_generate_prefix_to_handler`. Hardcoding this mapping reduces startup time and helps keep imports to a minimum, allowing installations without the full dependencies to function. \"\"\" prefix_to_handler : Dict [ str , str ] = { \"3dmet\" : \"manubot.cite.curie.Handler_CURIE\" , \"abs\" : \"manubot.cite.curie.Handler_CURIE\" , \"aceview.worm\" : \"manubot.cite.curie.Handler_CURIE\" , \"addgene\" : \"manubot.cite.curie.Handler_CURIE\" , \"adw\" : \"manubot.cite.curie.Handler_CURIE\" , \"affy.probeset\" : \"manubot.cite.curie.Handler_CURIE\" , \"aftol.taxonomy\" : \"manubot.cite.curie.Handler_CURIE\" , \"agricola\" : \"manubot.cite.curie.Handler_CURIE\" , \"allergome\" : \"manubot.cite.curie.Handler_CURIE\" , \"amoebadb\" : \"manubot.cite.curie.Handler_CURIE\" , \"antibodyregistry\" : \"manubot.cite.curie.Handler_CURIE\" , \"antweb\" : \"manubot.cite.curie.Handler_CURIE\" , \"aop\" : \"manubot.cite.curie.Handler_CURIE\" , \"aop.events\" : \"manubot.cite.curie.Handler_CURIE\" , \"aop.relationships\" : \"manubot.cite.curie.Handler_CURIE\" , \"aop.stressor\" : \"manubot.cite.curie.Handler_CURIE\" , \"apd\" : \"manubot.cite.curie.Handler_CURIE\" , \"aphidbase.transcript\" : \"manubot.cite.curie.Handler_CURIE\" , \"apid.interactions\" : \"manubot.cite.curie.Handler_CURIE\" , \"arachnoserver\" : \"manubot.cite.curie.Handler_CURIE\" , \"ardb\" : \"manubot.cite.curie.Handler_CURIE\" , \"ark\" : \"manubot.cite.curie.Handler_CURIE\" , \"arrayexpress\" : \"manubot.cite.curie.Handler_CURIE\" , \"arrayexpress.platform\" : \"manubot.cite.curie.Handler_CURIE\" , \"arraymap\" : \"manubot.cite.curie.Handler_CURIE\" , \"arxiv\" : \"manubot.cite.arxiv.Handler_arXiv\" , \"asap\" : \"manubot.cite.curie.Handler_CURIE\" , \"ascl\" : \"manubot.cite.curie.Handler_CURIE\" , \"asin\" : \"manubot.cite.curie.Handler_CURIE\" , \"aspgd.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"aspgd.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"atc\" : \"manubot.cite.curie.Handler_CURIE\" , \"atcc\" : \"manubot.cite.curie.Handler_CURIE\" , \"atcvet\" : \"manubot.cite.curie.Handler_CURIE\" , \"atfdb.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"autdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"bacdive\" : \"manubot.cite.curie.Handler_CURIE\" , \"bacmap.biog\" : \"manubot.cite.curie.Handler_CURIE\" , \"bacmap.map\" : \"manubot.cite.curie.Handler_CURIE\" , \"bao\" : \"manubot.cite.curie.Handler_CURIE\" , \"bdgp.est\" : \"manubot.cite.curie.Handler_CURIE\" , \"bdgp.insertion\" : \"manubot.cite.curie.Handler_CURIE\" , \"bdsc\" : \"manubot.cite.curie.Handler_CURIE\" , \"beetlebase\" : \"manubot.cite.curie.Handler_CURIE\" , \"begdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"bgee.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"bgee.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"bgee.organ\" : \"manubot.cite.curie.Handler_CURIE\" , \"bgee.stage\" : \"manubot.cite.curie.Handler_CURIE\" , \"bigg.compartment\" : \"manubot.cite.curie.Handler_CURIE\" , \"bigg.metabolite\" : \"manubot.cite.curie.Handler_CURIE\" , \"bigg.model\" : \"manubot.cite.curie.Handler_CURIE\" , \"bigg.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"bindingdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"biocarta.pathway\" : \"manubot.cite.curie.Handler_CURIE\" , \"biocatalogue.service\" : \"manubot.cite.curie.Handler_CURIE\" , \"biocyc\" : \"manubot.cite.curie.Handler_CURIE\" , \"biogrid\" : \"manubot.cite.curie.Handler_CURIE\" , \"biolink\" : \"manubot.cite.curie.Handler_CURIE\" , \"biominder\" : \"manubot.cite.curie.Handler_CURIE\" , \"biomodels.db\" : \"manubot.cite.curie.Handler_CURIE\" , \"biomodels.kisao\" : \"manubot.cite.curie.Handler_CURIE\" , \"biomodels.teddy\" : \"manubot.cite.curie.Handler_CURIE\" , \"biomodels.vocabulary\" : \"manubot.cite.curie.Handler_CURIE\" , \"bionumbers\" : \"manubot.cite.curie.Handler_CURIE\" , \"bioportal\" : \"manubot.cite.curie.Handler_CURIE\" , \"bioproject\" : \"manubot.cite.curie.Handler_CURIE\" , \"biosample\" : \"manubot.cite.curie.Handler_CURIE\" , \"biosimulators\" : \"manubot.cite.curie.Handler_CURIE\" , \"biostudies\" : \"manubot.cite.curie.Handler_CURIE\" , \"biosystems\" : \"manubot.cite.curie.Handler_CURIE\" , \"biotools\" : \"manubot.cite.curie.Handler_CURIE\" , \"bitterdb.cpd\" : \"manubot.cite.curie.Handler_CURIE\" , \"bitterdb.rec\" : \"manubot.cite.curie.Handler_CURIE\" , \"bmrb\" : \"manubot.cite.curie.Handler_CURIE\" , \"bold.taxonomy\" : \"manubot.cite.curie.Handler_CURIE\" , \"brenda\" : \"manubot.cite.curie.Handler_CURIE\" , \"broad\" : \"manubot.cite.curie.Handler_CURIE\" , \"bto\" : \"manubot.cite.curie.Handler_CURIE\" , \"bugbase.expt\" : \"manubot.cite.curie.Handler_CURIE\" , \"bugbase.protocol\" : \"manubot.cite.curie.Handler_CURIE\" , \"bykdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"cabri\" : \"manubot.cite.curie.Handler_CURIE\" , \"cadsr\" : \"manubot.cite.curie.Handler_CURIE\" , \"cameo\" : \"manubot.cite.curie.Handler_CURIE\" , \"caps\" : \"manubot.cite.curie.Handler_CURIE\" , \"cas\" : \"manubot.cite.curie.Handler_CURIE\" , \"cath\" : \"manubot.cite.curie.Handler_CURIE\" , \"cath.domain\" : \"manubot.cite.curie.Handler_CURIE\" , \"cath.superfamily\" : \"manubot.cite.curie.Handler_CURIE\" , \"cattleqtldb\" : \"manubot.cite.curie.Handler_CURIE\" , \"cazy\" : \"manubot.cite.curie.Handler_CURIE\" , \"cbioportal\" : \"manubot.cite.curie.Handler_CURIE\" , \"ccds\" : \"manubot.cite.curie.Handler_CURIE\" , \"cco\" : \"manubot.cite.curie.Handler_CURIE\" , \"cdd\" : \"manubot.cite.curie.Handler_CURIE\" , \"cdpd\" : \"manubot.cite.curie.Handler_CURIE\" , \"cellimage\" : \"manubot.cite.curie.Handler_CURIE\" , \"cellosaurus\" : \"manubot.cite.curie.Handler_CURIE\" , \"cgd\" : \"manubot.cite.curie.Handler_CURIE\" , \"cgsc\" : \"manubot.cite.curie.Handler_CURIE\" , \"charprot\" : \"manubot.cite.curie.Handler_CURIE\" , \"chebi\" : \"manubot.cite.curie.Handler_CURIE\" , \"chembl.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"chembl.target\" : \"manubot.cite.curie.Handler_CURIE\" , \"chemdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"chemidplus\" : \"manubot.cite.curie.Handler_CURIE\" , \"chemspider\" : \"manubot.cite.curie.Handler_CURIE\" , \"chickenqtldb\" : \"manubot.cite.curie.Handler_CURIE\" , \"cl\" : \"manubot.cite.curie.Handler_CURIE\" , \"classyfire\" : \"manubot.cite.curie.Handler_CURIE\" , \"cldb\" : \"manubot.cite.curie.Handler_CURIE\" , \"clinicaltrials\" : \"manubot.cite.curie.Handler_CURIE\" , \"clinvar\" : \"manubot.cite.curie.Handler_CURIE\" , \"clinvar.record\" : \"manubot.cite.curie.Handler_CURIE\" , \"clinvar.submission\" : \"manubot.cite.curie.Handler_CURIE\" , \"combine.specifications\" : \"manubot.cite.curie.Handler_CURIE\" , \"complexportal\" : \"manubot.cite.curie.Handler_CURIE\" , \"comptox\" : \"manubot.cite.curie.Handler_CURIE\" , \"compulyeast\" : \"manubot.cite.curie.Handler_CURIE\" , \"conoserver\" : \"manubot.cite.curie.Handler_CURIE\" , \"coriell\" : \"manubot.cite.curie.Handler_CURIE\" , \"corum\" : \"manubot.cite.curie.Handler_CURIE\" , \"cosmic\" : \"manubot.cite.curie.Handler_CURIE\" , \"cpc\" : \"manubot.cite.curie.Handler_CURIE\" , \"crisprdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"cryptodb\" : \"manubot.cite.curie.Handler_CURIE\" , \"csa\" : \"manubot.cite.curie.Handler_CURIE\" , \"cst\" : \"manubot.cite.curie.Handler_CURIE\" , \"cst.ab\" : \"manubot.cite.curie.Handler_CURIE\" , \"ctd.chemical\" : \"manubot.cite.curie.Handler_CURIE\" , \"ctd.disease\" : \"manubot.cite.curie.Handler_CURIE\" , \"ctd.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"cubedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"d1id\" : \"manubot.cite.curie.Handler_CURIE\" , \"dailymed\" : \"manubot.cite.curie.Handler_CURIE\" , \"dandi\" : \"manubot.cite.curie.Handler_CURIE\" , \"darc\" : \"manubot.cite.curie.Handler_CURIE\" , \"dashr\" : \"manubot.cite.curie.Handler_CURIE\" , \"dashr.expression\" : \"manubot.cite.curie.Handler_CURIE\" , \"datanator.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"datanator.metabolite\" : \"manubot.cite.curie.Handler_CURIE\" , \"datf\" : \"manubot.cite.curie.Handler_CURIE\" , \"dbd\" : \"manubot.cite.curie.Handler_CURIE\" , \"dbest\" : \"manubot.cite.curie.Handler_CURIE\" , \"dbg2introns\" : \"manubot.cite.curie.Handler_CURIE\" , \"dbgap\" : \"manubot.cite.curie.Handler_CURIE\" , \"dbprobe\" : \"manubot.cite.curie.Handler_CURIE\" , \"dbsnp\" : \"manubot.cite.curie.Handler_CURIE\" , \"degradome\" : \"manubot.cite.curie.Handler_CURIE\" , \"depod\" : \"manubot.cite.curie.Handler_CURIE\" , \"dev.ga4ghdos\" : \"manubot.cite.curie.Handler_CURIE\" , \"dictybase.est\" : \"manubot.cite.curie.Handler_CURIE\" , \"dictybase.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"did\" : \"manubot.cite.curie.Handler_CURIE\" , \"dip\" : \"manubot.cite.curie.Handler_CURIE\" , \"disprot\" : \"manubot.cite.curie.Handler_CURIE\" , \"disprot.region\" : \"manubot.cite.curie.Handler_CURIE\" , \"dlxb\" : \"manubot.cite.curie.Handler_CURIE\" , \"dlxc\" : \"manubot.cite.curie.Handler_CURIE\" , \"doi\" : \"manubot.cite.doi.Handler_DOI\" , \"doid\" : \"manubot.cite.curie.Handler_CURIE\" , \"dommino\" : \"manubot.cite.curie.Handler_CURIE\" , \"door\" : \"manubot.cite.curie.Handler_CURIE\" , \"doqcs.model\" : \"manubot.cite.curie.Handler_CURIE\" , \"doqcs.pathway\" : \"manubot.cite.curie.Handler_CURIE\" , \"dpv\" : \"manubot.cite.curie.Handler_CURIE\" , \"dragondb.allele\" : \"manubot.cite.curie.Handler_CURIE\" , \"dragondb.dna\" : \"manubot.cite.curie.Handler_CURIE\" , \"dragondb.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"dragondb.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"drsc\" : \"manubot.cite.curie.Handler_CURIE\" , \"drugbank\" : \"manubot.cite.curie.Handler_CURIE\" , \"drugbankv4.target\" : \"manubot.cite.curie.Handler_CURIE\" , \"ec-code\" : \"manubot.cite.curie.Handler_CURIE\" , \"echobase\" : \"manubot.cite.curie.Handler_CURIE\" , \"eco\" : \"manubot.cite.curie.Handler_CURIE\" , \"ecogene\" : \"manubot.cite.curie.Handler_CURIE\" , \"ecoliwiki\" : \"manubot.cite.curie.Handler_CURIE\" , \"ecyano.entity\" : \"manubot.cite.curie.Handler_CURIE\" , \"ecyano.experiment\" : \"manubot.cite.curie.Handler_CURIE\" , \"ecyano.model\" : \"manubot.cite.curie.Handler_CURIE\" , \"ecyano.rule\" : \"manubot.cite.curie.Handler_CURIE\" , \"edam\" : \"manubot.cite.curie.Handler_CURIE\" , \"efo\" : \"manubot.cite.curie.Handler_CURIE\" , \"ega.dataset\" : \"manubot.cite.curie.Handler_CURIE\" , \"ega.study\" : \"manubot.cite.curie.Handler_CURIE\" , \"eggnog\" : \"manubot.cite.curie.Handler_CURIE\" , \"elm\" : \"manubot.cite.curie.Handler_CURIE\" , \"emdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"ena.embl\" : \"manubot.cite.curie.Handler_CURIE\" , \"encode\" : \"manubot.cite.curie.Handler_CURIE\" , \"ensembl\" : \"manubot.cite.curie.Handler_CURIE\" , \"ensembl.bacteria\" : \"manubot.cite.curie.Handler_CURIE\" , \"ensembl.fungi\" : \"manubot.cite.curie.Handler_CURIE\" , \"ensembl.metazoa\" : \"manubot.cite.curie.Handler_CURIE\" , \"ensembl.plant\" : \"manubot.cite.curie.Handler_CURIE\" , \"ensembl.protist\" : \"manubot.cite.curie.Handler_CURIE\" , \"envipath\" : \"manubot.cite.curie.Handler_CURIE\" , \"envo\" : \"manubot.cite.curie.Handler_CURIE\" , \"eo\" : \"manubot.cite.curie.Handler_CURIE\" , \"epd\" : \"manubot.cite.curie.Handler_CURIE\" , \"erm\" : \"manubot.cite.curie.Handler_CURIE\" , \"erv\" : \"manubot.cite.curie.Handler_CURIE\" , \"eu89h\" : \"manubot.cite.curie.Handler_CURIE\" , \"euclinicaltrials\" : \"manubot.cite.curie.Handler_CURIE\" , \"exac.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"exac.transcript\" : \"manubot.cite.curie.Handler_CURIE\" , \"exac.variant\" : \"manubot.cite.curie.Handler_CURIE\" , \"facebase\" : \"manubot.cite.curie.Handler_CURIE\" , \"fairsharing\" : \"manubot.cite.curie.Handler_CURIE\" , \"fb\" : \"manubot.cite.curie.Handler_CURIE\" , \"fbol\" : \"manubot.cite.curie.Handler_CURIE\" , \"flowrepository\" : \"manubot.cite.curie.Handler_CURIE\" , \"fma\" : \"manubot.cite.curie.Handler_CURIE\" , \"foodb.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"foodon\" : \"manubot.cite.curie.Handler_CURIE\" , \"fplx\" : \"manubot.cite.curie.Handler_CURIE\" , \"fsnp\" : \"manubot.cite.curie.Handler_CURIE\" , \"funcbase.fly\" : \"manubot.cite.curie.Handler_CURIE\" , \"funcbase.human\" : \"manubot.cite.curie.Handler_CURIE\" , \"funcbase.mouse\" : \"manubot.cite.curie.Handler_CURIE\" , \"funcbase.yeast\" : \"manubot.cite.curie.Handler_CURIE\" , \"fungidb\" : \"manubot.cite.curie.Handler_CURIE\" , \"ga4ghdos\" : \"manubot.cite.curie.Handler_CURIE\" , \"gabi\" : \"manubot.cite.curie.Handler_CURIE\" , \"gcst\" : \"manubot.cite.curie.Handler_CURIE\" , \"gdc\" : \"manubot.cite.curie.Handler_CURIE\" , \"gdsc\" : \"manubot.cite.curie.Handler_CURIE\" , \"genatlas\" : \"manubot.cite.curie.Handler_CURIE\" , \"genecards\" : \"manubot.cite.curie.Handler_CURIE\" , \"genedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"genefarm\" : \"manubot.cite.curie.Handler_CURIE\" , \"genetree\" : \"manubot.cite.curie.Handler_CURIE\" , \"genewiki\" : \"manubot.cite.curie.Handler_CURIE\" , \"genpept\" : \"manubot.cite.curie.Handler_CURIE\" , \"genprop\" : \"manubot.cite.curie.Handler_CURIE\" , \"geo\" : \"manubot.cite.curie.Handler_CURIE\" , \"giardiadb\" : \"manubot.cite.curie.Handler_CURIE\" , \"glida.gpcr\" : \"manubot.cite.curie.Handler_CURIE\" , \"glida.ligand\" : \"manubot.cite.curie.Handler_CURIE\" , \"glycoepitope\" : \"manubot.cite.curie.Handler_CURIE\" , \"glycomedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"glycopost\" : \"manubot.cite.curie.Handler_CURIE\" , \"glytoucan\" : \"manubot.cite.curie.Handler_CURIE\" , \"gmd\" : \"manubot.cite.curie.Handler_CURIE\" , \"gmd.analyte\" : \"manubot.cite.curie.Handler_CURIE\" , \"gmd.gcms\" : \"manubot.cite.curie.Handler_CURIE\" , \"gmd.profile\" : \"manubot.cite.curie.Handler_CURIE\" , \"gmd.ref\" : \"manubot.cite.curie.Handler_CURIE\" , \"gnpis\" : \"manubot.cite.curie.Handler_CURIE\" , \"go\" : \"manubot.cite.curie.Handler_CURIE\" , \"go_ref\" : \"manubot.cite.curie.Handler_CURIE\" , \"goa\" : \"manubot.cite.curie.Handler_CURIE\" , \"gold\" : \"manubot.cite.curie.Handler_CURIE\" , \"gold.genome\" : \"manubot.cite.curie.Handler_CURIE\" , \"gold.meta\" : \"manubot.cite.curie.Handler_CURIE\" , \"google.patent\" : \"manubot.cite.curie.Handler_CURIE\" , \"gpcrdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"gpmdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"gramene.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"gramene.growthstage\" : \"manubot.cite.curie.Handler_CURIE\" , \"gramene.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"gramene.qtl\" : \"manubot.cite.curie.Handler_CURIE\" , \"gramene.taxonomy\" : \"manubot.cite.curie.Handler_CURIE\" , \"greengenes\" : \"manubot.cite.curie.Handler_CURIE\" , \"grid\" : \"manubot.cite.curie.Handler_CURIE\" , \"grin.taxonomy\" : \"manubot.cite.curie.Handler_CURIE\" , \"gro\" : \"manubot.cite.curie.Handler_CURIE\" , \"grsdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"gsso\" : \"manubot.cite.curie.Handler_CURIE\" , \"gtex\" : \"manubot.cite.curie.Handler_CURIE\" , \"gudmap\" : \"manubot.cite.curie.Handler_CURIE\" , \"gwascentral.marker\" : \"manubot.cite.curie.Handler_CURIE\" , \"gwascentral.phenotype\" : \"manubot.cite.curie.Handler_CURIE\" , \"gwascentral.study\" : \"manubot.cite.curie.Handler_CURIE\" , \"gxa.expt\" : \"manubot.cite.curie.Handler_CURIE\" , \"gxa.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"hamap\" : \"manubot.cite.curie.Handler_CURIE\" , \"hcvdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"hdr\" : \"manubot.cite.curie.Handler_CURIE\" , \"hgmd\" : \"manubot.cite.curie.Handler_CURIE\" , \"hgnc\" : \"manubot.cite.curie.Handler_CURIE\" , \"hgnc.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"hgnc.genefamily\" : \"manubot.cite.curie.Handler_CURIE\" , \"hgnc.genegroup\" : \"manubot.cite.curie.Handler_CURIE\" , \"hgnc.symbol\" : \"manubot.cite.curie.Handler_CURIE\" , \"hinv.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"hinv.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"hinv.transcript\" : \"manubot.cite.curie.Handler_CURIE\" , \"hmdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"hogenom\" : \"manubot.cite.curie.Handler_CURIE\" , \"homd.seq\" : \"manubot.cite.curie.Handler_CURIE\" , \"homd.taxon\" : \"manubot.cite.curie.Handler_CURIE\" , \"homologene\" : \"manubot.cite.curie.Handler_CURIE\" , \"hovergen\" : \"manubot.cite.curie.Handler_CURIE\" , \"hp\" : \"manubot.cite.curie.Handler_CURIE\" , \"hpa\" : \"manubot.cite.curie.Handler_CURIE\" , \"hpm.peptide\" : \"manubot.cite.curie.Handler_CURIE\" , \"hpm.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"hprd\" : \"manubot.cite.curie.Handler_CURIE\" , \"hpscreg\" : \"manubot.cite.curie.Handler_CURIE\" , \"hssp\" : \"manubot.cite.curie.Handler_CURIE\" , \"http\" : \"manubot.cite.url.Handler_URL\" , \"https\" : \"manubot.cite.url.Handler_URL\" , \"huge\" : \"manubot.cite.curie.Handler_CURIE\" , \"iao\" : \"manubot.cite.curie.Handler_CURIE\" , \"icd\" : \"manubot.cite.curie.Handler_CURIE\" , \"iceberg.element\" : \"manubot.cite.curie.Handler_CURIE\" , \"iceberg.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"ideal\" : \"manubot.cite.curie.Handler_CURIE\" , \"ido\" : \"manubot.cite.curie.Handler_CURIE\" , \"idoo\" : \"manubot.cite.curie.Handler_CURIE\" , \"idot\" : \"manubot.cite.curie.Handler_CURIE\" , \"idr\" : \"manubot.cite.curie.Handler_CURIE\" , \"imex\" : \"manubot.cite.curie.Handler_CURIE\" , \"img.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"img.taxon\" : \"manubot.cite.curie.Handler_CURIE\" , \"imgt.hla\" : \"manubot.cite.curie.Handler_CURIE\" , \"imgt.ligm\" : \"manubot.cite.curie.Handler_CURIE\" , \"inchi\" : \"manubot.cite.curie.Handler_CURIE\" , \"inchikey\" : \"manubot.cite.curie.Handler_CURIE\" , \"insdc\" : \"manubot.cite.curie.Handler_CURIE\" , \"insdc.cds\" : \"manubot.cite.curie.Handler_CURIE\" , \"insdc.gca\" : \"manubot.cite.curie.Handler_CURIE\" , \"insdc.sra\" : \"manubot.cite.curie.Handler_CURIE\" , \"intact\" : \"manubot.cite.curie.Handler_CURIE\" , \"intact.molecule\" : \"manubot.cite.curie.Handler_CURIE\" , \"interpro\" : \"manubot.cite.curie.Handler_CURIE\" , \"ird.segment\" : \"manubot.cite.curie.Handler_CURIE\" , \"irefweb\" : \"manubot.cite.curie.Handler_CURIE\" , \"isbn\" : \"manubot.cite.isbn.Handler_ISBN\" , \"isfinder\" : \"manubot.cite.curie.Handler_CURIE\" , \"isni\" : \"manubot.cite.curie.Handler_CURIE\" , \"issn\" : \"manubot.cite.curie.Handler_CURIE\" , \"iuphar.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"iuphar.ligand\" : \"manubot.cite.curie.Handler_CURIE\" , \"iuphar.receptor\" : \"manubot.cite.curie.Handler_CURIE\" , \"jaxmice\" : \"manubot.cite.curie.Handler_CURIE\" , \"jcggdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"jcm\" : \"manubot.cite.curie.Handler_CURIE\" , \"jcsd\" : \"manubot.cite.curie.Handler_CURIE\" , \"jstor\" : \"manubot.cite.curie.Handler_CURIE\" , \"jws\" : \"manubot.cite.curie.Handler_CURIE\" , \"kaggle\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.disease\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.drug\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.environ\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.genes\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.genome\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.glycan\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.metagenome\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.module\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.orthology\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.pathway\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"knapsack\" : \"manubot.cite.curie.Handler_CURIE\" , \"lei\" : \"manubot.cite.curie.Handler_CURIE\" , \"lgic\" : \"manubot.cite.curie.Handler_CURIE\" , \"licebase\" : \"manubot.cite.curie.Handler_CURIE\" , \"ligandbook\" : \"manubot.cite.curie.Handler_CURIE\" , \"ligandbox\" : \"manubot.cite.curie.Handler_CURIE\" , \"ligandexpo\" : \"manubot.cite.curie.Handler_CURIE\" , \"lincs.cell\" : \"manubot.cite.curie.Handler_CURIE\" , \"lincs.data\" : \"manubot.cite.curie.Handler_CURIE\" , \"lincs.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"lincs.smallmolecule\" : \"manubot.cite.curie.Handler_CURIE\" , \"lipidbank\" : \"manubot.cite.curie.Handler_CURIE\" , \"lipidmaps\" : \"manubot.cite.curie.Handler_CURIE\" , \"lrg\" : \"manubot.cite.curie.Handler_CURIE\" , \"ma\" : \"manubot.cite.curie.Handler_CURIE\" , \"macie\" : \"manubot.cite.curie.Handler_CURIE\" , \"maizegdb.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"mamo\" : \"manubot.cite.curie.Handler_CURIE\" , \"massbank\" : \"manubot.cite.curie.Handler_CURIE\" , \"massive\" : \"manubot.cite.curie.Handler_CURIE\" , \"matrixdb.association\" : \"manubot.cite.curie.Handler_CURIE\" , \"mdm\" : \"manubot.cite.curie.Handler_CURIE\" , \"meddra\" : \"manubot.cite.curie.Handler_CURIE\" , \"medgen\" : \"manubot.cite.curie.Handler_CURIE\" , \"medlineplus\" : \"manubot.cite.curie.Handler_CURIE\" , \"merops\" : \"manubot.cite.curie.Handler_CURIE\" , \"merops.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"merops.inhibitor\" : \"manubot.cite.curie.Handler_CURIE\" , \"mesh\" : \"manubot.cite.curie.Handler_CURIE\" , \"mesh.2012\" : \"manubot.cite.curie.Handler_CURIE\" , \"mesh.2013\" : \"manubot.cite.curie.Handler_CURIE\" , \"metabolights\" : \"manubot.cite.curie.Handler_CURIE\" , \"metacyc.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"metacyc.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"metanetx.chemical\" : \"manubot.cite.curie.Handler_CURIE\" , \"metanetx.compartment\" : \"manubot.cite.curie.Handler_CURIE\" , \"metanetx.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"metlin\" : \"manubot.cite.curie.Handler_CURIE\" , \"mex\" : \"manubot.cite.curie.Handler_CURIE\" , \"mge\" : \"manubot.cite.curie.Handler_CURIE\" , \"mgi\" : \"manubot.cite.curie.Handler_CURIE\" , \"mgnify.proj\" : \"manubot.cite.curie.Handler_CURIE\" , \"mgnify.samp\" : \"manubot.cite.curie.Handler_CURIE\" , \"mi\" : \"manubot.cite.curie.Handler_CURIE\" , \"microscope\" : \"manubot.cite.curie.Handler_CURIE\" , \"microsporidia\" : \"manubot.cite.curie.Handler_CURIE\" , \"mim\" : \"manubot.cite.curie.Handler_CURIE\" , \"mimodb\" : \"manubot.cite.curie.Handler_CURIE\" , \"minid\" : \"manubot.cite.curie.Handler_CURIE\" , \"minid.test\" : \"manubot.cite.curie.Handler_CURIE\" , \"mint\" : \"manubot.cite.curie.Handler_CURIE\" , \"mipmod\" : \"manubot.cite.curie.Handler_CURIE\" , \"mir\" : \"manubot.cite.curie.Handler_CURIE\" , \"mirbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"mirbase.mature\" : \"manubot.cite.curie.Handler_CURIE\" , \"mirex\" : \"manubot.cite.curie.Handler_CURIE\" , \"miriam.collection\" : \"manubot.cite.curie.Handler_CURIE\" , \"miriam.resource\" : \"manubot.cite.curie.Handler_CURIE\" , \"mirnest\" : \"manubot.cite.curie.Handler_CURIE\" , \"mirtarbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmmp:biomaps\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmp.cat\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmp.db\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmp.fun\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmp.ref\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmrrc\" : \"manubot.cite.curie.Handler_CURIE\" , \"mo\" : \"manubot.cite.curie.Handler_CURIE\" , \"mobidb\" : \"manubot.cite.curie.Handler_CURIE\" , \"mod\" : \"manubot.cite.curie.Handler_CURIE\" , \"modeldb\" : \"manubot.cite.curie.Handler_CURIE\" , \"molbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"molmedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"morpheus\" : \"manubot.cite.curie.Handler_CURIE\" , \"mp\" : \"manubot.cite.curie.Handler_CURIE\" , \"mpid\" : \"manubot.cite.curie.Handler_CURIE\" , \"ms\" : \"manubot.cite.curie.Handler_CURIE\" , \"multicellds.cell_line\" : \"manubot.cite.curie.Handler_CURIE\" , \"multicellds.collection\" : \"manubot.cite.curie.Handler_CURIE\" , \"multicellds.snapshot\" : \"manubot.cite.curie.Handler_CURIE\" , \"mw.project\" : \"manubot.cite.curie.Handler_CURIE\" , \"mw.study\" : \"manubot.cite.curie.Handler_CURIE\" , \"myco.lepra\" : \"manubot.cite.curie.Handler_CURIE\" , \"myco.marinum\" : \"manubot.cite.curie.Handler_CURIE\" , \"myco.smeg\" : \"manubot.cite.curie.Handler_CURIE\" , \"myco.tuber\" : \"manubot.cite.curie.Handler_CURIE\" , \"mycobank\" : \"manubot.cite.curie.Handler_CURIE\" , \"mzspec\" : \"manubot.cite.curie.Handler_CURIE\" , \"napdi\" : \"manubot.cite.curie.Handler_CURIE\" , \"napp\" : \"manubot.cite.curie.Handler_CURIE\" , \"narcis\" : \"manubot.cite.curie.Handler_CURIE\" , \"nasc\" : \"manubot.cite.curie.Handler_CURIE\" , \"nbn\" : \"manubot.cite.curie.Handler_CURIE\" , \"nbrc\" : \"manubot.cite.curie.Handler_CURIE\" , \"ncbigene\" : \"manubot.cite.curie.Handler_CURIE\" , \"ncbiprotein\" : \"manubot.cite.curie.Handler_CURIE\" , \"ncim\" : \"manubot.cite.curie.Handler_CURIE\" , \"ncit\" : \"manubot.cite.curie.Handler_CURIE\" , \"ndc\" : \"manubot.cite.curie.Handler_CURIE\" , \"nemo\" : \"manubot.cite.curie.Handler_CURIE\" , \"neurolex\" : \"manubot.cite.curie.Handler_CURIE\" , \"neuromorpho\" : \"manubot.cite.curie.Handler_CURIE\" , \"neurondb\" : \"manubot.cite.curie.Handler_CURIE\" , \"neurovault.collection\" : \"manubot.cite.curie.Handler_CURIE\" , \"neurovault.image\" : \"manubot.cite.curie.Handler_CURIE\" , \"nextdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"nextprot\" : \"manubot.cite.curie.Handler_CURIE\" , \"ngl\" : \"manubot.cite.curie.Handler_CURIE\" , \"niaest\" : \"manubot.cite.curie.Handler_CURIE\" , \"nmr\" : \"manubot.cite.curie.Handler_CURIE\" , \"nmrshiftdb2\" : \"manubot.cite.curie.Handler_CURIE\" , \"noncodev3\" : \"manubot.cite.curie.Handler_CURIE\" , \"noncodev4.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"noncodev4.rna\" : \"manubot.cite.curie.Handler_CURIE\" , \"norine\" : \"manubot.cite.curie.Handler_CURIE\" , \"nuclearbd\" : \"manubot.cite.curie.Handler_CURIE\" , \"obi\" : \"manubot.cite.curie.Handler_CURIE\" , \"occ\" : \"manubot.cite.curie.Handler_CURIE\" , \"oci\" : \"manubot.cite.curie.Handler_CURIE\" , \"ocid\" : \"manubot.cite.curie.Handler_CURIE\" , \"oclc\" : \"manubot.cite.curie.Handler_CURIE\" , \"odam\" : \"manubot.cite.curie.Handler_CURIE\" , \"odor\" : \"manubot.cite.curie.Handler_CURIE\" , \"oid\" : \"manubot.cite.curie.Handler_CURIE\" , \"oma.grp\" : \"manubot.cite.curie.Handler_CURIE\" , \"oma.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"omia\" : \"manubot.cite.curie.Handler_CURIE\" , \"omit\" : \"manubot.cite.curie.Handler_CURIE\" , \"opb\" : \"manubot.cite.curie.Handler_CURIE\" , \"opm\" : \"manubot.cite.curie.Handler_CURIE\" , \"orcid\" : \"manubot.cite.curie.Handler_CURIE\" , \"ordb\" : \"manubot.cite.curie.Handler_CURIE\" , \"oridb.sacch\" : \"manubot.cite.curie.Handler_CURIE\" , \"oridb.schizo\" : \"manubot.cite.curie.Handler_CURIE\" , \"orphanet\" : \"manubot.cite.curie.Handler_CURIE\" , \"orphanet.ordo\" : \"manubot.cite.curie.Handler_CURIE\" , \"orthodb\" : \"manubot.cite.curie.Handler_CURIE\" , \"oryzabase.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"oryzabase.mutant\" : \"manubot.cite.curie.Handler_CURIE\" , \"oryzabase.reference\" : \"manubot.cite.curie.Handler_CURIE\" , \"oryzabase.stage\" : \"manubot.cite.curie.Handler_CURIE\" , \"oryzabase.strain\" : \"manubot.cite.curie.Handler_CURIE\" , \"otl\" : \"manubot.cite.curie.Handler_CURIE\" , \"p3db.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"p3db.site\" : \"manubot.cite.curie.Handler_CURIE\" , \"paleodb\" : \"manubot.cite.curie.Handler_CURIE\" , \"panther.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"panther.node\" : \"manubot.cite.curie.Handler_CURIE\" , \"panther.pathway\" : \"manubot.cite.curie.Handler_CURIE\" , \"panther.pthcmp\" : \"manubot.cite.curie.Handler_CURIE\" , \"pass2\" : \"manubot.cite.curie.Handler_CURIE\" , \"pathwaycommons\" : \"manubot.cite.curie.Handler_CURIE\" , \"pato\" : \"manubot.cite.curie.Handler_CURIE\" , \"paxdb.organism\" : \"manubot.cite.curie.Handler_CURIE\" , \"paxdb.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"pazar\" : \"manubot.cite.curie.Handler_CURIE\" , \"pdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pdb-ccd\" : \"manubot.cite.curie.Handler_CURIE\" , \"pdb.ligand\" : \"manubot.cite.curie.Handler_CURIE\" , \"peptideatlas\" : \"manubot.cite.curie.Handler_CURIE\" , \"peptideatlas.dataset\" : \"manubot.cite.curie.Handler_CURIE\" , \"peroxibase\" : \"manubot.cite.curie.Handler_CURIE\" , \"pfam\" : \"manubot.cite.curie.Handler_CURIE\" , \"pgs\" : \"manubot.cite.curie.Handler_CURIE\" , \"pgx\" : \"manubot.cite.curie.Handler_CURIE\" , \"pharmgkb.disease\" : \"manubot.cite.curie.Handler_CURIE\" , \"pharmgkb.drug\" : \"manubot.cite.curie.Handler_CURIE\" , \"pharmgkb.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"pharmgkb.pathways\" : \"manubot.cite.curie.Handler_CURIE\" , \"phenolexplorer\" : \"manubot.cite.curie.Handler_CURIE\" , \"phosphopoint.kinase\" : \"manubot.cite.curie.Handler_CURIE\" , \"phosphopoint.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"phosphosite.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"phosphosite.residue\" : \"manubot.cite.curie.Handler_CURIE\" , \"phylomedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"phytozome.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"pid.pathway\" : \"manubot.cite.curie.Handler_CURIE\" , \"pigqtldb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pina\" : \"manubot.cite.curie.Handler_CURIE\" , \"piroplasma\" : \"manubot.cite.curie.Handler_CURIE\" , \"pirsf\" : \"manubot.cite.curie.Handler_CURIE\" , \"pkdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"planttfdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"plasmodb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pmap.cutdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pmap.substratedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pmc\" : \"manubot.cite.pubmed.Handler_PMC\" , \"pmcid\" : \"manubot.cite.pubmed.Handler_PMC\" , \"pmdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pmid\" : \"manubot.cite.pubmed.Handler_PubMed\" , \"pmp\" : \"manubot.cite.curie.Handler_CURIE\" , \"po\" : \"manubot.cite.curie.Handler_CURIE\" , \"pocketome\" : \"manubot.cite.curie.Handler_CURIE\" , \"polbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"pombase\" : \"manubot.cite.curie.Handler_CURIE\" , \"pr\" : \"manubot.cite.curie.Handler_CURIE\" , \"pride\" : \"manubot.cite.curie.Handler_CURIE\" , \"pride.project\" : \"manubot.cite.curie.Handler_CURIE\" , \"prints\" : \"manubot.cite.curie.Handler_CURIE\" , \"probonto\" : \"manubot.cite.curie.Handler_CURIE\" , \"prodom\" : \"manubot.cite.curie.Handler_CURIE\" , \"proglyc\" : \"manubot.cite.curie.Handler_CURIE\" , \"prosite\" : \"manubot.cite.curie.Handler_CURIE\" , \"protclustdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"proteomicsdb.peptide\" : \"manubot.cite.curie.Handler_CURIE\" , \"proteomicsdb.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"protonet.cluster\" : \"manubot.cite.curie.Handler_CURIE\" , \"protonet.proteincard\" : \"manubot.cite.curie.Handler_CURIE\" , \"pscdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pseudomonas\" : \"manubot.cite.curie.Handler_CURIE\" , \"psipar\" : \"manubot.cite.curie.Handler_CURIE\" , \"pubchem.bioassay\" : \"manubot.cite.curie.Handler_CURIE\" , \"pubchem.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"pubchem.substance\" : \"manubot.cite.curie.Handler_CURIE\" , \"pubmed\" : \"manubot.cite.pubmed.Handler_PubMed\" , \"pw\" : \"manubot.cite.curie.Handler_CURIE\" , \"px\" : \"manubot.cite.curie.Handler_CURIE\" , \"rapdb.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"rapdb.transcript\" : \"manubot.cite.curie.Handler_CURIE\" , \"rbk\" : \"manubot.cite.curie.Handler_CURIE\" , \"reactome\" : \"manubot.cite.curie.Handler_CURIE\" , \"rebase\" : \"manubot.cite.curie.Handler_CURIE\" , \"refseq\" : \"manubot.cite.curie.Handler_CURIE\" , \"resid\" : \"manubot.cite.curie.Handler_CURIE\" , \"rfam\" : \"manubot.cite.curie.Handler_CURIE\" , \"rgd\" : \"manubot.cite.curie.Handler_CURIE\" , \"rgd.qtl\" : \"manubot.cite.curie.Handler_CURIE\" , \"rgd.strain\" : \"manubot.cite.curie.Handler_CURIE\" , \"rhea\" : \"manubot.cite.curie.Handler_CURIE\" , \"ricegap\" : \"manubot.cite.curie.Handler_CURIE\" , \"ricenetdb.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"ricenetdb.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"ricenetdb.mirna\" : \"manubot.cite.curie.Handler_CURIE\" , \"ricenetdb.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"ricenetdb.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"rnacentral\" : \"manubot.cite.curie.Handler_CURIE\" , \"rnamods\" : \"manubot.cite.curie.Handler_CURIE\" , \"ro\" : \"manubot.cite.curie.Handler_CURIE\" , \"rouge\" : \"manubot.cite.curie.Handler_CURIE\" , \"rrid\" : \"manubot.cite.curie.Handler_CURIE\" , \"sabiork.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"sabiork.ec\" : \"manubot.cite.curie.Handler_CURIE\" , \"sabiork.kineticrecord\" : \"manubot.cite.curie.Handler_CURIE\" , \"sabiork.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"sasbdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"sbo\" : \"manubot.cite.curie.Handler_CURIE\" , \"scop\" : \"manubot.cite.curie.Handler_CURIE\" , \"scretf\" : \"manubot.cite.curie.Handler_CURIE\" , \"sdbs\" : \"manubot.cite.curie.Handler_CURIE\" , \"seed\" : \"manubot.cite.curie.Handler_CURIE\" , \"seed.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"seed.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"sgd\" : \"manubot.cite.curie.Handler_CURIE\" , \"sgd.pathways\" : \"manubot.cite.curie.Handler_CURIE\" , \"sgn\" : \"manubot.cite.curie.Handler_CURIE\" , \"sheepqtldb\" : \"manubot.cite.curie.Handler_CURIE\" , \"shortdoi\" : \"manubot.cite.doi.Handler_DOI\" , \"sider.drug\" : \"manubot.cite.curie.Handler_CURIE\" , \"sider.effect\" : \"manubot.cite.curie.Handler_CURIE\" , \"signaling-gateway\" : \"manubot.cite.curie.Handler_CURIE\" , \"sisu\" : \"manubot.cite.curie.Handler_CURIE\" , \"sitex\" : \"manubot.cite.curie.Handler_CURIE\" , \"slm\" : \"manubot.cite.curie.Handler_CURIE\" , \"smart\" : \"manubot.cite.curie.Handler_CURIE\" , \"smpdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"snomedct\" : \"manubot.cite.curie.Handler_CURIE\" , \"snp2tfbs\" : \"manubot.cite.curie.Handler_CURIE\" , \"so\" : \"manubot.cite.curie.Handler_CURIE\" , \"soybase\" : \"manubot.cite.curie.Handler_CURIE\" , \"spdx\" : \"manubot.cite.curie.Handler_CURIE\" , \"spike.map\" : \"manubot.cite.curie.Handler_CURIE\" , \"splash\" : \"manubot.cite.curie.Handler_CURIE\" , \"spp\" : \"manubot.cite.curie.Handler_CURIE\" , \"stap\" : \"manubot.cite.curie.Handler_CURIE\" , \"stitch\" : \"manubot.cite.curie.Handler_CURIE\" , \"storedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"string\" : \"manubot.cite.curie.Handler_CURIE\" , \"subtilist\" : \"manubot.cite.curie.Handler_CURIE\" , \"subtiwiki\" : \"manubot.cite.curie.Handler_CURIE\" , \"sugarbind\" : \"manubot.cite.curie.Handler_CURIE\" , \"supfam\" : \"manubot.cite.curie.Handler_CURIE\" , \"swh\" : \"manubot.cite.curie.Handler_CURIE\" , \"swiss-model\" : \"manubot.cite.curie.Handler_CURIE\" , \"swissregulon\" : \"manubot.cite.curie.Handler_CURIE\" , \"t3db\" : \"manubot.cite.curie.Handler_CURIE\" , \"tair.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"tair.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"tair.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"tarbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"taxonomy\" : \"manubot.cite.curie.Handler_CURIE\" , \"tcdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"tgd\" : \"manubot.cite.curie.Handler_CURIE\" , \"tigrfam\" : \"manubot.cite.curie.Handler_CURIE\" , \"tissuelist\" : \"manubot.cite.curie.Handler_CURIE\" , \"tol\" : \"manubot.cite.curie.Handler_CURIE\" , \"topdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"topfind\" : \"manubot.cite.curie.Handler_CURIE\" , \"toxoplasma\" : \"manubot.cite.curie.Handler_CURIE\" , \"transyt\" : \"manubot.cite.curie.Handler_CURIE\" , \"treebase\" : \"manubot.cite.curie.Handler_CURIE\" , \"treefam\" : \"manubot.cite.curie.Handler_CURIE\" , \"trichdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"tritrypdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"ttd.drug\" : \"manubot.cite.curie.Handler_CURIE\" , \"ttd.target\" : \"manubot.cite.curie.Handler_CURIE\" , \"uberon\" : \"manubot.cite.curie.Handler_CURIE\" , \"ubio.namebank\" : \"manubot.cite.curie.Handler_CURIE\" , \"umbbd.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"umbbd.enzyme\" : \"manubot.cite.curie.Handler_CURIE\" , \"umbbd.pathway\" : \"manubot.cite.curie.Handler_CURIE\" , \"umbbd.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"umbbd.rule\" : \"manubot.cite.curie.Handler_CURIE\" , \"umls\" : \"manubot.cite.curie.Handler_CURIE\" , \"unigene\" : \"manubot.cite.curie.Handler_CURIE\" , \"unii\" : \"manubot.cite.curie.Handler_CURIE\" , \"unimod\" : \"manubot.cite.curie.Handler_CURIE\" , \"uniparc\" : \"manubot.cite.curie.Handler_CURIE\" , \"unipathway.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"unipathway.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"uniprot\" : \"manubot.cite.curie.Handler_CURIE\" , \"uniprot.chain\" : \"manubot.cite.curie.Handler_CURIE\" , \"uniprot.isoform\" : \"manubot.cite.curie.Handler_CURIE\" , \"unists\" : \"manubot.cite.curie.Handler_CURIE\" , \"unite\" : \"manubot.cite.curie.Handler_CURIE\" , \"uo\" : \"manubot.cite.curie.Handler_CURIE\" , \"url\" : \"manubot.cite.url.Handler_URL\" , \"uspto\" : \"manubot.cite.curie.Handler_CURIE\" , \"validatordb\" : \"manubot.cite.curie.Handler_CURIE\" , \"vario\" : \"manubot.cite.curie.Handler_CURIE\" , \"vbase2\" : \"manubot.cite.curie.Handler_CURIE\" , \"vbrc\" : \"manubot.cite.curie.Handler_CURIE\" , \"vectorbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"vegbank\" : \"manubot.cite.curie.Handler_CURIE\" , \"vfb\" : \"manubot.cite.curie.Handler_CURIE\" , \"vfdb.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"vfdb.genus\" : \"manubot.cite.curie.Handler_CURIE\" , \"vgnc\" : \"manubot.cite.curie.Handler_CURIE\" , \"viaf\" : \"manubot.cite.curie.Handler_CURIE\" , \"vipr\" : \"manubot.cite.curie.Handler_CURIE\" , \"viralzone\" : \"manubot.cite.curie.Handler_CURIE\" , \"virsirna\" : \"manubot.cite.curie.Handler_CURIE\" , \"vmhmetabolite\" : \"manubot.cite.curie.Handler_CURIE\" , \"vmhreaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"wb\" : \"manubot.cite.curie.Handler_CURIE\" , \"wb.rnai\" : \"manubot.cite.curie.Handler_CURIE\" , \"wikidata\" : \"manubot.cite.wikidata.Handler_Wikidata\" , \"wikigenes\" : \"manubot.cite.curie.Handler_CURIE\" , \"wikipathways\" : \"manubot.cite.curie.Handler_CURIE\" , \"wikipedia.en\" : \"manubot.cite.curie.Handler_CURIE\" , \"worfdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"wormpep\" : \"manubot.cite.curie.Handler_CURIE\" , \"worms\" : \"manubot.cite.curie.Handler_CURIE\" , \"xenbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"ydpm\" : \"manubot.cite.curie.Handler_CURIE\" , \"yeastintron\" : \"manubot.cite.curie.Handler_CURIE\" , \"yetfasco\" : \"manubot.cite.curie.Handler_CURIE\" , \"yid\" : \"manubot.cite.curie.Handler_CURIE\" , \"yrcpdr\" : \"manubot.cite.curie.Handler_CURIE\" , \"zfin\" : \"manubot.cite.curie.Handler_CURIE\" , \"zinc\" : \"manubot.cite.curie.Handler_CURIE\" , }","title":"Module manubot.cite.handlers"},{"location":"reference/manubot/cite/handlers/#variables","text":"prefix_to_handler","title":"Variables"},{"location":"reference/manubot/cite/handlers/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/handlers/#get_handler","text":"def get_handler ( prefix_lower : str ) -> 'Handler' View Source @functools . lru_cache ( maxsize = 10 _000 ) def get_handler ( prefix_lower : str ) -> \"Handler\" : if not isinstance ( prefix_lower , str ) : raise TypeError ( f \"prefix_lower should be a str, instead received {prefix_lower.__class__.__name__}\" ) assert prefix_lower == prefix_lower . lower () handler = prefix_to_handler [ prefix_lower ] handler = import_function ( handler )( prefix_lower ) return handler","title":"get_handler"},{"location":"reference/manubot/cite/handlers/#infer_prefix","text":"def infer_prefix ( dealiased_id : str ) -> Optional [ str ] Infer the prefix for citekey by matching it against a sequence of regexes. If a match is found, return the coressponding standard prefix. Otherwise, return None. View Source def infer_prefix ( dealiased_id : str ) -> Optional [ str ] : \"\"\" Infer the prefix for citekey by matching it against a sequence of regexes. If a match is found, return the coressponding standard prefix. Otherwise, return None. \"\"\" for prefix , pattern_attrib in _infer_prefix_patterns : handler = get_handler ( prefix ) pattern = handler . _get_pattern ( attribute = pattern_attrib ) if pattern . fullmatch ( dealiased_id ) : return handler . standard_prefix return None","title":"infer_prefix"},{"location":"reference/manubot/cite/handlers/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/handlers/#handler","text":"class Handler ( prefix_lower : str ) View Source @dataclasses . dataclass class Handler : \"\"\" A Handler is a class that provides support for a certain type of citekey. For example, a Handler subclass could provide support for DOI citekeys. Subclasses enable custom logic for different citekey prefixes, including how to standardize the citekey and how to retrieve CSL Item metadata. \"\"\" prefix_lower : str prefixes = [] def _get_pattern ( self , attribute = \"accession_pattern\" ) -> Pattern : \"\"\" Return a compiled regex pattern stored by `attribute`. By default, return `self.accession_pattern`, which Handler subclasses can set to provide the expected pattern for `self.accession`. \"\"\" # todo : consider caching compilation pattern = getattr ( self , attribute , None ) if not pattern : return None if not isinstance ( pattern , Pattern ) : pattern = re . compile ( pattern ) return pattern def inspect ( self , citekey : CiteKey ) -> Optional [ str ] : \"\"\" Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. \"\"\" pattern = self . _get_pattern ( \"accession_pattern\" ) if not pattern : return None if not pattern . fullmatch ( citekey . accession ) : return f \"{citekey.accession} does not match regex {pattern.pattern}\" def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str, str ] : \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession @abc . abstractmethod def get_csl_item ( self , citekey ) -> Dict [ str, Any ] : \"\"\" Return a CSL_Item with bibliographic details for citekey. \"\"\" raise NotImplementedError ( f \"Manubot does not know how to generate a csl_item for {citekey.standard_id!r}\" )","title":"Handler"},{"location":"reference/manubot/cite/handlers/#descendants","text":"manubot.cite.arxiv.Handler_arXiv manubot.cite.curie.Handler_CURIE manubot.cite.pubmed.Handler_PubMed manubot.cite.pubmed.Handler_PMC manubot.cite.doi.Handler_DOI manubot.cite.isbn.Handler_ISBN manubot.cite.url.Handler_URL manubot.cite.wikidata.Handler_Wikidata","title":"Descendants"},{"location":"reference/manubot/cite/handlers/#class-variables","text":"prefixes","title":"Class variables"},{"location":"reference/manubot/cite/handlers/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/handlers/#get_csl_item","text":"def get_csl_item ( self , citekey ) -> Dict [ str , Any ] Return a CSL_Item with bibliographic details for citekey. View Source @abc . abstractmethod def get_csl_item ( self , citekey ) -> Dict [ str, Any ] : \"\"\" Return a CSL_Item with bibliographic details for citekey. \"\"\" raise NotImplementedError ( f \"Manubot does not know how to generate a csl_item for {citekey.standard_id!r}\" )","title":"get_csl_item"},{"location":"reference/manubot/cite/handlers/#inspect","text":"def inspect ( self , citekey : manubot . cite . citekey . CiteKey ) -> Optional [ str ] Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey : CiteKey ) -> Optional [ str ] : \"\"\" Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. \"\"\" pattern = self . _get_pattern ( \"accession_pattern\" ) if not pattern : return None if not pattern . fullmatch ( citekey . accession ) : return f \"{citekey.accession} does not match regex {pattern.pattern}\"","title":"inspect"},{"location":"reference/manubot/cite/handlers/#standardize_prefix_accession","text":"def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ] Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ] : \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession","title":"standardize_prefix_accession"},{"location":"reference/manubot/cite/isbn/","text":"Module manubot.cite.isbn None None View Source import json import logging import re from .handlers import Handler class Handler_ISBN ( Handler ): standard_prefix = \"isbn\" prefixes = [ \"isbn\" , ] def inspect ( self , citekey ): import isbnlib fail = isbnlib . notisbn ( citekey . accession , level = \"strict\" ) if fail : return f \"identifier violates the ISBN syntax according to isbnlib v { isbnlib . __version__ } \" def standardize_prefix_accession ( self , accession ): from isbnlib import to_isbn13 accession = to_isbn13 ( accession ) return self . standard_prefix , accession def get_csl_item ( self , citekey ): return get_isbn_csl_item ( citekey . standard_accession ) def get_isbn_csl_item ( isbn : str ): \"\"\" Generate CSL JSON Data for an ISBN. Converts all ISBNs to 13-digit format. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `isbn_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\"\" import isbnlib isbn = isbnlib . to_isbn13 ( isbn ) for retriever in isbn_retrievers : try : return retriever ( isbn ) except Exception as error : logging . warning ( f \"Error in { retriever . __name__ } for { isbn } \" f \"due to a { error . __class__ . __name__ } : \\n { error } \" ) logging . info ( error , exc_info = True ) raise Exception ( f \"all get_isbn_csl_item methods failed for { isbn } \" ) def get_isbn_csl_item_zotero ( isbn : str ): \"\"\" Generate CSL JSON Data for an ISBN using Zotero's translation-server. \"\"\" from manubot.cite.zotero import get_csl_item return get_csl_item ( f \"isbn: { isbn } \" ) def get_isbn_csl_item_citoid ( isbn : str ): \"\"\" Return CSL JSON Data for an ISBN using the Wikipedia Citoid API. https://en.wikipedia.org/api/rest_v1/#!/Citation/getCitation \"\"\" import requests from manubot.util import get_manubot_user_agent headers = { \"User-Agent\" : get_manubot_user_agent ()} url = f \"https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/ { isbn } \" response = requests . get ( url , headers = headers ) result = response . json () if isinstance ( result , dict ): if result [ \"title\" ] == \"Not found.\" : raise KeyError ( f \"Metadata for ISBN { isbn } not found at { url } \" ) else : raise Exception ( f \"Unable to extract CSL from JSON metadata for ISBN { isbn } : \\n \" f \" { json . dumps ( result . text ) } \" ) ( mediawiki ,) = result csl_item = dict () csl_item [ \"type\" ] = mediawiki . get ( \"itemType\" , \"book\" ) if \"title\" in mediawiki : csl_item [ \"title\" ] = mediawiki [ \"title\" ] if \"author\" in mediawiki : csl_author = list () for last , first in mediawiki [ \"author\" ]: csl_author . append ({ \"given\" : first , \"family\" : last }) if csl_author : csl_item [ \"author\" ] = csl_author if \"date\" in mediawiki : year_pattern = re . compile ( r \"[0-9] {4} \" ) match = year_pattern . search ( mediawiki [ \"date\" ]) if match : year = int ( match . group ()) csl_item [ \"issued\" ] = { \"date-parts\" : [[ year ]]} else : logging . debug ( f \"get_isbn_csl_item_citoid: issue extracting date for ISBN { isbn } \\n \" f \"metadata retrieved from { url } \\n \" f 'unable to extract year from date field: { mediawiki [ \"date\" ] } ' ) if \"publisher\" in mediawiki : csl_item [ \"publisher\" ] = mediawiki [ \"publisher\" ] if \"place\" in mediawiki : csl_item [ \"publisher-place\" ] = mediawiki [ \"place\" ] if \"volume\" in mediawiki : csl_item [ \"volume\" ] = mediawiki [ \"volume\" ] if \"edition\" in mediawiki : csl_item [ \"edition\" ] = mediawiki [ \"edition\" ] if \"abstractNote\" in mediawiki : csl_item [ \"abstract\" ] = mediawiki [ \"abstractNote\" ] csl_item [ \"ISBN\" ] = isbn if \"source\" in mediawiki : csl_item [ \"source\" ] = mediawiki [ \"source\" ][ 0 ] if \"url\" in mediawiki : csl_item [ \"URL\" ] = mediawiki [ \"url\" ] return csl_item def get_isbn_csl_item_isbnlib ( isbn : str ): \"\"\" Generate CSL JSON Data for an ISBN using isbnlib. \"\"\" import isbnlib metadata = isbnlib . meta ( isbn ) csl_json = isbnlib . registry . bibformatters [ \"csl\" ]( metadata ) csl_data = json . loads ( csl_json ) return csl_data isbn_retrievers = [ get_isbn_csl_item_zotero , get_isbn_csl_item_citoid , get_isbn_csl_item_isbnlib , ] Variables isbn_retrievers Functions get_isbn_csl_item def get_isbn_csl_item ( isbn : str ) Generate CSL JSON Data for an ISBN. Converts all ISBNs to 13-digit format. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable isbn_retrievers . The methods are attempted in order, with this function returning the metadata from the first non-failing method. View Source def get_isbn_csl_item ( isbn : str ): \"\"\" Generate CSL JSON Data for an ISBN. Converts all ISBNs to 13-digit format. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `isbn_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\"\" import isbnlib isbn = isbnlib . to_isbn13 ( isbn ) for retriever in isbn_retrievers : try : return retriever ( isbn ) except Exception as error : logging . warning ( f \"Error in { retriever . __name__ } for { isbn } \" f \"due to a { error . __class__ . __name__ } : \\n { error } \" ) logging . info ( error , exc_info = True ) raise Exception ( f \"all get_isbn_csl_item methods failed for { isbn } \" ) get_isbn_csl_item_citoid def get_isbn_csl_item_citoid ( isbn : str ) Return CSL JSON Data for an ISBN using the Wikipedia Citoid API. https://en.wikipedia.org/api/rest_v1/#!/Citation/getCitation View Source def get_isbn_csl_item_citoid ( isbn : str ): \"\"\" Return CSL JSON Data for an ISBN using the Wikipedia Citoid API. https://en.wikipedia.org/api/rest_v1/#!/Citation/getCitation \"\"\" import requests from manubot.util import get_manubot_user_agent headers = { \"User-Agent\" : get_manubot_user_agent ()} url = f \"https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/ { isbn } \" response = requests . get ( url , headers = headers ) result = response . json () if isinstance ( result , dict ): if result [ \"title\" ] == \"Not found.\" : raise KeyError ( f \"Metadata for ISBN { isbn } not found at { url } \" ) else : raise Exception ( f \"Unable to extract CSL from JSON metadata for ISBN { isbn } : \\n \" f \" { json . dumps ( result . text ) } \" ) ( mediawiki ,) = result csl_item = dict () csl_item [ \"type\" ] = mediawiki . get ( \"itemType\" , \"book\" ) if \"title\" in mediawiki : csl_item [ \"title\" ] = mediawiki [ \"title\" ] if \"author\" in mediawiki : csl_author = list () for last , first in mediawiki [ \"author\" ]: csl_author . append ({ \"given\" : first , \"family\" : last }) if csl_author : csl_item [ \"author\" ] = csl_author if \"date\" in mediawiki : year_pattern = re . compile ( r \"[0-9] {4} \" ) match = year_pattern . search ( mediawiki [ \"date\" ]) if match : year = int ( match . group ()) csl_item [ \"issued\" ] = { \"date-parts\" : [[ year ]]} else : logging . debug ( f \"get_isbn_csl_item_citoid: issue extracting date for ISBN { isbn } \\n \" f \"metadata retrieved from { url } \\n \" f 'unable to extract year from date field: { mediawiki [ \"date\" ] } ' ) if \"publisher\" in mediawiki : csl_item [ \"publisher\" ] = mediawiki [ \"publisher\" ] if \"place\" in mediawiki : csl_item [ \"publisher-place\" ] = mediawiki [ \"place\" ] if \"volume\" in mediawiki : csl_item [ \"volume\" ] = mediawiki [ \"volume\" ] if \"edition\" in mediawiki : csl_item [ \"edition\" ] = mediawiki [ \"edition\" ] if \"abstractNote\" in mediawiki : csl_item [ \"abstract\" ] = mediawiki [ \"abstractNote\" ] csl_item [ \"ISBN\" ] = isbn if \"source\" in mediawiki : csl_item [ \"source\" ] = mediawiki [ \"source\" ][ 0 ] if \"url\" in mediawiki : csl_item [ \"URL\" ] = mediawiki [ \"url\" ] return csl_item get_isbn_csl_item_isbnlib def get_isbn_csl_item_isbnlib ( isbn : str ) Generate CSL JSON Data for an ISBN using isbnlib. View Source def get_isbn_csl_item_isbnlib ( isbn : str ): \"\"\" Generate CSL JSON Data for an ISBN using isbnlib. \"\"\" import isbnlib metadata = isbnlib . meta ( isbn ) csl_json = isbnlib . registry . bibformatters [ \"csl\" ]( metadata ) csl_data = json . loads ( csl_json ) return csl_data get_isbn_csl_item_zotero def get_isbn_csl_item_zotero ( isbn : str ) Generate CSL JSON Data for an ISBN using Zotero's translation-server. View Source def get_isbn_csl_item_zotero ( isbn : str ): \"\"\" Generate CSL JSON Data for an ISBN using Zotero's translation-server. \"\"\" from manubot.cite.zotero import get_csl_item return get_csl_item ( f \"isbn: { isbn } \" ) Classes Handler_ISBN class Handler_ISBN ( prefix_lower : str ) View Source class Handler_ISBN ( Handler ): standard_prefix = \"isbn\" prefixes = [ \"isbn\" , ] def inspect ( self , citekey ): import isbnlib fail = isbnlib . notisbn ( citekey . accession , level = \"strict\" ) if fail : return f \"identifier violates the ISBN syntax according to isbnlib v { isbnlib . __version__ } \" def standardize_prefix_accession ( self , accession ): from isbnlib import to_isbn13 accession = to_isbn13 ( accession ) return self . standard_prefix , accession def get_csl_item ( self , citekey ): return get_isbn_csl_item ( citekey . standard_accession ) Ancestors (in MRO) manubot.cite.handlers.Handler Class variables prefixes standard_prefix Methods get_csl_item def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ) : return get_isbn_csl_item ( citekey . standard_accession ) inspect def inspect ( self , citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey ): import isbnlib fail = isbnlib . notisbn ( citekey . accession , level = \"strict\" ) if fail : return f \"identifier violates the ISBN syntax according to isbnlib v { isbnlib . __version__ } \" standardize_prefix_accession def standardize_prefix_accession ( self , accession ) Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession ): from isbnlib import to_isbn13 accession = to_isbn13 ( accession ) return self . standard_prefix , accession","title":"Isbn"},{"location":"reference/manubot/cite/isbn/#module-manubotciteisbn","text":"None None View Source import json import logging import re from .handlers import Handler class Handler_ISBN ( Handler ): standard_prefix = \"isbn\" prefixes = [ \"isbn\" , ] def inspect ( self , citekey ): import isbnlib fail = isbnlib . notisbn ( citekey . accession , level = \"strict\" ) if fail : return f \"identifier violates the ISBN syntax according to isbnlib v { isbnlib . __version__ } \" def standardize_prefix_accession ( self , accession ): from isbnlib import to_isbn13 accession = to_isbn13 ( accession ) return self . standard_prefix , accession def get_csl_item ( self , citekey ): return get_isbn_csl_item ( citekey . standard_accession ) def get_isbn_csl_item ( isbn : str ): \"\"\" Generate CSL JSON Data for an ISBN. Converts all ISBNs to 13-digit format. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `isbn_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\"\" import isbnlib isbn = isbnlib . to_isbn13 ( isbn ) for retriever in isbn_retrievers : try : return retriever ( isbn ) except Exception as error : logging . warning ( f \"Error in { retriever . __name__ } for { isbn } \" f \"due to a { error . __class__ . __name__ } : \\n { error } \" ) logging . info ( error , exc_info = True ) raise Exception ( f \"all get_isbn_csl_item methods failed for { isbn } \" ) def get_isbn_csl_item_zotero ( isbn : str ): \"\"\" Generate CSL JSON Data for an ISBN using Zotero's translation-server. \"\"\" from manubot.cite.zotero import get_csl_item return get_csl_item ( f \"isbn: { isbn } \" ) def get_isbn_csl_item_citoid ( isbn : str ): \"\"\" Return CSL JSON Data for an ISBN using the Wikipedia Citoid API. https://en.wikipedia.org/api/rest_v1/#!/Citation/getCitation \"\"\" import requests from manubot.util import get_manubot_user_agent headers = { \"User-Agent\" : get_manubot_user_agent ()} url = f \"https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/ { isbn } \" response = requests . get ( url , headers = headers ) result = response . json () if isinstance ( result , dict ): if result [ \"title\" ] == \"Not found.\" : raise KeyError ( f \"Metadata for ISBN { isbn } not found at { url } \" ) else : raise Exception ( f \"Unable to extract CSL from JSON metadata for ISBN { isbn } : \\n \" f \" { json . dumps ( result . text ) } \" ) ( mediawiki ,) = result csl_item = dict () csl_item [ \"type\" ] = mediawiki . get ( \"itemType\" , \"book\" ) if \"title\" in mediawiki : csl_item [ \"title\" ] = mediawiki [ \"title\" ] if \"author\" in mediawiki : csl_author = list () for last , first in mediawiki [ \"author\" ]: csl_author . append ({ \"given\" : first , \"family\" : last }) if csl_author : csl_item [ \"author\" ] = csl_author if \"date\" in mediawiki : year_pattern = re . compile ( r \"[0-9] {4} \" ) match = year_pattern . search ( mediawiki [ \"date\" ]) if match : year = int ( match . group ()) csl_item [ \"issued\" ] = { \"date-parts\" : [[ year ]]} else : logging . debug ( f \"get_isbn_csl_item_citoid: issue extracting date for ISBN { isbn } \\n \" f \"metadata retrieved from { url } \\n \" f 'unable to extract year from date field: { mediawiki [ \"date\" ] } ' ) if \"publisher\" in mediawiki : csl_item [ \"publisher\" ] = mediawiki [ \"publisher\" ] if \"place\" in mediawiki : csl_item [ \"publisher-place\" ] = mediawiki [ \"place\" ] if \"volume\" in mediawiki : csl_item [ \"volume\" ] = mediawiki [ \"volume\" ] if \"edition\" in mediawiki : csl_item [ \"edition\" ] = mediawiki [ \"edition\" ] if \"abstractNote\" in mediawiki : csl_item [ \"abstract\" ] = mediawiki [ \"abstractNote\" ] csl_item [ \"ISBN\" ] = isbn if \"source\" in mediawiki : csl_item [ \"source\" ] = mediawiki [ \"source\" ][ 0 ] if \"url\" in mediawiki : csl_item [ \"URL\" ] = mediawiki [ \"url\" ] return csl_item def get_isbn_csl_item_isbnlib ( isbn : str ): \"\"\" Generate CSL JSON Data for an ISBN using isbnlib. \"\"\" import isbnlib metadata = isbnlib . meta ( isbn ) csl_json = isbnlib . registry . bibformatters [ \"csl\" ]( metadata ) csl_data = json . loads ( csl_json ) return csl_data isbn_retrievers = [ get_isbn_csl_item_zotero , get_isbn_csl_item_citoid , get_isbn_csl_item_isbnlib , ]","title":"Module manubot.cite.isbn"},{"location":"reference/manubot/cite/isbn/#variables","text":"isbn_retrievers","title":"Variables"},{"location":"reference/manubot/cite/isbn/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/isbn/#get_isbn_csl_item","text":"def get_isbn_csl_item ( isbn : str ) Generate CSL JSON Data for an ISBN. Converts all ISBNs to 13-digit format. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable isbn_retrievers . The methods are attempted in order, with this function returning the metadata from the first non-failing method. View Source def get_isbn_csl_item ( isbn : str ): \"\"\" Generate CSL JSON Data for an ISBN. Converts all ISBNs to 13-digit format. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `isbn_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\"\" import isbnlib isbn = isbnlib . to_isbn13 ( isbn ) for retriever in isbn_retrievers : try : return retriever ( isbn ) except Exception as error : logging . warning ( f \"Error in { retriever . __name__ } for { isbn } \" f \"due to a { error . __class__ . __name__ } : \\n { error } \" ) logging . info ( error , exc_info = True ) raise Exception ( f \"all get_isbn_csl_item methods failed for { isbn } \" )","title":"get_isbn_csl_item"},{"location":"reference/manubot/cite/isbn/#get_isbn_csl_item_citoid","text":"def get_isbn_csl_item_citoid ( isbn : str ) Return CSL JSON Data for an ISBN using the Wikipedia Citoid API. https://en.wikipedia.org/api/rest_v1/#!/Citation/getCitation View Source def get_isbn_csl_item_citoid ( isbn : str ): \"\"\" Return CSL JSON Data for an ISBN using the Wikipedia Citoid API. https://en.wikipedia.org/api/rest_v1/#!/Citation/getCitation \"\"\" import requests from manubot.util import get_manubot_user_agent headers = { \"User-Agent\" : get_manubot_user_agent ()} url = f \"https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/ { isbn } \" response = requests . get ( url , headers = headers ) result = response . json () if isinstance ( result , dict ): if result [ \"title\" ] == \"Not found.\" : raise KeyError ( f \"Metadata for ISBN { isbn } not found at { url } \" ) else : raise Exception ( f \"Unable to extract CSL from JSON metadata for ISBN { isbn } : \\n \" f \" { json . dumps ( result . text ) } \" ) ( mediawiki ,) = result csl_item = dict () csl_item [ \"type\" ] = mediawiki . get ( \"itemType\" , \"book\" ) if \"title\" in mediawiki : csl_item [ \"title\" ] = mediawiki [ \"title\" ] if \"author\" in mediawiki : csl_author = list () for last , first in mediawiki [ \"author\" ]: csl_author . append ({ \"given\" : first , \"family\" : last }) if csl_author : csl_item [ \"author\" ] = csl_author if \"date\" in mediawiki : year_pattern = re . compile ( r \"[0-9] {4} \" ) match = year_pattern . search ( mediawiki [ \"date\" ]) if match : year = int ( match . group ()) csl_item [ \"issued\" ] = { \"date-parts\" : [[ year ]]} else : logging . debug ( f \"get_isbn_csl_item_citoid: issue extracting date for ISBN { isbn } \\n \" f \"metadata retrieved from { url } \\n \" f 'unable to extract year from date field: { mediawiki [ \"date\" ] } ' ) if \"publisher\" in mediawiki : csl_item [ \"publisher\" ] = mediawiki [ \"publisher\" ] if \"place\" in mediawiki : csl_item [ \"publisher-place\" ] = mediawiki [ \"place\" ] if \"volume\" in mediawiki : csl_item [ \"volume\" ] = mediawiki [ \"volume\" ] if \"edition\" in mediawiki : csl_item [ \"edition\" ] = mediawiki [ \"edition\" ] if \"abstractNote\" in mediawiki : csl_item [ \"abstract\" ] = mediawiki [ \"abstractNote\" ] csl_item [ \"ISBN\" ] = isbn if \"source\" in mediawiki : csl_item [ \"source\" ] = mediawiki [ \"source\" ][ 0 ] if \"url\" in mediawiki : csl_item [ \"URL\" ] = mediawiki [ \"url\" ] return csl_item","title":"get_isbn_csl_item_citoid"},{"location":"reference/manubot/cite/isbn/#get_isbn_csl_item_isbnlib","text":"def get_isbn_csl_item_isbnlib ( isbn : str ) Generate CSL JSON Data for an ISBN using isbnlib. View Source def get_isbn_csl_item_isbnlib ( isbn : str ): \"\"\" Generate CSL JSON Data for an ISBN using isbnlib. \"\"\" import isbnlib metadata = isbnlib . meta ( isbn ) csl_json = isbnlib . registry . bibformatters [ \"csl\" ]( metadata ) csl_data = json . loads ( csl_json ) return csl_data","title":"get_isbn_csl_item_isbnlib"},{"location":"reference/manubot/cite/isbn/#get_isbn_csl_item_zotero","text":"def get_isbn_csl_item_zotero ( isbn : str ) Generate CSL JSON Data for an ISBN using Zotero's translation-server. View Source def get_isbn_csl_item_zotero ( isbn : str ): \"\"\" Generate CSL JSON Data for an ISBN using Zotero's translation-server. \"\"\" from manubot.cite.zotero import get_csl_item return get_csl_item ( f \"isbn: { isbn } \" )","title":"get_isbn_csl_item_zotero"},{"location":"reference/manubot/cite/isbn/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/isbn/#handler_isbn","text":"class Handler_ISBN ( prefix_lower : str ) View Source class Handler_ISBN ( Handler ): standard_prefix = \"isbn\" prefixes = [ \"isbn\" , ] def inspect ( self , citekey ): import isbnlib fail = isbnlib . notisbn ( citekey . accession , level = \"strict\" ) if fail : return f \"identifier violates the ISBN syntax according to isbnlib v { isbnlib . __version__ } \" def standardize_prefix_accession ( self , accession ): from isbnlib import to_isbn13 accession = to_isbn13 ( accession ) return self . standard_prefix , accession def get_csl_item ( self , citekey ): return get_isbn_csl_item ( citekey . standard_accession )","title":"Handler_ISBN"},{"location":"reference/manubot/cite/isbn/#ancestors-in-mro","text":"manubot.cite.handlers.Handler","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/isbn/#class-variables","text":"prefixes standard_prefix","title":"Class variables"},{"location":"reference/manubot/cite/isbn/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/isbn/#get_csl_item","text":"def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ) : return get_isbn_csl_item ( citekey . standard_accession )","title":"get_csl_item"},{"location":"reference/manubot/cite/isbn/#inspect","text":"def inspect ( self , citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey ): import isbnlib fail = isbnlib . notisbn ( citekey . accession , level = \"strict\" ) if fail : return f \"identifier violates the ISBN syntax according to isbnlib v { isbnlib . __version__ } \"","title":"inspect"},{"location":"reference/manubot/cite/isbn/#standardize_prefix_accession","text":"def standardize_prefix_accession ( self , accession ) Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession ): from isbnlib import to_isbn13 accession = to_isbn13 ( accession ) return self . standard_prefix , accession","title":"standardize_prefix_accession"},{"location":"reference/manubot/cite/pubmed/","text":"Module manubot.cite.pubmed None None View Source import functools import json import logging import os import warnings from typing import TYPE_CHECKING , Any , Dict , List , Optional , Union from xml.etree import ElementTree import requests from manubot.util import get_manubot_user_agent from .citekey import CiteKey from .handlers import Handler class Handler_PubMed ( Handler ): standard_prefix = \"pubmed\" prefixes = [ \"pubmed\" , \"pmid\" , ] accession_pattern = r \"[1-9][0-9]{0,7}\" def inspect ( self , citekey : CiteKey ) -> Optional [ str ]: identifier = citekey . accession # https://www.nlm.nih.gov/bsd/mms/medlineelements.html#pmid if identifier . startswith ( \"PMC\" ): return ( \"PubMed Identifiers should start with digits rather than PMC. \" f \"Should { citekey . dealiased_id !r} switch the citation source to 'pmc'?\" ) elif not self . _get_pattern () . fullmatch ( identifier ): return \"PubMed Identifiers should be 1-8 digits with no leading zeros.\" def get_csl_item ( self , citekey : CiteKey ) -> Dict [ str , Any ]: return get_pubmed_csl_item ( citekey . standard_accession ) class Handler_PMC ( Handler ): standard_prefix = \"pmc\" prefixes = [ \"pmc\" , \"pmcid\" , ] accession_pattern = r \"PMC[0-9]+\" def inspect ( self , citekey : CiteKey ) -> Optional [ str ]: identifier = citekey . accession # https://www.nlm.nih.gov/bsd/mms/medlineelements.html#pmc if not identifier . startswith ( \"PMC\" ): return \"PubMed Central Identifiers must start with 'PMC'.\" elif not self . _get_pattern () . fullmatch ( identifier ): return ( \"Identifier does not conform to the PMCID regex. \" \"Double check the PMCID.\" ) def get_csl_item ( self , citekey : CiteKey ): return get_pmc_csl_item ( citekey . standard_accession ) def get_pmc_csl_item ( pmcid : str ) -> Dict [ str , Any ]: \"\"\" Get the CSL Item for a PubMed Central record by its PMID, PMCID, or DOI, using the NCBI Citation Exporter API. https://api.ncbi.nlm.nih.gov/lit/ctxp https://github.com/manubot/manubot/issues/21 https://twitter.com/dhimmel/status/1061787168820092929 \"\"\" assert pmcid . startswith ( \"PMC\" ) csl_item = _get_literature_citation_exporter_csl_item ( \"pmc\" , pmcid [ 3 :]) if \"URL\" not in csl_item : csl_item [ \"URL\" ] = f \"https://www.ncbi.nlm.nih.gov/pmc/articles/ { csl_item . get ( 'PMCID' , pmcid ) } /\" return csl_item def _get_literature_citation_exporter_csl_item ( database : str , identifier : str ) -> Dict [ str , Any ]: \"\"\" https://api.ncbi.nlm.nih.gov/lit/ctxp \"\"\" if database not in { \"pubmed\" , \"pmc\" }: logging . error ( f \"Error calling _get_literature_citation_exporter_csl_item. \\n \" f 'database must be either \"pubmed\" or \"pmc\", not { database } ' ) assert False if not identifier : logging . error ( f \"Error calling _get_literature_citation_exporter_csl_item. \\n \" f \"identifier cannot be blank\" ) assert False params = { \"format\" : \"csl\" , \"id\" : identifier } headers = { \"User-Agent\" : get_manubot_user_agent ()} url = f \"https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/ { database } /\" response = requests . get ( url , params , headers = headers ) try : csl_item = response . json () except Exception as error : logging . error ( f \"Error fetching { database } metadata for { identifier } . \\n \" f \"Invalid JSON response from { response . url } : \\n { response . text } \" ) raise error assert isinstance ( csl_item , dict ) if csl_item . get ( \"status\" , \"okay\" ) == \"error\" : logging . error ( f \"Error fetching { database } metadata for { identifier } . \\n \" f \"Literature Citation Exporter returned JSON indicating an error for { response . url } \\n \" f \" { json . dumps ( csl_item , indent = 2 ) } \" ) assert False return csl_item def get_pubmed_csl_item ( pmid : Union [ str , int ]) -> Dict [ str , Any ]: \"\"\" Query NCBI E-Utilities to create CSL Items for PubMed IDs. https://github.com/manubot/manubot/issues/21 https://github.com/ncbi/citation-exporter/issues/3#issuecomment-355313143 \"\"\" pmid = str ( pmid ) params = { \"db\" : \"pubmed\" , \"id\" : pmid , \"rettype\" : \"full\" } headers = { \"User-Agent\" : get_manubot_user_agent ()} url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\" with _get_eutils_rate_limiter (): response = requests . get ( url , params , headers = headers ) try : xml_article_set = ElementTree . fromstring ( response . text ) assert isinstance ( xml_article_set , ElementTree . Element ) assert xml_article_set . tag == \"PubmedArticleSet\" ( xml_article ,) = list ( xml_article_set ) assert xml_article . tag in [ \"PubmedArticle\" , \"PubmedBookArticle\" ] except Exception as error : logging . error ( f \"Error fetching PubMed metadata for { pmid } . \\n \" f \"Unsupported XML response from { response . url } : \\n { response . text } \" ) raise error try : csl_item = csl_item_from_pubmed_article ( xml_article ) except Exception as error : msg = f \"Error parsing the following PubMed metadata for PMID { pmid } : \\n { response . text } \" logging . error ( msg ) raise error return csl_item def csl_item_from_pubmed_article ( article : ElementTree . Element ) -> Dict [ str , Any ]: \"\"\" Extract a CSL Item dictionary from a PubmedArticle XML element. https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" if not article . tag == \"PubmedArticle\" : raise ValueError ( f \"Expected article to be an XML element with tag PubmedArticle, received tag { article . tag !r} \" ) csl_item = dict () if not article . find ( \"MedlineCitation/Article\" ): raise NotImplementedError ( \"Unsupported PubMed record: no <Article> element\" ) title = article . findtext ( \"MedlineCitation/Article/ArticleTitle\" ) if title : csl_item [ \"title\" ] = title volume = article . findtext ( \"MedlineCitation/Article/Journal/JournalIssue/Volume\" ) if volume : csl_item [ \"volume\" ] = volume issue = article . findtext ( \"MedlineCitation/Article/Journal/JournalIssue/Issue\" ) if issue : csl_item [ \"issue\" ] = issue page = article . findtext ( \"MedlineCitation/Article/Pagination/MedlinePgn\" ) if page : csl_item [ \"page\" ] = page journal = article . findtext ( \"MedlineCitation/Article/Journal/Title\" ) if journal : csl_item [ \"container-title\" ] = journal journal_short = article . findtext ( \"MedlineCitation/Article/Journal/ISOAbbreviation\" ) if journal_short : csl_item [ \"container-title-short\" ] = journal_short issn = article . findtext ( \"MedlineCitation/Article/Journal/ISSN\" ) if issn : csl_item [ \"ISSN\" ] = issn date_parts = extract_publication_date_parts ( article ) if date_parts : csl_item [ \"issued\" ] = { \"date-parts\" : [ date_parts ]} authors_csl = list () authors = article . findall ( \"MedlineCitation/Article/AuthorList/Author\" ) for author in authors : author_csl = dict () given = author . findtext ( \"ForeName\" ) if given : author_csl [ \"given\" ] = given family = author . findtext ( \"LastName\" ) if family : author_csl [ \"family\" ] = family authors_csl . append ( author_csl ) if authors_csl : csl_item [ \"author\" ] = authors_csl for id_type , key in ( \"pubmed\" , \"PMID\" ), ( \"pmc\" , \"PMCID\" ), ( \"doi\" , \"DOI\" ): xpath = f \"PubmedData/ArticleIdList/ArticleId[@IdType=' { id_type } ']\" value = article . findtext ( xpath ) if value : csl_item [ key ] = value . lower () if key == \"DOI\" else value abstract = article . findtext ( \"MedlineCitation/Article/Abstract/AbstractText\" ) if abstract : csl_item [ \"abstract\" ] = abstract csl_item [ \"URL\" ] = f \"https://www.ncbi.nlm.nih.gov/pubmed/ { csl_item [ 'PMID' ] } \" csl_item [ \"type\" ] = \"article-journal\" return csl_item month_abbrev_to_int : Dict [ str , int ] = { \"Jan\" : 1 , \"Feb\" : 2 , \"Mar\" : 3 , \"Apr\" : 4 , \"May\" : 5 , \"Jun\" : 6 , \"Jul\" : 7 , \"Aug\" : 8 , \"Sep\" : 9 , \"Oct\" : 10 , \"Nov\" : 11 , \"Dec\" : 12 , } def extract_publication_date_parts ( article : ElementTree . Element ) -> List [ int ]: \"\"\" Extract date published from a PubmedArticle XML element. \"\"\" date_parts = [] # Electronic articles date = article . find ( \"MedlineCitation/Article/ArticleDate\" ) if date : for part in \"Year\" , \"Month\" , \"Day\" : part = date . findtext ( part ) if not part : break date_parts . append ( int ( part )) return date_parts # Print articles date = article . find ( \"MedlineCitation/Article/Journal/JournalIssue/PubDate\" ) year = date . findtext ( \"Year\" ) if year : date_parts . append ( int ( year )) month = date . findtext ( \"Month\" ) if month : try : date_parts . append ( month_abbrev_to_int [ month ]) except KeyError : date_parts . append ( int ( month )) day = date . findtext ( \"Day\" ) if day : date_parts . append ( int ( day )) return date_parts def get_pmcid_and_pmid_for_doi ( doi : str ) -> Dict [ str , str ]: \"\"\" Query PMC's ID Converter API to retrieve the PMCID and PMID for a DOI. Does not work for DOIs that are in Pubmed but not PubMed Central. https://www.ncbi.nlm.nih.gov/pmc/tools/id-converter-api/ \"\"\" assert isinstance ( doi , str ) assert doi . startswith ( \"10.\" ) params = { \"ids\" : doi , \"tool\" : \"manubot\" } url = \"https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/\" response = requests . get ( url , params ) if not response . ok : logging . warning ( f \"Status code { response . status_code } querying { response . url } \\n \" ) return {} try : element_tree = ElementTree . fromstring ( response . text ) assert element_tree . tag == \"pmcids\" except Exception : logging . warning ( f \"Error fetching PMC ID conversion for { doi } . \\n \" f \"Response from { response . url } : \\n { response . text } \" ) return {} records = element_tree . findall ( \"record\" ) if len ( records ) != 1 : logging . warning ( f \"Expected PubMed Central ID converter to return a single XML record for { doi } . \\n \" f \"Response from { response . url } : \\n { response . text } \" ) return {} ( record ,) = records if record . findtext ( \"status\" , default = \"okay\" ) == \"error\" : return {} id_dict = {} for id_type in \"pmcid\" , \"pmid\" : id_ = record . get ( id_type ) if id_ : id_dict [ id_type . upper ()] = id_ return id_dict def get_pmid_for_doi ( doi : str ) -> Optional [ str ]: \"\"\" Query NCBI's E-utilities to retrieve the PMID for a DOI. \"\"\" assert isinstance ( doi , str ) assert doi . startswith ( \"10.\" ) params = { \"db\" : \"pubmed\" , \"term\" : f \" { doi } [DOI]\" } headers = { \"User-Agent\" : get_manubot_user_agent ()} url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\" with _get_eutils_rate_limiter (): response = requests . get ( url , params , headers = headers ) if not response . ok : logging . warning ( f \"Status code { response . status_code } querying { response . url } \\n \" ) return None try : element_tree = ElementTree . fromstring ( response . text ) assert isinstance ( element_tree , ElementTree . Element ) assert element_tree . tag == \"eSearchResult\" except Exception : logging . warning ( f \"Error in ESearch XML for DOI: { doi } . \\n \" f \"Response from { response . url } : \\n { response . text } \" ) return None id_elems = element_tree . findall ( \"IdList/Id\" ) if len ( id_elems ) != 1 : logging . debug ( f \"No PMIDs found for { doi } . \\n \" f \"Response from { response . url } : \\n { response . text } \" ) return None ( id_elem ,) = id_elems return id_elem . text def get_pubmed_ids_for_doi ( doi : str ) -> Dict [ str , str ]: \"\"\" Return a dictionary with PMCID and PMID, if they exist, for the specified DOI. See https://github.com/manubot/manubot/issues/45. \"\"\" pubmed_ids = get_pmcid_and_pmid_for_doi ( doi ) if not pubmed_ids : pmid = get_pmid_for_doi ( doi ) if pmid : pubmed_ids [ \"PMID\" ] = pmid return pubmed_ids if TYPE_CHECKING : # support RateLimiter return type while avoiding unused runtime import # https://stackoverflow.com/a/39757388/4651668 from ratelimiter import RateLimiter @functools . lru_cache () def _get_eutils_rate_limiter () -> \"RateLimiter\" : \"\"\" Rate limiter to cap NCBI E-utilities queries to <= 3 per second as per https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities/ \"\"\" with warnings . catch_warnings (): # https://github.com/RazerM/ratelimiter/issues/10 # https://github.com/manubot/manubot/issues/257 warnings . filterwarnings ( \"ignore\" , category = DeprecationWarning ) from ratelimiter import RateLimiter if \"CI\" in os . environ : # multiple CI jobs might be running concurrently return RateLimiter ( max_calls = 1 , period = 1.5 ) return RateLimiter ( max_calls = 2 , period = 1 ) Variables TYPE_CHECKING month_abbrev_to_int Functions csl_item_from_pubmed_article def csl_item_from_pubmed_article ( article : xml . etree . ElementTree . Element ) -> Dict [ str , Any ] Extract a CSL Item dictionary from a PubmedArticle XML element. https://github.com/citation-style-language/schema/blob/master/csl-data.json View Source def csl_item_from_pubmed_article ( article : ElementTree . Element ) -> Dict [ str, Any ] : \"\"\" Extract a CSL Item dictionary from a PubmedArticle XML element. https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" if not article . tag == \"PubmedArticle\" : raise ValueError ( f \"Expected article to be an XML element with tag PubmedArticle, received tag {article.tag!r}\" ) csl_item = dict () if not article . find ( \"MedlineCitation/Article\" ) : raise NotImplementedError ( \"Unsupported PubMed record: no <Article> element\" ) title = article . findtext ( \"MedlineCitation/Article/ArticleTitle\" ) if title : csl_item [ \"title\" ] = title volume = article . findtext ( \"MedlineCitation/Article/Journal/JournalIssue/Volume\" ) if volume : csl_item [ \"volume\" ] = volume issue = article . findtext ( \"MedlineCitation/Article/Journal/JournalIssue/Issue\" ) if issue : csl_item [ \"issue\" ] = issue page = article . findtext ( \"MedlineCitation/Article/Pagination/MedlinePgn\" ) if page : csl_item [ \"page\" ] = page journal = article . findtext ( \"MedlineCitation/Article/Journal/Title\" ) if journal : csl_item [ \"container-title\" ] = journal journal_short = article . findtext ( \"MedlineCitation/Article/Journal/ISOAbbreviation\" ) if journal_short : csl_item [ \"container-title-short\" ] = journal_short issn = article . findtext ( \"MedlineCitation/Article/Journal/ISSN\" ) if issn : csl_item [ \"ISSN\" ] = issn date_parts = extract_publication_date_parts ( article ) if date_parts : csl_item [ \"issued\" ] = { \"date-parts\" : [ date_parts ] } authors_csl = list () authors = article . findall ( \"MedlineCitation/Article/AuthorList/Author\" ) for author in authors : author_csl = dict () given = author . findtext ( \"ForeName\" ) if given : author_csl [ \"given\" ] = given family = author . findtext ( \"LastName\" ) if family : author_csl [ \"family\" ] = family authors_csl . append ( author_csl ) if authors_csl : csl_item [ \"author\" ] = authors_csl for id_type , key in ( \"pubmed\" , \"PMID\" ), ( \"pmc\" , \"PMCID\" ), ( \"doi\" , \"DOI\" ) : xpath = f \"PubmedData/ArticleIdList/ArticleId[@IdType='{id_type}']\" value = article . findtext ( xpath ) if value : csl_item [ key ] = value . lower () if key == \"DOI\" else value abstract = article . findtext ( \"MedlineCitation/Article/Abstract/AbstractText\" ) if abstract : csl_item [ \"abstract\" ] = abstract csl_item [ \"URL\" ] = f \"https://www.ncbi.nlm.nih.gov/pubmed/{csl_item['PMID']}\" csl_item [ \"type\" ] = \"article-journal\" return csl_item extract_publication_date_parts def extract_publication_date_parts ( article : xml . etree . ElementTree . Element ) -> List [ int ] Extract date published from a PubmedArticle XML element. View Source def extract_publication_date_parts ( article : ElementTree . Element ) -> List [ int ] : \"\"\" Extract date published from a PubmedArticle XML element. \"\"\" date_parts = [] # Electronic articles date = article . find ( \"MedlineCitation/Article/ArticleDate\" ) if date : for part in \"Year\" , \"Month\" , \"Day\" : part = date . findtext ( part ) if not part : break date_parts . append ( int ( part )) return date_parts # Print articles date = article . find ( \"MedlineCitation/Article/Journal/JournalIssue/PubDate\" ) year = date . findtext ( \"Year\" ) if year : date_parts . append ( int ( year )) month = date . findtext ( \"Month\" ) if month : try : date_parts . append ( month_abbrev_to_int [ month ] ) except KeyError : date_parts . append ( int ( month )) day = date . findtext ( \"Day\" ) if day : date_parts . append ( int ( day )) return date_parts get_pmc_csl_item def get_pmc_csl_item ( pmcid : str ) -> Dict [ str , Any ] Get the CSL Item for a PubMed Central record by its PMID, PMCID, or DOI, using the NCBI Citation Exporter API. https://api.ncbi.nlm.nih.gov/lit/ctxp https://github.com/manubot/manubot/issues/21 https://twitter.com/dhimmel/status/1061787168820092929 View Source def get_pmc_csl_item ( pmcid : str ) -> Dict [ str , Any ]: \"\"\" Get the CSL Item for a PubMed Central record by its PMID, PMCID, or DOI, using the NCBI Citation Exporter API. https://api.ncbi.nlm.nih.gov/lit/ctxp https://github.com/manubot/manubot/issues/21 https://twitter.com/dhimmel/status/1061787168820092929 \"\"\" assert pmcid . startswith ( \"PMC\" ) csl_item = _get_literature_citation_exporter_csl_item ( \"pmc\" , pmcid [ 3 :]) if \"URL\" not in csl_item : csl_item [ \"URL\" ] = f \"https://www.ncbi.nlm.nih.gov/pmc/articles/{csl_item.get('PMCID', pmcid)}/\" return csl_item get_pmcid_and_pmid_for_doi def get_pmcid_and_pmid_for_doi ( doi : str ) -> Dict [ str , str ] Query PMC's ID Converter API to retrieve the PMCID and PMID for a DOI. Does not work for DOIs that are in Pubmed but not PubMed Central. https://www.ncbi.nlm.nih.gov/pmc/tools/id-converter-api/ View Source def get_pmcid_and_pmid_for_doi ( doi : str ) -> Dict [ str , str ]: \"\"\" Query PMC's ID Converter API to retrieve the PMCID and PMID for a DOI. Does not work for DOIs that are in Pubmed but not PubMed Central. https://www.ncbi.nlm.nih.gov/pmc/tools/id-converter-api/ \"\"\" assert isinstance ( doi , str ) assert doi . startswith ( \"10.\" ) params = { \"ids\" : doi , \"tool\" : \"manubot\" } url = \"https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/\" response = requests . get ( url , params ) if not response . ok : logging . warning ( f \"Status code {response.status_code} querying {response.url} \\n \" ) return {} try : element_tree = ElementTree . fromstring ( response . text ) assert element_tree . tag == \"pmcids\" except Exception : logging . warning ( f \"Error fetching PMC ID conversion for {doi}. \\n \" f \"Response from {response.url}: \\n {response.text}\" ) return {} records = element_tree . findall ( \"record\" ) if len ( records ) != 1 : logging . warning ( f \"Expected PubMed Central ID converter to return a single XML record for {doi}. \\n \" f \"Response from {response.url}: \\n {response.text}\" ) return {} ( record ,) = records if record . findtext ( \"status\" , default = \"okay\" ) == \"error\" : return {} id_dict = {} for id_type in \"pmcid\" , \"pmid\" : id_ = record . get ( id_type ) if id_ : id_dict [ id_type . upper ()] = id_ return id_dict get_pmid_for_doi def get_pmid_for_doi ( doi : str ) -> Optional [ str ] Query NCBI's E-utilities to retrieve the PMID for a DOI. View Source def get_pmid_for_doi ( doi : str ) -> Optional [ str ] : \"\"\" Query NCBI's E-utilities to retrieve the PMID for a DOI. \"\"\" assert isinstance ( doi , str ) assert doi . startswith ( \"10.\" ) params = { \"db\" : \"pubmed\" , \"term\" : f \"{doi}[DOI]\" } headers = { \"User-Agent\" : get_manubot_user_agent () } url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\" with _get_eutils_rate_limiter () : response = requests . get ( url , params , headers = headers ) if not response . ok : logging . warning ( f \"Status code {response.status_code} querying {response.url}\\n\" ) return None try : element_tree = ElementTree . fromstring ( response . text ) assert isinstance ( element_tree , ElementTree . Element ) assert element_tree . tag == \"eSearchResult\" except Exception : logging . warning ( f \"Error in ESearch XML for DOI: {doi}.\\n\" f \"Response from {response.url}:\\n{response.text}\" ) return None id_elems = element_tree . findall ( \"IdList/Id\" ) if len ( id_elems ) != 1 : logging . debug ( f \"No PMIDs found for {doi}.\\n\" f \"Response from {response.url}:\\n{response.text}\" ) return None ( id_elem ,) = id_elems return id_elem . text get_pubmed_csl_item def get_pubmed_csl_item ( pmid : Union [ str , int ] ) -> Dict [ str , Any ] Query NCBI E-Utilities to create CSL Items for PubMed IDs. https://github.com/manubot/manubot/issues/21 https://github.com/ncbi/citation-exporter/issues/3#issuecomment-355313143 View Source def get_pubmed_csl_item ( pmid : Union [ str , int ]) -> Dict [ str , Any ]: \"\"\" Query NCBI E-Utilities to create CSL Items for PubMed IDs. https://github.com/manubot/manubot/issues/21 https://github.com/ncbi/citation-exporter/issues/3#issuecomment-355313143 \"\"\" pmid = str ( pmid ) params = { \"db\" : \"pubmed\" , \"id\" : pmid , \"rettype\" : \"full\" } headers = { \"User-Agent\" : get_manubot_user_agent ()} url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\" with _get_eutils_rate_limiter (): response = requests . get ( url , params , headers = headers ) try : xml_article_set = ElementTree . fromstring ( response . text ) assert isinstance ( xml_article_set , ElementTree . Element ) assert xml_article_set . tag == \"PubmedArticleSet\" ( xml_article ,) = list ( xml_article_set ) assert xml_article . tag in [ \"PubmedArticle\" , \"PubmedBookArticle\" ] except Exception as error : logging . error ( f \"Error fetching PubMed metadata for {pmid}. \\n \" f \"Unsupported XML response from {response.url}: \\n {response.text}\" ) raise error try : csl_item = csl_item_from_pubmed_article ( xml_article ) except Exception as error : msg = f \"Error parsing the following PubMed metadata for PMID {pmid}: \\n {response.text}\" logging . error ( msg ) raise error return csl_item get_pubmed_ids_for_doi def get_pubmed_ids_for_doi ( doi : str ) -> Dict [ str , str ] Return a dictionary with PMCID and PMID, if they exist, for the specified DOI. See https://github.com/manubot/manubot/issues/45. View Source def get_pubmed_ids_for_doi ( doi : str ) -> Dict [ str , str ] : \"\"\" Return a dictionary with PMCID and PMID, if they exist, for the specified DOI. See https://github.com/manubot/manubot/issues/45. \"\"\" pubmed_ids = get_pmcid_and_pmid_for_doi ( doi ) if not pubmed_ids : pmid = get_pmid_for_doi ( doi ) if pmid : pubmed_ids [ \"PMID\" ] = pmid return pubmed_ids Classes Handler_PMC class Handler_PMC ( prefix_lower : str ) View Source class Handler_PMC ( Handler ) : standard_prefix = \"pmc\" prefixes = [ \"pmc\", \"pmcid\", ] accession_pattern = r \"PMC[0-9]+\" def inspect ( self , citekey : CiteKey ) -> Optional [ str ] : identifier = citekey . accession # https : // www . nlm . nih . gov / bsd / mms / medlineelements . html#pmc if not identifier . startswith ( \"PMC\" ) : return \"PubMed Central Identifiers must start with 'PMC'.\" elif not self . _get_pattern (). fullmatch ( identifier ) : return ( \"Identifier does not conform to the PMCID regex. \" \"Double check the PMCID.\" ) def get_csl_item ( self , citekey : CiteKey ) : return get_pmc_csl_item ( citekey . standard_accession ) Ancestors (in MRO) manubot.cite.handlers.Handler Class variables accession_pattern prefixes standard_prefix Methods get_csl_item def get_csl_item ( self , citekey : manubot . cite . citekey . CiteKey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey : CiteKey ) : return get_pmc_csl_item ( citekey . standard_accession ) inspect def inspect ( self , citekey : manubot . cite . citekey . CiteKey ) -> Optional [ str ] Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey : CiteKey ) -> Optional [ str ] : identifier = citekey . accession # https : // www . nlm . nih . gov / bsd / mms / medlineelements . html#pmc if not identifier . startswith ( \"PMC\" ) : return \"PubMed Central Identifiers must start with 'PMC'.\" elif not self . _get_pattern (). fullmatch ( identifier ) : return ( \"Identifier does not conform to the PMCID regex. \" \"Double check the PMCID.\" ) standardize_prefix_accession def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ] Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ] : \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession Handler_PubMed class Handler_PubMed ( prefix_lower : str ) View Source class Handler_PubMed ( Handler ) : standard_prefix = \"pubmed\" prefixes = [ \"pubmed\", \"pmid\", ] accession_pattern = r \"[1-9][0-9]{0,7}\" def inspect ( self , citekey : CiteKey ) -> Optional [ str ] : identifier = citekey . accession # https : // www . nlm . nih . gov / bsd / mms / medlineelements . html#pmid if identifier . startswith ( \"PMC\" ) : return ( \"PubMed Identifiers should start with digits rather than PMC. \" f \"Should {citekey.dealiased_id!r} switch the citation source to 'pmc'?\" ) elif not self . _get_pattern (). fullmatch ( identifier ) : return \"PubMed Identifiers should be 1-8 digits with no leading zeros.\" def get_csl_item ( self , citekey : CiteKey ) -> Dict [ str, Any ] : return get_pubmed_csl_item ( citekey . standard_accession ) Ancestors (in MRO) manubot.cite.handlers.Handler Class variables accession_pattern prefixes standard_prefix Methods get_csl_item def get_csl_item ( self , citekey : manubot . cite . citekey . CiteKey ) -> Dict [ str , Any ] Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey : CiteKey ) -> Dict [ str , Any ] : return get_pubmed_csl_item ( citekey . standard_accession ) inspect def inspect ( self , citekey : manubot . cite . citekey . CiteKey ) -> Optional [ str ] Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey : CiteKey ) -> Optional [ str ] : identifier = citekey . accession # https : // www . nlm . nih . gov / bsd / mms / medlineelements . html#pmid if identifier . startswith ( \"PMC\" ) : return ( \"PubMed Identifiers should start with digits rather than PMC. \" f \"Should {citekey.dealiased_id!r} switch the citation source to 'pmc'?\" ) elif not self . _get_pattern (). fullmatch ( identifier ) : return \"PubMed Identifiers should be 1-8 digits with no leading zeros.\" standardize_prefix_accession def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ] Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ] : \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession","title":"Pubmed"},{"location":"reference/manubot/cite/pubmed/#module-manubotcitepubmed","text":"None None View Source import functools import json import logging import os import warnings from typing import TYPE_CHECKING , Any , Dict , List , Optional , Union from xml.etree import ElementTree import requests from manubot.util import get_manubot_user_agent from .citekey import CiteKey from .handlers import Handler class Handler_PubMed ( Handler ): standard_prefix = \"pubmed\" prefixes = [ \"pubmed\" , \"pmid\" , ] accession_pattern = r \"[1-9][0-9]{0,7}\" def inspect ( self , citekey : CiteKey ) -> Optional [ str ]: identifier = citekey . accession # https://www.nlm.nih.gov/bsd/mms/medlineelements.html#pmid if identifier . startswith ( \"PMC\" ): return ( \"PubMed Identifiers should start with digits rather than PMC. \" f \"Should { citekey . dealiased_id !r} switch the citation source to 'pmc'?\" ) elif not self . _get_pattern () . fullmatch ( identifier ): return \"PubMed Identifiers should be 1-8 digits with no leading zeros.\" def get_csl_item ( self , citekey : CiteKey ) -> Dict [ str , Any ]: return get_pubmed_csl_item ( citekey . standard_accession ) class Handler_PMC ( Handler ): standard_prefix = \"pmc\" prefixes = [ \"pmc\" , \"pmcid\" , ] accession_pattern = r \"PMC[0-9]+\" def inspect ( self , citekey : CiteKey ) -> Optional [ str ]: identifier = citekey . accession # https://www.nlm.nih.gov/bsd/mms/medlineelements.html#pmc if not identifier . startswith ( \"PMC\" ): return \"PubMed Central Identifiers must start with 'PMC'.\" elif not self . _get_pattern () . fullmatch ( identifier ): return ( \"Identifier does not conform to the PMCID regex. \" \"Double check the PMCID.\" ) def get_csl_item ( self , citekey : CiteKey ): return get_pmc_csl_item ( citekey . standard_accession ) def get_pmc_csl_item ( pmcid : str ) -> Dict [ str , Any ]: \"\"\" Get the CSL Item for a PubMed Central record by its PMID, PMCID, or DOI, using the NCBI Citation Exporter API. https://api.ncbi.nlm.nih.gov/lit/ctxp https://github.com/manubot/manubot/issues/21 https://twitter.com/dhimmel/status/1061787168820092929 \"\"\" assert pmcid . startswith ( \"PMC\" ) csl_item = _get_literature_citation_exporter_csl_item ( \"pmc\" , pmcid [ 3 :]) if \"URL\" not in csl_item : csl_item [ \"URL\" ] = f \"https://www.ncbi.nlm.nih.gov/pmc/articles/ { csl_item . get ( 'PMCID' , pmcid ) } /\" return csl_item def _get_literature_citation_exporter_csl_item ( database : str , identifier : str ) -> Dict [ str , Any ]: \"\"\" https://api.ncbi.nlm.nih.gov/lit/ctxp \"\"\" if database not in { \"pubmed\" , \"pmc\" }: logging . error ( f \"Error calling _get_literature_citation_exporter_csl_item. \\n \" f 'database must be either \"pubmed\" or \"pmc\", not { database } ' ) assert False if not identifier : logging . error ( f \"Error calling _get_literature_citation_exporter_csl_item. \\n \" f \"identifier cannot be blank\" ) assert False params = { \"format\" : \"csl\" , \"id\" : identifier } headers = { \"User-Agent\" : get_manubot_user_agent ()} url = f \"https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/ { database } /\" response = requests . get ( url , params , headers = headers ) try : csl_item = response . json () except Exception as error : logging . error ( f \"Error fetching { database } metadata for { identifier } . \\n \" f \"Invalid JSON response from { response . url } : \\n { response . text } \" ) raise error assert isinstance ( csl_item , dict ) if csl_item . get ( \"status\" , \"okay\" ) == \"error\" : logging . error ( f \"Error fetching { database } metadata for { identifier } . \\n \" f \"Literature Citation Exporter returned JSON indicating an error for { response . url } \\n \" f \" { json . dumps ( csl_item , indent = 2 ) } \" ) assert False return csl_item def get_pubmed_csl_item ( pmid : Union [ str , int ]) -> Dict [ str , Any ]: \"\"\" Query NCBI E-Utilities to create CSL Items for PubMed IDs. https://github.com/manubot/manubot/issues/21 https://github.com/ncbi/citation-exporter/issues/3#issuecomment-355313143 \"\"\" pmid = str ( pmid ) params = { \"db\" : \"pubmed\" , \"id\" : pmid , \"rettype\" : \"full\" } headers = { \"User-Agent\" : get_manubot_user_agent ()} url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\" with _get_eutils_rate_limiter (): response = requests . get ( url , params , headers = headers ) try : xml_article_set = ElementTree . fromstring ( response . text ) assert isinstance ( xml_article_set , ElementTree . Element ) assert xml_article_set . tag == \"PubmedArticleSet\" ( xml_article ,) = list ( xml_article_set ) assert xml_article . tag in [ \"PubmedArticle\" , \"PubmedBookArticle\" ] except Exception as error : logging . error ( f \"Error fetching PubMed metadata for { pmid } . \\n \" f \"Unsupported XML response from { response . url } : \\n { response . text } \" ) raise error try : csl_item = csl_item_from_pubmed_article ( xml_article ) except Exception as error : msg = f \"Error parsing the following PubMed metadata for PMID { pmid } : \\n { response . text } \" logging . error ( msg ) raise error return csl_item def csl_item_from_pubmed_article ( article : ElementTree . Element ) -> Dict [ str , Any ]: \"\"\" Extract a CSL Item dictionary from a PubmedArticle XML element. https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" if not article . tag == \"PubmedArticle\" : raise ValueError ( f \"Expected article to be an XML element with tag PubmedArticle, received tag { article . tag !r} \" ) csl_item = dict () if not article . find ( \"MedlineCitation/Article\" ): raise NotImplementedError ( \"Unsupported PubMed record: no <Article> element\" ) title = article . findtext ( \"MedlineCitation/Article/ArticleTitle\" ) if title : csl_item [ \"title\" ] = title volume = article . findtext ( \"MedlineCitation/Article/Journal/JournalIssue/Volume\" ) if volume : csl_item [ \"volume\" ] = volume issue = article . findtext ( \"MedlineCitation/Article/Journal/JournalIssue/Issue\" ) if issue : csl_item [ \"issue\" ] = issue page = article . findtext ( \"MedlineCitation/Article/Pagination/MedlinePgn\" ) if page : csl_item [ \"page\" ] = page journal = article . findtext ( \"MedlineCitation/Article/Journal/Title\" ) if journal : csl_item [ \"container-title\" ] = journal journal_short = article . findtext ( \"MedlineCitation/Article/Journal/ISOAbbreviation\" ) if journal_short : csl_item [ \"container-title-short\" ] = journal_short issn = article . findtext ( \"MedlineCitation/Article/Journal/ISSN\" ) if issn : csl_item [ \"ISSN\" ] = issn date_parts = extract_publication_date_parts ( article ) if date_parts : csl_item [ \"issued\" ] = { \"date-parts\" : [ date_parts ]} authors_csl = list () authors = article . findall ( \"MedlineCitation/Article/AuthorList/Author\" ) for author in authors : author_csl = dict () given = author . findtext ( \"ForeName\" ) if given : author_csl [ \"given\" ] = given family = author . findtext ( \"LastName\" ) if family : author_csl [ \"family\" ] = family authors_csl . append ( author_csl ) if authors_csl : csl_item [ \"author\" ] = authors_csl for id_type , key in ( \"pubmed\" , \"PMID\" ), ( \"pmc\" , \"PMCID\" ), ( \"doi\" , \"DOI\" ): xpath = f \"PubmedData/ArticleIdList/ArticleId[@IdType=' { id_type } ']\" value = article . findtext ( xpath ) if value : csl_item [ key ] = value . lower () if key == \"DOI\" else value abstract = article . findtext ( \"MedlineCitation/Article/Abstract/AbstractText\" ) if abstract : csl_item [ \"abstract\" ] = abstract csl_item [ \"URL\" ] = f \"https://www.ncbi.nlm.nih.gov/pubmed/ { csl_item [ 'PMID' ] } \" csl_item [ \"type\" ] = \"article-journal\" return csl_item month_abbrev_to_int : Dict [ str , int ] = { \"Jan\" : 1 , \"Feb\" : 2 , \"Mar\" : 3 , \"Apr\" : 4 , \"May\" : 5 , \"Jun\" : 6 , \"Jul\" : 7 , \"Aug\" : 8 , \"Sep\" : 9 , \"Oct\" : 10 , \"Nov\" : 11 , \"Dec\" : 12 , } def extract_publication_date_parts ( article : ElementTree . Element ) -> List [ int ]: \"\"\" Extract date published from a PubmedArticle XML element. \"\"\" date_parts = [] # Electronic articles date = article . find ( \"MedlineCitation/Article/ArticleDate\" ) if date : for part in \"Year\" , \"Month\" , \"Day\" : part = date . findtext ( part ) if not part : break date_parts . append ( int ( part )) return date_parts # Print articles date = article . find ( \"MedlineCitation/Article/Journal/JournalIssue/PubDate\" ) year = date . findtext ( \"Year\" ) if year : date_parts . append ( int ( year )) month = date . findtext ( \"Month\" ) if month : try : date_parts . append ( month_abbrev_to_int [ month ]) except KeyError : date_parts . append ( int ( month )) day = date . findtext ( \"Day\" ) if day : date_parts . append ( int ( day )) return date_parts def get_pmcid_and_pmid_for_doi ( doi : str ) -> Dict [ str , str ]: \"\"\" Query PMC's ID Converter API to retrieve the PMCID and PMID for a DOI. Does not work for DOIs that are in Pubmed but not PubMed Central. https://www.ncbi.nlm.nih.gov/pmc/tools/id-converter-api/ \"\"\" assert isinstance ( doi , str ) assert doi . startswith ( \"10.\" ) params = { \"ids\" : doi , \"tool\" : \"manubot\" } url = \"https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/\" response = requests . get ( url , params ) if not response . ok : logging . warning ( f \"Status code { response . status_code } querying { response . url } \\n \" ) return {} try : element_tree = ElementTree . fromstring ( response . text ) assert element_tree . tag == \"pmcids\" except Exception : logging . warning ( f \"Error fetching PMC ID conversion for { doi } . \\n \" f \"Response from { response . url } : \\n { response . text } \" ) return {} records = element_tree . findall ( \"record\" ) if len ( records ) != 1 : logging . warning ( f \"Expected PubMed Central ID converter to return a single XML record for { doi } . \\n \" f \"Response from { response . url } : \\n { response . text } \" ) return {} ( record ,) = records if record . findtext ( \"status\" , default = \"okay\" ) == \"error\" : return {} id_dict = {} for id_type in \"pmcid\" , \"pmid\" : id_ = record . get ( id_type ) if id_ : id_dict [ id_type . upper ()] = id_ return id_dict def get_pmid_for_doi ( doi : str ) -> Optional [ str ]: \"\"\" Query NCBI's E-utilities to retrieve the PMID for a DOI. \"\"\" assert isinstance ( doi , str ) assert doi . startswith ( \"10.\" ) params = { \"db\" : \"pubmed\" , \"term\" : f \" { doi } [DOI]\" } headers = { \"User-Agent\" : get_manubot_user_agent ()} url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\" with _get_eutils_rate_limiter (): response = requests . get ( url , params , headers = headers ) if not response . ok : logging . warning ( f \"Status code { response . status_code } querying { response . url } \\n \" ) return None try : element_tree = ElementTree . fromstring ( response . text ) assert isinstance ( element_tree , ElementTree . Element ) assert element_tree . tag == \"eSearchResult\" except Exception : logging . warning ( f \"Error in ESearch XML for DOI: { doi } . \\n \" f \"Response from { response . url } : \\n { response . text } \" ) return None id_elems = element_tree . findall ( \"IdList/Id\" ) if len ( id_elems ) != 1 : logging . debug ( f \"No PMIDs found for { doi } . \\n \" f \"Response from { response . url } : \\n { response . text } \" ) return None ( id_elem ,) = id_elems return id_elem . text def get_pubmed_ids_for_doi ( doi : str ) -> Dict [ str , str ]: \"\"\" Return a dictionary with PMCID and PMID, if they exist, for the specified DOI. See https://github.com/manubot/manubot/issues/45. \"\"\" pubmed_ids = get_pmcid_and_pmid_for_doi ( doi ) if not pubmed_ids : pmid = get_pmid_for_doi ( doi ) if pmid : pubmed_ids [ \"PMID\" ] = pmid return pubmed_ids if TYPE_CHECKING : # support RateLimiter return type while avoiding unused runtime import # https://stackoverflow.com/a/39757388/4651668 from ratelimiter import RateLimiter @functools . lru_cache () def _get_eutils_rate_limiter () -> \"RateLimiter\" : \"\"\" Rate limiter to cap NCBI E-utilities queries to <= 3 per second as per https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities/ \"\"\" with warnings . catch_warnings (): # https://github.com/RazerM/ratelimiter/issues/10 # https://github.com/manubot/manubot/issues/257 warnings . filterwarnings ( \"ignore\" , category = DeprecationWarning ) from ratelimiter import RateLimiter if \"CI\" in os . environ : # multiple CI jobs might be running concurrently return RateLimiter ( max_calls = 1 , period = 1.5 ) return RateLimiter ( max_calls = 2 , period = 1 )","title":"Module manubot.cite.pubmed"},{"location":"reference/manubot/cite/pubmed/#variables","text":"TYPE_CHECKING month_abbrev_to_int","title":"Variables"},{"location":"reference/manubot/cite/pubmed/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/pubmed/#csl_item_from_pubmed_article","text":"def csl_item_from_pubmed_article ( article : xml . etree . ElementTree . Element ) -> Dict [ str , Any ] Extract a CSL Item dictionary from a PubmedArticle XML element. https://github.com/citation-style-language/schema/blob/master/csl-data.json View Source def csl_item_from_pubmed_article ( article : ElementTree . Element ) -> Dict [ str, Any ] : \"\"\" Extract a CSL Item dictionary from a PubmedArticle XML element. https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" if not article . tag == \"PubmedArticle\" : raise ValueError ( f \"Expected article to be an XML element with tag PubmedArticle, received tag {article.tag!r}\" ) csl_item = dict () if not article . find ( \"MedlineCitation/Article\" ) : raise NotImplementedError ( \"Unsupported PubMed record: no <Article> element\" ) title = article . findtext ( \"MedlineCitation/Article/ArticleTitle\" ) if title : csl_item [ \"title\" ] = title volume = article . findtext ( \"MedlineCitation/Article/Journal/JournalIssue/Volume\" ) if volume : csl_item [ \"volume\" ] = volume issue = article . findtext ( \"MedlineCitation/Article/Journal/JournalIssue/Issue\" ) if issue : csl_item [ \"issue\" ] = issue page = article . findtext ( \"MedlineCitation/Article/Pagination/MedlinePgn\" ) if page : csl_item [ \"page\" ] = page journal = article . findtext ( \"MedlineCitation/Article/Journal/Title\" ) if journal : csl_item [ \"container-title\" ] = journal journal_short = article . findtext ( \"MedlineCitation/Article/Journal/ISOAbbreviation\" ) if journal_short : csl_item [ \"container-title-short\" ] = journal_short issn = article . findtext ( \"MedlineCitation/Article/Journal/ISSN\" ) if issn : csl_item [ \"ISSN\" ] = issn date_parts = extract_publication_date_parts ( article ) if date_parts : csl_item [ \"issued\" ] = { \"date-parts\" : [ date_parts ] } authors_csl = list () authors = article . findall ( \"MedlineCitation/Article/AuthorList/Author\" ) for author in authors : author_csl = dict () given = author . findtext ( \"ForeName\" ) if given : author_csl [ \"given\" ] = given family = author . findtext ( \"LastName\" ) if family : author_csl [ \"family\" ] = family authors_csl . append ( author_csl ) if authors_csl : csl_item [ \"author\" ] = authors_csl for id_type , key in ( \"pubmed\" , \"PMID\" ), ( \"pmc\" , \"PMCID\" ), ( \"doi\" , \"DOI\" ) : xpath = f \"PubmedData/ArticleIdList/ArticleId[@IdType='{id_type}']\" value = article . findtext ( xpath ) if value : csl_item [ key ] = value . lower () if key == \"DOI\" else value abstract = article . findtext ( \"MedlineCitation/Article/Abstract/AbstractText\" ) if abstract : csl_item [ \"abstract\" ] = abstract csl_item [ \"URL\" ] = f \"https://www.ncbi.nlm.nih.gov/pubmed/{csl_item['PMID']}\" csl_item [ \"type\" ] = \"article-journal\" return csl_item","title":"csl_item_from_pubmed_article"},{"location":"reference/manubot/cite/pubmed/#extract_publication_date_parts","text":"def extract_publication_date_parts ( article : xml . etree . ElementTree . Element ) -> List [ int ] Extract date published from a PubmedArticle XML element. View Source def extract_publication_date_parts ( article : ElementTree . Element ) -> List [ int ] : \"\"\" Extract date published from a PubmedArticle XML element. \"\"\" date_parts = [] # Electronic articles date = article . find ( \"MedlineCitation/Article/ArticleDate\" ) if date : for part in \"Year\" , \"Month\" , \"Day\" : part = date . findtext ( part ) if not part : break date_parts . append ( int ( part )) return date_parts # Print articles date = article . find ( \"MedlineCitation/Article/Journal/JournalIssue/PubDate\" ) year = date . findtext ( \"Year\" ) if year : date_parts . append ( int ( year )) month = date . findtext ( \"Month\" ) if month : try : date_parts . append ( month_abbrev_to_int [ month ] ) except KeyError : date_parts . append ( int ( month )) day = date . findtext ( \"Day\" ) if day : date_parts . append ( int ( day )) return date_parts","title":"extract_publication_date_parts"},{"location":"reference/manubot/cite/pubmed/#get_pmc_csl_item","text":"def get_pmc_csl_item ( pmcid : str ) -> Dict [ str , Any ] Get the CSL Item for a PubMed Central record by its PMID, PMCID, or DOI, using the NCBI Citation Exporter API. https://api.ncbi.nlm.nih.gov/lit/ctxp https://github.com/manubot/manubot/issues/21 https://twitter.com/dhimmel/status/1061787168820092929 View Source def get_pmc_csl_item ( pmcid : str ) -> Dict [ str , Any ]: \"\"\" Get the CSL Item for a PubMed Central record by its PMID, PMCID, or DOI, using the NCBI Citation Exporter API. https://api.ncbi.nlm.nih.gov/lit/ctxp https://github.com/manubot/manubot/issues/21 https://twitter.com/dhimmel/status/1061787168820092929 \"\"\" assert pmcid . startswith ( \"PMC\" ) csl_item = _get_literature_citation_exporter_csl_item ( \"pmc\" , pmcid [ 3 :]) if \"URL\" not in csl_item : csl_item [ \"URL\" ] = f \"https://www.ncbi.nlm.nih.gov/pmc/articles/{csl_item.get('PMCID', pmcid)}/\" return csl_item","title":"get_pmc_csl_item"},{"location":"reference/manubot/cite/pubmed/#get_pmcid_and_pmid_for_doi","text":"def get_pmcid_and_pmid_for_doi ( doi : str ) -> Dict [ str , str ] Query PMC's ID Converter API to retrieve the PMCID and PMID for a DOI. Does not work for DOIs that are in Pubmed but not PubMed Central. https://www.ncbi.nlm.nih.gov/pmc/tools/id-converter-api/ View Source def get_pmcid_and_pmid_for_doi ( doi : str ) -> Dict [ str , str ]: \"\"\" Query PMC's ID Converter API to retrieve the PMCID and PMID for a DOI. Does not work for DOIs that are in Pubmed but not PubMed Central. https://www.ncbi.nlm.nih.gov/pmc/tools/id-converter-api/ \"\"\" assert isinstance ( doi , str ) assert doi . startswith ( \"10.\" ) params = { \"ids\" : doi , \"tool\" : \"manubot\" } url = \"https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/\" response = requests . get ( url , params ) if not response . ok : logging . warning ( f \"Status code {response.status_code} querying {response.url} \\n \" ) return {} try : element_tree = ElementTree . fromstring ( response . text ) assert element_tree . tag == \"pmcids\" except Exception : logging . warning ( f \"Error fetching PMC ID conversion for {doi}. \\n \" f \"Response from {response.url}: \\n {response.text}\" ) return {} records = element_tree . findall ( \"record\" ) if len ( records ) != 1 : logging . warning ( f \"Expected PubMed Central ID converter to return a single XML record for {doi}. \\n \" f \"Response from {response.url}: \\n {response.text}\" ) return {} ( record ,) = records if record . findtext ( \"status\" , default = \"okay\" ) == \"error\" : return {} id_dict = {} for id_type in \"pmcid\" , \"pmid\" : id_ = record . get ( id_type ) if id_ : id_dict [ id_type . upper ()] = id_ return id_dict","title":"get_pmcid_and_pmid_for_doi"},{"location":"reference/manubot/cite/pubmed/#get_pmid_for_doi","text":"def get_pmid_for_doi ( doi : str ) -> Optional [ str ] Query NCBI's E-utilities to retrieve the PMID for a DOI. View Source def get_pmid_for_doi ( doi : str ) -> Optional [ str ] : \"\"\" Query NCBI's E-utilities to retrieve the PMID for a DOI. \"\"\" assert isinstance ( doi , str ) assert doi . startswith ( \"10.\" ) params = { \"db\" : \"pubmed\" , \"term\" : f \"{doi}[DOI]\" } headers = { \"User-Agent\" : get_manubot_user_agent () } url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\" with _get_eutils_rate_limiter () : response = requests . get ( url , params , headers = headers ) if not response . ok : logging . warning ( f \"Status code {response.status_code} querying {response.url}\\n\" ) return None try : element_tree = ElementTree . fromstring ( response . text ) assert isinstance ( element_tree , ElementTree . Element ) assert element_tree . tag == \"eSearchResult\" except Exception : logging . warning ( f \"Error in ESearch XML for DOI: {doi}.\\n\" f \"Response from {response.url}:\\n{response.text}\" ) return None id_elems = element_tree . findall ( \"IdList/Id\" ) if len ( id_elems ) != 1 : logging . debug ( f \"No PMIDs found for {doi}.\\n\" f \"Response from {response.url}:\\n{response.text}\" ) return None ( id_elem ,) = id_elems return id_elem . text","title":"get_pmid_for_doi"},{"location":"reference/manubot/cite/pubmed/#get_pubmed_csl_item","text":"def get_pubmed_csl_item ( pmid : Union [ str , int ] ) -> Dict [ str , Any ] Query NCBI E-Utilities to create CSL Items for PubMed IDs. https://github.com/manubot/manubot/issues/21 https://github.com/ncbi/citation-exporter/issues/3#issuecomment-355313143 View Source def get_pubmed_csl_item ( pmid : Union [ str , int ]) -> Dict [ str , Any ]: \"\"\" Query NCBI E-Utilities to create CSL Items for PubMed IDs. https://github.com/manubot/manubot/issues/21 https://github.com/ncbi/citation-exporter/issues/3#issuecomment-355313143 \"\"\" pmid = str ( pmid ) params = { \"db\" : \"pubmed\" , \"id\" : pmid , \"rettype\" : \"full\" } headers = { \"User-Agent\" : get_manubot_user_agent ()} url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\" with _get_eutils_rate_limiter (): response = requests . get ( url , params , headers = headers ) try : xml_article_set = ElementTree . fromstring ( response . text ) assert isinstance ( xml_article_set , ElementTree . Element ) assert xml_article_set . tag == \"PubmedArticleSet\" ( xml_article ,) = list ( xml_article_set ) assert xml_article . tag in [ \"PubmedArticle\" , \"PubmedBookArticle\" ] except Exception as error : logging . error ( f \"Error fetching PubMed metadata for {pmid}. \\n \" f \"Unsupported XML response from {response.url}: \\n {response.text}\" ) raise error try : csl_item = csl_item_from_pubmed_article ( xml_article ) except Exception as error : msg = f \"Error parsing the following PubMed metadata for PMID {pmid}: \\n {response.text}\" logging . error ( msg ) raise error return csl_item","title":"get_pubmed_csl_item"},{"location":"reference/manubot/cite/pubmed/#get_pubmed_ids_for_doi","text":"def get_pubmed_ids_for_doi ( doi : str ) -> Dict [ str , str ] Return a dictionary with PMCID and PMID, if they exist, for the specified DOI. See https://github.com/manubot/manubot/issues/45. View Source def get_pubmed_ids_for_doi ( doi : str ) -> Dict [ str , str ] : \"\"\" Return a dictionary with PMCID and PMID, if they exist, for the specified DOI. See https://github.com/manubot/manubot/issues/45. \"\"\" pubmed_ids = get_pmcid_and_pmid_for_doi ( doi ) if not pubmed_ids : pmid = get_pmid_for_doi ( doi ) if pmid : pubmed_ids [ \"PMID\" ] = pmid return pubmed_ids","title":"get_pubmed_ids_for_doi"},{"location":"reference/manubot/cite/pubmed/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/pubmed/#handler_pmc","text":"class Handler_PMC ( prefix_lower : str ) View Source class Handler_PMC ( Handler ) : standard_prefix = \"pmc\" prefixes = [ \"pmc\", \"pmcid\", ] accession_pattern = r \"PMC[0-9]+\" def inspect ( self , citekey : CiteKey ) -> Optional [ str ] : identifier = citekey . accession # https : // www . nlm . nih . gov / bsd / mms / medlineelements . html#pmc if not identifier . startswith ( \"PMC\" ) : return \"PubMed Central Identifiers must start with 'PMC'.\" elif not self . _get_pattern (). fullmatch ( identifier ) : return ( \"Identifier does not conform to the PMCID regex. \" \"Double check the PMCID.\" ) def get_csl_item ( self , citekey : CiteKey ) : return get_pmc_csl_item ( citekey . standard_accession )","title":"Handler_PMC"},{"location":"reference/manubot/cite/pubmed/#ancestors-in-mro","text":"manubot.cite.handlers.Handler","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/pubmed/#class-variables","text":"accession_pattern prefixes standard_prefix","title":"Class variables"},{"location":"reference/manubot/cite/pubmed/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/pubmed/#get_csl_item","text":"def get_csl_item ( self , citekey : manubot . cite . citekey . CiteKey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey : CiteKey ) : return get_pmc_csl_item ( citekey . standard_accession )","title":"get_csl_item"},{"location":"reference/manubot/cite/pubmed/#inspect","text":"def inspect ( self , citekey : manubot . cite . citekey . CiteKey ) -> Optional [ str ] Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey : CiteKey ) -> Optional [ str ] : identifier = citekey . accession # https : // www . nlm . nih . gov / bsd / mms / medlineelements . html#pmc if not identifier . startswith ( \"PMC\" ) : return \"PubMed Central Identifiers must start with 'PMC'.\" elif not self . _get_pattern (). fullmatch ( identifier ) : return ( \"Identifier does not conform to the PMCID regex. \" \"Double check the PMCID.\" )","title":"inspect"},{"location":"reference/manubot/cite/pubmed/#standardize_prefix_accession","text":"def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ] Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ] : \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession","title":"standardize_prefix_accession"},{"location":"reference/manubot/cite/pubmed/#handler_pubmed","text":"class Handler_PubMed ( prefix_lower : str ) View Source class Handler_PubMed ( Handler ) : standard_prefix = \"pubmed\" prefixes = [ \"pubmed\", \"pmid\", ] accession_pattern = r \"[1-9][0-9]{0,7}\" def inspect ( self , citekey : CiteKey ) -> Optional [ str ] : identifier = citekey . accession # https : // www . nlm . nih . gov / bsd / mms / medlineelements . html#pmid if identifier . startswith ( \"PMC\" ) : return ( \"PubMed Identifiers should start with digits rather than PMC. \" f \"Should {citekey.dealiased_id!r} switch the citation source to 'pmc'?\" ) elif not self . _get_pattern (). fullmatch ( identifier ) : return \"PubMed Identifiers should be 1-8 digits with no leading zeros.\" def get_csl_item ( self , citekey : CiteKey ) -> Dict [ str, Any ] : return get_pubmed_csl_item ( citekey . standard_accession )","title":"Handler_PubMed"},{"location":"reference/manubot/cite/pubmed/#ancestors-in-mro_1","text":"manubot.cite.handlers.Handler","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/pubmed/#class-variables_1","text":"accession_pattern prefixes standard_prefix","title":"Class variables"},{"location":"reference/manubot/cite/pubmed/#methods_1","text":"","title":"Methods"},{"location":"reference/manubot/cite/pubmed/#get_csl_item_1","text":"def get_csl_item ( self , citekey : manubot . cite . citekey . CiteKey ) -> Dict [ str , Any ] Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey : CiteKey ) -> Dict [ str , Any ] : return get_pubmed_csl_item ( citekey . standard_accession )","title":"get_csl_item"},{"location":"reference/manubot/cite/pubmed/#inspect_1","text":"def inspect ( self , citekey : manubot . cite . citekey . CiteKey ) -> Optional [ str ] Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey : CiteKey ) -> Optional [ str ] : identifier = citekey . accession # https : // www . nlm . nih . gov / bsd / mms / medlineelements . html#pmid if identifier . startswith ( \"PMC\" ) : return ( \"PubMed Identifiers should start with digits rather than PMC. \" f \"Should {citekey.dealiased_id!r} switch the citation source to 'pmc'?\" ) elif not self . _get_pattern (). fullmatch ( identifier ) : return \"PubMed Identifiers should be 1-8 digits with no leading zeros.\"","title":"inspect"},{"location":"reference/manubot/cite/pubmed/#standardize_prefix_accession_1","text":"def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ] Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ] : \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession","title":"standardize_prefix_accession"},{"location":"reference/manubot/cite/unpaywall/","text":"Module manubot.cite.unpaywall Utilities for accessing https://unpaywall.org/ data to provide access information for DOIs. Also supports identifier sources not directly supported by Unpaywall, such as arXiv IDs. View Source \"\"\" Utilities for accessing <https://unpaywall.org/> data to provide access information for DOIs. Also supports identifier sources not directly supported by Unpaywall, such as arXiv IDs. \"\"\" import abc import requests \"\"\" Unpaywall license choices used by Location.has_open_license. Defaults to licenses that conform to <https://opendefinition.org/>. \"\"\" open_licenses = { \"cc0\" , \"cc-by\" , \"cc-by-sa\" , \"pd\" } class Unpaywall : \"\"\" A class to handle open access locations in the Unpaywall data format. Create new Unpaywall objects using the `from_csl_item` and `from_citekey` methods, or by using __init__ of a subclass like `Unpaywall_DOI`. \"\"\" csl_item = None @abc . abstractmethod def set_oa_locations ( self ): \"\"\" Set `self.oa_locations`, which is a list of `Unpaywall_Location` objects. \"\"\" self . oa_locations = [] @property def best_openly_licensed_pdf ( self ) -> \"Unpaywall_Location\" : for location in self . oa_locations : if location . has_openly_licensed_pdf : return location @property def best_pdf ( self ) -> \"Unpaywall_Location\" : for location in self . oa_locations : if location . has_pdf : return location @staticmethod def from_citekey ( citekey , csl_item = None ): \"\"\" Create an Unpaywall object for `citekey`. `csl_item` is an optional field that can avoid an external web request to generate to a new CSL Item. \"\"\" from .citekey import CiteKey if isinstance ( citekey , str ): citekey = CiteKey ( citekey ) if not isinstance ( citekey , CiteKey ): raise ValueError ( \"citekey must be a str or CiteKey\" ) if citekey . standard_prefix in source_to_unpaywaller : unpaywaller = source_to_unpaywaller [ citekey . standard_prefix ] unpaywall = unpaywaller ( citekey . standard_accession , set_oa_locations = False ) else : raise ValueError ( f \"Cannot Unpaywall { citekey . input_id } . \" f \"Supported citations sources are { ', ' . join ( source_to_unpaywaller ) } . \" \"Received {citekey.standard_prefix!r} .\" ) unpaywall . csl_item = csl_item unpaywall . set_oa_locations () return unpaywall @classmethod def from_csl_item ( cls , csl_item ): \"\"\" Create an Unpaywall object for `csl_item`. \"\"\" from .csl_item import CSL_Item csl_item = CSL_Item ( csl_item ) doi = csl_item . get ( \"DOI\" ) if doi : return cls . from_citekey ( f \"doi: { doi } \" , csl_item = csl_item ) csl_item . infer_id () return cls . from_citekey ( csl_item [ \"id\" ], csl_item = csl_item ) class Unpaywall_DOI ( Unpaywall ): \"\"\" From https://unpaywall.org/data-format: > The DOI object is more or less a row in our main database... it's everything we know about a given DOI-assigned resource, including metadata about the resource itself, and information about its OA status. It includes a list of zero or more OA Location Objects, as well as a `best_oa_location` property that's probably the OA Location you'll want to use. \"\"\" def __init__ ( self , doi , set_oa_locations = True ): self . doi = doi . lower () if set_oa_locations : self . set_oa_locations () def set_oa_locations ( self ): from manubot.util import contact_email url = f \"https://api.unpaywall.org/v2/ { self . doi } \" params = { \"email\" : contact_email } response = requests . get ( url , params = params ) response . raise_for_status () self . results = response . json () self . oa_locations = [ Unpaywall_Location ( location ) for location in self . results . get ( \"oa_locations\" , []) ] class Unpaywall_arXiv ( Unpaywall ): def __init__ ( self , arxiv_id , set_oa_locations = True , use_doi = True ): from .arxiv import split_arxiv_id_version self . arxiv_id = arxiv_id self . arxiv_id_latest , self . arxiv_id_version = split_arxiv_id_version ( arxiv_id ) self . use_doi = use_doi if set_oa_locations : self . set_oa_locations () def set_oa_locations ( self ): from .arxiv import get_arxiv_csl_item if not self . csl_item : self . csl_item = get_arxiv_csl_item ( self . arxiv_id ) doi = self . csl_item . get ( \"DOI\" ) if self . use_doi and doi : unpaywall_doi = Unpaywall_DOI ( doi ) self . doi = unpaywall_doi . doi self . oa_locations = unpaywall_doi . oa_locations return location = self . location_from_arvix_id () self . oa_locations = [ location ] def location_from_arvix_id ( self ): import datetime url_for_pdf = f \"https://arxiv.org/pdf/ { self . arxiv_id } .pdf\" location = Unpaywall_Location ( { \"endpoint_id\" : None , \"evidence\" : \"oa repository\" , \"host_type\" : \"repository\" , \"is_best\" : True , \"license\" : self . get_license (), \"pmh_id\" : f \"oai:arXiv.org: { self . arxiv_id_latest } \" , \"repository_institution\" : \"Cornell University - arXiv\" , \"updated\" : datetime . datetime . now () . isoformat (), \"url\" : url_for_pdf , \"url_for_landing_page\" : f \"https://arxiv.org/abs/ { self . arxiv_id } \" , \"url_for_pdf\" : url_for_pdf , \"version\" : \"submittedVersion\" , } ) return location def get_license ( self ): \"\"\" Return license using choices from the Unpaywall data format. Looks for license metadata in the CSL Item. \"\"\" license = self . csl_item . note_dict . get ( \"license\" ) if not license : return # Example licenses from https://arxiv.org/help/license # http://creativecommons.org/publicdomain/zero/1.0/ # http://creativecommons.org/licenses/by/4.0/ # http://creativecommons.org/licenses/by-sa/4.0/ # http://creativecommons.org/licenses/by-nc-sa/4.0/ # http://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html from urllib.parse import urlparse parsed_url = urlparse ( license ) if not parsed_url . scheme . startswith ( \"http\" ): return if parsed_url . hostname . endswith ( \"creativecommons.org\" ): try : abbrev = parsed_url . path . split ( \"/\" )[ 2 ] except IndexError : return if abbrev == \"zero\" : return \"cc0\" return f \"cc- { abbrev } \" source_to_unpaywaller = { \"doi\" : Unpaywall_DOI , \"arxiv\" : Unpaywall_arXiv , } class Unpaywall_Location ( dict ): \"\"\" From https://unpaywall.org/data-format > The OA Location object describes particular place where we found a given OA article. The same article is often available from multiple locations, and there may be differences in format, version, and license depending on the location; the OA Location object describes these key attributes. An OA Location Object is always a Child of a DOI Object. Example oa_locations from the Unpaywall API are: ```json { \"endpoint_id\": null, \"evidence\": \"open (via page says license)\", \"host_type\": \"publisher\", \"is_best\": true, \"license\": \"cc-by\", \"pmh_id\": null, \"repository_institution\": null, \"updated\": \"2020-01-19T08:55:45.548214\", \"url\": \"https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1007250&type=printable\", \"url_for_landing_page\": \"https://doi.org/10.1371/journal.pcbi.1007250\", \"url_for_pdf\": \"https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1007250&type=printable\", \"version\": \"publishedVersion\" }, { \"endpoint_id\": \"ca8f8d56758a80a4f86\", \"evidence\": \"oa repository (via OAI-PMH doi match)\", \"host_type\": \"repository\", \"is_best\": true, \"license\": null, \"pmh_id\": \"oai:arXiv.org:1806.05726\", \"repository_institution\": \"Cornell University - arXiv\", \"updated\": \"2019-11-01T00:28:16.784912\", \"url\": \"http://arxiv.org/pdf/1806.05726\", \"url_for_landing_page\": \"http://arxiv.org/abs/1806.05726\", \"url_for_pdf\": \"http://arxiv.org/pdf/1806.05726\", \"version\": \"submittedVersion\" } ``` \"\"\" @property def has_pdf ( self ): return bool ( self . get ( \"url_for_pdf\" )) @property def has_open_license ( self ): license = self . get ( \"license\" ) return license in open_licenses @property def has_creative_commons_license ( self ): license = self . get ( \"license\" ) if not license : return False return license == \"cc0\" or license . startswith ( \"cc-\" ) @property def has_openly_licensed_pdf ( self ): return self . has_pdf and self . has_open_license Variables open_licenses source_to_unpaywaller Classes Unpaywall class Unpaywall ( / , * args , ** kwargs ) View Source class Unpaywall : \" \"\" A class to handle open access locations in the Unpaywall data format. Create new Unpaywall objects using the `from_csl_item` and `from_citekey` methods, or by using __init__ of a subclass like `Unpaywall_DOI`. \"\" \" csl_item = None @abc.abstractmethod def set _oa_locations ( self ) : \" \"\" Set `self.oa_locations`, which is a list of `Unpaywall_Location` objects. \"\" \" self . oa_locations = [] @property def best_openly_licensed_pdf ( self ) -> \"Unpaywall_Location\" : for location in self . oa_locations : if location . has_openly_licensed_pdf : return location @property def best_pdf ( self ) -> \"Unpaywall_Location\" : for location in self . oa_locations : if location . has_pdf : return location @staticmethod def from_citekey ( citekey , csl_item = None ) : \" \"\" Create an Unpaywall object for `citekey`. `csl_item` is an optional field that can avoid an external web request to generate to a new CSL Item. \"\" \" from . citekey import CiteKey if isinstance ( citekey , str ) : citekey = CiteKey ( citekey ) if not isinstance ( citekey , CiteKey ) : raise ValueError ( \"citekey must be a str or CiteKey\" ) if citekey . standard_prefix in source_to_unpaywaller : unpaywaller = source_to_unpaywaller [ citekey . standard_prefix ] unpaywall = unpaywaller ( citekey . standard_accession , set _oa_locations = False ) else : raise ValueError ( f \"Cannot Unpaywall {citekey.input_id}. \" f \"Supported citations sources are {', '.join(source_to_unpaywaller)}. \" \"Received {citekey.standard_prefix!r}.\" ) unpaywall . csl_item = csl_item unpaywall . set _oa_locations () return unpaywall @classmethod def from_csl_item ( cls , csl_item ) : \" \"\" Create an Unpaywall object for `csl_item`. \"\" \" from . csl_item import CSL_Item csl_item = CSL_Item ( csl_item ) doi = csl_item . get ( \"DOI\" ) if doi : return cls . from_citekey ( f \"doi:{doi}\" , csl_item = csl_item ) csl_item . infer_id () return cls . from_citekey ( csl_item [ \"id\" ] , csl_item = csl_item ) Descendants manubot.cite.unpaywall.Unpaywall_DOI manubot.cite.unpaywall.Unpaywall_arXiv Class variables csl_item Static methods from_citekey def from_citekey ( citekey , csl_item = None ) Create an Unpaywall object for citekey . csl_item is an optional field that can avoid an external web request to generate to a new CSL Item. View Source @staticmethod def from_citekey ( citekey , csl_item = None ): \"\"\" Create an Unpaywall object for `citekey`. `csl_item` is an optional field that can avoid an external web request to generate to a new CSL Item. \"\"\" from .citekey import CiteKey if isinstance ( citekey , str ): citekey = CiteKey ( citekey ) if not isinstance ( citekey , CiteKey ): raise ValueError ( \"citekey must be a str or CiteKey\" ) if citekey . standard_prefix in source_to_unpaywaller : unpaywaller = source_to_unpaywaller [ citekey . standard_prefix ] unpaywall = unpaywaller ( citekey . standard_accession , set_oa_locations = False ) else : raise ValueError ( f \"Cannot Unpaywall { citekey . input_id } . \" f \"Supported citations sources are { ', ' . join ( source_to_unpaywaller ) } . \" \"Received {citekey.standard_prefix!r} .\" ) unpaywall . csl_item = csl_item unpaywall . set_oa_locations () return unpaywall from_csl_item def from_csl_item ( csl_item ) Create an Unpaywall object for csl_item . View Source @classmethod def from_csl_item ( cls , csl_item ): \"\"\" Create an Unpaywall object for `csl_item`. \"\"\" from .csl_item import CSL_Item csl_item = CSL_Item ( csl_item ) doi = csl_item . get ( \"DOI\" ) if doi : return cls . from_citekey ( f \"doi: { doi } \" , csl_item = csl_item ) csl_item . infer_id () return cls . from_citekey ( csl_item [ \"id\" ], csl_item = csl_item ) Instance variables best_openly_licensed_pdf best_pdf Methods set_oa_locations def set_oa_locations ( self ) Set self.oa_locations , which is a list of Unpaywall_Location objects. View Source @abc.abstractmethod def set _oa_locations ( self ) : \" \"\" Set `self.oa_locations`, which is a list of `Unpaywall_Location` objects. \"\" \" self . oa_locations = [] Unpaywall_DOI class Unpaywall_DOI ( doi , set_oa_locations = True ) View Source class Unpaywall_DOI ( Unpaywall ): \"\"\" From https://unpaywall.org/data-format: > The DOI object is more or less a row in our main database... it's everything we know about a given DOI-assigned resource, including metadata about the resource itself, and information about its OA status. It includes a list of zero or more OA Location Objects, as well as a `best_oa_location` property that's probably the OA Location you'll want to use. \"\"\" def __init__ ( self , doi , set_oa_locations = True ): self . doi = doi . lower () if set_oa_locations : self . set_oa_locations () def set_oa_locations ( self ): from manubot.util import contact_email url = f \"https://api.unpaywall.org/v2/ { self . doi } \" params = { \"email\" : contact_email } response = requests . get ( url , params = params ) response . raise_for_status () self . results = response . json () self . oa_locations = [ Unpaywall_Location ( location ) for location in self . results . get ( \"oa_locations\" , []) ] Ancestors (in MRO) manubot.cite.unpaywall.Unpaywall Class variables csl_item Static methods from_citekey def from_citekey ( citekey , csl_item = None ) Create an Unpaywall object for citekey . csl_item is an optional field that can avoid an external web request to generate to a new CSL Item. View Source @staticmethod def from_citekey ( citekey , csl_item = None ): \"\"\" Create an Unpaywall object for `citekey`. `csl_item` is an optional field that can avoid an external web request to generate to a new CSL Item. \"\"\" from .citekey import CiteKey if isinstance ( citekey , str ): citekey = CiteKey ( citekey ) if not isinstance ( citekey , CiteKey ): raise ValueError ( \"citekey must be a str or CiteKey\" ) if citekey . standard_prefix in source_to_unpaywaller : unpaywaller = source_to_unpaywaller [ citekey . standard_prefix ] unpaywall = unpaywaller ( citekey . standard_accession , set_oa_locations = False ) else : raise ValueError ( f \"Cannot Unpaywall { citekey . input_id } . \" f \"Supported citations sources are { ', ' . join ( source_to_unpaywaller ) } . \" \"Received {citekey.standard_prefix!r} .\" ) unpaywall . csl_item = csl_item unpaywall . set_oa_locations () return unpaywall from_csl_item def from_csl_item ( csl_item ) Create an Unpaywall object for csl_item . View Source @classmethod def from_csl_item ( cls , csl_item ): \"\"\" Create an Unpaywall object for `csl_item`. \"\"\" from .csl_item import CSL_Item csl_item = CSL_Item ( csl_item ) doi = csl_item . get ( \"DOI\" ) if doi : return cls . from_citekey ( f \"doi: { doi } \" , csl_item = csl_item ) csl_item . infer_id () return cls . from_citekey ( csl_item [ \"id\" ], csl_item = csl_item ) Instance variables best_openly_licensed_pdf best_pdf Methods set_oa_locations def set_oa_locations ( self ) Set self.oa_locations , which is a list of Unpaywall_Location objects. View Source def set_oa_locations ( self ): from manubot.util import contact_email url = f \"https://api.unpaywall.org/v2/ { self . doi } \" params = { \"email\" : contact_email } response = requests . get ( url , params = params ) response . raise_for_status () self . results = response . json () self . oa_locations = [ Unpaywall_Location ( location ) for location in self . results . get ( \"oa_locations\" , []) ] Unpaywall_Location class Unpaywall_Location ( / , * args , ** kwargs ) View Source class Unpaywall_Location ( dict ) : \"\"\" From https://unpaywall.org/data-format > The OA Location object describes particular place where we found a given OA article. The same article is often available from multiple locations, and there may be differences in format, version, and license depending on the location; the OA Location object describes these key attributes. An OA Location Object is always a Child of a DOI Object. Example oa_locations from the Unpaywall API are: ```json { \" endpoint_id \": null, \" evidence \": \" open ( via page says license ) \", \" host_type \": \" publisher \", \" is_best \": true, \" license \": \" cc - by \", \" pmh_id \": null, \" repository_institution \": null, \" updated \": \" 2020 - 01 - 19 T08 : 55 : 45.548214 \", \" url \": \" https : // journals . plos . org / ploscompbiol / article / file ? id = 10.1371 / journal . pcbi .1007250 & type = printable \", \" url_for_landing_page \": \" https : // doi . org / 10.1371 / journal . pcbi .1007250 \", \" url_for_pdf \": \" https : // journals . plos . org / ploscompbiol / article / file ? id = 10.1371 / journal . pcbi .1007250 & type = printable \", \" version \": \" publishedVersion \" }, { \" endpoint_id \": \" ca8f8d56758a80a4f86 \", \" evidence \": \" oa repository ( via OAI - PMH doi match ) \", \" host_type \": \" repository \", \" is_best \": true, \" license \": null, \" pmh_id \": \" oai : arXiv . org : 1806.05726 \", \" repository_institution \": \" Cornell University - arXiv \", \" updated \": \" 2019 - 11 - 01 T00 : 28 : 16.784912 \", \" url \": \" http : // arxiv . org / pdf / 1806.05726 \", \" url_for_landing_page \": \" http : // arxiv . org / abs / 1806.05726 \", \" url_for_pdf \": \" http : // arxiv . org / pdf / 1806.05726 \", \" version \": \" submittedVersion \" } ``` \"\"\" @property def has_pdf ( self ) : return bool ( self . get ( \"url_for_pdf\" )) @property def has_open_license ( self ) : license = self . get ( \"license\" ) return license in open_licenses @property def has_creative_commons_license ( self ) : license = self . get ( \"license\" ) if not license : return False return license == \"cc0\" or license . startswith ( \"cc-\" ) @property def has_openly_licensed_pdf ( self ) : return self . has_pdf and self . has_open_license Ancestors (in MRO) builtins.dict Instance variables has_creative_commons_license has_open_license has_openly_licensed_pdf has_pdf Methods clear def clear ( ... ) D.clear() -> None. Remove all items from D. copy def copy ( ... ) D.copy() -> a shallow copy of D fromkeys def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value. get def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default. items def items ( ... ) D.items() -> a set-like object providing a view on D's items keys def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys pop def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, default is returned if given, otherwise KeyError is raised popitem def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. setdefault def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. update def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k] values def values ( ... ) D.values() -> an object providing a view on D's values Unpaywall_arXiv class Unpaywall_arXiv ( arxiv_id , set_oa_locations = True , use_doi = True ) View Source class Unpaywall_arXiv ( Unpaywall ): def __init__ ( self , arxiv_id , set_oa_locations = True , use_doi = True ): from .arxiv import split_arxiv_id_version self . arxiv_id = arxiv_id self . arxiv_id_latest , self . arxiv_id_version = split_arxiv_id_version ( arxiv_id ) self . use_doi = use_doi if set_oa_locations : self . set_oa_locations () def set_oa_locations ( self ): from .arxiv import get_arxiv_csl_item if not self . csl_item : self . csl_item = get_arxiv_csl_item ( self . arxiv_id ) doi = self . csl_item . get ( \"DOI\" ) if self . use_doi and doi : unpaywall_doi = Unpaywall_DOI ( doi ) self . doi = unpaywall_doi . doi self . oa_locations = unpaywall_doi . oa_locations return location = self . location_from_arvix_id () self . oa_locations = [ location ] def location_from_arvix_id ( self ): import datetime url_for_pdf = f \"https://arxiv.org/pdf/ { self . arxiv_id } .pdf\" location = Unpaywall_Location ( { \"endpoint_id\" : None , \"evidence\" : \"oa repository\" , \"host_type\" : \"repository\" , \"is_best\" : True , \"license\" : self . get_license (), \"pmh_id\" : f \"oai:arXiv.org: { self . arxiv_id_latest } \" , \"repository_institution\" : \"Cornell University - arXiv\" , \"updated\" : datetime . datetime . now () . isoformat (), \"url\" : url_for_pdf , \"url_for_landing_page\" : f \"https://arxiv.org/abs/ { self . arxiv_id } \" , \"url_for_pdf\" : url_for_pdf , \"version\" : \"submittedVersion\" , } ) return location def get_license ( self ): \"\"\" Return license using choices from the Unpaywall data format. Looks for license metadata in the CSL Item. \"\"\" license = self . csl_item . note_dict . get ( \"license\" ) if not license : return # Example licenses from https://arxiv.org/help/license # http://creativecommons.org/publicdomain/zero/1.0/ # http://creativecommons.org/licenses/by/4.0/ # http://creativecommons.org/licenses/by-sa/4.0/ # http://creativecommons.org/licenses/by-nc-sa/4.0/ # http://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html from urllib.parse import urlparse parsed_url = urlparse ( license ) if not parsed_url . scheme . startswith ( \"http\" ): return if parsed_url . hostname . endswith ( \"creativecommons.org\" ): try : abbrev = parsed_url . path . split ( \"/\" )[ 2 ] except IndexError : return if abbrev == \"zero\" : return \"cc0\" return f \"cc- { abbrev } \" Ancestors (in MRO) manubot.cite.unpaywall.Unpaywall Class variables csl_item Static methods from_citekey def from_citekey ( citekey , csl_item = None ) Create an Unpaywall object for citekey . csl_item is an optional field that can avoid an external web request to generate to a new CSL Item. View Source @staticmethod def from_citekey ( citekey , csl_item = None ): \"\"\" Create an Unpaywall object for `citekey`. `csl_item` is an optional field that can avoid an external web request to generate to a new CSL Item. \"\"\" from .citekey import CiteKey if isinstance ( citekey , str ): citekey = CiteKey ( citekey ) if not isinstance ( citekey , CiteKey ): raise ValueError ( \"citekey must be a str or CiteKey\" ) if citekey . standard_prefix in source_to_unpaywaller : unpaywaller = source_to_unpaywaller [ citekey . standard_prefix ] unpaywall = unpaywaller ( citekey . standard_accession , set_oa_locations = False ) else : raise ValueError ( f \"Cannot Unpaywall { citekey . input_id } . \" f \"Supported citations sources are { ', ' . join ( source_to_unpaywaller ) } . \" \"Received {citekey.standard_prefix!r} .\" ) unpaywall . csl_item = csl_item unpaywall . set_oa_locations () return unpaywall from_csl_item def from_csl_item ( csl_item ) Create an Unpaywall object for csl_item . View Source @classmethod def from_csl_item ( cls , csl_item ): \"\"\" Create an Unpaywall object for `csl_item`. \"\"\" from .csl_item import CSL_Item csl_item = CSL_Item ( csl_item ) doi = csl_item . get ( \"DOI\" ) if doi : return cls . from_citekey ( f \"doi: { doi } \" , csl_item = csl_item ) csl_item . infer_id () return cls . from_citekey ( csl_item [ \"id\" ], csl_item = csl_item ) Instance variables best_openly_licensed_pdf best_pdf Methods get_license def get_license ( self ) Return license using choices from the Unpaywall data format. Looks for license metadata in the CSL Item. View Source def get_license ( self ): \"\"\" Return license using choices from the Unpaywall data format. Looks for license metadata in the CSL Item. \"\"\" license = self . csl_item . note_dict . get ( \"license\" ) if not license : return # Example licenses from https://arxiv.org/help/license # http://creativecommons.org/publicdomain/zero/1.0/ # http://creativecommons.org/licenses/by/4.0/ # http://creativecommons.org/licenses/by-sa/4.0/ # http://creativecommons.org/licenses/by-nc-sa/4.0/ # http://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html from urllib.parse import urlparse parsed_url = urlparse ( license ) if not parsed_url . scheme . startswith ( \"http\" ): return if parsed_url . hostname . endswith ( \"creativecommons.org\" ): try : abbrev = parsed_url . path . split ( \"/\" )[ 2 ] except IndexError : return if abbrev == \"zero\" : return \"cc0\" return f \"cc- { abbrev } \" location_from_arvix_id def location_from_arvix_id ( self ) View Source def location_from_arvix_id ( self ): import datetime url_for_pdf = f \"https://arxiv.org/pdf/ { self . arxiv_id } .pdf\" location = Unpaywall_Location ( { \"endpoint_id\" : None , \"evidence\" : \"oa repository\" , \"host_type\" : \"repository\" , \"is_best\" : True , \"license\" : self . get_license (), \"pmh_id\" : f \"oai:arXiv.org: { self . arxiv_id_latest } \" , \"repository_institution\" : \"Cornell University - arXiv\" , \"updated\" : datetime . datetime . now () . isoformat (), \"url\" : url_for_pdf , \"url_for_landing_page\" : f \"https://arxiv.org/abs/ { self . arxiv_id } \" , \"url_for_pdf\" : url_for_pdf , \"version\" : \"submittedVersion\" , } ) return location set_oa_locations def set_oa_locations ( self ) Set self.oa_locations , which is a list of Unpaywall_Location objects. View Source def set_oa_locations ( self ): from .arxiv import get_arxiv_csl_item if not self . csl_item : self . csl_item = get_arxiv_csl_item ( self . arxiv_id ) doi = self . csl_item . get ( \"DOI\" ) if self . use_doi and doi : unpaywall_doi = Unpaywall_DOI ( doi ) self . doi = unpaywall_doi . doi self . oa_locations = unpaywall_doi . oa_locations return location = self . location_from_arvix_id () self . oa_locations = [ location ]","title":"Unpaywall"},{"location":"reference/manubot/cite/unpaywall/#module-manubotciteunpaywall","text":"Utilities for accessing https://unpaywall.org/ data to provide access information for DOIs. Also supports identifier sources not directly supported by Unpaywall, such as arXiv IDs. View Source \"\"\" Utilities for accessing <https://unpaywall.org/> data to provide access information for DOIs. Also supports identifier sources not directly supported by Unpaywall, such as arXiv IDs. \"\"\" import abc import requests \"\"\" Unpaywall license choices used by Location.has_open_license. Defaults to licenses that conform to <https://opendefinition.org/>. \"\"\" open_licenses = { \"cc0\" , \"cc-by\" , \"cc-by-sa\" , \"pd\" } class Unpaywall : \"\"\" A class to handle open access locations in the Unpaywall data format. Create new Unpaywall objects using the `from_csl_item` and `from_citekey` methods, or by using __init__ of a subclass like `Unpaywall_DOI`. \"\"\" csl_item = None @abc . abstractmethod def set_oa_locations ( self ): \"\"\" Set `self.oa_locations`, which is a list of `Unpaywall_Location` objects. \"\"\" self . oa_locations = [] @property def best_openly_licensed_pdf ( self ) -> \"Unpaywall_Location\" : for location in self . oa_locations : if location . has_openly_licensed_pdf : return location @property def best_pdf ( self ) -> \"Unpaywall_Location\" : for location in self . oa_locations : if location . has_pdf : return location @staticmethod def from_citekey ( citekey , csl_item = None ): \"\"\" Create an Unpaywall object for `citekey`. `csl_item` is an optional field that can avoid an external web request to generate to a new CSL Item. \"\"\" from .citekey import CiteKey if isinstance ( citekey , str ): citekey = CiteKey ( citekey ) if not isinstance ( citekey , CiteKey ): raise ValueError ( \"citekey must be a str or CiteKey\" ) if citekey . standard_prefix in source_to_unpaywaller : unpaywaller = source_to_unpaywaller [ citekey . standard_prefix ] unpaywall = unpaywaller ( citekey . standard_accession , set_oa_locations = False ) else : raise ValueError ( f \"Cannot Unpaywall { citekey . input_id } . \" f \"Supported citations sources are { ', ' . join ( source_to_unpaywaller ) } . \" \"Received {citekey.standard_prefix!r} .\" ) unpaywall . csl_item = csl_item unpaywall . set_oa_locations () return unpaywall @classmethod def from_csl_item ( cls , csl_item ): \"\"\" Create an Unpaywall object for `csl_item`. \"\"\" from .csl_item import CSL_Item csl_item = CSL_Item ( csl_item ) doi = csl_item . get ( \"DOI\" ) if doi : return cls . from_citekey ( f \"doi: { doi } \" , csl_item = csl_item ) csl_item . infer_id () return cls . from_citekey ( csl_item [ \"id\" ], csl_item = csl_item ) class Unpaywall_DOI ( Unpaywall ): \"\"\" From https://unpaywall.org/data-format: > The DOI object is more or less a row in our main database... it's everything we know about a given DOI-assigned resource, including metadata about the resource itself, and information about its OA status. It includes a list of zero or more OA Location Objects, as well as a `best_oa_location` property that's probably the OA Location you'll want to use. \"\"\" def __init__ ( self , doi , set_oa_locations = True ): self . doi = doi . lower () if set_oa_locations : self . set_oa_locations () def set_oa_locations ( self ): from manubot.util import contact_email url = f \"https://api.unpaywall.org/v2/ { self . doi } \" params = { \"email\" : contact_email } response = requests . get ( url , params = params ) response . raise_for_status () self . results = response . json () self . oa_locations = [ Unpaywall_Location ( location ) for location in self . results . get ( \"oa_locations\" , []) ] class Unpaywall_arXiv ( Unpaywall ): def __init__ ( self , arxiv_id , set_oa_locations = True , use_doi = True ): from .arxiv import split_arxiv_id_version self . arxiv_id = arxiv_id self . arxiv_id_latest , self . arxiv_id_version = split_arxiv_id_version ( arxiv_id ) self . use_doi = use_doi if set_oa_locations : self . set_oa_locations () def set_oa_locations ( self ): from .arxiv import get_arxiv_csl_item if not self . csl_item : self . csl_item = get_arxiv_csl_item ( self . arxiv_id ) doi = self . csl_item . get ( \"DOI\" ) if self . use_doi and doi : unpaywall_doi = Unpaywall_DOI ( doi ) self . doi = unpaywall_doi . doi self . oa_locations = unpaywall_doi . oa_locations return location = self . location_from_arvix_id () self . oa_locations = [ location ] def location_from_arvix_id ( self ): import datetime url_for_pdf = f \"https://arxiv.org/pdf/ { self . arxiv_id } .pdf\" location = Unpaywall_Location ( { \"endpoint_id\" : None , \"evidence\" : \"oa repository\" , \"host_type\" : \"repository\" , \"is_best\" : True , \"license\" : self . get_license (), \"pmh_id\" : f \"oai:arXiv.org: { self . arxiv_id_latest } \" , \"repository_institution\" : \"Cornell University - arXiv\" , \"updated\" : datetime . datetime . now () . isoformat (), \"url\" : url_for_pdf , \"url_for_landing_page\" : f \"https://arxiv.org/abs/ { self . arxiv_id } \" , \"url_for_pdf\" : url_for_pdf , \"version\" : \"submittedVersion\" , } ) return location def get_license ( self ): \"\"\" Return license using choices from the Unpaywall data format. Looks for license metadata in the CSL Item. \"\"\" license = self . csl_item . note_dict . get ( \"license\" ) if not license : return # Example licenses from https://arxiv.org/help/license # http://creativecommons.org/publicdomain/zero/1.0/ # http://creativecommons.org/licenses/by/4.0/ # http://creativecommons.org/licenses/by-sa/4.0/ # http://creativecommons.org/licenses/by-nc-sa/4.0/ # http://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html from urllib.parse import urlparse parsed_url = urlparse ( license ) if not parsed_url . scheme . startswith ( \"http\" ): return if parsed_url . hostname . endswith ( \"creativecommons.org\" ): try : abbrev = parsed_url . path . split ( \"/\" )[ 2 ] except IndexError : return if abbrev == \"zero\" : return \"cc0\" return f \"cc- { abbrev } \" source_to_unpaywaller = { \"doi\" : Unpaywall_DOI , \"arxiv\" : Unpaywall_arXiv , } class Unpaywall_Location ( dict ): \"\"\" From https://unpaywall.org/data-format > The OA Location object describes particular place where we found a given OA article. The same article is often available from multiple locations, and there may be differences in format, version, and license depending on the location; the OA Location object describes these key attributes. An OA Location Object is always a Child of a DOI Object. Example oa_locations from the Unpaywall API are: ```json { \"endpoint_id\": null, \"evidence\": \"open (via page says license)\", \"host_type\": \"publisher\", \"is_best\": true, \"license\": \"cc-by\", \"pmh_id\": null, \"repository_institution\": null, \"updated\": \"2020-01-19T08:55:45.548214\", \"url\": \"https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1007250&type=printable\", \"url_for_landing_page\": \"https://doi.org/10.1371/journal.pcbi.1007250\", \"url_for_pdf\": \"https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1007250&type=printable\", \"version\": \"publishedVersion\" }, { \"endpoint_id\": \"ca8f8d56758a80a4f86\", \"evidence\": \"oa repository (via OAI-PMH doi match)\", \"host_type\": \"repository\", \"is_best\": true, \"license\": null, \"pmh_id\": \"oai:arXiv.org:1806.05726\", \"repository_institution\": \"Cornell University - arXiv\", \"updated\": \"2019-11-01T00:28:16.784912\", \"url\": \"http://arxiv.org/pdf/1806.05726\", \"url_for_landing_page\": \"http://arxiv.org/abs/1806.05726\", \"url_for_pdf\": \"http://arxiv.org/pdf/1806.05726\", \"version\": \"submittedVersion\" } ``` \"\"\" @property def has_pdf ( self ): return bool ( self . get ( \"url_for_pdf\" )) @property def has_open_license ( self ): license = self . get ( \"license\" ) return license in open_licenses @property def has_creative_commons_license ( self ): license = self . get ( \"license\" ) if not license : return False return license == \"cc0\" or license . startswith ( \"cc-\" ) @property def has_openly_licensed_pdf ( self ): return self . has_pdf and self . has_open_license","title":"Module manubot.cite.unpaywall"},{"location":"reference/manubot/cite/unpaywall/#variables","text":"open_licenses source_to_unpaywaller","title":"Variables"},{"location":"reference/manubot/cite/unpaywall/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/unpaywall/#unpaywall","text":"class Unpaywall ( / , * args , ** kwargs ) View Source class Unpaywall : \" \"\" A class to handle open access locations in the Unpaywall data format. Create new Unpaywall objects using the `from_csl_item` and `from_citekey` methods, or by using __init__ of a subclass like `Unpaywall_DOI`. \"\" \" csl_item = None @abc.abstractmethod def set _oa_locations ( self ) : \" \"\" Set `self.oa_locations`, which is a list of `Unpaywall_Location` objects. \"\" \" self . oa_locations = [] @property def best_openly_licensed_pdf ( self ) -> \"Unpaywall_Location\" : for location in self . oa_locations : if location . has_openly_licensed_pdf : return location @property def best_pdf ( self ) -> \"Unpaywall_Location\" : for location in self . oa_locations : if location . has_pdf : return location @staticmethod def from_citekey ( citekey , csl_item = None ) : \" \"\" Create an Unpaywall object for `citekey`. `csl_item` is an optional field that can avoid an external web request to generate to a new CSL Item. \"\" \" from . citekey import CiteKey if isinstance ( citekey , str ) : citekey = CiteKey ( citekey ) if not isinstance ( citekey , CiteKey ) : raise ValueError ( \"citekey must be a str or CiteKey\" ) if citekey . standard_prefix in source_to_unpaywaller : unpaywaller = source_to_unpaywaller [ citekey . standard_prefix ] unpaywall = unpaywaller ( citekey . standard_accession , set _oa_locations = False ) else : raise ValueError ( f \"Cannot Unpaywall {citekey.input_id}. \" f \"Supported citations sources are {', '.join(source_to_unpaywaller)}. \" \"Received {citekey.standard_prefix!r}.\" ) unpaywall . csl_item = csl_item unpaywall . set _oa_locations () return unpaywall @classmethod def from_csl_item ( cls , csl_item ) : \" \"\" Create an Unpaywall object for `csl_item`. \"\" \" from . csl_item import CSL_Item csl_item = CSL_Item ( csl_item ) doi = csl_item . get ( \"DOI\" ) if doi : return cls . from_citekey ( f \"doi:{doi}\" , csl_item = csl_item ) csl_item . infer_id () return cls . from_citekey ( csl_item [ \"id\" ] , csl_item = csl_item )","title":"Unpaywall"},{"location":"reference/manubot/cite/unpaywall/#descendants","text":"manubot.cite.unpaywall.Unpaywall_DOI manubot.cite.unpaywall.Unpaywall_arXiv","title":"Descendants"},{"location":"reference/manubot/cite/unpaywall/#class-variables","text":"csl_item","title":"Class variables"},{"location":"reference/manubot/cite/unpaywall/#static-methods","text":"","title":"Static methods"},{"location":"reference/manubot/cite/unpaywall/#from_citekey","text":"def from_citekey ( citekey , csl_item = None ) Create an Unpaywall object for citekey . csl_item is an optional field that can avoid an external web request to generate to a new CSL Item. View Source @staticmethod def from_citekey ( citekey , csl_item = None ): \"\"\" Create an Unpaywall object for `citekey`. `csl_item` is an optional field that can avoid an external web request to generate to a new CSL Item. \"\"\" from .citekey import CiteKey if isinstance ( citekey , str ): citekey = CiteKey ( citekey ) if not isinstance ( citekey , CiteKey ): raise ValueError ( \"citekey must be a str or CiteKey\" ) if citekey . standard_prefix in source_to_unpaywaller : unpaywaller = source_to_unpaywaller [ citekey . standard_prefix ] unpaywall = unpaywaller ( citekey . standard_accession , set_oa_locations = False ) else : raise ValueError ( f \"Cannot Unpaywall { citekey . input_id } . \" f \"Supported citations sources are { ', ' . join ( source_to_unpaywaller ) } . \" \"Received {citekey.standard_prefix!r} .\" ) unpaywall . csl_item = csl_item unpaywall . set_oa_locations () return unpaywall","title":"from_citekey"},{"location":"reference/manubot/cite/unpaywall/#from_csl_item","text":"def from_csl_item ( csl_item ) Create an Unpaywall object for csl_item . View Source @classmethod def from_csl_item ( cls , csl_item ): \"\"\" Create an Unpaywall object for `csl_item`. \"\"\" from .csl_item import CSL_Item csl_item = CSL_Item ( csl_item ) doi = csl_item . get ( \"DOI\" ) if doi : return cls . from_citekey ( f \"doi: { doi } \" , csl_item = csl_item ) csl_item . infer_id () return cls . from_citekey ( csl_item [ \"id\" ], csl_item = csl_item )","title":"from_csl_item"},{"location":"reference/manubot/cite/unpaywall/#instance-variables","text":"best_openly_licensed_pdf best_pdf","title":"Instance variables"},{"location":"reference/manubot/cite/unpaywall/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/unpaywall/#set_oa_locations","text":"def set_oa_locations ( self ) Set self.oa_locations , which is a list of Unpaywall_Location objects. View Source @abc.abstractmethod def set _oa_locations ( self ) : \" \"\" Set `self.oa_locations`, which is a list of `Unpaywall_Location` objects. \"\" \" self . oa_locations = []","title":"set_oa_locations"},{"location":"reference/manubot/cite/unpaywall/#unpaywall_doi","text":"class Unpaywall_DOI ( doi , set_oa_locations = True ) View Source class Unpaywall_DOI ( Unpaywall ): \"\"\" From https://unpaywall.org/data-format: > The DOI object is more or less a row in our main database... it's everything we know about a given DOI-assigned resource, including metadata about the resource itself, and information about its OA status. It includes a list of zero or more OA Location Objects, as well as a `best_oa_location` property that's probably the OA Location you'll want to use. \"\"\" def __init__ ( self , doi , set_oa_locations = True ): self . doi = doi . lower () if set_oa_locations : self . set_oa_locations () def set_oa_locations ( self ): from manubot.util import contact_email url = f \"https://api.unpaywall.org/v2/ { self . doi } \" params = { \"email\" : contact_email } response = requests . get ( url , params = params ) response . raise_for_status () self . results = response . json () self . oa_locations = [ Unpaywall_Location ( location ) for location in self . results . get ( \"oa_locations\" , []) ]","title":"Unpaywall_DOI"},{"location":"reference/manubot/cite/unpaywall/#ancestors-in-mro","text":"manubot.cite.unpaywall.Unpaywall","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/unpaywall/#class-variables_1","text":"csl_item","title":"Class variables"},{"location":"reference/manubot/cite/unpaywall/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/manubot/cite/unpaywall/#from_citekey_1","text":"def from_citekey ( citekey , csl_item = None ) Create an Unpaywall object for citekey . csl_item is an optional field that can avoid an external web request to generate to a new CSL Item. View Source @staticmethod def from_citekey ( citekey , csl_item = None ): \"\"\" Create an Unpaywall object for `citekey`. `csl_item` is an optional field that can avoid an external web request to generate to a new CSL Item. \"\"\" from .citekey import CiteKey if isinstance ( citekey , str ): citekey = CiteKey ( citekey ) if not isinstance ( citekey , CiteKey ): raise ValueError ( \"citekey must be a str or CiteKey\" ) if citekey . standard_prefix in source_to_unpaywaller : unpaywaller = source_to_unpaywaller [ citekey . standard_prefix ] unpaywall = unpaywaller ( citekey . standard_accession , set_oa_locations = False ) else : raise ValueError ( f \"Cannot Unpaywall { citekey . input_id } . \" f \"Supported citations sources are { ', ' . join ( source_to_unpaywaller ) } . \" \"Received {citekey.standard_prefix!r} .\" ) unpaywall . csl_item = csl_item unpaywall . set_oa_locations () return unpaywall","title":"from_citekey"},{"location":"reference/manubot/cite/unpaywall/#from_csl_item_1","text":"def from_csl_item ( csl_item ) Create an Unpaywall object for csl_item . View Source @classmethod def from_csl_item ( cls , csl_item ): \"\"\" Create an Unpaywall object for `csl_item`. \"\"\" from .csl_item import CSL_Item csl_item = CSL_Item ( csl_item ) doi = csl_item . get ( \"DOI\" ) if doi : return cls . from_citekey ( f \"doi: { doi } \" , csl_item = csl_item ) csl_item . infer_id () return cls . from_citekey ( csl_item [ \"id\" ], csl_item = csl_item )","title":"from_csl_item"},{"location":"reference/manubot/cite/unpaywall/#instance-variables_1","text":"best_openly_licensed_pdf best_pdf","title":"Instance variables"},{"location":"reference/manubot/cite/unpaywall/#methods_1","text":"","title":"Methods"},{"location":"reference/manubot/cite/unpaywall/#set_oa_locations_1","text":"def set_oa_locations ( self ) Set self.oa_locations , which is a list of Unpaywall_Location objects. View Source def set_oa_locations ( self ): from manubot.util import contact_email url = f \"https://api.unpaywall.org/v2/ { self . doi } \" params = { \"email\" : contact_email } response = requests . get ( url , params = params ) response . raise_for_status () self . results = response . json () self . oa_locations = [ Unpaywall_Location ( location ) for location in self . results . get ( \"oa_locations\" , []) ]","title":"set_oa_locations"},{"location":"reference/manubot/cite/unpaywall/#unpaywall_location","text":"class Unpaywall_Location ( / , * args , ** kwargs ) View Source class Unpaywall_Location ( dict ) : \"\"\" From https://unpaywall.org/data-format > The OA Location object describes particular place where we found a given OA article. The same article is often available from multiple locations, and there may be differences in format, version, and license depending on the location; the OA Location object describes these key attributes. An OA Location Object is always a Child of a DOI Object. Example oa_locations from the Unpaywall API are: ```json { \" endpoint_id \": null, \" evidence \": \" open ( via page says license ) \", \" host_type \": \" publisher \", \" is_best \": true, \" license \": \" cc - by \", \" pmh_id \": null, \" repository_institution \": null, \" updated \": \" 2020 - 01 - 19 T08 : 55 : 45.548214 \", \" url \": \" https : // journals . plos . org / ploscompbiol / article / file ? id = 10.1371 / journal . pcbi .1007250 & type = printable \", \" url_for_landing_page \": \" https : // doi . org / 10.1371 / journal . pcbi .1007250 \", \" url_for_pdf \": \" https : // journals . plos . org / ploscompbiol / article / file ? id = 10.1371 / journal . pcbi .1007250 & type = printable \", \" version \": \" publishedVersion \" }, { \" endpoint_id \": \" ca8f8d56758a80a4f86 \", \" evidence \": \" oa repository ( via OAI - PMH doi match ) \", \" host_type \": \" repository \", \" is_best \": true, \" license \": null, \" pmh_id \": \" oai : arXiv . org : 1806.05726 \", \" repository_institution \": \" Cornell University - arXiv \", \" updated \": \" 2019 - 11 - 01 T00 : 28 : 16.784912 \", \" url \": \" http : // arxiv . org / pdf / 1806.05726 \", \" url_for_landing_page \": \" http : // arxiv . org / abs / 1806.05726 \", \" url_for_pdf \": \" http : // arxiv . org / pdf / 1806.05726 \", \" version \": \" submittedVersion \" } ``` \"\"\" @property def has_pdf ( self ) : return bool ( self . get ( \"url_for_pdf\" )) @property def has_open_license ( self ) : license = self . get ( \"license\" ) return license in open_licenses @property def has_creative_commons_license ( self ) : license = self . get ( \"license\" ) if not license : return False return license == \"cc0\" or license . startswith ( \"cc-\" ) @property def has_openly_licensed_pdf ( self ) : return self . has_pdf and self . has_open_license","title":"Unpaywall_Location"},{"location":"reference/manubot/cite/unpaywall/#ancestors-in-mro_1","text":"builtins.dict","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/unpaywall/#instance-variables_2","text":"has_creative_commons_license has_open_license has_openly_licensed_pdf has_pdf","title":"Instance variables"},{"location":"reference/manubot/cite/unpaywall/#methods_2","text":"","title":"Methods"},{"location":"reference/manubot/cite/unpaywall/#clear","text":"def clear ( ... ) D.clear() -> None. Remove all items from D.","title":"clear"},{"location":"reference/manubot/cite/unpaywall/#copy","text":"def copy ( ... ) D.copy() -> a shallow copy of D","title":"copy"},{"location":"reference/manubot/cite/unpaywall/#fromkeys","text":"def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value.","title":"fromkeys"},{"location":"reference/manubot/cite/unpaywall/#get","text":"def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default.","title":"get"},{"location":"reference/manubot/cite/unpaywall/#items","text":"def items ( ... ) D.items() -> a set-like object providing a view on D's items","title":"items"},{"location":"reference/manubot/cite/unpaywall/#keys","text":"def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys","title":"keys"},{"location":"reference/manubot/cite/unpaywall/#pop","text":"def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, default is returned if given, otherwise KeyError is raised","title":"pop"},{"location":"reference/manubot/cite/unpaywall/#popitem","text":"def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty.","title":"popitem"},{"location":"reference/manubot/cite/unpaywall/#setdefault","text":"def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default.","title":"setdefault"},{"location":"reference/manubot/cite/unpaywall/#update","text":"def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k]","title":"update"},{"location":"reference/manubot/cite/unpaywall/#values","text":"def values ( ... ) D.values() -> an object providing a view on D's values","title":"values"},{"location":"reference/manubot/cite/unpaywall/#unpaywall_arxiv","text":"class Unpaywall_arXiv ( arxiv_id , set_oa_locations = True , use_doi = True ) View Source class Unpaywall_arXiv ( Unpaywall ): def __init__ ( self , arxiv_id , set_oa_locations = True , use_doi = True ): from .arxiv import split_arxiv_id_version self . arxiv_id = arxiv_id self . arxiv_id_latest , self . arxiv_id_version = split_arxiv_id_version ( arxiv_id ) self . use_doi = use_doi if set_oa_locations : self . set_oa_locations () def set_oa_locations ( self ): from .arxiv import get_arxiv_csl_item if not self . csl_item : self . csl_item = get_arxiv_csl_item ( self . arxiv_id ) doi = self . csl_item . get ( \"DOI\" ) if self . use_doi and doi : unpaywall_doi = Unpaywall_DOI ( doi ) self . doi = unpaywall_doi . doi self . oa_locations = unpaywall_doi . oa_locations return location = self . location_from_arvix_id () self . oa_locations = [ location ] def location_from_arvix_id ( self ): import datetime url_for_pdf = f \"https://arxiv.org/pdf/ { self . arxiv_id } .pdf\" location = Unpaywall_Location ( { \"endpoint_id\" : None , \"evidence\" : \"oa repository\" , \"host_type\" : \"repository\" , \"is_best\" : True , \"license\" : self . get_license (), \"pmh_id\" : f \"oai:arXiv.org: { self . arxiv_id_latest } \" , \"repository_institution\" : \"Cornell University - arXiv\" , \"updated\" : datetime . datetime . now () . isoformat (), \"url\" : url_for_pdf , \"url_for_landing_page\" : f \"https://arxiv.org/abs/ { self . arxiv_id } \" , \"url_for_pdf\" : url_for_pdf , \"version\" : \"submittedVersion\" , } ) return location def get_license ( self ): \"\"\" Return license using choices from the Unpaywall data format. Looks for license metadata in the CSL Item. \"\"\" license = self . csl_item . note_dict . get ( \"license\" ) if not license : return # Example licenses from https://arxiv.org/help/license # http://creativecommons.org/publicdomain/zero/1.0/ # http://creativecommons.org/licenses/by/4.0/ # http://creativecommons.org/licenses/by-sa/4.0/ # http://creativecommons.org/licenses/by-nc-sa/4.0/ # http://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html from urllib.parse import urlparse parsed_url = urlparse ( license ) if not parsed_url . scheme . startswith ( \"http\" ): return if parsed_url . hostname . endswith ( \"creativecommons.org\" ): try : abbrev = parsed_url . path . split ( \"/\" )[ 2 ] except IndexError : return if abbrev == \"zero\" : return \"cc0\" return f \"cc- { abbrev } \"","title":"Unpaywall_arXiv"},{"location":"reference/manubot/cite/unpaywall/#ancestors-in-mro_2","text":"manubot.cite.unpaywall.Unpaywall","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/unpaywall/#class-variables_2","text":"csl_item","title":"Class variables"},{"location":"reference/manubot/cite/unpaywall/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/manubot/cite/unpaywall/#from_citekey_2","text":"def from_citekey ( citekey , csl_item = None ) Create an Unpaywall object for citekey . csl_item is an optional field that can avoid an external web request to generate to a new CSL Item. View Source @staticmethod def from_citekey ( citekey , csl_item = None ): \"\"\" Create an Unpaywall object for `citekey`. `csl_item` is an optional field that can avoid an external web request to generate to a new CSL Item. \"\"\" from .citekey import CiteKey if isinstance ( citekey , str ): citekey = CiteKey ( citekey ) if not isinstance ( citekey , CiteKey ): raise ValueError ( \"citekey must be a str or CiteKey\" ) if citekey . standard_prefix in source_to_unpaywaller : unpaywaller = source_to_unpaywaller [ citekey . standard_prefix ] unpaywall = unpaywaller ( citekey . standard_accession , set_oa_locations = False ) else : raise ValueError ( f \"Cannot Unpaywall { citekey . input_id } . \" f \"Supported citations sources are { ', ' . join ( source_to_unpaywaller ) } . \" \"Received {citekey.standard_prefix!r} .\" ) unpaywall . csl_item = csl_item unpaywall . set_oa_locations () return unpaywall","title":"from_citekey"},{"location":"reference/manubot/cite/unpaywall/#from_csl_item_2","text":"def from_csl_item ( csl_item ) Create an Unpaywall object for csl_item . View Source @classmethod def from_csl_item ( cls , csl_item ): \"\"\" Create an Unpaywall object for `csl_item`. \"\"\" from .csl_item import CSL_Item csl_item = CSL_Item ( csl_item ) doi = csl_item . get ( \"DOI\" ) if doi : return cls . from_citekey ( f \"doi: { doi } \" , csl_item = csl_item ) csl_item . infer_id () return cls . from_citekey ( csl_item [ \"id\" ], csl_item = csl_item )","title":"from_csl_item"},{"location":"reference/manubot/cite/unpaywall/#instance-variables_3","text":"best_openly_licensed_pdf best_pdf","title":"Instance variables"},{"location":"reference/manubot/cite/unpaywall/#methods_3","text":"","title":"Methods"},{"location":"reference/manubot/cite/unpaywall/#get_license","text":"def get_license ( self ) Return license using choices from the Unpaywall data format. Looks for license metadata in the CSL Item. View Source def get_license ( self ): \"\"\" Return license using choices from the Unpaywall data format. Looks for license metadata in the CSL Item. \"\"\" license = self . csl_item . note_dict . get ( \"license\" ) if not license : return # Example licenses from https://arxiv.org/help/license # http://creativecommons.org/publicdomain/zero/1.0/ # http://creativecommons.org/licenses/by/4.0/ # http://creativecommons.org/licenses/by-sa/4.0/ # http://creativecommons.org/licenses/by-nc-sa/4.0/ # http://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html from urllib.parse import urlparse parsed_url = urlparse ( license ) if not parsed_url . scheme . startswith ( \"http\" ): return if parsed_url . hostname . endswith ( \"creativecommons.org\" ): try : abbrev = parsed_url . path . split ( \"/\" )[ 2 ] except IndexError : return if abbrev == \"zero\" : return \"cc0\" return f \"cc- { abbrev } \"","title":"get_license"},{"location":"reference/manubot/cite/unpaywall/#location_from_arvix_id","text":"def location_from_arvix_id ( self ) View Source def location_from_arvix_id ( self ): import datetime url_for_pdf = f \"https://arxiv.org/pdf/ { self . arxiv_id } .pdf\" location = Unpaywall_Location ( { \"endpoint_id\" : None , \"evidence\" : \"oa repository\" , \"host_type\" : \"repository\" , \"is_best\" : True , \"license\" : self . get_license (), \"pmh_id\" : f \"oai:arXiv.org: { self . arxiv_id_latest } \" , \"repository_institution\" : \"Cornell University - arXiv\" , \"updated\" : datetime . datetime . now () . isoformat (), \"url\" : url_for_pdf , \"url_for_landing_page\" : f \"https://arxiv.org/abs/ { self . arxiv_id } \" , \"url_for_pdf\" : url_for_pdf , \"version\" : \"submittedVersion\" , } ) return location","title":"location_from_arvix_id"},{"location":"reference/manubot/cite/unpaywall/#set_oa_locations_2","text":"def set_oa_locations ( self ) Set self.oa_locations , which is a list of Unpaywall_Location objects. View Source def set_oa_locations ( self ): from .arxiv import get_arxiv_csl_item if not self . csl_item : self . csl_item = get_arxiv_csl_item ( self . arxiv_id ) doi = self . csl_item . get ( \"DOI\" ) if self . use_doi and doi : unpaywall_doi = Unpaywall_DOI ( doi ) self . doi = unpaywall_doi . doi self . oa_locations = unpaywall_doi . oa_locations return location = self . location_from_arvix_id () self . oa_locations = [ location ]","title":"set_oa_locations"},{"location":"reference/manubot/cite/url/","text":"Module manubot.cite.url None None View Source import json import logging import re from typing import Any , Dict from .handlers import Handler CSLItem = Dict [ str , Any ] class Handler_URL ( Handler ): standard_prefix = \"url\" prefixes = [ \"url\" , \"http\" , \"https\" , ] def standardize_prefix_accession ( self , accession ): if self . prefix_lower != \"url\" : accession = f \" { self . prefix_lower } : { accession } \" return self . standard_prefix , accession def get_csl_item ( self , citekey ): return get_url_csl_item ( citekey . standard_accession ) def get_url_csl_item ( url : str ) -> CSLItem : \"\"\" Get csl_item for a URL trying a sequence of strategies. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `url_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\"\" for retriever in url_retrievers : try : return retriever ( url ) except Exception as error : logging . warning ( f \"Error in { retriever . __name__ } for { url } \" f \"due to a { error . __class__ . __name__ } : \\n { error } \" ) logging . info ( error , exc_info = True ) raise Exception ( f \"all get_url_csl_item methods failed for { url } \" ) def get_url_csl_item_zotero ( url : str ) -> CSLItem : \"\"\" Use Zotero's translation-server to generate a CSL Item for the specified URL. \"\"\" from manubot.cite.zotero import export_as_csl , web_query zotero_data = web_query ( url ) csl_data = export_as_csl ( zotero_data ) ( csl_item ,) = csl_data if not csl_item . get ( \"URL\" ): # some Zotero translators don't set URL. https://github.com/manubot/manubot/issues/244 csl_item [ \"URL\" ] = url return csl_item def get_url_csl_item_greycite ( url : str ) -> CSLItem : \"\"\" Uses Greycite which has experiened uptime problems in the past. API calls seem to take at least 15 seconds. Browser requests are much faster. Setting header did not have an effect. Consider mimicking browser using selenium. More information on Greycite at: http://greycite.knowledgeblog.org/ http://knowledgeblog.org/greycite https://arxiv.org/abs/1304.7151 https://git.io/v9N2C \"\"\" import requests from manubot.util import get_manubot_user_agent headers = { \"Connection\" : \"close\" , # https://github.com/kennethreitz/requests/issues/4023 \"User-Agent\" : get_manubot_user_agent (), } response = requests . get ( \"http://greycite.knowledgeblog.org/json\" , params = { \"uri\" : url }, headers = headers ) response . raise_for_status () # Some Greycite responses were valid JSON besides for an error appended # like \"<p>*** Date set from uri<p>\" or \"<p>*** fetch error : 404<p>\". pattern = re . compile ( r \"<p>\\*\\*\\*.*<p>\" ) text = pattern . sub ( \"\" , response . text ) csl_item = json . loads ( text ) csl_item [ \"type\" ] = \"webpage\" return csl_item def get_url_csl_item_manual ( url : str ) -> CSLItem : \"\"\" Manually create csl_item for a URL. \"\"\" return { \"URL\" : url , \"type\" : \"webpage\" } url_retrievers = [ get_url_csl_item_zotero , get_url_csl_item_greycite , get_url_csl_item_manual , ] Variables CSLItem url_retrievers Functions get_url_csl_item def get_url_csl_item ( url : str ) -> Dict [ str , Any ] Get csl_item for a URL trying a sequence of strategies. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable url_retrievers . The methods are attempted in order, with this function returning the metadata from the first non-failing method. View Source def get_url_csl_item ( url : str ) -> CSLItem : \" \"\" Get csl_item for a URL trying a sequence of strategies. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `url_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\" \" for retriever in url_retrievers : try : return retriever ( url ) except Exception as error : logging . warning ( f \"Error in {retriever.__name__} for {url} \" f \"due to a {error.__class__.__name__}: \\n {error}\" ) logging . info ( error , exc_info = True ) raise Exception ( f \"all get_url_csl_item methods failed for {url}\" ) get_url_csl_item_greycite def get_url_csl_item_greycite ( url : str ) -> Dict [ str , Any ] Uses Greycite which has experiened uptime problems in the past. API calls seem to take at least 15 seconds. Browser requests are much faster. Setting header did not have an effect. Consider mimicking browser using selenium. More information on Greycite at: http://greycite.knowledgeblog.org/ http://knowledgeblog.org/greycite https://arxiv.org/abs/1304.7151 https://git.io/v9N2C View Source def get_url_csl_item_greycite ( url : str ) -> CSLItem : \"\"\" Uses Greycite which has experiened uptime problems in the past. API calls seem to take at least 15 seconds. Browser requests are much faster. Setting header did not have an effect. Consider mimicking browser using selenium. More information on Greycite at: http://greycite.knowledgeblog.org/ http://knowledgeblog.org/greycite https://arxiv.org/abs/1304.7151 https://git.io/v9N2C \"\"\" import requests from manubot.util import get_manubot_user_agent headers = { \"Connection\" : \"close\" , # https://github.com/kennethreitz/requests/issues/4023 \"User-Agent\" : get_manubot_user_agent (), } response = requests . get ( \"http://greycite.knowledgeblog.org/json\" , params = { \"uri\" : url }, headers = headers ) response . raise_for_status () # Some Greycite responses were valid JSON besides for an error appended # like \"<p>*** Date set from uri<p>\" or \"<p>*** fetch error : 404<p>\". pattern = re . compile ( r \"<p>\\*\\*\\*.*<p>\" ) text = pattern . sub ( \"\" , response . text ) csl_item = json . loads ( text ) csl_item [ \"type\" ] = \"webpage\" return csl_item get_url_csl_item_manual def get_url_csl_item_manual ( url : str ) -> Dict [ str , Any ] Manually create csl_item for a URL. View Source def get_url_csl_item_manual ( url : str ) -> CSLItem : \"\"\" Manually create csl_item for a URL. \"\"\" return { \"URL\" : url , \"type\" : \"webpage\" } get_url_csl_item_zotero def get_url_csl_item_zotero ( url : str ) -> Dict [ str , Any ] Use Zotero's translation-server to generate a CSL Item for the specified URL. View Source def get_url_csl_item_zotero ( url : str ) -> CSLItem : \"\"\" Use Zotero's translation-server to generate a CSL Item for the specified URL. \"\"\" from manubot.cite.zotero import export_as_csl , web_query zotero_data = web_query ( url ) csl_data = export_as_csl ( zotero_data ) ( csl_item ,) = csl_data if not csl_item . get ( \"URL\" ): # some Zotero translators don't set URL. https://github.com/manubot/manubot/issues/244 csl_item [ \"URL\" ] = url return csl_item Classes Handler_URL class Handler_URL ( prefix_lower : str ) View Source class Handler_URL ( Handler ): standard_prefix = \"url\" prefixes = [ \"url\" , \"http\" , \"https\" , ] def standardize_prefix_accession ( self , accession ): if self . prefix_lower != \"url\" : accession = f \"{self.prefix_lower}:{accession}\" return self . standard_prefix , accession def get_csl_item ( self , citekey ): return get_url_csl_item ( citekey . standard_accession ) Ancestors (in MRO) manubot.cite.handlers.Handler Class variables prefixes standard_prefix Methods get_csl_item def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ) : return get_url_csl_item ( citekey . standard_accession ) inspect def inspect ( self , citekey : manubot . cite . citekey . CiteKey ) -> Optional [ str ] Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey : CiteKey ) -> Optional [ str ] : \"\"\" Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. \"\"\" pattern = self . _get_pattern ( \"accession_pattern\" ) if not pattern : return None if not pattern . fullmatch ( citekey . accession ) : return f \"{citekey.accession} does not match regex {pattern.pattern}\" standardize_prefix_accession def standardize_prefix_accession ( self , accession ) Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession ) : if self . prefix_lower != \" url \" : accession = f \" {self.prefix_lower}:{accession} \" return self . standard_prefix , accession","title":"Url"},{"location":"reference/manubot/cite/url/#module-manubotciteurl","text":"None None View Source import json import logging import re from typing import Any , Dict from .handlers import Handler CSLItem = Dict [ str , Any ] class Handler_URL ( Handler ): standard_prefix = \"url\" prefixes = [ \"url\" , \"http\" , \"https\" , ] def standardize_prefix_accession ( self , accession ): if self . prefix_lower != \"url\" : accession = f \" { self . prefix_lower } : { accession } \" return self . standard_prefix , accession def get_csl_item ( self , citekey ): return get_url_csl_item ( citekey . standard_accession ) def get_url_csl_item ( url : str ) -> CSLItem : \"\"\" Get csl_item for a URL trying a sequence of strategies. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `url_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\"\" for retriever in url_retrievers : try : return retriever ( url ) except Exception as error : logging . warning ( f \"Error in { retriever . __name__ } for { url } \" f \"due to a { error . __class__ . __name__ } : \\n { error } \" ) logging . info ( error , exc_info = True ) raise Exception ( f \"all get_url_csl_item methods failed for { url } \" ) def get_url_csl_item_zotero ( url : str ) -> CSLItem : \"\"\" Use Zotero's translation-server to generate a CSL Item for the specified URL. \"\"\" from manubot.cite.zotero import export_as_csl , web_query zotero_data = web_query ( url ) csl_data = export_as_csl ( zotero_data ) ( csl_item ,) = csl_data if not csl_item . get ( \"URL\" ): # some Zotero translators don't set URL. https://github.com/manubot/manubot/issues/244 csl_item [ \"URL\" ] = url return csl_item def get_url_csl_item_greycite ( url : str ) -> CSLItem : \"\"\" Uses Greycite which has experiened uptime problems in the past. API calls seem to take at least 15 seconds. Browser requests are much faster. Setting header did not have an effect. Consider mimicking browser using selenium. More information on Greycite at: http://greycite.knowledgeblog.org/ http://knowledgeblog.org/greycite https://arxiv.org/abs/1304.7151 https://git.io/v9N2C \"\"\" import requests from manubot.util import get_manubot_user_agent headers = { \"Connection\" : \"close\" , # https://github.com/kennethreitz/requests/issues/4023 \"User-Agent\" : get_manubot_user_agent (), } response = requests . get ( \"http://greycite.knowledgeblog.org/json\" , params = { \"uri\" : url }, headers = headers ) response . raise_for_status () # Some Greycite responses were valid JSON besides for an error appended # like \"<p>*** Date set from uri<p>\" or \"<p>*** fetch error : 404<p>\". pattern = re . compile ( r \"<p>\\*\\*\\*.*<p>\" ) text = pattern . sub ( \"\" , response . text ) csl_item = json . loads ( text ) csl_item [ \"type\" ] = \"webpage\" return csl_item def get_url_csl_item_manual ( url : str ) -> CSLItem : \"\"\" Manually create csl_item for a URL. \"\"\" return { \"URL\" : url , \"type\" : \"webpage\" } url_retrievers = [ get_url_csl_item_zotero , get_url_csl_item_greycite , get_url_csl_item_manual , ]","title":"Module manubot.cite.url"},{"location":"reference/manubot/cite/url/#variables","text":"CSLItem url_retrievers","title":"Variables"},{"location":"reference/manubot/cite/url/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/url/#get_url_csl_item","text":"def get_url_csl_item ( url : str ) -> Dict [ str , Any ] Get csl_item for a URL trying a sequence of strategies. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable url_retrievers . The methods are attempted in order, with this function returning the metadata from the first non-failing method. View Source def get_url_csl_item ( url : str ) -> CSLItem : \" \"\" Get csl_item for a URL trying a sequence of strategies. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `url_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\" \" for retriever in url_retrievers : try : return retriever ( url ) except Exception as error : logging . warning ( f \"Error in {retriever.__name__} for {url} \" f \"due to a {error.__class__.__name__}: \\n {error}\" ) logging . info ( error , exc_info = True ) raise Exception ( f \"all get_url_csl_item methods failed for {url}\" )","title":"get_url_csl_item"},{"location":"reference/manubot/cite/url/#get_url_csl_item_greycite","text":"def get_url_csl_item_greycite ( url : str ) -> Dict [ str , Any ] Uses Greycite which has experiened uptime problems in the past. API calls seem to take at least 15 seconds. Browser requests are much faster. Setting header did not have an effect. Consider mimicking browser using selenium. More information on Greycite at: http://greycite.knowledgeblog.org/ http://knowledgeblog.org/greycite https://arxiv.org/abs/1304.7151 https://git.io/v9N2C View Source def get_url_csl_item_greycite ( url : str ) -> CSLItem : \"\"\" Uses Greycite which has experiened uptime problems in the past. API calls seem to take at least 15 seconds. Browser requests are much faster. Setting header did not have an effect. Consider mimicking browser using selenium. More information on Greycite at: http://greycite.knowledgeblog.org/ http://knowledgeblog.org/greycite https://arxiv.org/abs/1304.7151 https://git.io/v9N2C \"\"\" import requests from manubot.util import get_manubot_user_agent headers = { \"Connection\" : \"close\" , # https://github.com/kennethreitz/requests/issues/4023 \"User-Agent\" : get_manubot_user_agent (), } response = requests . get ( \"http://greycite.knowledgeblog.org/json\" , params = { \"uri\" : url }, headers = headers ) response . raise_for_status () # Some Greycite responses were valid JSON besides for an error appended # like \"<p>*** Date set from uri<p>\" or \"<p>*** fetch error : 404<p>\". pattern = re . compile ( r \"<p>\\*\\*\\*.*<p>\" ) text = pattern . sub ( \"\" , response . text ) csl_item = json . loads ( text ) csl_item [ \"type\" ] = \"webpage\" return csl_item","title":"get_url_csl_item_greycite"},{"location":"reference/manubot/cite/url/#get_url_csl_item_manual","text":"def get_url_csl_item_manual ( url : str ) -> Dict [ str , Any ] Manually create csl_item for a URL. View Source def get_url_csl_item_manual ( url : str ) -> CSLItem : \"\"\" Manually create csl_item for a URL. \"\"\" return { \"URL\" : url , \"type\" : \"webpage\" }","title":"get_url_csl_item_manual"},{"location":"reference/manubot/cite/url/#get_url_csl_item_zotero","text":"def get_url_csl_item_zotero ( url : str ) -> Dict [ str , Any ] Use Zotero's translation-server to generate a CSL Item for the specified URL. View Source def get_url_csl_item_zotero ( url : str ) -> CSLItem : \"\"\" Use Zotero's translation-server to generate a CSL Item for the specified URL. \"\"\" from manubot.cite.zotero import export_as_csl , web_query zotero_data = web_query ( url ) csl_data = export_as_csl ( zotero_data ) ( csl_item ,) = csl_data if not csl_item . get ( \"URL\" ): # some Zotero translators don't set URL. https://github.com/manubot/manubot/issues/244 csl_item [ \"URL\" ] = url return csl_item","title":"get_url_csl_item_zotero"},{"location":"reference/manubot/cite/url/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/url/#handler_url","text":"class Handler_URL ( prefix_lower : str ) View Source class Handler_URL ( Handler ): standard_prefix = \"url\" prefixes = [ \"url\" , \"http\" , \"https\" , ] def standardize_prefix_accession ( self , accession ): if self . prefix_lower != \"url\" : accession = f \"{self.prefix_lower}:{accession}\" return self . standard_prefix , accession def get_csl_item ( self , citekey ): return get_url_csl_item ( citekey . standard_accession )","title":"Handler_URL"},{"location":"reference/manubot/cite/url/#ancestors-in-mro","text":"manubot.cite.handlers.Handler","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/url/#class-variables","text":"prefixes standard_prefix","title":"Class variables"},{"location":"reference/manubot/cite/url/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/url/#get_csl_item","text":"def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ) : return get_url_csl_item ( citekey . standard_accession )","title":"get_csl_item"},{"location":"reference/manubot/cite/url/#inspect","text":"def inspect ( self , citekey : manubot . cite . citekey . CiteKey ) -> Optional [ str ] Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey : CiteKey ) -> Optional [ str ] : \"\"\" Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. \"\"\" pattern = self . _get_pattern ( \"accession_pattern\" ) if not pattern : return None if not pattern . fullmatch ( citekey . accession ) : return f \"{citekey.accession} does not match regex {pattern.pattern}\"","title":"inspect"},{"location":"reference/manubot/cite/url/#standardize_prefix_accession","text":"def standardize_prefix_accession ( self , accession ) Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession ) : if self . prefix_lower != \" url \" : accession = f \" {self.prefix_lower}:{accession} \" return self . standard_prefix , accession","title":"standardize_prefix_accession"},{"location":"reference/manubot/cite/wikidata/","text":"Module manubot.cite.wikidata None None View Source from typing import Any , Dict from .handlers import Handler class Handler_Wikidata ( Handler ): prefixes = [ \"wikidata\" ] standard_prefix = \"wikidata\" accession_pattern = r \"Q[0-9]+\" def inspect ( self , citekey ): \"\"\" https://www.wikidata.org/wiki/Wikidata:Identifiers \"\"\" accession = citekey . accession if not accession . startswith ( \"Q\" ): return \"Wikidata item IDs must start with 'Q'.\" elif not self . _get_pattern () . fullmatch ( accession ): return ( \"Accession does not conform to the Wikidata regex. \" \"Double check the entity ID.\" ) def get_csl_item ( self , citekey ): return get_wikidata_csl_item ( citekey . standard_accession ) def get_wikidata_csl_item ( identifier : str ) -> Dict [ str , Any ]: \"\"\" Get a CSL JSON item with the citation metadata for a Wikidata item. identifier should be a Wikidata item ID corresponding to a citeable work, such as Q50051684. \"\"\" url = f \"https://www.wikidata.org/wiki/ { identifier } \" from manubot.cite.url import get_url_csl_item_zotero csl_item = get_url_csl_item_zotero ( url ) if \"DOI\" in csl_item : csl_item [ \"DOI\" ] = csl_item [ \"DOI\" ] . lower () if \"URL\" not in csl_item : csl_item [ \"URL\" ] = url return csl_item Functions get_wikidata_csl_item def get_wikidata_csl_item ( identifier : str ) -> Dict [ str , Any ] Get a CSL JSON item with the citation metadata for a Wikidata item. identifier should be a Wikidata item ID corresponding to a citeable work, such as Q50051684. View Source def get_wikidata_csl_item ( identifier : str ) -> Dict [ str , Any ]: \"\"\" Get a CSL JSON item with the citation metadata for a Wikidata item. identifier should be a Wikidata item ID corresponding to a citeable work, such as Q50051684. \"\"\" url = f \"https://www.wikidata.org/wiki/ { identifier } \" from manubot.cite.url import get_url_csl_item_zotero csl_item = get_url_csl_item_zotero ( url ) if \"DOI\" in csl_item : csl_item [ \"DOI\" ] = csl_item [ \"DOI\" ] . lower () if \"URL\" not in csl_item : csl_item [ \"URL\" ] = url return csl_item Classes Handler_Wikidata class Handler_Wikidata ( prefix_lower : str ) View Source class Handler_Wikidata ( Handler ) : prefixes = [ \"wikidata\" ] standard_prefix = \"wikidata\" accession_pattern = r \"Q[0-9]+\" def inspect ( self , citekey ) : \"\"\" https: //www.wikidata.org/wiki/Wikidata:Identifiers \"\"\" accession = citekey . accession if not accession . startswith ( \"Q\" ) : return \"Wikidata item IDs must start with 'Q'.\" elif not self . _get_pattern (). fullmatch ( accession ) : return ( \"Accession does not conform to the Wikidata regex. \" \"Double check the entity ID.\" ) def get_csl_item ( self , citekey ) : return get_wikidata_csl_item ( citekey . standard_accession ) Ancestors (in MRO) manubot.cite.handlers.Handler Class variables accession_pattern prefixes standard_prefix Methods get_csl_item def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ) : return get_wikidata_csl_item ( citekey . standard_accession ) inspect def inspect ( self , citekey ) https://www.wikidata.org/wiki/Wikidata:Identifiers View Source def inspect ( self , citekey ) : \"\"\" https: //www.wikidata.org/wiki/Wikidata:Identifiers \"\"\" accession = citekey . accession if not accession . startswith ( \"Q\" ) : return \"Wikidata item IDs must start with 'Q'.\" elif not self . _get_pattern (). fullmatch ( accession ) : return ( \"Accession does not conform to the Wikidata regex. \" \"Double check the entity ID.\" ) standardize_prefix_accession def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ] Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ] : \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession","title":"Wikidata"},{"location":"reference/manubot/cite/wikidata/#module-manubotcitewikidata","text":"None None View Source from typing import Any , Dict from .handlers import Handler class Handler_Wikidata ( Handler ): prefixes = [ \"wikidata\" ] standard_prefix = \"wikidata\" accession_pattern = r \"Q[0-9]+\" def inspect ( self , citekey ): \"\"\" https://www.wikidata.org/wiki/Wikidata:Identifiers \"\"\" accession = citekey . accession if not accession . startswith ( \"Q\" ): return \"Wikidata item IDs must start with 'Q'.\" elif not self . _get_pattern () . fullmatch ( accession ): return ( \"Accession does not conform to the Wikidata regex. \" \"Double check the entity ID.\" ) def get_csl_item ( self , citekey ): return get_wikidata_csl_item ( citekey . standard_accession ) def get_wikidata_csl_item ( identifier : str ) -> Dict [ str , Any ]: \"\"\" Get a CSL JSON item with the citation metadata for a Wikidata item. identifier should be a Wikidata item ID corresponding to a citeable work, such as Q50051684. \"\"\" url = f \"https://www.wikidata.org/wiki/ { identifier } \" from manubot.cite.url import get_url_csl_item_zotero csl_item = get_url_csl_item_zotero ( url ) if \"DOI\" in csl_item : csl_item [ \"DOI\" ] = csl_item [ \"DOI\" ] . lower () if \"URL\" not in csl_item : csl_item [ \"URL\" ] = url return csl_item","title":"Module manubot.cite.wikidata"},{"location":"reference/manubot/cite/wikidata/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/wikidata/#get_wikidata_csl_item","text":"def get_wikidata_csl_item ( identifier : str ) -> Dict [ str , Any ] Get a CSL JSON item with the citation metadata for a Wikidata item. identifier should be a Wikidata item ID corresponding to a citeable work, such as Q50051684. View Source def get_wikidata_csl_item ( identifier : str ) -> Dict [ str , Any ]: \"\"\" Get a CSL JSON item with the citation metadata for a Wikidata item. identifier should be a Wikidata item ID corresponding to a citeable work, such as Q50051684. \"\"\" url = f \"https://www.wikidata.org/wiki/ { identifier } \" from manubot.cite.url import get_url_csl_item_zotero csl_item = get_url_csl_item_zotero ( url ) if \"DOI\" in csl_item : csl_item [ \"DOI\" ] = csl_item [ \"DOI\" ] . lower () if \"URL\" not in csl_item : csl_item [ \"URL\" ] = url return csl_item","title":"get_wikidata_csl_item"},{"location":"reference/manubot/cite/wikidata/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/wikidata/#handler_wikidata","text":"class Handler_Wikidata ( prefix_lower : str ) View Source class Handler_Wikidata ( Handler ) : prefixes = [ \"wikidata\" ] standard_prefix = \"wikidata\" accession_pattern = r \"Q[0-9]+\" def inspect ( self , citekey ) : \"\"\" https: //www.wikidata.org/wiki/Wikidata:Identifiers \"\"\" accession = citekey . accession if not accession . startswith ( \"Q\" ) : return \"Wikidata item IDs must start with 'Q'.\" elif not self . _get_pattern (). fullmatch ( accession ) : return ( \"Accession does not conform to the Wikidata regex. \" \"Double check the entity ID.\" ) def get_csl_item ( self , citekey ) : return get_wikidata_csl_item ( citekey . standard_accession )","title":"Handler_Wikidata"},{"location":"reference/manubot/cite/wikidata/#ancestors-in-mro","text":"manubot.cite.handlers.Handler","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/wikidata/#class-variables","text":"accession_pattern prefixes standard_prefix","title":"Class variables"},{"location":"reference/manubot/cite/wikidata/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/wikidata/#get_csl_item","text":"def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ) : return get_wikidata_csl_item ( citekey . standard_accession )","title":"get_csl_item"},{"location":"reference/manubot/cite/wikidata/#inspect","text":"def inspect ( self , citekey ) https://www.wikidata.org/wiki/Wikidata:Identifiers View Source def inspect ( self , citekey ) : \"\"\" https: //www.wikidata.org/wiki/Wikidata:Identifiers \"\"\" accession = citekey . accession if not accession . startswith ( \"Q\" ) : return \"Wikidata item IDs must start with 'Q'.\" elif not self . _get_pattern (). fullmatch ( accession ) : return ( \"Accession does not conform to the Wikidata regex. \" \"Double check the entity ID.\" )","title":"inspect"},{"location":"reference/manubot/cite/wikidata/#standardize_prefix_accession","text":"def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ] Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession : str ) -> Tuple [ str , str ] : \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession","title":"standardize_prefix_accession"},{"location":"reference/manubot/cite/zotero/","text":"Module manubot.cite.zotero Methods to interact with a Zotero translation-server. https://github.com/zotero/translation-server The Manubot team currently hosts a public translation server at https://translate.manubot.org. More information on this instance at https://github.com/manubot/manubot/issues/82. View Source \"\"\" Methods to interact with a Zotero translation-server. https://github.com/zotero/translation-server The Manubot team currently hosts a public translation server at https://translate.manubot.org. More information on this instance at https://github.com/manubot/manubot/issues/82. \"\"\" import json import logging from typing import Any , Dict , List import requests from manubot.util import get_manubot_user_agent , is_http_url ZoteroRecord = Dict [ str , Any ] ZoteroData = List [ ZoteroRecord ] # for the purposes of this module, the CSL Items and Zotero Data have the same type CSLItem = ZoteroRecord CSLItems = ZoteroData base_url = \"https://translate.manubot.org\" \"\"\"URL that provides access to the Zotero translation-server API\"\"\" def web_query ( url : str ) -> ZoteroData : \"\"\" Return Zotero citation metadata for a URL as a list containing a single element that is a dictionary with the URL's metadata. \"\"\" headers = { \"User-Agent\" : get_manubot_user_agent (), \"Content-Type\" : \"text/plain\" } params = { \"single\" : 1 } api_url = f \" { base_url } /web\" response = requests . post ( api_url , params = params , headers = headers , data = str ( url )) try : zotero_data = response . json () except Exception as error : logging . warning ( f \"Error parsing web_query output as JSON for { url } : \\n { response . text } \" ) raise error if response . status_code == 300 : # When single=1 is specified, multiple results should never be returned logging . warning ( f \"web_query returned multiple results for { url } : \\n \" + json . dumps ( zotero_data , indent = 2 ) ) raise ValueError ( f \"multiple results for { url } \" ) zotero_data = _passthrough_zotero_data ( zotero_data ) return zotero_data def search_query ( identifier : str ) -> ZoteroData : \"\"\" Retrive Zotero metadata for a DOI, ISBN, PMID, or arXiv ID. Example usage: ```shell curl --silent \\ --data '10.2307/4486062' \\ --header 'Content-Type: text/plain' \\ http://127.0.0.1:1969/search ``` \"\"\" api_url = f \" { base_url } /search\" headers = { \"User-Agent\" : get_manubot_user_agent (), \"Content-Type\" : \"text/plain\" } response = requests . post ( api_url , headers = headers , data = str ( identifier )) try : zotero_data = response . json () except Exception as error : logging . warning ( f \"Error parsing search_query output as JSON for { identifier } : \\n { response . text } \" ) raise error zotero_data = _passthrough_zotero_data ( zotero_data ) return zotero_data def _passthrough_zotero_data ( zotero_data : ZoteroData ) -> ZoteroData : \"\"\" Address known issues with Zotero metadata. Assumes zotero data should contain a single bibliographic record. \"\"\" if not isinstance ( zotero_data , list ): raise ValueError ( \"_passthrough_zotero_data: zotero_data should be a list\" ) if len ( zotero_data ) > 1 : # Sometimes translation-server creates multiple data items for a single record. # If so, keep only the parent item, and remove child items (such as notes). # https://github.com/zotero/translation-server/issues/67 zotero_data = zotero_data [: 1 ] return zotero_data def export_as_csl ( zotero_data : ZoteroData ) -> CSLItems : \"\"\" Export Zotero JSON data to CSL JSON using a translation-server /export query. Performs a similar query to the following curl command: ``` curl --verbose \\ --data @items.json \\ --header 'Content-Type: application/json' \\ 'https://translate.manubot.org/export?format=csljson' ``` \"\"\" api_url = f \" { base_url } /export\" params = { \"format\" : \"csljson\" } headers = { \"User-Agent\" : get_manubot_user_agent ()} response = requests . post ( api_url , params = params , headers = headers , json = zotero_data ) if not response . ok : message = f \"export_as_csl: translation-server returned status code { response . status_code } \" logging . warning ( f \" { message } with the following output: \\n { response . text } \" ) raise requests . HTTPError ( message ) try : csl_items = response . json () except Exception as error : logging . warning ( f \"Error parsing export_as_csl output as JSON: \\n { response . text } \" ) raise error return csl_items def get_csl_item ( identifier : str ) -> CSLItem : \"\"\" Use a translation-server search query followed by an export query to return a CSL Item (the first & only record of the returned CSL JSON). \"\"\" zotero_data = search_query ( identifier ) csl_items = export_as_csl ( zotero_data ) ( csl_item ,) = csl_items return csl_item def search_or_web_query ( identifier : str ) -> ZoteroData : \"\"\" Detect whether `identifier` is a URL. If so, retrieve zotero metadata using a /web query. Otherwise, retrieve zotero metadata using a /search query. \"\"\" if is_http_url ( identifier ): zotero_data = web_query ( identifier ) else : zotero_data = search_query ( identifier ) return zotero_data Variables CSLItem CSLItems ZoteroData ZoteroRecord base_url URL that provides access to the Zotero translation-server API Functions export_as_csl def export_as_csl ( zotero_data : List [ Dict [ str , Any ]] ) -> List [ Dict [ str , Any ]] Export Zotero JSON data to CSL JSON using a translation-server /export query. Performs a similar query to the following curl command: curl -- verbose -- data @ items . json -- header 'Content-Type: application/json' 'https://translate.manubot.org/export?format=csljson' View Source def export_as_csl ( zotero_data : ZoteroData ) -> CSLItems : \"\"\" Export Zotero JSON data to CSL JSON using a translation-server /export query. Performs a similar query to the following curl command: ``` curl --verbose \\ --data @items.json \\ --header 'Content-Type: application/json' \\ 'https://translate.manubot.org/export?format=csljson' ``` \"\"\" api_url = f \"{base_url}/export\" params = { \"format\" : \"csljson\" } headers = { \"User-Agent\" : get_manubot_user_agent ()} response = requests . post ( api_url , params = params , headers = headers , json = zotero_data ) if not response . ok : message = f \"export_as_csl: translation-server returned status code {response.status_code}\" logging . warning ( f \"{message} with the following output: \\n {response.text}\" ) raise requests . HTTPError ( message ) try : csl_items = response . json () except Exception as error : logging . warning ( f \"Error parsing export_as_csl output as JSON: \\n {response.text}\" ) raise error return csl_items get_csl_item def get_csl_item ( identifier : str ) -> Dict [ str , Any ] Use a translation-server search query followed by an export query to return a CSL Item (the first & only record of the returned CSL JSON). View Source def get_csl_item ( identifier : str ) -> CSLItem : \"\"\" Use a translation-server search query followed by an export query to return a CSL Item (the first & only record of the returned CSL JSON). \"\"\" zotero_data = search_query ( identifier ) csl_items = export_as_csl ( zotero_data ) ( csl_item ,) = csl_items return csl_item search_or_web_query def search_or_web_query ( identifier : str ) -> List [ Dict [ str , Any ]] Detect whether identifier is a URL. If so, retrieve zotero metadata using a /web query. Otherwise, retrieve zotero metadata using a /search query. View Source def search_or_web_query ( identifier : str ) -> ZoteroData : \" \"\" Detect whether `identifier` is a URL. If so, retrieve zotero metadata using a /web query. Otherwise, retrieve zotero metadata using a /search query. \"\" \" if is_http_url ( identifier ) : zotero_data = web_query ( identifier ) else : zotero_data = search_query ( identifier ) return zotero_data search_query def search_query ( identifier : str ) -> List [ Dict [ str , Any ]] Retrive Zotero metadata for a DOI, ISBN, PMID, or arXiv ID. Example usage: curl --silent --data '10.2307/4486062' --header 'Content-Type: text/plain' http://127.0.0.1:1969/search View Source def search_query ( identifier : str ) -> ZoteroData : \"\"\" Retrive Zotero metadata for a DOI, ISBN, PMID, or arXiv ID. Example usage: ```shell curl --silent \\ --data '10.2307/4486062' \\ --header 'Content-Type: text/plain' \\ http://127.0.0.1:1969/search ``` \"\"\" api_url = f \"{base_url}/search\" headers = { \"User-Agent\" : get_manubot_user_agent (), \"Content-Type\" : \"text/plain\" } response = requests . post ( api_url , headers= headers , data= str ( identifier )) try : zotero_data = response . json () except Exception as error : logging . warning ( f \"Error parsing search_query output as JSON for {identifier}: \\ n{response.text}\" ) raise error zotero_data = _passthrough_zotero_data ( zotero_data ) return zotero_data web_query def web_query ( url : str ) -> List [ Dict [ str , Any ]] Return Zotero citation metadata for a URL as a list containing a single element that is a dictionary with the URL's metadata. View Source def web_query ( url : str ) -> ZoteroData : \"\"\" Return Zotero citation metadata for a URL as a list containing a single element that is a dictionary with the URL's metadata. \"\"\" headers = { \"User-Agent\" : get_manubot_user_agent (), \"Content-Type\" : \"text/plain\" } params = { \"single\" : 1 } api_url = f \"{base_url}/web\" response = requests . post ( api_url , params= params , headers= headers , data= str ( url )) try : zotero_data = response . json () except Exception as error : logging . warning ( f \"Error parsing web_query output as JSON for {url}: \\ n{response.text}\" ) raise error if response . status_code = = 300 : # When single=1 is specified, multiple results should never be returned logging . warning ( f \"web_query returned multiple results for {url}: \\ n\" + json . dumps ( zotero_data , indent= 2 ) ) raise ValueError ( f \"multiple results for {url}\" ) zotero_data = _passthrough_zotero_data ( zotero_data ) return zotero_data","title":"Zotero"},{"location":"reference/manubot/cite/zotero/#module-manubotcitezotero","text":"Methods to interact with a Zotero translation-server. https://github.com/zotero/translation-server The Manubot team currently hosts a public translation server at https://translate.manubot.org. More information on this instance at https://github.com/manubot/manubot/issues/82. View Source \"\"\" Methods to interact with a Zotero translation-server. https://github.com/zotero/translation-server The Manubot team currently hosts a public translation server at https://translate.manubot.org. More information on this instance at https://github.com/manubot/manubot/issues/82. \"\"\" import json import logging from typing import Any , Dict , List import requests from manubot.util import get_manubot_user_agent , is_http_url ZoteroRecord = Dict [ str , Any ] ZoteroData = List [ ZoteroRecord ] # for the purposes of this module, the CSL Items and Zotero Data have the same type CSLItem = ZoteroRecord CSLItems = ZoteroData base_url = \"https://translate.manubot.org\" \"\"\"URL that provides access to the Zotero translation-server API\"\"\" def web_query ( url : str ) -> ZoteroData : \"\"\" Return Zotero citation metadata for a URL as a list containing a single element that is a dictionary with the URL's metadata. \"\"\" headers = { \"User-Agent\" : get_manubot_user_agent (), \"Content-Type\" : \"text/plain\" } params = { \"single\" : 1 } api_url = f \" { base_url } /web\" response = requests . post ( api_url , params = params , headers = headers , data = str ( url )) try : zotero_data = response . json () except Exception as error : logging . warning ( f \"Error parsing web_query output as JSON for { url } : \\n { response . text } \" ) raise error if response . status_code == 300 : # When single=1 is specified, multiple results should never be returned logging . warning ( f \"web_query returned multiple results for { url } : \\n \" + json . dumps ( zotero_data , indent = 2 ) ) raise ValueError ( f \"multiple results for { url } \" ) zotero_data = _passthrough_zotero_data ( zotero_data ) return zotero_data def search_query ( identifier : str ) -> ZoteroData : \"\"\" Retrive Zotero metadata for a DOI, ISBN, PMID, or arXiv ID. Example usage: ```shell curl --silent \\ --data '10.2307/4486062' \\ --header 'Content-Type: text/plain' \\ http://127.0.0.1:1969/search ``` \"\"\" api_url = f \" { base_url } /search\" headers = { \"User-Agent\" : get_manubot_user_agent (), \"Content-Type\" : \"text/plain\" } response = requests . post ( api_url , headers = headers , data = str ( identifier )) try : zotero_data = response . json () except Exception as error : logging . warning ( f \"Error parsing search_query output as JSON for { identifier } : \\n { response . text } \" ) raise error zotero_data = _passthrough_zotero_data ( zotero_data ) return zotero_data def _passthrough_zotero_data ( zotero_data : ZoteroData ) -> ZoteroData : \"\"\" Address known issues with Zotero metadata. Assumes zotero data should contain a single bibliographic record. \"\"\" if not isinstance ( zotero_data , list ): raise ValueError ( \"_passthrough_zotero_data: zotero_data should be a list\" ) if len ( zotero_data ) > 1 : # Sometimes translation-server creates multiple data items for a single record. # If so, keep only the parent item, and remove child items (such as notes). # https://github.com/zotero/translation-server/issues/67 zotero_data = zotero_data [: 1 ] return zotero_data def export_as_csl ( zotero_data : ZoteroData ) -> CSLItems : \"\"\" Export Zotero JSON data to CSL JSON using a translation-server /export query. Performs a similar query to the following curl command: ``` curl --verbose \\ --data @items.json \\ --header 'Content-Type: application/json' \\ 'https://translate.manubot.org/export?format=csljson' ``` \"\"\" api_url = f \" { base_url } /export\" params = { \"format\" : \"csljson\" } headers = { \"User-Agent\" : get_manubot_user_agent ()} response = requests . post ( api_url , params = params , headers = headers , json = zotero_data ) if not response . ok : message = f \"export_as_csl: translation-server returned status code { response . status_code } \" logging . warning ( f \" { message } with the following output: \\n { response . text } \" ) raise requests . HTTPError ( message ) try : csl_items = response . json () except Exception as error : logging . warning ( f \"Error parsing export_as_csl output as JSON: \\n { response . text } \" ) raise error return csl_items def get_csl_item ( identifier : str ) -> CSLItem : \"\"\" Use a translation-server search query followed by an export query to return a CSL Item (the first & only record of the returned CSL JSON). \"\"\" zotero_data = search_query ( identifier ) csl_items = export_as_csl ( zotero_data ) ( csl_item ,) = csl_items return csl_item def search_or_web_query ( identifier : str ) -> ZoteroData : \"\"\" Detect whether `identifier` is a URL. If so, retrieve zotero metadata using a /web query. Otherwise, retrieve zotero metadata using a /search query. \"\"\" if is_http_url ( identifier ): zotero_data = web_query ( identifier ) else : zotero_data = search_query ( identifier ) return zotero_data","title":"Module manubot.cite.zotero"},{"location":"reference/manubot/cite/zotero/#variables","text":"CSLItem CSLItems ZoteroData ZoteroRecord base_url URL that provides access to the Zotero translation-server API","title":"Variables"},{"location":"reference/manubot/cite/zotero/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/zotero/#export_as_csl","text":"def export_as_csl ( zotero_data : List [ Dict [ str , Any ]] ) -> List [ Dict [ str , Any ]] Export Zotero JSON data to CSL JSON using a translation-server /export query. Performs a similar query to the following curl command: curl -- verbose -- data @ items . json -- header 'Content-Type: application/json' 'https://translate.manubot.org/export?format=csljson' View Source def export_as_csl ( zotero_data : ZoteroData ) -> CSLItems : \"\"\" Export Zotero JSON data to CSL JSON using a translation-server /export query. Performs a similar query to the following curl command: ``` curl --verbose \\ --data @items.json \\ --header 'Content-Type: application/json' \\ 'https://translate.manubot.org/export?format=csljson' ``` \"\"\" api_url = f \"{base_url}/export\" params = { \"format\" : \"csljson\" } headers = { \"User-Agent\" : get_manubot_user_agent ()} response = requests . post ( api_url , params = params , headers = headers , json = zotero_data ) if not response . ok : message = f \"export_as_csl: translation-server returned status code {response.status_code}\" logging . warning ( f \"{message} with the following output: \\n {response.text}\" ) raise requests . HTTPError ( message ) try : csl_items = response . json () except Exception as error : logging . warning ( f \"Error parsing export_as_csl output as JSON: \\n {response.text}\" ) raise error return csl_items","title":"export_as_csl"},{"location":"reference/manubot/cite/zotero/#get_csl_item","text":"def get_csl_item ( identifier : str ) -> Dict [ str , Any ] Use a translation-server search query followed by an export query to return a CSL Item (the first & only record of the returned CSL JSON). View Source def get_csl_item ( identifier : str ) -> CSLItem : \"\"\" Use a translation-server search query followed by an export query to return a CSL Item (the first & only record of the returned CSL JSON). \"\"\" zotero_data = search_query ( identifier ) csl_items = export_as_csl ( zotero_data ) ( csl_item ,) = csl_items return csl_item","title":"get_csl_item"},{"location":"reference/manubot/cite/zotero/#search_or_web_query","text":"def search_or_web_query ( identifier : str ) -> List [ Dict [ str , Any ]] Detect whether identifier is a URL. If so, retrieve zotero metadata using a /web query. Otherwise, retrieve zotero metadata using a /search query. View Source def search_or_web_query ( identifier : str ) -> ZoteroData : \" \"\" Detect whether `identifier` is a URL. If so, retrieve zotero metadata using a /web query. Otherwise, retrieve zotero metadata using a /search query. \"\" \" if is_http_url ( identifier ) : zotero_data = web_query ( identifier ) else : zotero_data = search_query ( identifier ) return zotero_data","title":"search_or_web_query"},{"location":"reference/manubot/cite/zotero/#search_query","text":"def search_query ( identifier : str ) -> List [ Dict [ str , Any ]] Retrive Zotero metadata for a DOI, ISBN, PMID, or arXiv ID. Example usage: curl --silent --data '10.2307/4486062' --header 'Content-Type: text/plain' http://127.0.0.1:1969/search View Source def search_query ( identifier : str ) -> ZoteroData : \"\"\" Retrive Zotero metadata for a DOI, ISBN, PMID, or arXiv ID. Example usage: ```shell curl --silent \\ --data '10.2307/4486062' \\ --header 'Content-Type: text/plain' \\ http://127.0.0.1:1969/search ``` \"\"\" api_url = f \"{base_url}/search\" headers = { \"User-Agent\" : get_manubot_user_agent (), \"Content-Type\" : \"text/plain\" } response = requests . post ( api_url , headers= headers , data= str ( identifier )) try : zotero_data = response . json () except Exception as error : logging . warning ( f \"Error parsing search_query output as JSON for {identifier}: \\ n{response.text}\" ) raise error zotero_data = _passthrough_zotero_data ( zotero_data ) return zotero_data","title":"search_query"},{"location":"reference/manubot/cite/zotero/#web_query","text":"def web_query ( url : str ) -> List [ Dict [ str , Any ]] Return Zotero citation metadata for a URL as a list containing a single element that is a dictionary with the URL's metadata. View Source def web_query ( url : str ) -> ZoteroData : \"\"\" Return Zotero citation metadata for a URL as a list containing a single element that is a dictionary with the URL's metadata. \"\"\" headers = { \"User-Agent\" : get_manubot_user_agent (), \"Content-Type\" : \"text/plain\" } params = { \"single\" : 1 } api_url = f \"{base_url}/web\" response = requests . post ( api_url , params= params , headers= headers , data= str ( url )) try : zotero_data = response . json () except Exception as error : logging . warning ( f \"Error parsing web_query output as JSON for {url}: \\ n{response.text}\" ) raise error if response . status_code = = 300 : # When single=1 is specified, multiple results should never be returned logging . warning ( f \"web_query returned multiple results for {url}: \\ n\" + json . dumps ( zotero_data , indent= 2 ) ) raise ValueError ( f \"multiple results for {url}\" ) zotero_data = _passthrough_zotero_data ( zotero_data ) return zotero_data","title":"web_query"},{"location":"reference/manubot/cite/tests/","text":"Module manubot.cite.tests None None Sub-modules manubot.cite.tests.test_arxiv manubot.cite.tests.test_citations manubot.cite.tests.test_cite_command manubot.cite.tests.test_citekey manubot.cite.tests.test_citekey_api manubot.cite.tests.test_citeproc manubot.cite.tests.test_csl_item manubot.cite.tests.test_curie manubot.cite.tests.test_doi manubot.cite.tests.test_handlers manubot.cite.tests.test_isbn manubot.cite.tests.test_pubmed manubot.cite.tests.test_unpaywall manubot.cite.tests.test_url manubot.cite.tests.test_wikidata manubot.cite.tests.test_zotero","title":"Index"},{"location":"reference/manubot/cite/tests/#module-manubotcitetests","text":"None None","title":"Module manubot.cite.tests"},{"location":"reference/manubot/cite/tests/#sub-modules","text":"manubot.cite.tests.test_arxiv manubot.cite.tests.test_citations manubot.cite.tests.test_cite_command manubot.cite.tests.test_citekey manubot.cite.tests.test_citekey_api manubot.cite.tests.test_citeproc manubot.cite.tests.test_csl_item manubot.cite.tests.test_curie manubot.cite.tests.test_doi manubot.cite.tests.test_handlers manubot.cite.tests.test_isbn manubot.cite.tests.test_pubmed manubot.cite.tests.test_unpaywall manubot.cite.tests.test_url manubot.cite.tests.test_wikidata manubot.cite.tests.test_zotero","title":"Sub-modules"},{"location":"reference/manubot/cite/tests/test_arxiv/","text":"Module manubot.cite.tests.test_arxiv None None View Source from ..arxiv import get_arxiv_csl_item_export_api , get_arxiv_csl_item_oai def test_get_arxiv_csl_item_abstract_whitespace (): \"\"\" Test wrapping newlines are properly removed from the abstract, while preserving paragraph breaks. From https://arxiv.org/help/prep#abstracts: > Carriage returns will be stripped unless they are followed by leading white spaces. So if you want a new paragraph or a table of contents, be sure to indent the lines after the carriage return. When the abstract is formatted for email announcement, it will be wrapped to 80 characters. \"\"\" csl_item = get_arxiv_csl_item_export_api ( \"1908.00936v2\" ) assert csl_item [ \"title\" ] == \"Multi-Scale Learned Iterative Reconstruction\" assert csl_item [ \"URL\" ] == \"https://arxiv.org/abs/1908.00936v2\" assert csl_item [ \"version\" ] == \"v2\" abstract = ( \"Model-based learned iterative reconstruction methods have recently been shown to outperform classical reconstruction algorithms. Applicability of these methods to large scale inverse problems is however limited by the available memory for training and extensive training times due to computationally expensive forward models. As a possible solution to these restrictions we propose a multi-scale learned iterative reconstruction scheme that computes iterates on discretisations of increasing resolution. This procedure does not only reduce memory requirements, it also considerably speeds up reconstruction and training times, but most importantly is scalable to large scale inverse problems with non-trivial forward operators, such as those that arise in many 3D tomographic applications. In particular, we propose a hybrid network that combines the multi-scale iterative approach with a particularly expressive network architecture which in combination exhibits excellent scalability in 3D.\" \" \\n Applicability of the algorithm is demonstrated for 3D cone beam computed tomography from real measurement data of an organic phantom. Additionally, we examine scalability and reconstruction quality in comparison to established learned reconstruction methods in two dimensions for low dose computed tomography on human phantoms.\" ) assert csl_item [ \"abstract\" ] == abstract def test_get_arxiv_csl_item_oai (): \"\"\" https://export.arxiv.org/oai2?verb=GetRecord&identifier=oai:arXiv.org:1912.04616&metadataPrefix=arXiv \"\"\" csl_item = get_arxiv_csl_item_oai ( \"1912.04616\" ) assert ( csl_item [ \"title\" ] == \"OpenBioLink: A benchmarking framework for large-scale biomedical link prediction\" ) assert csl_item [ \"URL\" ] == \"https://arxiv.org/abs/1912.04616\" assert csl_item . note_dict [ \"license\" ] assert csl_item [ \"author\" ][ 0 ][ \"family\" ] == \"Breit\" Functions test_get_arxiv_csl_item_abstract_whitespace def test_get_arxiv_csl_item_abstract_whitespace ( ) Test wrapping newlines are properly removed from the abstract, while preserving paragraph breaks. From https://arxiv.org/help/prep#abstracts: Carriage returns will be stripped unless they are followed by leading white spaces. So if you want a new paragraph or a table of contents, be sure to indent the lines after the carriage return. When the abstract is formatted for email announcement, it will be wrapped to 80 characters. View Source def test_get_arxiv_csl_item_abstract_whitespace (): \"\"\" Test wrapping newlines are properly removed from the abstract, while preserving paragraph breaks. From https://arxiv.org/help/prep#abstracts: > Carriage returns will be stripped unless they are followed by leading white spaces. So if you want a new paragraph or a table of contents, be sure to indent the lines after the carriage return. When the abstract is formatted for email announcement, it will be wrapped to 80 characters. \"\"\" csl_item = get_arxiv_csl_item_export_api ( \"1908.00936v2\" ) assert csl_item [ \"title\" ] == \"Multi-Scale Learned Iterative Reconstruction\" assert csl_item [ \"URL\" ] == \"https://arxiv.org/abs/1908.00936v2\" assert csl_item [ \"version\" ] == \"v2\" abstract = ( \"Model-based learned iterative reconstruction methods have recently been shown to outperform classical reconstruction algorithms. Applicability of these methods to large scale inverse problems is however limited by the available memory for training and extensive training times due to computationally expensive forward models. As a possible solution to these restrictions we propose a multi-scale learned iterative reconstruction scheme that computes iterates on discretisations of increasing resolution. This procedure does not only reduce memory requirements, it also considerably speeds up reconstruction and training times, but most importantly is scalable to large scale inverse problems with non-trivial forward operators, such as those that arise in many 3D tomographic applications. In particular, we propose a hybrid network that combines the multi-scale iterative approach with a particularly expressive network architecture which in combination exhibits excellent scalability in 3D.\" \" \\n Applicability of the algorithm is demonstrated for 3D cone beam computed tomography from real measurement data of an organic phantom. Additionally, we examine scalability and reconstruction quality in comparison to established learned reconstruction methods in two dimensions for low dose computed tomography on human phantoms.\" ) assert csl_item [ \"abstract\" ] == abstract test_get_arxiv_csl_item_oai def test_get_arxiv_csl_item_oai ( ) https://export.arxiv.org/oai2?verb=GetRecord&identifier=oai:arXiv.org:1912.04616&metadataPrefix=arXiv View Source def test_get_arxiv_csl_item_oai (): \"\"\" https://export.arxiv.org/oai2?verb=GetRecord&identifier=oai:arXiv.org:1912.04616&metadataPrefix=arXiv \"\"\" csl_item = get_arxiv_csl_item_oai ( \"1912.04616\" ) assert ( csl_item [ \"title\" ] == \"OpenBioLink: A benchmarking framework for large-scale biomedical link prediction\" ) assert csl_item [ \"URL\" ] == \"https://arxiv.org/abs/1912.04616\" assert csl_item . note_dict [ \"license\" ] assert csl_item [ \"author\" ][ 0 ][ \"family\" ] == \"Breit\"","title":"Test Arxiv"},{"location":"reference/manubot/cite/tests/test_arxiv/#module-manubotciteteststest_arxiv","text":"None None View Source from ..arxiv import get_arxiv_csl_item_export_api , get_arxiv_csl_item_oai def test_get_arxiv_csl_item_abstract_whitespace (): \"\"\" Test wrapping newlines are properly removed from the abstract, while preserving paragraph breaks. From https://arxiv.org/help/prep#abstracts: > Carriage returns will be stripped unless they are followed by leading white spaces. So if you want a new paragraph or a table of contents, be sure to indent the lines after the carriage return. When the abstract is formatted for email announcement, it will be wrapped to 80 characters. \"\"\" csl_item = get_arxiv_csl_item_export_api ( \"1908.00936v2\" ) assert csl_item [ \"title\" ] == \"Multi-Scale Learned Iterative Reconstruction\" assert csl_item [ \"URL\" ] == \"https://arxiv.org/abs/1908.00936v2\" assert csl_item [ \"version\" ] == \"v2\" abstract = ( \"Model-based learned iterative reconstruction methods have recently been shown to outperform classical reconstruction algorithms. Applicability of these methods to large scale inverse problems is however limited by the available memory for training and extensive training times due to computationally expensive forward models. As a possible solution to these restrictions we propose a multi-scale learned iterative reconstruction scheme that computes iterates on discretisations of increasing resolution. This procedure does not only reduce memory requirements, it also considerably speeds up reconstruction and training times, but most importantly is scalable to large scale inverse problems with non-trivial forward operators, such as those that arise in many 3D tomographic applications. In particular, we propose a hybrid network that combines the multi-scale iterative approach with a particularly expressive network architecture which in combination exhibits excellent scalability in 3D.\" \" \\n Applicability of the algorithm is demonstrated for 3D cone beam computed tomography from real measurement data of an organic phantom. Additionally, we examine scalability and reconstruction quality in comparison to established learned reconstruction methods in two dimensions for low dose computed tomography on human phantoms.\" ) assert csl_item [ \"abstract\" ] == abstract def test_get_arxiv_csl_item_oai (): \"\"\" https://export.arxiv.org/oai2?verb=GetRecord&identifier=oai:arXiv.org:1912.04616&metadataPrefix=arXiv \"\"\" csl_item = get_arxiv_csl_item_oai ( \"1912.04616\" ) assert ( csl_item [ \"title\" ] == \"OpenBioLink: A benchmarking framework for large-scale biomedical link prediction\" ) assert csl_item [ \"URL\" ] == \"https://arxiv.org/abs/1912.04616\" assert csl_item . note_dict [ \"license\" ] assert csl_item [ \"author\" ][ 0 ][ \"family\" ] == \"Breit\"","title":"Module manubot.cite.tests.test_arxiv"},{"location":"reference/manubot/cite/tests/test_arxiv/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_arxiv/#test_get_arxiv_csl_item_abstract_whitespace","text":"def test_get_arxiv_csl_item_abstract_whitespace ( ) Test wrapping newlines are properly removed from the abstract, while preserving paragraph breaks. From https://arxiv.org/help/prep#abstracts: Carriage returns will be stripped unless they are followed by leading white spaces. So if you want a new paragraph or a table of contents, be sure to indent the lines after the carriage return. When the abstract is formatted for email announcement, it will be wrapped to 80 characters. View Source def test_get_arxiv_csl_item_abstract_whitespace (): \"\"\" Test wrapping newlines are properly removed from the abstract, while preserving paragraph breaks. From https://arxiv.org/help/prep#abstracts: > Carriage returns will be stripped unless they are followed by leading white spaces. So if you want a new paragraph or a table of contents, be sure to indent the lines after the carriage return. When the abstract is formatted for email announcement, it will be wrapped to 80 characters. \"\"\" csl_item = get_arxiv_csl_item_export_api ( \"1908.00936v2\" ) assert csl_item [ \"title\" ] == \"Multi-Scale Learned Iterative Reconstruction\" assert csl_item [ \"URL\" ] == \"https://arxiv.org/abs/1908.00936v2\" assert csl_item [ \"version\" ] == \"v2\" abstract = ( \"Model-based learned iterative reconstruction methods have recently been shown to outperform classical reconstruction algorithms. Applicability of these methods to large scale inverse problems is however limited by the available memory for training and extensive training times due to computationally expensive forward models. As a possible solution to these restrictions we propose a multi-scale learned iterative reconstruction scheme that computes iterates on discretisations of increasing resolution. This procedure does not only reduce memory requirements, it also considerably speeds up reconstruction and training times, but most importantly is scalable to large scale inverse problems with non-trivial forward operators, such as those that arise in many 3D tomographic applications. In particular, we propose a hybrid network that combines the multi-scale iterative approach with a particularly expressive network architecture which in combination exhibits excellent scalability in 3D.\" \" \\n Applicability of the algorithm is demonstrated for 3D cone beam computed tomography from real measurement data of an organic phantom. Additionally, we examine scalability and reconstruction quality in comparison to established learned reconstruction methods in two dimensions for low dose computed tomography on human phantoms.\" ) assert csl_item [ \"abstract\" ] == abstract","title":"test_get_arxiv_csl_item_abstract_whitespace"},{"location":"reference/manubot/cite/tests/test_arxiv/#test_get_arxiv_csl_item_oai","text":"def test_get_arxiv_csl_item_oai ( ) https://export.arxiv.org/oai2?verb=GetRecord&identifier=oai:arXiv.org:1912.04616&metadataPrefix=arXiv View Source def test_get_arxiv_csl_item_oai (): \"\"\" https://export.arxiv.org/oai2?verb=GetRecord&identifier=oai:arXiv.org:1912.04616&metadataPrefix=arXiv \"\"\" csl_item = get_arxiv_csl_item_oai ( \"1912.04616\" ) assert ( csl_item [ \"title\" ] == \"OpenBioLink: A benchmarking framework for large-scale biomedical link prediction\" ) assert csl_item [ \"URL\" ] == \"https://arxiv.org/abs/1912.04616\" assert csl_item . note_dict [ \"license\" ] assert csl_item [ \"author\" ][ 0 ][ \"family\" ] == \"Breit\"","title":"test_get_arxiv_csl_item_oai"},{"location":"reference/manubot/cite/tests/test_citations/","text":"Module manubot.cite.tests.test_citations None None View Source import pathlib import pytest from manubot.cite.citations import Citations def test_citations_filter_pandoc_xnos (): input_ids = [ \"fig:pandoc-fignos-key\" , # should filter \"eq:pandoc-eqnos-key\" , # should filter \"tbl:pandoc-tablenos-key\" , # should filter \"not-pandoc-xnos-key\" , # should keep ] citations = Citations ( input_ids ) citations . filter_pandoc_xnos () assert len ( citations . citekeys ) == 1 assert citations . citekeys [ 0 ] . input_id == \"not-pandoc-xnos-key\" def test_citations_filter_unhandled (): input_ids = [ \"citekey-with-no-prefix\" , \"bad-prefix:citekey\" , \":empty-prefix\" , \"doi:handled-prefix\" , ] citations = Citations ( input_ids ) citations . filter_unhandled () assert len ( citations . citekeys ) == 1 assert citations . citekeys [ 0 ] . input_id == \"doi:handled-prefix\" def test_citations_check_collisions ( caplog ): input_ids = [ \"citekey-1\" , \"citekey-1\" , \"citekey-2\" , \"Citekey-2\" , ] citations = Citations ( input_ids ) citations . check_collisions () assert not caplog . records def test_citations_check_multiple_input_ids ( caplog ): input_ids = [ \"doi:10/b6vnmd\" , \"DOI:10/B6VNMD\" , \"doi:10.1016/s0933-3657(96)00367-3\" , \"ugly-doi-alias\" , \"other-citekey\" , ] citekey_aliases = { \"ugly-doi-alias\" : \"DOI:10.1016/s0933-3657(96)00367-3\" } citations = Citations ( input_ids , citekey_aliases ) citations . check_multiple_input_ids () expected = \"Multiple citekey input_ids refer to the same standard_id doi:10.1016/s0933-3657(96)00367-3:\" \"['doi:10/b6vnmd', 'DOI:10/B6VNMD', 'doi:10.1016/s0933-3657(96)00367-3', 'ugly-doi-alias']\" assert expected in caplog . text def test_citations_citekeys_tsv (): input_ids = [ \"citekey-1\" , \"arXiv:1806.05726v1\" , \"DOI:10.7717/peerj.338\" , \"pmid:29618526\" , ] citations = Citations ( input_ids ) citekeys_tsv = citations . citekeys_tsv assert isinstance ( citekeys_tsv , str ) assert \"arxiv:1806.05726v1\" in citekeys_tsv . splitlines ()[ 2 ] . split ( \" \\t \" ) def test_citations_inspect (): input_ids = [ \"citekey-1\" , # passes inspection \"arXiv:1806.05726v1\" , # passes inspection \"arXiv:bad-id\" , \"DOI:bad-id\" , \"pmid:bad-id\" , \"DOID:not-disease-ontology-id\" , ] citations = Citations ( input_ids ) report = citations . inspect ( log_level = \"INFO\" ) print ( report ) assert len ( report . splitlines ()) == 4 assert \"pmid:bad-id -- PubMed Identifiers should be 1-8 digits\" in report @pytest . mark . parametrize ( \"csl_format\" , [ \"json\" , \"yaml\" ]) def test_citations_csl_serialization ( csl_format ): ccr_dir = pathlib . Path ( __file__ ) . parent . joinpath ( \"cite-command-rendered\" ) citations = Citations ( [ \"arxiv:1806.05726v1\" , \"doi:10.7717/peerj.338\" , \"pubmed:29618526\" ] ) citations . load_manual_references ( paths = [ ccr_dir . joinpath ( \"input-bibliography.json\" )] ) citations . get_csl_items () path_out = ccr_dir . joinpath ( f \"output-bibliography. { csl_format } \" ) # uncomment the following line to regenerate test output # citations.write_csl_items(path_out) csl_out = getattr ( citations , f \"csl_ { csl_format } \" ) assert csl_out == path_out . read_text () Functions test_citations_check_collisions def test_citations_check_collisions ( caplog ) View Source def test_citations_check_collisions(caplog): input_ids = [ \"citekey-1\", \"citekey-1\", \"citekey-2\", \"Citekey-2\", ] citations = Citations(input_ids) citations.check_collisions() assert not caplog.records test_citations_check_multiple_input_ids def test_citations_check_multiple_input_ids ( caplog ) View Source def test_citations_check_multiple_input_ids(caplog): input_ids = [ \"doi:10/b6vnmd\", \"DOI:10/B6VNMD\", \"doi:10.1016/s0933-3657(96)00367-3\", \"ugly-doi-alias\", \"other-citekey\", ] citekey_aliases = {\"ugly-doi-alias\": \"DOI:10.1016/s0933-3657(96)00367-3\"} citations = Citations(input_ids, citekey_aliases) citations.check_multiple_input_ids() expected = \"Multiple citekey input_ids refer to the same standard_id doi:10.1016/s0933-3657(96)00367-3:\" \"['doi:10/b6vnmd', 'DOI:10/B6VNMD', 'doi:10.1016/s0933-3657(96)00367-3', 'ugly-doi-alias']\" assert expected in caplog.text test_citations_citekeys_tsv def test_citations_citekeys_tsv ( ) View Source def test_citations_citekeys_tsv(): input_ids = [ \"citekey-1\", \"arXiv:1806.05726v1\", \"DOI:10.7717/peerj.338\", \"pmid:29618526\", ] citations = Citations(input_ids) citekeys_tsv = citations.citekeys_tsv assert isinstance(citekeys_tsv, str) assert \"arxiv:1806.05726v1\" in citekeys_tsv.splitlines()[2].split(\"\\t\") test_citations_csl_serialization def test_citations_csl_serialization ( csl_format ) View Source @ pytest . mark . parametrize ( \"csl_format\" , [ \"json\" , \"yaml\" ]) def test_citations_csl_serialization ( csl_format ): ccr_dir = pathlib . Path ( __file__ ) . parent . joinpath ( \"cite-command-rendered\" ) citations = Citations ( [ \"arxiv:1806.05726v1\" , \"doi:10.7717/peerj.338\" , \"pubmed:29618526\" ] ) citations . load_manual_references ( paths = [ ccr_dir . joinpath ( \"input-bibliography.json\" )] ) citations . get_csl_items () path_out = ccr_dir . joinpath ( f \"output-bibliography.{csl_format}\" ) # uncomment the following line to regenerate test output # citations.write_csl_items(path_out) csl_out = getattr ( citations , f \"csl_{csl_format}\" ) assert csl_out == path_out . read_text () test_citations_filter_pandoc_xnos def test_citations_filter_pandoc_xnos ( ) View Source def test_citations_filter_pandoc_xnos(): input_ids = [ \"fig:pandoc-fignos-key\", # should filter \"eq:pandoc-eqnos-key\", # should filter \"tbl:pandoc-tablenos-key\", # should filter \"not-pandoc-xnos-key\", # should keep ] citations = Citations(input_ids) citations.filter_pandoc_xnos() assert len(citations.citekeys) == 1 assert citations.citekeys[0].input_id == \"not-pandoc-xnos-key\" test_citations_filter_unhandled def test_citations_filter_unhandled ( ) View Source def test_citations_filter_unhandled(): input_ids = [ \"citekey-with-no-prefix\", \"bad-prefix:citekey\", \":empty-prefix\", \"doi:handled-prefix\", ] citations = Citations(input_ids) citations.filter_unhandled() assert len(citations.citekeys) == 1 assert citations.citekeys[0].input_id == \"doi:handled-prefix\" test_citations_inspect def test_citations_inspect ( ) View Source def test_citations_inspect(): input_ids = [ \"citekey-1\", # passes inspection \"arXiv:1806.05726v1\", # passes inspection \"arXiv:bad-id\", \"DOI:bad-id\", \"pmid:bad-id\", \"DOID:not-disease-ontology-id\", ] citations = Citations(input_ids) report = citations.inspect(log_level=\"INFO\") print(report) assert len(report.splitlines()) == 4 assert \"pmid:bad-id -- PubMed Identifiers should be 1-8 digits\" in report","title":"Test Citations"},{"location":"reference/manubot/cite/tests/test_citations/#module-manubotciteteststest_citations","text":"None None View Source import pathlib import pytest from manubot.cite.citations import Citations def test_citations_filter_pandoc_xnos (): input_ids = [ \"fig:pandoc-fignos-key\" , # should filter \"eq:pandoc-eqnos-key\" , # should filter \"tbl:pandoc-tablenos-key\" , # should filter \"not-pandoc-xnos-key\" , # should keep ] citations = Citations ( input_ids ) citations . filter_pandoc_xnos () assert len ( citations . citekeys ) == 1 assert citations . citekeys [ 0 ] . input_id == \"not-pandoc-xnos-key\" def test_citations_filter_unhandled (): input_ids = [ \"citekey-with-no-prefix\" , \"bad-prefix:citekey\" , \":empty-prefix\" , \"doi:handled-prefix\" , ] citations = Citations ( input_ids ) citations . filter_unhandled () assert len ( citations . citekeys ) == 1 assert citations . citekeys [ 0 ] . input_id == \"doi:handled-prefix\" def test_citations_check_collisions ( caplog ): input_ids = [ \"citekey-1\" , \"citekey-1\" , \"citekey-2\" , \"Citekey-2\" , ] citations = Citations ( input_ids ) citations . check_collisions () assert not caplog . records def test_citations_check_multiple_input_ids ( caplog ): input_ids = [ \"doi:10/b6vnmd\" , \"DOI:10/B6VNMD\" , \"doi:10.1016/s0933-3657(96)00367-3\" , \"ugly-doi-alias\" , \"other-citekey\" , ] citekey_aliases = { \"ugly-doi-alias\" : \"DOI:10.1016/s0933-3657(96)00367-3\" } citations = Citations ( input_ids , citekey_aliases ) citations . check_multiple_input_ids () expected = \"Multiple citekey input_ids refer to the same standard_id doi:10.1016/s0933-3657(96)00367-3:\" \"['doi:10/b6vnmd', 'DOI:10/B6VNMD', 'doi:10.1016/s0933-3657(96)00367-3', 'ugly-doi-alias']\" assert expected in caplog . text def test_citations_citekeys_tsv (): input_ids = [ \"citekey-1\" , \"arXiv:1806.05726v1\" , \"DOI:10.7717/peerj.338\" , \"pmid:29618526\" , ] citations = Citations ( input_ids ) citekeys_tsv = citations . citekeys_tsv assert isinstance ( citekeys_tsv , str ) assert \"arxiv:1806.05726v1\" in citekeys_tsv . splitlines ()[ 2 ] . split ( \" \\t \" ) def test_citations_inspect (): input_ids = [ \"citekey-1\" , # passes inspection \"arXiv:1806.05726v1\" , # passes inspection \"arXiv:bad-id\" , \"DOI:bad-id\" , \"pmid:bad-id\" , \"DOID:not-disease-ontology-id\" , ] citations = Citations ( input_ids ) report = citations . inspect ( log_level = \"INFO\" ) print ( report ) assert len ( report . splitlines ()) == 4 assert \"pmid:bad-id -- PubMed Identifiers should be 1-8 digits\" in report @pytest . mark . parametrize ( \"csl_format\" , [ \"json\" , \"yaml\" ]) def test_citations_csl_serialization ( csl_format ): ccr_dir = pathlib . Path ( __file__ ) . parent . joinpath ( \"cite-command-rendered\" ) citations = Citations ( [ \"arxiv:1806.05726v1\" , \"doi:10.7717/peerj.338\" , \"pubmed:29618526\" ] ) citations . load_manual_references ( paths = [ ccr_dir . joinpath ( \"input-bibliography.json\" )] ) citations . get_csl_items () path_out = ccr_dir . joinpath ( f \"output-bibliography. { csl_format } \" ) # uncomment the following line to regenerate test output # citations.write_csl_items(path_out) csl_out = getattr ( citations , f \"csl_ { csl_format } \" ) assert csl_out == path_out . read_text ()","title":"Module manubot.cite.tests.test_citations"},{"location":"reference/manubot/cite/tests/test_citations/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_citations/#test_citations_check_collisions","text":"def test_citations_check_collisions ( caplog ) View Source def test_citations_check_collisions(caplog): input_ids = [ \"citekey-1\", \"citekey-1\", \"citekey-2\", \"Citekey-2\", ] citations = Citations(input_ids) citations.check_collisions() assert not caplog.records","title":"test_citations_check_collisions"},{"location":"reference/manubot/cite/tests/test_citations/#test_citations_check_multiple_input_ids","text":"def test_citations_check_multiple_input_ids ( caplog ) View Source def test_citations_check_multiple_input_ids(caplog): input_ids = [ \"doi:10/b6vnmd\", \"DOI:10/B6VNMD\", \"doi:10.1016/s0933-3657(96)00367-3\", \"ugly-doi-alias\", \"other-citekey\", ] citekey_aliases = {\"ugly-doi-alias\": \"DOI:10.1016/s0933-3657(96)00367-3\"} citations = Citations(input_ids, citekey_aliases) citations.check_multiple_input_ids() expected = \"Multiple citekey input_ids refer to the same standard_id doi:10.1016/s0933-3657(96)00367-3:\" \"['doi:10/b6vnmd', 'DOI:10/B6VNMD', 'doi:10.1016/s0933-3657(96)00367-3', 'ugly-doi-alias']\" assert expected in caplog.text","title":"test_citations_check_multiple_input_ids"},{"location":"reference/manubot/cite/tests/test_citations/#test_citations_citekeys_tsv","text":"def test_citations_citekeys_tsv ( ) View Source def test_citations_citekeys_tsv(): input_ids = [ \"citekey-1\", \"arXiv:1806.05726v1\", \"DOI:10.7717/peerj.338\", \"pmid:29618526\", ] citations = Citations(input_ids) citekeys_tsv = citations.citekeys_tsv assert isinstance(citekeys_tsv, str) assert \"arxiv:1806.05726v1\" in citekeys_tsv.splitlines()[2].split(\"\\t\")","title":"test_citations_citekeys_tsv"},{"location":"reference/manubot/cite/tests/test_citations/#test_citations_csl_serialization","text":"def test_citations_csl_serialization ( csl_format ) View Source @ pytest . mark . parametrize ( \"csl_format\" , [ \"json\" , \"yaml\" ]) def test_citations_csl_serialization ( csl_format ): ccr_dir = pathlib . Path ( __file__ ) . parent . joinpath ( \"cite-command-rendered\" ) citations = Citations ( [ \"arxiv:1806.05726v1\" , \"doi:10.7717/peerj.338\" , \"pubmed:29618526\" ] ) citations . load_manual_references ( paths = [ ccr_dir . joinpath ( \"input-bibliography.json\" )] ) citations . get_csl_items () path_out = ccr_dir . joinpath ( f \"output-bibliography.{csl_format}\" ) # uncomment the following line to regenerate test output # citations.write_csl_items(path_out) csl_out = getattr ( citations , f \"csl_{csl_format}\" ) assert csl_out == path_out . read_text ()","title":"test_citations_csl_serialization"},{"location":"reference/manubot/cite/tests/test_citations/#test_citations_filter_pandoc_xnos","text":"def test_citations_filter_pandoc_xnos ( ) View Source def test_citations_filter_pandoc_xnos(): input_ids = [ \"fig:pandoc-fignos-key\", # should filter \"eq:pandoc-eqnos-key\", # should filter \"tbl:pandoc-tablenos-key\", # should filter \"not-pandoc-xnos-key\", # should keep ] citations = Citations(input_ids) citations.filter_pandoc_xnos() assert len(citations.citekeys) == 1 assert citations.citekeys[0].input_id == \"not-pandoc-xnos-key\"","title":"test_citations_filter_pandoc_xnos"},{"location":"reference/manubot/cite/tests/test_citations/#test_citations_filter_unhandled","text":"def test_citations_filter_unhandled ( ) View Source def test_citations_filter_unhandled(): input_ids = [ \"citekey-with-no-prefix\", \"bad-prefix:citekey\", \":empty-prefix\", \"doi:handled-prefix\", ] citations = Citations(input_ids) citations.filter_unhandled() assert len(citations.citekeys) == 1 assert citations.citekeys[0].input_id == \"doi:handled-prefix\"","title":"test_citations_filter_unhandled"},{"location":"reference/manubot/cite/tests/test_citations/#test_citations_inspect","text":"def test_citations_inspect ( ) View Source def test_citations_inspect(): input_ids = [ \"citekey-1\", # passes inspection \"arXiv:1806.05726v1\", # passes inspection \"arXiv:bad-id\", \"DOI:bad-id\", \"pmid:bad-id\", \"DOID:not-disease-ontology-id\", ] citations = Citations(input_ids) report = citations.inspect(log_level=\"INFO\") print(report) assert len(report.splitlines()) == 4 assert \"pmid:bad-id -- PubMed Identifiers should be 1-8 digits\" in report","title":"test_citations_inspect"},{"location":"reference/manubot/cite/tests/test_cite_command/","text":"Module manubot.cite.tests.test_cite_command None None View Source import json import pathlib import shutil import subprocess import pytest from manubot.cite.csl_item import CSL_Item from manubot.pandoc.util import get_pandoc_version from manubot.util import shlex_join data_dir = pathlib . Path ( __file__ ) . parent . joinpath ( \"cite-command-rendered\" ) def test_cite_command_preserves_order (): \"\"\" https://github.com/manubot/manubot/issues/240 \"\"\" citekeys = [ \"pmid:29618526\" , \"doi:10.7717/peerj.338\" , \"arxiv:1806.05726v1\" , \"pubmed:29618526\" , \"DOI:10.7717/PEERJ.338\" , ] args = [ \"manubot\" , \"cite\" , \"--bibliography=input-bibliography.json\" , * citekeys , ] output = subprocess . check_output ( args , encoding = \"utf-8\" , cwd = data_dir , ) csl_items = json . loads ( output ) csl_items = [ CSL_Item ( x ) for x in csl_items ] standard_ids = [ csl_item . note_dict . get ( \"standard_id\" ) for csl_item in csl_items ] assert standard_ids == [ \"pubmed:29618526\" , \"doi:10.7717/peerj.338\" , \"arxiv:1806.05726v1\" , ] def test_cite_command_empty (): process = subprocess . run ( [ \"manubot\" , \"cite\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ) print ( process . stderr ) assert process . returncode == 2 assert \"the following arguments are required: citekeys\" in process . stderr def test_cite_command_stdout (): process = subprocess . run ( [ \"manubot\" , \"cite\" , \"arxiv:1806.05726v1\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ) print ( process . stderr ) assert process . returncode == 0 ( csl ,) = json . loads ( process . stdout ) assert csl [ \"URL\" ] == \"https://arxiv.org/abs/1806.05726v1\" def test_cite_command_file ( tmpdir ): path = pathlib . Path ( tmpdir ) / \"csl-items.json\" process = subprocess . run ( [ \"manubot\" , \"cite\" , \"--output\" , str ( path ), \"arxiv:1806.05726v1\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , ) print ( process . stderr . decode ()) assert process . returncode == 0 with path . open () as read_file : ( csl ,) = json . load ( read_file ) assert csl [ \"URL\" ] == \"https://arxiv.org/abs/1806.05726v1\" @pytest . mark . integration @pytest . mark . parametrize ( [ \"args\" , \"filename\" ], [ pytest . param ( [ \"--format\" , \"plain\" ], \"references-plain- {} .txt\" , id = \"--format=plain\" ), pytest . param ( [ \"--format\" , \"markdown\" ], \"references-markdown- {} .md\" , id = \"--format=markdown\" , ), pytest . param ( [ \"--format\" , \"html\" ], \"references-html- {} .html\" , id = \"--format=html\" ), pytest . param ( [ \"--format\" , \"jats\" ], \"references-jats- {} .xml\" , id = \"--format=jats\" ), ], ) @pytest . mark . skipif ( not shutil . which ( \"pandoc\" ), reason = \"pandoc installation not found on system\" ) @pytest . mark . pandoc_version_sensitive def test_cite_command_render_stdout ( args , filename ): \"\"\" Test the stdout output of `manubot cite` with various Pandoc-output formats. The output is sensitive to the version of Pandoc used, so expected output files include the pandoc version stamp in their filename. When the expected version is missing, the test fails but writes the command output to that file. Therefore, subsequent runs of the same test will pass. Before committing the auto-generated output, do look to ensure its integrity. This test uses --bibliography to avoid slow network calls. Regenerate the CSL JSON using: ```shell manubot cite \\ --output=manubot/cite/tests/cite-command-rendered/input-bibliography.json \\ arxiv:1806.05726v1 doi:10.7717/peerj.338 pmid:29618526 ``` \"\"\" # get pandoc version info pandoc_version = get_pandoc_version () pandoc_stamp = \".\" . join ( map ( str , pandoc_version )) path = data_dir . joinpath ( filename . format ( pandoc_stamp )) # skip test on old pandoc versions for output in \"markdown\" , \"html\" , \"jats\" : if output in args and pandoc_version < ( 2 , 5 ): pytest . skip ( f \"Test { output } output assumes pandoc >= 2.5\" ) if pandoc_version < ( 2 , 0 ): pytest . skip ( \"Test requires pandoc >= 2.0 to support --lua-filter and --csl=URL\" ) args = [ \"manubot\" , \"cite\" , \"--bibliography=input-bibliography.json\" , \"--csl=https://github.com/greenelab/manubot-rootstock/raw/e83e51dcd89256403bb787c3d9a46e4ee8d04a9e/build/assets/style.csl\" , \"arxiv:1806.05726v1\" , \"doi:10.7717/peerj.338\" , \"pmid:29618526\" , ] + args process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , cwd = data_dir , ) print ( shlex_join ( process . args )) print ( process . stderr ) assert process . returncode == 0 print ( process . stdout ) if not path . exists (): # https://github.com/manubot/manubot/pull/146#discussion_r333132261 print ( f \"Missing expected output at { path } \\n \" \"Writing output to file such that future tests will pass.\" ) path . write_text ( process . stdout , encoding = \"utf-8\" ) assert False expected = path . read_text ( encoding = \"utf-8-sig\" ) assert process . stdout == expected def test_cite_command_bibliography (): bib_dir = pathlib . Path ( __file__ ) . parent . parent . parent . joinpath ( \"pandoc/tests/bibliographies\" ) args = [ \"manubot\" , \"cite\" , \"--bibliography=bibliography.json\" , \"DOI:10.7554/elife.32822\" , ] csl_json = subprocess . check_output ( args , encoding = \"utf-8\" , cwd = bib_dir ) csl_items = json . loads ( csl_json ) assert len ( csl_items ) == 1 csl_item = csl_items [ 0 ] assert \"source_bibliography: bibliography.json\" in csl_item [ \"note\" ] def teardown_module ( module ): \"\"\" Avoid too many requests to NCBI E-Utils in the test_pubmed.py, which is executed following this module. E-Utility requests are capped at 3 per second, which is usually controlled by _get_eutils_rate_limiter, but this does not seem to work across test modules. \"\"\" import time time . sleep ( 1 ) Variables data_dir Functions teardown_module def teardown_module ( module ) Avoid too many requests to NCBI E-Utils in the test_pubmed.py, which is executed following this module. E-Utility requests are capped at 3 per second, which is usually controlled by _get_eutils_rate_limiter, but this does not seem to work across test modules. View Source def teardown_module ( module ): \"\"\" Avoid too many requests to NCBI E-Utils in the test_pubmed.py, which is executed following this module. E-Utility requests are capped at 3 per second, which is usually controlled by _get_eutils_rate_limiter, but this does not seem to work across test modules. \"\"\" import time time . sleep ( 1 ) test_cite_command_bibliography def test_cite_command_bibliography ( ) View Source def test_cite_command_bibliography (): bib_dir = pathlib . Path ( __file__ ) . parent . parent . parent . joinpath ( \"pandoc/tests/bibliographies\" ) args = [ \"manubot\" , \"cite\" , \"--bibliography=bibliography.json\" , \"DOI:10.7554/elife.32822\" , ] csl_json = subprocess . check_output ( args , encoding = \"utf-8\" , cwd = bib_dir ) csl_items = json . loads ( csl_json ) assert len ( csl_items ) == 1 csl_item = csl_items [ 0 ] assert \"source_bibliography: bibliography.json\" in csl_item [ \"note\" ] test_cite_command_empty def test_cite_command_empty ( ) View Source def test_cite_command_empty(): process = subprocess.run( [\"manubot\", \"cite\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding=\"utf-8\", ) print(process.stderr) assert process.returncode == 2 assert \"the following arguments are required: citekeys\" in process.stderr test_cite_command_file def test_cite_command_file ( tmpdir ) View Source def test_cite_command_file ( tmpdir ): path = pathlib . Path ( tmpdir ) / \"csl-items.json\" process = subprocess . run ( [ \"manubot\" , \"cite\" , \"--output\" , str ( path ), \"arxiv:1806.05726v1\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , ) print ( process . stderr . decode ()) assert process . returncode == 0 with path . open () as read_file : ( csl ,) = json . load ( read_file ) assert csl [ \"URL\" ] == \"https://arxiv.org/abs/1806.05726v1\" test_cite_command_preserves_order def test_cite_command_preserves_order ( ) https://github.com/manubot/manubot/issues/240 View Source def test_cite_command_preserves_order (): \"\"\" https://github.com/manubot/manubot/issues/240 \"\"\" citekeys = [ \"pmid:29618526\" , \"doi:10.7717/peerj.338\" , \"arxiv:1806.05726v1\" , \"pubmed:29618526\" , \"DOI:10.7717/PEERJ.338\" , ] args = [ \"manubot\" , \"cite\" , \"--bibliography=input-bibliography.json\" , * citekeys , ] output = subprocess . check_output ( args , encoding = \"utf-8\" , cwd = data_dir , ) csl_items = json . loads ( output ) csl_items = [ CSL_Item ( x ) for x in csl_items ] standard_ids = [ csl_item . note_dict . get ( \"standard_id\" ) for csl_item in csl_items ] assert standard_ids == [ \"pubmed:29618526\" , \"doi:10.7717/peerj.338\" , \"arxiv:1806.05726v1\" , ] test_cite_command_render_stdout def test_cite_command_render_stdout ( args , filename ) Test the stdout output of manubot cite with various Pandoc-output formats. The output is sensitive to the version of Pandoc used, so expected output files include the pandoc version stamp in their filename. When the expected version is missing, the test fails but writes the command output to that file. Therefore, subsequent runs of the same test will pass. Before committing the auto-generated output, do look to ensure its integrity. This test uses --bibliography to avoid slow network calls. Regenerate the CSL JSON using: manubot cite --output = manubot/cite/tests/cite-command-rendered/input-bibliography.json arxiv:1806.05726v1 doi:10.7717/peerj.338 pmid:29618526 View Source @ pytest . mark . integration @ pytest . mark . parametrize ( [ \"args\" , \"filename\" ], [ pytest . param ( [ \"--format\" , \"plain\" ], \"references-plain-{}.txt\" , id = \"--format=plain\" ), pytest . param ( [ \"--format\" , \"markdown\" ], \"references-markdown-{}.md\" , id = \"--format=markdown\" , ), pytest . param ( [ \"--format\" , \"html\" ], \"references-html-{}.html\" , id = \"--format=html\" ), pytest . param ( [ \"--format\" , \"jats\" ], \"references-jats-{}.xml\" , id = \"--format=jats\" ), ], ) @ pytest . mark . skipif ( not shutil . which ( \"pandoc\" ), reason = \"pandoc installation not found on system\" ) @ pytest . mark . pandoc_version_sensitive def test_cite_command_render_stdout ( args , filename ): \"\"\" Test the stdout output of `manubot cite` with various Pandoc-output formats. The output is sensitive to the version of Pandoc used, so expected output files include the pandoc version stamp in their filename. When the expected version is missing, the test fails but writes the command output to that file. Therefore, subsequent runs of the same test will pass. Before committing the auto-generated output, do look to ensure its integrity. This test uses --bibliography to avoid slow network calls. Regenerate the CSL JSON using: ```shell manubot cite \\ --output=manubot/cite/tests/cite-command-rendered/input-bibliography.json \\ arxiv:1806.05726v1 doi:10.7717/peerj.338 pmid:29618526 ``` \"\"\" # get pandoc version info pandoc_version = get_pandoc_version () pandoc_stamp = \".\" . join ( map ( str , pandoc_version )) path = data_dir . joinpath ( filename . format ( pandoc_stamp )) # skip test on old pandoc versions for output in \"markdown\" , \"html\" , \"jats\" : if output in args and pandoc_version < ( 2 , 5 ): pytest . skip ( f \"Test {output} output assumes pandoc >= 2.5\" ) if pandoc_version < ( 2 , 0 ): pytest . skip ( \"Test requires pandoc >= 2.0 to support --lua-filter and --csl=URL\" ) args = [ \"manubot\" , \"cite\" , \"--bibliography=input-bibliography.json\" , \"--csl=https://github.com/greenelab/manubot-rootstock/raw/e83e51dcd89256403bb787c3d9a46e4ee8d04a9e/build/assets/style.csl\" , \"arxiv:1806.05726v1\" , \"doi:10.7717/peerj.338\" , \"pmid:29618526\" , ] + args process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , cwd = data_dir , ) print ( shlex_join ( process . args )) print ( process . stderr ) assert process . returncode == 0 print ( process . stdout ) if not path . exists (): # https://github.com/manubot/manubot/pull/146#discussion_r333132261 print ( f \"Missing expected output at {path} \\n \" \"Writing output to file such that future tests will pass.\" ) path . write_text ( process . stdout , encoding = \"utf-8\" ) assert False expected = path . read_text ( encoding = \"utf-8-sig\" ) assert process . stdout == expected test_cite_command_stdout def test_cite_command_stdout ( ) View Source def test_cite_command_stdout (): process = subprocess . run ( [ \"manubot\" , \"cite\" , \"arxiv:1806.05726v1\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ) print ( process . stderr ) assert process . returncode == 0 ( csl ,) = json . loads ( process . stdout ) assert csl [ \"URL\" ] == \"https://arxiv.org/abs/1806.05726v1\"","title":"Test Cite Command"},{"location":"reference/manubot/cite/tests/test_cite_command/#module-manubotciteteststest_cite_command","text":"None None View Source import json import pathlib import shutil import subprocess import pytest from manubot.cite.csl_item import CSL_Item from manubot.pandoc.util import get_pandoc_version from manubot.util import shlex_join data_dir = pathlib . Path ( __file__ ) . parent . joinpath ( \"cite-command-rendered\" ) def test_cite_command_preserves_order (): \"\"\" https://github.com/manubot/manubot/issues/240 \"\"\" citekeys = [ \"pmid:29618526\" , \"doi:10.7717/peerj.338\" , \"arxiv:1806.05726v1\" , \"pubmed:29618526\" , \"DOI:10.7717/PEERJ.338\" , ] args = [ \"manubot\" , \"cite\" , \"--bibliography=input-bibliography.json\" , * citekeys , ] output = subprocess . check_output ( args , encoding = \"utf-8\" , cwd = data_dir , ) csl_items = json . loads ( output ) csl_items = [ CSL_Item ( x ) for x in csl_items ] standard_ids = [ csl_item . note_dict . get ( \"standard_id\" ) for csl_item in csl_items ] assert standard_ids == [ \"pubmed:29618526\" , \"doi:10.7717/peerj.338\" , \"arxiv:1806.05726v1\" , ] def test_cite_command_empty (): process = subprocess . run ( [ \"manubot\" , \"cite\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ) print ( process . stderr ) assert process . returncode == 2 assert \"the following arguments are required: citekeys\" in process . stderr def test_cite_command_stdout (): process = subprocess . run ( [ \"manubot\" , \"cite\" , \"arxiv:1806.05726v1\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ) print ( process . stderr ) assert process . returncode == 0 ( csl ,) = json . loads ( process . stdout ) assert csl [ \"URL\" ] == \"https://arxiv.org/abs/1806.05726v1\" def test_cite_command_file ( tmpdir ): path = pathlib . Path ( tmpdir ) / \"csl-items.json\" process = subprocess . run ( [ \"manubot\" , \"cite\" , \"--output\" , str ( path ), \"arxiv:1806.05726v1\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , ) print ( process . stderr . decode ()) assert process . returncode == 0 with path . open () as read_file : ( csl ,) = json . load ( read_file ) assert csl [ \"URL\" ] == \"https://arxiv.org/abs/1806.05726v1\" @pytest . mark . integration @pytest . mark . parametrize ( [ \"args\" , \"filename\" ], [ pytest . param ( [ \"--format\" , \"plain\" ], \"references-plain- {} .txt\" , id = \"--format=plain\" ), pytest . param ( [ \"--format\" , \"markdown\" ], \"references-markdown- {} .md\" , id = \"--format=markdown\" , ), pytest . param ( [ \"--format\" , \"html\" ], \"references-html- {} .html\" , id = \"--format=html\" ), pytest . param ( [ \"--format\" , \"jats\" ], \"references-jats- {} .xml\" , id = \"--format=jats\" ), ], ) @pytest . mark . skipif ( not shutil . which ( \"pandoc\" ), reason = \"pandoc installation not found on system\" ) @pytest . mark . pandoc_version_sensitive def test_cite_command_render_stdout ( args , filename ): \"\"\" Test the stdout output of `manubot cite` with various Pandoc-output formats. The output is sensitive to the version of Pandoc used, so expected output files include the pandoc version stamp in their filename. When the expected version is missing, the test fails but writes the command output to that file. Therefore, subsequent runs of the same test will pass. Before committing the auto-generated output, do look to ensure its integrity. This test uses --bibliography to avoid slow network calls. Regenerate the CSL JSON using: ```shell manubot cite \\ --output=manubot/cite/tests/cite-command-rendered/input-bibliography.json \\ arxiv:1806.05726v1 doi:10.7717/peerj.338 pmid:29618526 ``` \"\"\" # get pandoc version info pandoc_version = get_pandoc_version () pandoc_stamp = \".\" . join ( map ( str , pandoc_version )) path = data_dir . joinpath ( filename . format ( pandoc_stamp )) # skip test on old pandoc versions for output in \"markdown\" , \"html\" , \"jats\" : if output in args and pandoc_version < ( 2 , 5 ): pytest . skip ( f \"Test { output } output assumes pandoc >= 2.5\" ) if pandoc_version < ( 2 , 0 ): pytest . skip ( \"Test requires pandoc >= 2.0 to support --lua-filter and --csl=URL\" ) args = [ \"manubot\" , \"cite\" , \"--bibliography=input-bibliography.json\" , \"--csl=https://github.com/greenelab/manubot-rootstock/raw/e83e51dcd89256403bb787c3d9a46e4ee8d04a9e/build/assets/style.csl\" , \"arxiv:1806.05726v1\" , \"doi:10.7717/peerj.338\" , \"pmid:29618526\" , ] + args process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , cwd = data_dir , ) print ( shlex_join ( process . args )) print ( process . stderr ) assert process . returncode == 0 print ( process . stdout ) if not path . exists (): # https://github.com/manubot/manubot/pull/146#discussion_r333132261 print ( f \"Missing expected output at { path } \\n \" \"Writing output to file such that future tests will pass.\" ) path . write_text ( process . stdout , encoding = \"utf-8\" ) assert False expected = path . read_text ( encoding = \"utf-8-sig\" ) assert process . stdout == expected def test_cite_command_bibliography (): bib_dir = pathlib . Path ( __file__ ) . parent . parent . parent . joinpath ( \"pandoc/tests/bibliographies\" ) args = [ \"manubot\" , \"cite\" , \"--bibliography=bibliography.json\" , \"DOI:10.7554/elife.32822\" , ] csl_json = subprocess . check_output ( args , encoding = \"utf-8\" , cwd = bib_dir ) csl_items = json . loads ( csl_json ) assert len ( csl_items ) == 1 csl_item = csl_items [ 0 ] assert \"source_bibliography: bibliography.json\" in csl_item [ \"note\" ] def teardown_module ( module ): \"\"\" Avoid too many requests to NCBI E-Utils in the test_pubmed.py, which is executed following this module. E-Utility requests are capped at 3 per second, which is usually controlled by _get_eutils_rate_limiter, but this does not seem to work across test modules. \"\"\" import time time . sleep ( 1 )","title":"Module manubot.cite.tests.test_cite_command"},{"location":"reference/manubot/cite/tests/test_cite_command/#variables","text":"data_dir","title":"Variables"},{"location":"reference/manubot/cite/tests/test_cite_command/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_cite_command/#teardown_module","text":"def teardown_module ( module ) Avoid too many requests to NCBI E-Utils in the test_pubmed.py, which is executed following this module. E-Utility requests are capped at 3 per second, which is usually controlled by _get_eutils_rate_limiter, but this does not seem to work across test modules. View Source def teardown_module ( module ): \"\"\" Avoid too many requests to NCBI E-Utils in the test_pubmed.py, which is executed following this module. E-Utility requests are capped at 3 per second, which is usually controlled by _get_eutils_rate_limiter, but this does not seem to work across test modules. \"\"\" import time time . sleep ( 1 )","title":"teardown_module"},{"location":"reference/manubot/cite/tests/test_cite_command/#test_cite_command_bibliography","text":"def test_cite_command_bibliography ( ) View Source def test_cite_command_bibliography (): bib_dir = pathlib . Path ( __file__ ) . parent . parent . parent . joinpath ( \"pandoc/tests/bibliographies\" ) args = [ \"manubot\" , \"cite\" , \"--bibliography=bibliography.json\" , \"DOI:10.7554/elife.32822\" , ] csl_json = subprocess . check_output ( args , encoding = \"utf-8\" , cwd = bib_dir ) csl_items = json . loads ( csl_json ) assert len ( csl_items ) == 1 csl_item = csl_items [ 0 ] assert \"source_bibliography: bibliography.json\" in csl_item [ \"note\" ]","title":"test_cite_command_bibliography"},{"location":"reference/manubot/cite/tests/test_cite_command/#test_cite_command_empty","text":"def test_cite_command_empty ( ) View Source def test_cite_command_empty(): process = subprocess.run( [\"manubot\", \"cite\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding=\"utf-8\", ) print(process.stderr) assert process.returncode == 2 assert \"the following arguments are required: citekeys\" in process.stderr","title":"test_cite_command_empty"},{"location":"reference/manubot/cite/tests/test_cite_command/#test_cite_command_file","text":"def test_cite_command_file ( tmpdir ) View Source def test_cite_command_file ( tmpdir ): path = pathlib . Path ( tmpdir ) / \"csl-items.json\" process = subprocess . run ( [ \"manubot\" , \"cite\" , \"--output\" , str ( path ), \"arxiv:1806.05726v1\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , ) print ( process . stderr . decode ()) assert process . returncode == 0 with path . open () as read_file : ( csl ,) = json . load ( read_file ) assert csl [ \"URL\" ] == \"https://arxiv.org/abs/1806.05726v1\"","title":"test_cite_command_file"},{"location":"reference/manubot/cite/tests/test_cite_command/#test_cite_command_preserves_order","text":"def test_cite_command_preserves_order ( ) https://github.com/manubot/manubot/issues/240 View Source def test_cite_command_preserves_order (): \"\"\" https://github.com/manubot/manubot/issues/240 \"\"\" citekeys = [ \"pmid:29618526\" , \"doi:10.7717/peerj.338\" , \"arxiv:1806.05726v1\" , \"pubmed:29618526\" , \"DOI:10.7717/PEERJ.338\" , ] args = [ \"manubot\" , \"cite\" , \"--bibliography=input-bibliography.json\" , * citekeys , ] output = subprocess . check_output ( args , encoding = \"utf-8\" , cwd = data_dir , ) csl_items = json . loads ( output ) csl_items = [ CSL_Item ( x ) for x in csl_items ] standard_ids = [ csl_item . note_dict . get ( \"standard_id\" ) for csl_item in csl_items ] assert standard_ids == [ \"pubmed:29618526\" , \"doi:10.7717/peerj.338\" , \"arxiv:1806.05726v1\" , ]","title":"test_cite_command_preserves_order"},{"location":"reference/manubot/cite/tests/test_cite_command/#test_cite_command_render_stdout","text":"def test_cite_command_render_stdout ( args , filename ) Test the stdout output of manubot cite with various Pandoc-output formats. The output is sensitive to the version of Pandoc used, so expected output files include the pandoc version stamp in their filename. When the expected version is missing, the test fails but writes the command output to that file. Therefore, subsequent runs of the same test will pass. Before committing the auto-generated output, do look to ensure its integrity. This test uses --bibliography to avoid slow network calls. Regenerate the CSL JSON using: manubot cite --output = manubot/cite/tests/cite-command-rendered/input-bibliography.json arxiv:1806.05726v1 doi:10.7717/peerj.338 pmid:29618526 View Source @ pytest . mark . integration @ pytest . mark . parametrize ( [ \"args\" , \"filename\" ], [ pytest . param ( [ \"--format\" , \"plain\" ], \"references-plain-{}.txt\" , id = \"--format=plain\" ), pytest . param ( [ \"--format\" , \"markdown\" ], \"references-markdown-{}.md\" , id = \"--format=markdown\" , ), pytest . param ( [ \"--format\" , \"html\" ], \"references-html-{}.html\" , id = \"--format=html\" ), pytest . param ( [ \"--format\" , \"jats\" ], \"references-jats-{}.xml\" , id = \"--format=jats\" ), ], ) @ pytest . mark . skipif ( not shutil . which ( \"pandoc\" ), reason = \"pandoc installation not found on system\" ) @ pytest . mark . pandoc_version_sensitive def test_cite_command_render_stdout ( args , filename ): \"\"\" Test the stdout output of `manubot cite` with various Pandoc-output formats. The output is sensitive to the version of Pandoc used, so expected output files include the pandoc version stamp in their filename. When the expected version is missing, the test fails but writes the command output to that file. Therefore, subsequent runs of the same test will pass. Before committing the auto-generated output, do look to ensure its integrity. This test uses --bibliography to avoid slow network calls. Regenerate the CSL JSON using: ```shell manubot cite \\ --output=manubot/cite/tests/cite-command-rendered/input-bibliography.json \\ arxiv:1806.05726v1 doi:10.7717/peerj.338 pmid:29618526 ``` \"\"\" # get pandoc version info pandoc_version = get_pandoc_version () pandoc_stamp = \".\" . join ( map ( str , pandoc_version )) path = data_dir . joinpath ( filename . format ( pandoc_stamp )) # skip test on old pandoc versions for output in \"markdown\" , \"html\" , \"jats\" : if output in args and pandoc_version < ( 2 , 5 ): pytest . skip ( f \"Test {output} output assumes pandoc >= 2.5\" ) if pandoc_version < ( 2 , 0 ): pytest . skip ( \"Test requires pandoc >= 2.0 to support --lua-filter and --csl=URL\" ) args = [ \"manubot\" , \"cite\" , \"--bibliography=input-bibliography.json\" , \"--csl=https://github.com/greenelab/manubot-rootstock/raw/e83e51dcd89256403bb787c3d9a46e4ee8d04a9e/build/assets/style.csl\" , \"arxiv:1806.05726v1\" , \"doi:10.7717/peerj.338\" , \"pmid:29618526\" , ] + args process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , cwd = data_dir , ) print ( shlex_join ( process . args )) print ( process . stderr ) assert process . returncode == 0 print ( process . stdout ) if not path . exists (): # https://github.com/manubot/manubot/pull/146#discussion_r333132261 print ( f \"Missing expected output at {path} \\n \" \"Writing output to file such that future tests will pass.\" ) path . write_text ( process . stdout , encoding = \"utf-8\" ) assert False expected = path . read_text ( encoding = \"utf-8-sig\" ) assert process . stdout == expected","title":"test_cite_command_render_stdout"},{"location":"reference/manubot/cite/tests/test_cite_command/#test_cite_command_stdout","text":"def test_cite_command_stdout ( ) View Source def test_cite_command_stdout (): process = subprocess . run ( [ \"manubot\" , \"cite\" , \"arxiv:1806.05726v1\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ) print ( process . stderr ) assert process . returncode == 0 ( csl ,) = json . loads ( process . stdout ) assert csl [ \"URL\" ] == \"https://arxiv.org/abs/1806.05726v1\"","title":"test_cite_command_stdout"},{"location":"reference/manubot/cite/tests/test_citekey/","text":"Module manubot.cite.tests.test_citekey Tests rest of functions in manubot.cite, not covered by test_citekey_api.py. None View Source \"\"\"Tests rest of functions in manubot.cite, not covered by test_citekey_api.py.\"\"\" import pytest from manubot.cite.citekey import CiteKey , shorten_citekey , url_to_citekey @pytest . mark . parametrize ( [ \"input_id\" , \"citekey_attrs\" ], [ pytest . param ( \"DOI:10.5061/DRYad.q447c/1\" , dict ( prefix = \"DOI\" , prefix_lower = \"doi\" , standard_accession = \"10.5061/dryad.q447c/1\" , standard_id = \"doi:10.5061/dryad.q447c/1\" , ), id = \"doi\" , ), pytest . param ( \"10.5061/DRYad.q447c/1\" , dict ( prefix = \"doi\" , prefix_lower = \"doi\" , standard_prefix = \"doi\" , standard_accession = \"10.5061/dryad.q447c/1\" , standard_id = \"doi:10.5061/dryad.q447c/1\" , ), id = \"doi-infer-prefix\" , ), pytest . param ( \"arXiv:1407.3561v1\" , dict ( prefix = \"arXiv\" , prefix_lower = \"arxiv\" , standard_prefix = \"arxiv\" , standard_accession = \"1407.3561v1\" , standard_id = \"arxiv:1407.3561v1\" , ), id = \"arxiv\" , ), pytest . param ( \"pmid:24159271\" , dict ( standard_id = \"pubmed:24159271\" , ), id = \"pmid\" , ), pytest . param ( \"pmcid:PMC4304851\" , dict ( prefix = \"pmcid\" , prefix_lower = \"pmcid\" , standard_prefix = \"pmc\" , standard_id = \"pmc:PMC4304851\" , ), id = \"pmcid\" , ), pytest . param ( \"https://greenelab.github.io/manubot-rootstock/\" , dict ( prefix = \"https\" , prefix_lower = \"https\" , standard_prefix = \"url\" , standard_id = \"url:https://greenelab.github.io/manubot-rootstock/\" , ), id = \"https\" , ), pytest . param ( \"isbn:1-339-91988-5\" , dict ( standard_id = \"isbn:9781339919881\" , ), id = \"isbn\" , ), pytest . param ( \"DOID:14330\" , dict ( standard_id = \"doid:14330\" , ), id = \"doid\" , ), pytest . param ( \"PubChem.substance:100101\" , dict ( standard_id = \"pubchem.substance:100101\" , ), id = \"PubChem.substance\" , ), pytest . param ( \"Wikidata:Q50051684\" , dict ( standard_id = \"wikidata:Q50051684\" , ), id = \"wikidata\" , ), ], ) def test_citekey_class ( input_id , citekey_attrs ): citekey = CiteKey ( input_id ) print ( citekey ) for key , value in citekey_attrs . items (): assert getattr ( citekey , key ) == value assert citekey . short_id def test_citekey_check_input_id_type (): with pytest . raises ( TypeError ) as excinfo : CiteKey ( None ) assert \"input_id should be type 'str' not 'NoneType': None\" == str ( excinfo . value ) with pytest . raises ( TypeError ): CiteKey ( 0 ) def test_citekey_check_input_id_at_prefix (): with pytest . raises ( ValueError ) as excinfo : CiteKey ( \"@my-citekey\" ) assert \"input_id: '@my-citekey' \\n starts with '@'\" in str ( excinfo . value ) @pytest . mark . parametrize ( \"standard_citekey,expected\" , [ ( \"doi:10.5061/dryad.q447c/1\" , \"kQFQ8EaO\" ), ( \"arxiv:1407.3561v1\" , \"16kozZ9Ys\" ), ( \"pmid:24159271\" , \"11sli93ov\" ), ( \"url:http://blog.dhimmel.com/irreproducible-timestamps/\" , \"QBWMEuxW\" ), ], ) def test_shorten_citekey ( standard_citekey , expected ): short_citekey = shorten_citekey ( standard_citekey ) assert short_citekey == expected @pytest . mark . parametrize ( \"citekey\" , [ \"doi:10.7717/peerj.705\" , \"doi:10/b6vnmd\" , \"pmc:PMC4304851\" , \"pubmed:25648772\" , \"arxiv:1407.3561\" , \"arxiv:1407.3561v1\" , \"arxiv:math.GT/0309136\" , \"arxiv:math.GT/0309136v1\" , \"arxiv:hep-th/9305059\" , \"arxiv:hep-th/9305059v2\" , \"isbn:978-1-339-91988-1\" , \"isbn:1-339-91988-5\" , \"wikidata:Q1\" , \"wikidata:Q50051684\" , \"url:https://peerj.com/articles/705/\" , \"https://peerj.com/articles/705/\" , \"GO:0006915\" , # namespaceEmbeddedInLui=true \"go:0006915\" , # namespaceEmbeddedInLui=true \"clinicaltrials:NCT04372602\" , # namespaceEmbeddedInLui=false ], ) def test_inspect_citekey_passes ( citekey ): \"\"\" These citekeys should pass inspection by inspect_citekey. \"\"\" report = CiteKey ( citekey ) . inspect () assert report is None @pytest . mark . parametrize ( [ \"citekey\" , \"contains\" ], [ ( \"doi:10.771/peerj.705\" , \"Double check the DOI\" ), ( \"doi:10/b6v_nmd\" , \"Double check the shortDOI\" ), ( \"doi:7717/peerj.705\" , \"must start with '10.'\" ), ( \"doi:b6vnmd\" , \"must start with '10.'\" ), ( \"pmcid:25648772\" , \"must start with 'PMC'\" ), ( \"pmid:PMC4304851\" , \"Should 'pmid:PMC4304851' switch the citation source to 'pmc'?\" , ), ( \"isbn:1-339-91988-X\" , \"identifier violates the ISBN syntax\" ), ( \"wikidata:P212\" , \"item IDs must start with 'Q'\" ), ( \"wikidata:QABCD\" , \"does not conform to the Wikidata regex\" ), ( \"arxiv:YYMM.number\" , \"must conform to syntax\" ), ( \"GO:GO:0006915\" , \"GO:GO:0006915 does not match regex\" ), ], ) def test_inspect_citekey_fails ( citekey , contains ): \"\"\" These citekeys should fail inspection by inspect_citekey. \"\"\" report = CiteKey ( citekey ) . inspect () assert report is not None assert isinstance ( report , str ) assert contains in report @pytest . mark . parametrize ( [ \"url\" , \"citekey\" ], [ ( \"https://www.doi.org/\" , \"url:https://www.doi.org/\" , ), ( \"https://www.doi.org/factsheets/Identifier_Interoper.html\" , \"url:https://www.doi.org/factsheets/Identifier_Interoper.html\" , ), ( \"https://doi.org/10.1097 %2F 00004032-200403000-00012\" , \"doi:10.1097/00004032-200403000-00012\" , ), ( \"http://dx.doi.org/10.7554/eLife.46574\" , \"doi:10.7554/eLife.46574\" ), ( \"https://doi.org/10/b6vnmd#anchor\" , \"doi:10/b6vnmd\" ), # ShortDOI URLs without `10/` prefix not yet supported` ( \"https://doi.org/b6vnmd\" , \"url:https://doi.org/b6vnmd\" ), ( \"https://www.biorxiv.org/about-biorxiv\" , \"url:https://www.biorxiv.org/about-biorxiv\" , ), ( \"https://sci-hub.tw/10.1038/nature19057\" , \"doi:10.1038/nature19057\" ), ( \"https://www.biorxiv.org/content/10.1101/087619v3\" , \"doi:10.1101/087619\" ), ( \"https://www.biorxiv.org/content/10.1101/087619v3.full\" , \"doi:10.1101/087619\" ), ( \"https://www.biorxiv.org/content/early/2017/08/31/087619.full.pdf\" , \"doi:10.1101/087619\" , ), ( \"https://www.biorxiv.org/content/10.1101/2019.12.11.872580v1\" , \"doi:10.1101/2019.12.11.872580\" , ), ( \"https://www.biorxiv.org/content/10.1101/2019.12.11.872580v1.full.pdf+html\" , \"doi:10.1101/2019.12.11.872580\" , ), ( \"https://www.biorxiv.org/content/10.1101/2019.12.11.872580v1.full.pdf\" , \"doi:10.1101/2019.12.11.872580\" , ), ( \"https://www.ncbi.nlm.nih.gov\" , \"url:https://www.ncbi.nlm.nih.gov\" ), ( \"https://www.ncbi.nlm.nih.gov/pubmed\" , \"url:https://www.ncbi.nlm.nih.gov/pubmed\" , ), ( \"https://www.ncbi.nlm.nih.gov/pubmed/31233491\" , \"pmid:31233491\" ), ( \"https://www.ncbi.nlm.nih.gov/pmc/\" , \"url:https://www.ncbi.nlm.nih.gov/pmc/\" ), ( \"https://www.ncbi.nlm.nih.gov/pmc/about/intro/\" , \"url:https://www.ncbi.nlm.nih.gov/pmc/about/intro/\" , ), ( \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4304851/\" , \"pmcid:PMC4304851\" ), ( \"https://www.wikidata.org/wiki/Wikidata:Main_Page\" , \"url:https://www.wikidata.org/wiki/Wikidata:Main_Page\" , ), ( \"https://www.wikidata.org/wiki/Q50051684\" , \"wikidata:Q50051684\" ), ( \"https://arxiv.org/\" , \"url:https://arxiv.org/\" ), ( \"https://arxiv.org/list/q-fin/recent\" , \"url:https://arxiv.org/list/q-fin/recent\" , ), ( \"https://arxiv.org/abs/1912.03529v1\" , \"arxiv:1912.03529v1\" ), ( \"https://arxiv.org/pdf/1912.03529v1.pdf\" , \"arxiv:1912.03529v1\" ), ( \"https://arxiv.org/ps/1912.03529v1\" , \"arxiv:1912.03529v1\" ), ( \"https://arxiv.org/abs/math.GT/0309136\" , \"arxiv:math.GT/0309136\" ), ( \"https://arxiv.org/abs/hep-th/9305059\" , \"arxiv:hep-th/9305059\" ), ( \"https://arxiv.org/pdf/hep-th/9305059.pdf\" , \"arxiv:hep-th/9305059\" ), ], ) def test_url_to_citekey ( url , citekey ): assert url_to_citekey ( url ) == citekey Functions test_citekey_check_input_id_at_prefix def test_citekey_check_input_id_at_prefix ( ) View Source def test_citekey_check_input_id_at_prefix () : with pytest . raises ( ValueError ) as excinfo : CiteKey ( \"@my-citekey\" ) assert \"input_id: '@my-citekey'\\nstarts with '@'\" in str ( excinfo . value ) test_citekey_check_input_id_type def test_citekey_check_input_id_type ( ) View Source def test_citekey_check_input_id_type(): with pytest.raises(TypeError) as excinfo: CiteKey(None) assert \"input_id should be type 'str' not 'NoneType': None\" == str(excinfo.value) with pytest.raises(TypeError): CiteKey(0) test_citekey_class def test_citekey_class ( input_id , citekey_attrs ) View Source @pytest . mark . parametrize ( [ \"input_id\", \"citekey_attrs\" ] , [ pytest.param( \"DOI:10.5061/DRYad.q447c/1\", dict( prefix=\"DOI\", prefix_lower=\"doi\", standard_accession=\"10.5061/dryad.q447c/1\", standard_id=\"doi:10.5061/dryad.q447c/1\", ), id=\"doi\", ), pytest.param( \"10.5061/DRYad.q447c/1\", dict( prefix=\"doi\", prefix_lower=\"doi\", standard_prefix=\"doi\", standard_accession=\"10.5061/dryad.q447c/1\", standard_id=\"doi:10.5061/dryad.q447c/1\", ), id=\"doi-infer-prefix\", ), pytest.param( \"arXiv:1407.3561v1\", dict( prefix=\"arXiv\", prefix_lower=\"arxiv\", standard_prefix=\"arxiv\", standard_accession=\"1407.3561v1\", standard_id=\"arxiv:1407.3561v1\", ), id=\"arxiv\", ), pytest.param( \"pmid:24159271\", dict( standard_id=\"pubmed:24159271\", ), id=\"pmid\", ), pytest.param( \"pmcid:PMC4304851\", dict( prefix=\"pmcid\", prefix_lower=\"pmcid\", standard_prefix=\"pmc\", standard_id=\"pmc:PMC4304851\", ), id=\"pmcid\", ), pytest.param( \"https://greenelab.github.io/manubot-rootstock/\", dict( prefix=\"https\", prefix_lower=\"https\", standard_prefix=\"url\", standard_id=\"url:https://greenelab.github.io/manubot-rootstock/\", ), id=\"https\", ), pytest.param( \"isbn:1-339-91988-5\", dict( standard_id=\"isbn:9781339919881\", ), id=\"isbn\", ), pytest.param( \"DOID:14330\", dict( standard_id=\"doid:14330\", ), id=\"doid\", ), pytest.param( \"PubChem.substance:100101\", dict( standard_id=\"pubchem.substance:100101\", ), id=\"PubChem.substance\", ), pytest.param( \"Wikidata:Q50051684\", dict( standard_id=\"wikidata:Q50051684\", ), id=\"wikidata\", ), ] , ) def test_citekey_class ( input_id , citekey_attrs ) : citekey = CiteKey ( input_id ) print ( citekey ) for key , value in citekey_attrs . items () : assert getattr ( citekey , key ) == value assert citekey . short_id test_inspect_citekey_fails def test_inspect_citekey_fails ( citekey , contains ) These citekeys should fail inspection by inspect_citekey. View Source @pytest . mark . parametrize ( [ \"citekey\", \"contains\" ] , [ (\"doi:10.771/peerj.705\", \"Double check the DOI\"), (\"doi:10/b6v_nmd\", \"Double check the shortDOI\"), (\"doi:7717/peerj.705\", \"must start with '10.'\"), (\"doi:b6vnmd\", \"must start with '10.'\"), (\"pmcid:25648772\", \"must start with 'PMC'\"), ( \"pmid:PMC4304851\", \"Should 'pmid:PMC4304851' switch the citation source to 'pmc'?\", ), (\"isbn:1-339-91988-X\", \"identifier violates the ISBN syntax\"), (\"wikidata:P212\", \"item IDs must start with 'Q'\"), (\"wikidata:QABCD\", \"does not conform to the Wikidata regex\"), (\"arxiv:YYMM.number\", \"must conform to syntax\"), (\"GO:GO:0006915\", \"GO:GO:0006915 does not match regex\"), ] , ) def test_inspect_citekey_fails ( citekey , contains ) : \"\"\" These citekeys should fail inspection by inspect_citekey. \"\"\" report = CiteKey ( citekey ). inspect () assert report is not None assert isinstance ( report , str ) assert contains in report test_inspect_citekey_passes def test_inspect_citekey_passes ( citekey ) These citekeys should pass inspection by inspect_citekey. View Source @pytest . mark . parametrize ( \"citekey\" , [ \"doi:10.7717/peerj.705\", \"doi:10/b6vnmd\", \"pmc:PMC4304851\", \"pubmed:25648772\", \"arxiv:1407.3561\", \"arxiv:1407.3561v1\", \"arxiv:math.GT/0309136\", \"arxiv:math.GT/0309136v1\", \"arxiv:hep-th/9305059\", \"arxiv:hep-th/9305059v2\", \"isbn:978-1-339-91988-1\", \"isbn:1-339-91988-5\", \"wikidata:Q1\", \"wikidata:Q50051684\", \"url:https://peerj.com/articles/705/\", \"https://peerj.com/articles/705/\", \"GO:0006915\", # namespaceEmbeddedInLui=true \"go:0006915\", # namespaceEmbeddedInLui=true \"clinicaltrials:NCT04372602\", # namespaceEmbeddedInLui=false ] , ) def test_inspect_citekey_passes ( citekey ) : \"\"\" These citekeys should pass inspection by inspect_citekey. \"\"\" report = CiteKey ( citekey ). inspect () assert report is None test_shorten_citekey def test_shorten_citekey ( standard_citekey , expected ) View Source @pytest . mark . parametrize ( \"standard_citekey,expected\" , [ (\"doi:10.5061/dryad.q447c/1\", \"kQFQ8EaO\"), (\"arxiv:1407.3561v1\", \"16kozZ9Ys\"), (\"pmid:24159271\", \"11sli93ov\"), (\"url:http://blog.dhimmel.com/irreproducible-timestamps/\", \"QBWMEuxW\"), ] , ) def test_shorten_citekey ( standard_citekey , expected ) : short_citekey = shorten_citekey ( standard_citekey ) assert short_citekey == expected test_url_to_citekey def test_url_to_citekey ( url , citekey ) View Source @pytest . mark . parametrize ( [ \"url\", \"citekey\" ] , [ ( \"https://www.doi.org/\", \"url:https://www.doi.org/\", ), ( \"https://www.doi.org/factsheets/Identifier_Interoper.html\", \"url:https://www.doi.org/factsheets/Identifier_Interoper.html\", ), ( \"https://doi.org/10.1097%2F00004032-200403000-00012\", \"doi:10.1097/00004032-200403000-00012\", ), (\"http://dx.doi.org/10.7554/eLife.46574\", \"doi:10.7554/eLife.46574\"), (\"https://doi.org/10/b6vnmd#anchor\", \"doi:10/b6vnmd\"), # ShortDOI URLs without `10/` prefix not yet supported` (\"https://doi.org/b6vnmd\", \"url:https://doi.org/b6vnmd\"), ( \"https://www.biorxiv.org/about-biorxiv\", \"url:https://www.biorxiv.org/about-biorxiv\", ), (\"https://sci-hub.tw/10.1038/nature19057\", \"doi:10.1038/nature19057\"), (\"https://www.biorxiv.org/content/10.1101/087619v3\", \"doi:10.1101/087619\"), (\"https://www.biorxiv.org/content/10.1101/087619v3.full\", \"doi:10.1101/087619\"), ( \"https://www.biorxiv.org/content/early/2017/08/31/087619.full.pdf\", \"doi:10.1101/087619\", ), ( \"https://www.biorxiv.org/content/10.1101/2019.12.11.872580v1\", \"doi:10.1101/2019.12.11.872580\", ), ( \"https://www.biorxiv.org/content/10.1101/2019.12.11.872580v1.full.pdf+html\", \"doi:10.1101/2019.12.11.872580\", ), ( \"https://www.biorxiv.org/content/10.1101/2019.12.11.872580v1.full.pdf\", \"doi:10.1101/2019.12.11.872580\", ), (\"https://www.ncbi.nlm.nih.gov\", \"url:https://www.ncbi.nlm.nih.gov\"), ( \"https://www.ncbi.nlm.nih.gov/pubmed\", \"url:https://www.ncbi.nlm.nih.gov/pubmed\", ), (\"https://www.ncbi.nlm.nih.gov/pubmed/31233491\", \"pmid:31233491\"), (\"https://www.ncbi.nlm.nih.gov/pmc/\", \"url:https://www.ncbi.nlm.nih.gov/pmc/\"), ( \"https://www.ncbi.nlm.nih.gov/pmc/about/intro/\", \"url:https://www.ncbi.nlm.nih.gov/pmc/about/intro/\", ), (\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4304851/\", \"pmcid:PMC4304851\"), ( \"https://www.wikidata.org/wiki/Wikidata:Main_Page\", \"url:https://www.wikidata.org/wiki/Wikidata:Main_Page\", ), (\"https://www.wikidata.org/wiki/Q50051684\", \"wikidata:Q50051684\"), (\"https://arxiv.org/\", \"url:https://arxiv.org/\"), ( \"https://arxiv.org/list/q-fin/recent\", \"url:https://arxiv.org/list/q-fin/recent\", ), (\"https://arxiv.org/abs/1912.03529v1\", \"arxiv:1912.03529v1\"), (\"https://arxiv.org/pdf/1912.03529v1.pdf\", \"arxiv:1912.03529v1\"), (\"https://arxiv.org/ps/1912.03529v1\", \"arxiv:1912.03529v1\"), (\"https://arxiv.org/abs/math.GT/0309136\", \"arxiv:math.GT/0309136\"), (\"https://arxiv.org/abs/hep-th/9305059\", \"arxiv:hep-th/9305059\"), (\"https://arxiv.org/pdf/hep-th/9305059.pdf\", \"arxiv:hep-th/9305059\"), ] , ) def test_url_to_citekey ( url , citekey ) : assert url_to_citekey ( url ) == citekey","title":"Test Citekey"},{"location":"reference/manubot/cite/tests/test_citekey/#module-manubotciteteststest_citekey","text":"Tests rest of functions in manubot.cite, not covered by test_citekey_api.py. None View Source \"\"\"Tests rest of functions in manubot.cite, not covered by test_citekey_api.py.\"\"\" import pytest from manubot.cite.citekey import CiteKey , shorten_citekey , url_to_citekey @pytest . mark . parametrize ( [ \"input_id\" , \"citekey_attrs\" ], [ pytest . param ( \"DOI:10.5061/DRYad.q447c/1\" , dict ( prefix = \"DOI\" , prefix_lower = \"doi\" , standard_accession = \"10.5061/dryad.q447c/1\" , standard_id = \"doi:10.5061/dryad.q447c/1\" , ), id = \"doi\" , ), pytest . param ( \"10.5061/DRYad.q447c/1\" , dict ( prefix = \"doi\" , prefix_lower = \"doi\" , standard_prefix = \"doi\" , standard_accession = \"10.5061/dryad.q447c/1\" , standard_id = \"doi:10.5061/dryad.q447c/1\" , ), id = \"doi-infer-prefix\" , ), pytest . param ( \"arXiv:1407.3561v1\" , dict ( prefix = \"arXiv\" , prefix_lower = \"arxiv\" , standard_prefix = \"arxiv\" , standard_accession = \"1407.3561v1\" , standard_id = \"arxiv:1407.3561v1\" , ), id = \"arxiv\" , ), pytest . param ( \"pmid:24159271\" , dict ( standard_id = \"pubmed:24159271\" , ), id = \"pmid\" , ), pytest . param ( \"pmcid:PMC4304851\" , dict ( prefix = \"pmcid\" , prefix_lower = \"pmcid\" , standard_prefix = \"pmc\" , standard_id = \"pmc:PMC4304851\" , ), id = \"pmcid\" , ), pytest . param ( \"https://greenelab.github.io/manubot-rootstock/\" , dict ( prefix = \"https\" , prefix_lower = \"https\" , standard_prefix = \"url\" , standard_id = \"url:https://greenelab.github.io/manubot-rootstock/\" , ), id = \"https\" , ), pytest . param ( \"isbn:1-339-91988-5\" , dict ( standard_id = \"isbn:9781339919881\" , ), id = \"isbn\" , ), pytest . param ( \"DOID:14330\" , dict ( standard_id = \"doid:14330\" , ), id = \"doid\" , ), pytest . param ( \"PubChem.substance:100101\" , dict ( standard_id = \"pubchem.substance:100101\" , ), id = \"PubChem.substance\" , ), pytest . param ( \"Wikidata:Q50051684\" , dict ( standard_id = \"wikidata:Q50051684\" , ), id = \"wikidata\" , ), ], ) def test_citekey_class ( input_id , citekey_attrs ): citekey = CiteKey ( input_id ) print ( citekey ) for key , value in citekey_attrs . items (): assert getattr ( citekey , key ) == value assert citekey . short_id def test_citekey_check_input_id_type (): with pytest . raises ( TypeError ) as excinfo : CiteKey ( None ) assert \"input_id should be type 'str' not 'NoneType': None\" == str ( excinfo . value ) with pytest . raises ( TypeError ): CiteKey ( 0 ) def test_citekey_check_input_id_at_prefix (): with pytest . raises ( ValueError ) as excinfo : CiteKey ( \"@my-citekey\" ) assert \"input_id: '@my-citekey' \\n starts with '@'\" in str ( excinfo . value ) @pytest . mark . parametrize ( \"standard_citekey,expected\" , [ ( \"doi:10.5061/dryad.q447c/1\" , \"kQFQ8EaO\" ), ( \"arxiv:1407.3561v1\" , \"16kozZ9Ys\" ), ( \"pmid:24159271\" , \"11sli93ov\" ), ( \"url:http://blog.dhimmel.com/irreproducible-timestamps/\" , \"QBWMEuxW\" ), ], ) def test_shorten_citekey ( standard_citekey , expected ): short_citekey = shorten_citekey ( standard_citekey ) assert short_citekey == expected @pytest . mark . parametrize ( \"citekey\" , [ \"doi:10.7717/peerj.705\" , \"doi:10/b6vnmd\" , \"pmc:PMC4304851\" , \"pubmed:25648772\" , \"arxiv:1407.3561\" , \"arxiv:1407.3561v1\" , \"arxiv:math.GT/0309136\" , \"arxiv:math.GT/0309136v1\" , \"arxiv:hep-th/9305059\" , \"arxiv:hep-th/9305059v2\" , \"isbn:978-1-339-91988-1\" , \"isbn:1-339-91988-5\" , \"wikidata:Q1\" , \"wikidata:Q50051684\" , \"url:https://peerj.com/articles/705/\" , \"https://peerj.com/articles/705/\" , \"GO:0006915\" , # namespaceEmbeddedInLui=true \"go:0006915\" , # namespaceEmbeddedInLui=true \"clinicaltrials:NCT04372602\" , # namespaceEmbeddedInLui=false ], ) def test_inspect_citekey_passes ( citekey ): \"\"\" These citekeys should pass inspection by inspect_citekey. \"\"\" report = CiteKey ( citekey ) . inspect () assert report is None @pytest . mark . parametrize ( [ \"citekey\" , \"contains\" ], [ ( \"doi:10.771/peerj.705\" , \"Double check the DOI\" ), ( \"doi:10/b6v_nmd\" , \"Double check the shortDOI\" ), ( \"doi:7717/peerj.705\" , \"must start with '10.'\" ), ( \"doi:b6vnmd\" , \"must start with '10.'\" ), ( \"pmcid:25648772\" , \"must start with 'PMC'\" ), ( \"pmid:PMC4304851\" , \"Should 'pmid:PMC4304851' switch the citation source to 'pmc'?\" , ), ( \"isbn:1-339-91988-X\" , \"identifier violates the ISBN syntax\" ), ( \"wikidata:P212\" , \"item IDs must start with 'Q'\" ), ( \"wikidata:QABCD\" , \"does not conform to the Wikidata regex\" ), ( \"arxiv:YYMM.number\" , \"must conform to syntax\" ), ( \"GO:GO:0006915\" , \"GO:GO:0006915 does not match regex\" ), ], ) def test_inspect_citekey_fails ( citekey , contains ): \"\"\" These citekeys should fail inspection by inspect_citekey. \"\"\" report = CiteKey ( citekey ) . inspect () assert report is not None assert isinstance ( report , str ) assert contains in report @pytest . mark . parametrize ( [ \"url\" , \"citekey\" ], [ ( \"https://www.doi.org/\" , \"url:https://www.doi.org/\" , ), ( \"https://www.doi.org/factsheets/Identifier_Interoper.html\" , \"url:https://www.doi.org/factsheets/Identifier_Interoper.html\" , ), ( \"https://doi.org/10.1097 %2F 00004032-200403000-00012\" , \"doi:10.1097/00004032-200403000-00012\" , ), ( \"http://dx.doi.org/10.7554/eLife.46574\" , \"doi:10.7554/eLife.46574\" ), ( \"https://doi.org/10/b6vnmd#anchor\" , \"doi:10/b6vnmd\" ), # ShortDOI URLs without `10/` prefix not yet supported` ( \"https://doi.org/b6vnmd\" , \"url:https://doi.org/b6vnmd\" ), ( \"https://www.biorxiv.org/about-biorxiv\" , \"url:https://www.biorxiv.org/about-biorxiv\" , ), ( \"https://sci-hub.tw/10.1038/nature19057\" , \"doi:10.1038/nature19057\" ), ( \"https://www.biorxiv.org/content/10.1101/087619v3\" , \"doi:10.1101/087619\" ), ( \"https://www.biorxiv.org/content/10.1101/087619v3.full\" , \"doi:10.1101/087619\" ), ( \"https://www.biorxiv.org/content/early/2017/08/31/087619.full.pdf\" , \"doi:10.1101/087619\" , ), ( \"https://www.biorxiv.org/content/10.1101/2019.12.11.872580v1\" , \"doi:10.1101/2019.12.11.872580\" , ), ( \"https://www.biorxiv.org/content/10.1101/2019.12.11.872580v1.full.pdf+html\" , \"doi:10.1101/2019.12.11.872580\" , ), ( \"https://www.biorxiv.org/content/10.1101/2019.12.11.872580v1.full.pdf\" , \"doi:10.1101/2019.12.11.872580\" , ), ( \"https://www.ncbi.nlm.nih.gov\" , \"url:https://www.ncbi.nlm.nih.gov\" ), ( \"https://www.ncbi.nlm.nih.gov/pubmed\" , \"url:https://www.ncbi.nlm.nih.gov/pubmed\" , ), ( \"https://www.ncbi.nlm.nih.gov/pubmed/31233491\" , \"pmid:31233491\" ), ( \"https://www.ncbi.nlm.nih.gov/pmc/\" , \"url:https://www.ncbi.nlm.nih.gov/pmc/\" ), ( \"https://www.ncbi.nlm.nih.gov/pmc/about/intro/\" , \"url:https://www.ncbi.nlm.nih.gov/pmc/about/intro/\" , ), ( \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4304851/\" , \"pmcid:PMC4304851\" ), ( \"https://www.wikidata.org/wiki/Wikidata:Main_Page\" , \"url:https://www.wikidata.org/wiki/Wikidata:Main_Page\" , ), ( \"https://www.wikidata.org/wiki/Q50051684\" , \"wikidata:Q50051684\" ), ( \"https://arxiv.org/\" , \"url:https://arxiv.org/\" ), ( \"https://arxiv.org/list/q-fin/recent\" , \"url:https://arxiv.org/list/q-fin/recent\" , ), ( \"https://arxiv.org/abs/1912.03529v1\" , \"arxiv:1912.03529v1\" ), ( \"https://arxiv.org/pdf/1912.03529v1.pdf\" , \"arxiv:1912.03529v1\" ), ( \"https://arxiv.org/ps/1912.03529v1\" , \"arxiv:1912.03529v1\" ), ( \"https://arxiv.org/abs/math.GT/0309136\" , \"arxiv:math.GT/0309136\" ), ( \"https://arxiv.org/abs/hep-th/9305059\" , \"arxiv:hep-th/9305059\" ), ( \"https://arxiv.org/pdf/hep-th/9305059.pdf\" , \"arxiv:hep-th/9305059\" ), ], ) def test_url_to_citekey ( url , citekey ): assert url_to_citekey ( url ) == citekey","title":"Module manubot.cite.tests.test_citekey"},{"location":"reference/manubot/cite/tests/test_citekey/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_citekey/#test_citekey_check_input_id_at_prefix","text":"def test_citekey_check_input_id_at_prefix ( ) View Source def test_citekey_check_input_id_at_prefix () : with pytest . raises ( ValueError ) as excinfo : CiteKey ( \"@my-citekey\" ) assert \"input_id: '@my-citekey'\\nstarts with '@'\" in str ( excinfo . value )","title":"test_citekey_check_input_id_at_prefix"},{"location":"reference/manubot/cite/tests/test_citekey/#test_citekey_check_input_id_type","text":"def test_citekey_check_input_id_type ( ) View Source def test_citekey_check_input_id_type(): with pytest.raises(TypeError) as excinfo: CiteKey(None) assert \"input_id should be type 'str' not 'NoneType': None\" == str(excinfo.value) with pytest.raises(TypeError): CiteKey(0)","title":"test_citekey_check_input_id_type"},{"location":"reference/manubot/cite/tests/test_citekey/#test_citekey_class","text":"def test_citekey_class ( input_id , citekey_attrs ) View Source @pytest . mark . parametrize ( [ \"input_id\", \"citekey_attrs\" ] , [ pytest.param( \"DOI:10.5061/DRYad.q447c/1\", dict( prefix=\"DOI\", prefix_lower=\"doi\", standard_accession=\"10.5061/dryad.q447c/1\", standard_id=\"doi:10.5061/dryad.q447c/1\", ), id=\"doi\", ), pytest.param( \"10.5061/DRYad.q447c/1\", dict( prefix=\"doi\", prefix_lower=\"doi\", standard_prefix=\"doi\", standard_accession=\"10.5061/dryad.q447c/1\", standard_id=\"doi:10.5061/dryad.q447c/1\", ), id=\"doi-infer-prefix\", ), pytest.param( \"arXiv:1407.3561v1\", dict( prefix=\"arXiv\", prefix_lower=\"arxiv\", standard_prefix=\"arxiv\", standard_accession=\"1407.3561v1\", standard_id=\"arxiv:1407.3561v1\", ), id=\"arxiv\", ), pytest.param( \"pmid:24159271\", dict( standard_id=\"pubmed:24159271\", ), id=\"pmid\", ), pytest.param( \"pmcid:PMC4304851\", dict( prefix=\"pmcid\", prefix_lower=\"pmcid\", standard_prefix=\"pmc\", standard_id=\"pmc:PMC4304851\", ), id=\"pmcid\", ), pytest.param( \"https://greenelab.github.io/manubot-rootstock/\", dict( prefix=\"https\", prefix_lower=\"https\", standard_prefix=\"url\", standard_id=\"url:https://greenelab.github.io/manubot-rootstock/\", ), id=\"https\", ), pytest.param( \"isbn:1-339-91988-5\", dict( standard_id=\"isbn:9781339919881\", ), id=\"isbn\", ), pytest.param( \"DOID:14330\", dict( standard_id=\"doid:14330\", ), id=\"doid\", ), pytest.param( \"PubChem.substance:100101\", dict( standard_id=\"pubchem.substance:100101\", ), id=\"PubChem.substance\", ), pytest.param( \"Wikidata:Q50051684\", dict( standard_id=\"wikidata:Q50051684\", ), id=\"wikidata\", ), ] , ) def test_citekey_class ( input_id , citekey_attrs ) : citekey = CiteKey ( input_id ) print ( citekey ) for key , value in citekey_attrs . items () : assert getattr ( citekey , key ) == value assert citekey . short_id","title":"test_citekey_class"},{"location":"reference/manubot/cite/tests/test_citekey/#test_inspect_citekey_fails","text":"def test_inspect_citekey_fails ( citekey , contains ) These citekeys should fail inspection by inspect_citekey. View Source @pytest . mark . parametrize ( [ \"citekey\", \"contains\" ] , [ (\"doi:10.771/peerj.705\", \"Double check the DOI\"), (\"doi:10/b6v_nmd\", \"Double check the shortDOI\"), (\"doi:7717/peerj.705\", \"must start with '10.'\"), (\"doi:b6vnmd\", \"must start with '10.'\"), (\"pmcid:25648772\", \"must start with 'PMC'\"), ( \"pmid:PMC4304851\", \"Should 'pmid:PMC4304851' switch the citation source to 'pmc'?\", ), (\"isbn:1-339-91988-X\", \"identifier violates the ISBN syntax\"), (\"wikidata:P212\", \"item IDs must start with 'Q'\"), (\"wikidata:QABCD\", \"does not conform to the Wikidata regex\"), (\"arxiv:YYMM.number\", \"must conform to syntax\"), (\"GO:GO:0006915\", \"GO:GO:0006915 does not match regex\"), ] , ) def test_inspect_citekey_fails ( citekey , contains ) : \"\"\" These citekeys should fail inspection by inspect_citekey. \"\"\" report = CiteKey ( citekey ). inspect () assert report is not None assert isinstance ( report , str ) assert contains in report","title":"test_inspect_citekey_fails"},{"location":"reference/manubot/cite/tests/test_citekey/#test_inspect_citekey_passes","text":"def test_inspect_citekey_passes ( citekey ) These citekeys should pass inspection by inspect_citekey. View Source @pytest . mark . parametrize ( \"citekey\" , [ \"doi:10.7717/peerj.705\", \"doi:10/b6vnmd\", \"pmc:PMC4304851\", \"pubmed:25648772\", \"arxiv:1407.3561\", \"arxiv:1407.3561v1\", \"arxiv:math.GT/0309136\", \"arxiv:math.GT/0309136v1\", \"arxiv:hep-th/9305059\", \"arxiv:hep-th/9305059v2\", \"isbn:978-1-339-91988-1\", \"isbn:1-339-91988-5\", \"wikidata:Q1\", \"wikidata:Q50051684\", \"url:https://peerj.com/articles/705/\", \"https://peerj.com/articles/705/\", \"GO:0006915\", # namespaceEmbeddedInLui=true \"go:0006915\", # namespaceEmbeddedInLui=true \"clinicaltrials:NCT04372602\", # namespaceEmbeddedInLui=false ] , ) def test_inspect_citekey_passes ( citekey ) : \"\"\" These citekeys should pass inspection by inspect_citekey. \"\"\" report = CiteKey ( citekey ). inspect () assert report is None","title":"test_inspect_citekey_passes"},{"location":"reference/manubot/cite/tests/test_citekey/#test_shorten_citekey","text":"def test_shorten_citekey ( standard_citekey , expected ) View Source @pytest . mark . parametrize ( \"standard_citekey,expected\" , [ (\"doi:10.5061/dryad.q447c/1\", \"kQFQ8EaO\"), (\"arxiv:1407.3561v1\", \"16kozZ9Ys\"), (\"pmid:24159271\", \"11sli93ov\"), (\"url:http://blog.dhimmel.com/irreproducible-timestamps/\", \"QBWMEuxW\"), ] , ) def test_shorten_citekey ( standard_citekey , expected ) : short_citekey = shorten_citekey ( standard_citekey ) assert short_citekey == expected","title":"test_shorten_citekey"},{"location":"reference/manubot/cite/tests/test_citekey/#test_url_to_citekey","text":"def test_url_to_citekey ( url , citekey ) View Source @pytest . mark . parametrize ( [ \"url\", \"citekey\" ] , [ ( \"https://www.doi.org/\", \"url:https://www.doi.org/\", ), ( \"https://www.doi.org/factsheets/Identifier_Interoper.html\", \"url:https://www.doi.org/factsheets/Identifier_Interoper.html\", ), ( \"https://doi.org/10.1097%2F00004032-200403000-00012\", \"doi:10.1097/00004032-200403000-00012\", ), (\"http://dx.doi.org/10.7554/eLife.46574\", \"doi:10.7554/eLife.46574\"), (\"https://doi.org/10/b6vnmd#anchor\", \"doi:10/b6vnmd\"), # ShortDOI URLs without `10/` prefix not yet supported` (\"https://doi.org/b6vnmd\", \"url:https://doi.org/b6vnmd\"), ( \"https://www.biorxiv.org/about-biorxiv\", \"url:https://www.biorxiv.org/about-biorxiv\", ), (\"https://sci-hub.tw/10.1038/nature19057\", \"doi:10.1038/nature19057\"), (\"https://www.biorxiv.org/content/10.1101/087619v3\", \"doi:10.1101/087619\"), (\"https://www.biorxiv.org/content/10.1101/087619v3.full\", \"doi:10.1101/087619\"), ( \"https://www.biorxiv.org/content/early/2017/08/31/087619.full.pdf\", \"doi:10.1101/087619\", ), ( \"https://www.biorxiv.org/content/10.1101/2019.12.11.872580v1\", \"doi:10.1101/2019.12.11.872580\", ), ( \"https://www.biorxiv.org/content/10.1101/2019.12.11.872580v1.full.pdf+html\", \"doi:10.1101/2019.12.11.872580\", ), ( \"https://www.biorxiv.org/content/10.1101/2019.12.11.872580v1.full.pdf\", \"doi:10.1101/2019.12.11.872580\", ), (\"https://www.ncbi.nlm.nih.gov\", \"url:https://www.ncbi.nlm.nih.gov\"), ( \"https://www.ncbi.nlm.nih.gov/pubmed\", \"url:https://www.ncbi.nlm.nih.gov/pubmed\", ), (\"https://www.ncbi.nlm.nih.gov/pubmed/31233491\", \"pmid:31233491\"), (\"https://www.ncbi.nlm.nih.gov/pmc/\", \"url:https://www.ncbi.nlm.nih.gov/pmc/\"), ( \"https://www.ncbi.nlm.nih.gov/pmc/about/intro/\", \"url:https://www.ncbi.nlm.nih.gov/pmc/about/intro/\", ), (\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4304851/\", \"pmcid:PMC4304851\"), ( \"https://www.wikidata.org/wiki/Wikidata:Main_Page\", \"url:https://www.wikidata.org/wiki/Wikidata:Main_Page\", ), (\"https://www.wikidata.org/wiki/Q50051684\", \"wikidata:Q50051684\"), (\"https://arxiv.org/\", \"url:https://arxiv.org/\"), ( \"https://arxiv.org/list/q-fin/recent\", \"url:https://arxiv.org/list/q-fin/recent\", ), (\"https://arxiv.org/abs/1912.03529v1\", \"arxiv:1912.03529v1\"), (\"https://arxiv.org/pdf/1912.03529v1.pdf\", \"arxiv:1912.03529v1\"), (\"https://arxiv.org/ps/1912.03529v1\", \"arxiv:1912.03529v1\"), (\"https://arxiv.org/abs/math.GT/0309136\", \"arxiv:math.GT/0309136\"), (\"https://arxiv.org/abs/hep-th/9305059\", \"arxiv:hep-th/9305059\"), (\"https://arxiv.org/pdf/hep-th/9305059.pdf\", \"arxiv:hep-th/9305059\"), ] , ) def test_url_to_citekey ( url , citekey ) : assert url_to_citekey ( url ) == citekey","title":"test_url_to_citekey"},{"location":"reference/manubot/cite/tests/test_citekey_api/","text":"Module manubot.cite.tests.test_citekey_api Tests API-level functions in manubot.cite. Both functions are found in citekey.py None View Source \"\"\"Tests API-level functions in manubot.cite. Both functions are found in citekey.py\"\"\" import pytest from manubot.cite import citekey_to_csl_item from manubot.cite.citekey import CiteKey @pytest . mark . parametrize ( \"input_id,expected\" , [ ( \"doi:10.5061/DRYAD.q447c/1\" , \"doi:10.5061/dryad.q447c/1\" ), ( \"doi:10.5061/dryad.q447c/1\" , \"doi:10.5061/dryad.q447c/1\" ), # infers by default ( \"10.5061/dryad.q447c/1\" , \"doi:10.5061/dryad.q447c/1\" ), ( \"doi:10/b6vnmd\" , \"doi:10.1016/s0933-3657(96)00367-3\" ), ( \"doi:10/B6VNMD\" , \"doi:10.1016/s0933-3657(96)00367-3\" ), ( \"doi:10/xxxxxxxxxxxxxYY\" , \"doi:10/xxxxxxxxxxxxxyy\" , ), # passthrough non-existent shortDOI ( \"pmid:24159271\" , \"pubmed:24159271\" ), ( \"isbn:1339919885\" , \"isbn:9781339919881\" ), ( \"isbn:1-339-91988-5\" , \"isbn:9781339919881\" ), ( \"isbn:978-0-387-95069-3\" , \"isbn:9780387950693\" ), ( \"isbn:9780387950938\" , \"isbn:9780387950938\" ), ( \"isbn:1-55860-510-X\" , \"isbn:9781558605107\" ), ( \"isbn:1-55860-510-x\" , \"isbn:9781558605107\" ), ], ) def test_citekey_standard_id ( input_id , expected ): \"\"\" Test CiteKey.standard_id property for common prefixes. \"\"\" citekey = CiteKey ( input_id ) assert citekey . standard_id == expected test_citekey_infer_prefix_params = [ ( \"10.5061/dryad.q447c/1\" , \"doi\" ), ( \"10/b6vnmd\" , \"doi\" ), ( \"24159271\" , \"pubmed\" ), ( \"1\" , \"pubmed\" ), ( \"PMC3041534\" , \"pmc\" ), ( \"Q50051684\" , \"wikidata\" ), ( \"1407.3561v1\" , \"arxiv\" ), ( \"no-prefix-to-infer\" , None ), ] @pytest . mark . parametrize ( \"input_id,prefix\" , test_citekey_infer_prefix_params ) def test_citekey_infer_prefix ( input_id , prefix ): citekey = CiteKey ( input_id , infer_prefix = True ) assert citekey . prefix == prefix @pytest . mark . parametrize ( \"input_id,prefix\" , test_citekey_infer_prefix_params ) def test_citekey_no_infer_prefix ( input_id , prefix ): citekey = CiteKey ( input_id , infer_prefix = False ) assert citekey . prefix is None assert citekey . accession is None @pytest . mark . xfail ( reason = \"https://twitter.com/dhimmel/status/950443969313419264\" ) def test_citekey_to_csl_item_doi_datacite (): citekey = \"doi:10.7287/peerj.preprints.3100v1\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"11cb5HXoY\" assert csl_item [ \"URL\" ] == \"https://doi.org/10.7287/peerj.preprints.3100v1\" assert csl_item [ \"DOI\" ] == \"10.7287/peerj.preprints.3100v1\" assert csl_item [ \"type\" ] == \"report\" assert ( csl_item [ \"title\" ] == \"Sci-Hub provides access to nearly all scholarly literature\" ) authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"family\" ] == \"Himmelstein\" assert authors [ - 1 ][ \"family\" ] == \"Greene\" def test_citekey_to_csl_item_arxiv (): citekey = \"arxiv:cond-mat/0703470v2\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"ES92tcdg\" assert csl_item [ \"URL\" ] == \"https://arxiv.org/abs/cond-mat/0703470v2\" assert csl_item [ \"number\" ] == \"cond-mat/0703470v2\" assert csl_item [ \"version\" ] == \"v2\" assert csl_item [ \"type\" ] == \"report\" assert csl_item [ \"container-title\" ] == \"arXiv\" assert csl_item [ \"title\" ] == \"Portraits of Complex Networks\" authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"literal\" ] == \"J. P. Bagrow\" assert csl_item [ \"DOI\" ] == \"10.1209/0295-5075/81/68004\" def test_citekey_to_csl_item_pmc (): \"\"\" https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/pmc/?format=csl&id=3041534 \"\"\" citekey = f \"pmc:PMC3041534\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"1CGP1eifE\" assert csl_item [ \"URL\" ] == \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041534/\" assert csl_item [ \"container-title-short\" ] == \"Summit Transl Bioinform\" assert ( csl_item [ \"title\" ] == \"Secondary Use of EHR: Data Quality Issues and Informatics Opportunities\" ) authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"family\" ] == \"Botsis\" assert csl_item [ \"PMID\" ] == \"21347133\" assert csl_item [ \"PMCID\" ] == \"PMC3041534\" assert \"generated by Manubot\" in csl_item [ \"note\" ] assert \"standard_id: pmc:PMC3041534\" in csl_item [ \"note\" ] def test_citekey_to_csl_item_pubmed_1 (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=21347133&rettype=full \"\"\" citekey = \"pubmed:21347133\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"o7hs4FTC\" assert csl_item [ \"type\" ] == \"article-journal\" assert csl_item [ \"URL\" ] == \"https://www.ncbi.nlm.nih.gov/pubmed/21347133\" assert csl_item [ \"container-title\" ] == \"Summit on translational bioinformatics\" assert ( csl_item [ \"title\" ] == \"Secondary Use of EHR: Data Quality Issues and Informatics Opportunities.\" ) assert csl_item [ \"issued\" ][ \"date-parts\" ] == [[ 2010 , 3 , 1 ]] authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"given\" ] == \"Taxiarchis\" assert authors [ 0 ][ \"family\" ] == \"Botsis\" assert csl_item [ \"PMID\" ] == \"21347133\" assert csl_item [ \"PMCID\" ] == \"PMC3041534\" def test_citekey_to_csl_item_pubmed_2 (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=27094199&rettype=full \"\"\" citekey = \"pubmed:27094199\" csl_item = citekey_to_csl_item ( citekey ) print ( csl_item ) assert csl_item [ \"id\" ] == \"5v0vabZu\" assert csl_item [ \"type\" ] == \"article-journal\" assert csl_item [ \"URL\" ] == \"https://www.ncbi.nlm.nih.gov/pubmed/27094199\" assert csl_item [ \"container-title\" ] == \"Circulation. Cardiovascular genetics\" assert csl_item [ \"container-title-short\" ] == \"Circ Cardiovasc Genet\" assert csl_item [ \"page\" ] == \"179-84\" assert ( csl_item [ \"title\" ] == \"Genetic Association-Guided Analysis of Gene Networks for the Study of Complex Traits.\" ) assert csl_item [ \"issued\" ][ \"date-parts\" ] == [[ 2016 , 4 ]] authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"given\" ] == \"Casey S\" assert authors [ 0 ][ \"family\" ] == \"Greene\" assert csl_item [ \"PMID\" ] == \"27094199\" assert csl_item [ \"DOI\" ] == \"10.1161/circgenetics.115.001181\" def test_citekey_to_csl_item_pubmed_with_numeric_month (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29028984&rettype=full See https://github.com/manubot/manubot/issues/69 \"\"\" citekey = \"pmid:29028984\" csl_item = citekey_to_csl_item ( citekey ) print ( csl_item ) assert csl_item [ \"issued\" ][ \"date-parts\" ] == [[ 2018 , 3 , 15 ]] def test_citekey_to_csl_item_pubmed_book ( caplog ): \"\"\" Extracting CSL metadata from books in PubMed is not supported. Logic not implemented to parse XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29227604&rettype=full \"\"\" csl_item = citekey_to_csl_item ( \"pmid:29227604\" , log_level = \"ERROR\" ) assert csl_item is None assert ( \"Expected article to be an XML element with tag PubmedArticle, received tag 'PubmedBookArticle'\" in caplog . text ) def test_citekey_to_csl_item_isbn (): csl_item = citekey_to_csl_item ( \"isbn:9780387950693\" ) assert csl_item [ \"type\" ] == \"book\" assert csl_item [ \"title\" ] == \"Complex analysis\" def test_citekey_to_csl_item_clinical_trial (): \"\"\" Test clinicaltrials.gov citation support using CURIEs. https://github.com/manubot/manubot/issues/216 \"\"\" csl_item = citekey_to_csl_item ( \"clinicaltrials:NCT04292899\" ) assert csl_item [ \"title\" ] . startswith ( \"A Phase 3 Randomized Study\" ) assert csl_item [ \"source\" ] . startswith ( \"clinicaltrials.gov\" ) assert csl_item [ \"URL\" ] == \"https://clinicaltrials.gov/ct2/show/NCT04292899\" Variables test_citekey_infer_prefix_params Functions test_citekey_infer_prefix def test_citekey_infer_prefix ( input_id , prefix ) View Source @pytest . mark . parametrize ( \"input_id,prefix\" , test_citekey_infer_prefix_params ) def test_citekey_infer_prefix ( input_id , prefix ) : citekey = CiteKey ( input_id , infer_prefix = True ) assert citekey . prefix == prefix test_citekey_no_infer_prefix def test_citekey_no_infer_prefix ( input_id , prefix ) View Source @pytest . mark . parametrize ( \"input_id,prefix\" , test_citekey_infer_prefix_params ) def test_citekey_no_infer_prefix ( input_id , prefix ) : citekey = CiteKey ( input_id , infer_prefix = False ) assert citekey . prefix is None assert citekey . accession is None test_citekey_standard_id def test_citekey_standard_id ( input_id , expected ) Test CiteKey.standard_id property for common prefixes. View Source @pytest . mark . parametrize ( \"input_id,expected\" , [ (\"doi:10.5061/DRYAD.q447c/1\", \"doi:10.5061/dryad.q447c/1\"), (\"doi:10.5061/dryad.q447c/1\", \"doi:10.5061/dryad.q447c/1\"), # infers by default (\"10.5061/dryad.q447c/1\", \"doi:10.5061/dryad.q447c/1\"), (\"doi:10/b6vnmd\", \"doi:10.1016/s0933-3657(96)00367-3\"), (\"doi:10/B6VNMD\", \"doi:10.1016/s0933-3657(96)00367-3\"), ( \"doi:10/xxxxxxxxxxxxxYY\", \"doi:10/xxxxxxxxxxxxxyy\", ), # passthrough non-existent shortDOI (\"pmid:24159271\", \"pubmed:24159271\"), (\"isbn:1339919885\", \"isbn:9781339919881\"), (\"isbn:1-339-91988-5\", \"isbn:9781339919881\"), (\"isbn:978-0-387-95069-3\", \"isbn:9780387950693\"), (\"isbn:9780387950938\", \"isbn:9780387950938\"), (\"isbn:1-55860-510-X\", \"isbn:9781558605107\"), (\"isbn:1-55860-510-x\", \"isbn:9781558605107\"), ] , ) def test_citekey_standard_id ( input_id , expected ) : \"\"\" Test CiteKey.standard_id property for common prefixes. \"\"\" citekey = CiteKey ( input_id ) assert citekey . standard_id == expected test_citekey_to_csl_item_arxiv def test_citekey_to_csl_item_arxiv ( ) View Source def test_citekey_to_csl_item_arxiv(): citekey = \"arxiv:cond-mat/0703470v2\" csl_item = citekey_to_csl_item(citekey) assert csl_item[\"id\"] == \"ES92tcdg\" assert csl_item[\"URL\"] == \"https://arxiv.org/abs/cond-mat/0703470v2\" assert csl_item[\"number\"] == \"cond-mat/0703470v2\" assert csl_item[\"version\"] == \"v2\" assert csl_item[\"type\"] == \"report\" assert csl_item[\"container-title\"] == \"arXiv\" assert csl_item[\"title\"] == \"Portraits of Complex Networks\" authors = csl_item[\"author\"] assert authors[0][\"literal\"] == \"J. P. Bagrow\" assert csl_item[\"DOI\"] == \"10.1209/0295-5075/81/68004\" test_citekey_to_csl_item_clinical_trial def test_citekey_to_csl_item_clinical_trial ( ) Test clinicaltrials.gov citation support using CURIEs. https://github.com/manubot/manubot/issues/216 View Source def test_citekey_to_csl_item_clinical_trial () : \"\"\" Test clinicaltrials . gov citation support using CURIEs . https : // github . com / manubot / manubot / issues / 216 \"\"\" csl_item = citekey_to_csl_item ( \" clinicaltrials:NCT04292899 \" ) assert csl_item [ \" title \" ]. startswith ( \" A Phase 3 Randomized Study \" ) assert csl_item [ \" source \" ]. startswith ( \" clinicaltrials.gov \" ) assert csl_item [ \" URL \" ] == \" https://clinicaltrials.gov/ct2/show/NCT04292899 \" test_citekey_to_csl_item_doi_datacite def test_citekey_to_csl_item_doi_datacite ( ) View Source @pytest . mark . xfail ( reason = \"https://twitter.com/dhimmel/status/950443969313419264\" ) def test_citekey_to_csl_item_doi_datacite () : citekey = \"doi:10.7287/peerj.preprints.3100v1\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"11cb5HXoY\" assert csl_item [ \"URL\" ] == \"https://doi.org/10.7287/peerj.preprints.3100v1\" assert csl_item [ \"DOI\" ] == \"10.7287/peerj.preprints.3100v1\" assert csl_item [ \"type\" ] == \"report\" assert ( csl_item [ \"title\" ] == \"Sci-Hub provides access to nearly all scholarly literature\" ) authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"family\" ] == \"Himmelstein\" assert authors [ -1 ][ \"family\" ] == \"Greene\" test_citekey_to_csl_item_isbn def test_citekey_to_csl_item_isbn ( ) View Source def test_citekey_to_csl_item_isbn(): csl_item = citekey_to_csl_item(\"isbn:9780387950693\") assert csl_item[\"type\"] == \"book\" assert csl_item[\"title\"] == \"Complex analysis\" test_citekey_to_csl_item_pmc def test_citekey_to_csl_item_pmc ( ) https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/pmc/?format=csl&id=3041534 View Source def test_citekey_to_csl_item_pmc(): \"\"\" https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/pmc/?format=csl&id=3041534 \"\"\" citekey = f\"pmc:PMC3041534\" csl_item = citekey_to_csl_item(citekey) assert csl_item[\"id\"] == \"1CGP1eifE\" assert csl_item[\"URL\"] == \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041534/\" assert csl_item[\"container-title-short\"] == \"Summit Transl Bioinform\" assert ( csl_item[\"title\"] == \"Secondary Use of EHR: Data Quality Issues and Informatics Opportunities\" ) authors = csl_item[\"author\"] assert authors[0][\"family\"] == \"Botsis\" assert csl_item[\"PMID\"] == \"21347133\" assert csl_item[\"PMCID\"] == \"PMC3041534\" assert \"generated by Manubot\" in csl_item[\"note\"] assert \"standard_id: pmc:PMC3041534\" in csl_item[\"note\"] test_citekey_to_csl_item_pubmed_1 def test_citekey_to_csl_item_pubmed_1 ( ) Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=21347133&rettype=full View Source def test_citekey_to_csl_item_pubmed_1(): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=21347133&rettype=full \"\"\" citekey = \"pubmed:21347133\" csl_item = citekey_to_csl_item(citekey) assert csl_item[\"id\"] == \"o7hs4FTC\" assert csl_item[\"type\"] == \"article-journal\" assert csl_item[\"URL\"] == \"https://www.ncbi.nlm.nih.gov/pubmed/21347133\" assert csl_item[\"container-title\"] == \"Summit on translational bioinformatics\" assert ( csl_item[\"title\"] == \"Secondary Use of EHR: Data Quality Issues and Informatics Opportunities.\" ) assert csl_item[\"issued\"][\"date-parts\"] == [[2010, 3, 1]] authors = csl_item[\"author\"] assert authors[0][\"given\"] == \"Taxiarchis\" assert authors[0][\"family\"] == \"Botsis\" assert csl_item[\"PMID\"] == \"21347133\" assert csl_item[\"PMCID\"] == \"PMC3041534\" test_citekey_to_csl_item_pubmed_2 def test_citekey_to_csl_item_pubmed_2 ( ) Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=27094199&rettype=full View Source def test_citekey_to_csl_item_pubmed_2 () : \"\"\" Generated from XML returned by https : // eutils . ncbi . nlm . nih . gov / entrez / eutils / efetch . fcgi ? db = pubmed & id = 27094199 & rettype = full \"\"\" citekey = \" pubmed:27094199 \" csl_item = citekey_to_csl_item ( citekey ) print ( csl_item ) assert csl_item [ \" id \" ] == \" 5v0vabZu \" assert csl_item [ \" type \" ] == \" article-journal \" assert csl_item [ \" URL \" ] == \" https://www.ncbi.nlm.nih.gov/pubmed/27094199 \" assert csl_item [ \" container-title \" ] == \" Circulation. Cardiovascular genetics \" assert csl_item [ \" container-title-short \" ] == \" Circ Cardiovasc Genet \" assert csl_item [ \" page \" ] == \" 179-84 \" assert ( csl_item [ \" title \" ] == \" Genetic Association-Guided Analysis of Gene Networks for the Study of Complex Traits. \" ) assert csl_item [ \" issued \" ][ \" date-parts \" ] == [[ 2016 , 4 ]] authors = csl_item [ \" author \" ] assert authors [ 0 ][ \" given \" ] == \" Casey S \" assert authors [ 0 ][ \" family \" ] == \" Greene \" assert csl_item [ \" PMID \" ] == \" 27094199 \" assert csl_item [ \" DOI \" ] == \" 10.1161/circgenetics.115.001181 \" test_citekey_to_csl_item_pubmed_book def test_citekey_to_csl_item_pubmed_book ( caplog ) Extracting CSL metadata from books in PubMed is not supported. Logic not implemented to parse XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29227604&rettype=full View Source def test_citekey_to_csl_item_pubmed_book(caplog): \"\"\" Extracting CSL metadata from books in PubMed is not supported. Logic not implemented to parse XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29227604&rettype=full \"\"\" csl_item = citekey_to_csl_item(\"pmid:29227604\", log_level=\"ERROR\") assert csl_item is None assert ( \"Expected article to be an XML element with tag PubmedArticle, received tag 'PubmedBookArticle'\" in caplog.text ) test_citekey_to_csl_item_pubmed_with_numeric_month def test_citekey_to_csl_item_pubmed_with_numeric_month ( ) Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29028984&rettype=full See https://github.com/manubot/manubot/issues/69 View Source def test_citekey_to_csl_item_pubmed_with_numeric_month(): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29028984&rettype=full See https://github.com/manubot/manubot/issues/69 \"\"\" citekey = \"pmid:29028984\" csl_item = citekey_to_csl_item(citekey) print(csl_item) assert csl_item[\"issued\"][\"date-parts\"] == [[2018, 3, 15]]","title":"Test Citekey Api"},{"location":"reference/manubot/cite/tests/test_citekey_api/#module-manubotciteteststest_citekey_api","text":"Tests API-level functions in manubot.cite. Both functions are found in citekey.py None View Source \"\"\"Tests API-level functions in manubot.cite. Both functions are found in citekey.py\"\"\" import pytest from manubot.cite import citekey_to_csl_item from manubot.cite.citekey import CiteKey @pytest . mark . parametrize ( \"input_id,expected\" , [ ( \"doi:10.5061/DRYAD.q447c/1\" , \"doi:10.5061/dryad.q447c/1\" ), ( \"doi:10.5061/dryad.q447c/1\" , \"doi:10.5061/dryad.q447c/1\" ), # infers by default ( \"10.5061/dryad.q447c/1\" , \"doi:10.5061/dryad.q447c/1\" ), ( \"doi:10/b6vnmd\" , \"doi:10.1016/s0933-3657(96)00367-3\" ), ( \"doi:10/B6VNMD\" , \"doi:10.1016/s0933-3657(96)00367-3\" ), ( \"doi:10/xxxxxxxxxxxxxYY\" , \"doi:10/xxxxxxxxxxxxxyy\" , ), # passthrough non-existent shortDOI ( \"pmid:24159271\" , \"pubmed:24159271\" ), ( \"isbn:1339919885\" , \"isbn:9781339919881\" ), ( \"isbn:1-339-91988-5\" , \"isbn:9781339919881\" ), ( \"isbn:978-0-387-95069-3\" , \"isbn:9780387950693\" ), ( \"isbn:9780387950938\" , \"isbn:9780387950938\" ), ( \"isbn:1-55860-510-X\" , \"isbn:9781558605107\" ), ( \"isbn:1-55860-510-x\" , \"isbn:9781558605107\" ), ], ) def test_citekey_standard_id ( input_id , expected ): \"\"\" Test CiteKey.standard_id property for common prefixes. \"\"\" citekey = CiteKey ( input_id ) assert citekey . standard_id == expected test_citekey_infer_prefix_params = [ ( \"10.5061/dryad.q447c/1\" , \"doi\" ), ( \"10/b6vnmd\" , \"doi\" ), ( \"24159271\" , \"pubmed\" ), ( \"1\" , \"pubmed\" ), ( \"PMC3041534\" , \"pmc\" ), ( \"Q50051684\" , \"wikidata\" ), ( \"1407.3561v1\" , \"arxiv\" ), ( \"no-prefix-to-infer\" , None ), ] @pytest . mark . parametrize ( \"input_id,prefix\" , test_citekey_infer_prefix_params ) def test_citekey_infer_prefix ( input_id , prefix ): citekey = CiteKey ( input_id , infer_prefix = True ) assert citekey . prefix == prefix @pytest . mark . parametrize ( \"input_id,prefix\" , test_citekey_infer_prefix_params ) def test_citekey_no_infer_prefix ( input_id , prefix ): citekey = CiteKey ( input_id , infer_prefix = False ) assert citekey . prefix is None assert citekey . accession is None @pytest . mark . xfail ( reason = \"https://twitter.com/dhimmel/status/950443969313419264\" ) def test_citekey_to_csl_item_doi_datacite (): citekey = \"doi:10.7287/peerj.preprints.3100v1\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"11cb5HXoY\" assert csl_item [ \"URL\" ] == \"https://doi.org/10.7287/peerj.preprints.3100v1\" assert csl_item [ \"DOI\" ] == \"10.7287/peerj.preprints.3100v1\" assert csl_item [ \"type\" ] == \"report\" assert ( csl_item [ \"title\" ] == \"Sci-Hub provides access to nearly all scholarly literature\" ) authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"family\" ] == \"Himmelstein\" assert authors [ - 1 ][ \"family\" ] == \"Greene\" def test_citekey_to_csl_item_arxiv (): citekey = \"arxiv:cond-mat/0703470v2\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"ES92tcdg\" assert csl_item [ \"URL\" ] == \"https://arxiv.org/abs/cond-mat/0703470v2\" assert csl_item [ \"number\" ] == \"cond-mat/0703470v2\" assert csl_item [ \"version\" ] == \"v2\" assert csl_item [ \"type\" ] == \"report\" assert csl_item [ \"container-title\" ] == \"arXiv\" assert csl_item [ \"title\" ] == \"Portraits of Complex Networks\" authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"literal\" ] == \"J. P. Bagrow\" assert csl_item [ \"DOI\" ] == \"10.1209/0295-5075/81/68004\" def test_citekey_to_csl_item_pmc (): \"\"\" https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/pmc/?format=csl&id=3041534 \"\"\" citekey = f \"pmc:PMC3041534\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"1CGP1eifE\" assert csl_item [ \"URL\" ] == \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041534/\" assert csl_item [ \"container-title-short\" ] == \"Summit Transl Bioinform\" assert ( csl_item [ \"title\" ] == \"Secondary Use of EHR: Data Quality Issues and Informatics Opportunities\" ) authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"family\" ] == \"Botsis\" assert csl_item [ \"PMID\" ] == \"21347133\" assert csl_item [ \"PMCID\" ] == \"PMC3041534\" assert \"generated by Manubot\" in csl_item [ \"note\" ] assert \"standard_id: pmc:PMC3041534\" in csl_item [ \"note\" ] def test_citekey_to_csl_item_pubmed_1 (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=21347133&rettype=full \"\"\" citekey = \"pubmed:21347133\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"o7hs4FTC\" assert csl_item [ \"type\" ] == \"article-journal\" assert csl_item [ \"URL\" ] == \"https://www.ncbi.nlm.nih.gov/pubmed/21347133\" assert csl_item [ \"container-title\" ] == \"Summit on translational bioinformatics\" assert ( csl_item [ \"title\" ] == \"Secondary Use of EHR: Data Quality Issues and Informatics Opportunities.\" ) assert csl_item [ \"issued\" ][ \"date-parts\" ] == [[ 2010 , 3 , 1 ]] authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"given\" ] == \"Taxiarchis\" assert authors [ 0 ][ \"family\" ] == \"Botsis\" assert csl_item [ \"PMID\" ] == \"21347133\" assert csl_item [ \"PMCID\" ] == \"PMC3041534\" def test_citekey_to_csl_item_pubmed_2 (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=27094199&rettype=full \"\"\" citekey = \"pubmed:27094199\" csl_item = citekey_to_csl_item ( citekey ) print ( csl_item ) assert csl_item [ \"id\" ] == \"5v0vabZu\" assert csl_item [ \"type\" ] == \"article-journal\" assert csl_item [ \"URL\" ] == \"https://www.ncbi.nlm.nih.gov/pubmed/27094199\" assert csl_item [ \"container-title\" ] == \"Circulation. Cardiovascular genetics\" assert csl_item [ \"container-title-short\" ] == \"Circ Cardiovasc Genet\" assert csl_item [ \"page\" ] == \"179-84\" assert ( csl_item [ \"title\" ] == \"Genetic Association-Guided Analysis of Gene Networks for the Study of Complex Traits.\" ) assert csl_item [ \"issued\" ][ \"date-parts\" ] == [[ 2016 , 4 ]] authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"given\" ] == \"Casey S\" assert authors [ 0 ][ \"family\" ] == \"Greene\" assert csl_item [ \"PMID\" ] == \"27094199\" assert csl_item [ \"DOI\" ] == \"10.1161/circgenetics.115.001181\" def test_citekey_to_csl_item_pubmed_with_numeric_month (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29028984&rettype=full See https://github.com/manubot/manubot/issues/69 \"\"\" citekey = \"pmid:29028984\" csl_item = citekey_to_csl_item ( citekey ) print ( csl_item ) assert csl_item [ \"issued\" ][ \"date-parts\" ] == [[ 2018 , 3 , 15 ]] def test_citekey_to_csl_item_pubmed_book ( caplog ): \"\"\" Extracting CSL metadata from books in PubMed is not supported. Logic not implemented to parse XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29227604&rettype=full \"\"\" csl_item = citekey_to_csl_item ( \"pmid:29227604\" , log_level = \"ERROR\" ) assert csl_item is None assert ( \"Expected article to be an XML element with tag PubmedArticle, received tag 'PubmedBookArticle'\" in caplog . text ) def test_citekey_to_csl_item_isbn (): csl_item = citekey_to_csl_item ( \"isbn:9780387950693\" ) assert csl_item [ \"type\" ] == \"book\" assert csl_item [ \"title\" ] == \"Complex analysis\" def test_citekey_to_csl_item_clinical_trial (): \"\"\" Test clinicaltrials.gov citation support using CURIEs. https://github.com/manubot/manubot/issues/216 \"\"\" csl_item = citekey_to_csl_item ( \"clinicaltrials:NCT04292899\" ) assert csl_item [ \"title\" ] . startswith ( \"A Phase 3 Randomized Study\" ) assert csl_item [ \"source\" ] . startswith ( \"clinicaltrials.gov\" ) assert csl_item [ \"URL\" ] == \"https://clinicaltrials.gov/ct2/show/NCT04292899\"","title":"Module manubot.cite.tests.test_citekey_api"},{"location":"reference/manubot/cite/tests/test_citekey_api/#variables","text":"test_citekey_infer_prefix_params","title":"Variables"},{"location":"reference/manubot/cite/tests/test_citekey_api/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_infer_prefix","text":"def test_citekey_infer_prefix ( input_id , prefix ) View Source @pytest . mark . parametrize ( \"input_id,prefix\" , test_citekey_infer_prefix_params ) def test_citekey_infer_prefix ( input_id , prefix ) : citekey = CiteKey ( input_id , infer_prefix = True ) assert citekey . prefix == prefix","title":"test_citekey_infer_prefix"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_no_infer_prefix","text":"def test_citekey_no_infer_prefix ( input_id , prefix ) View Source @pytest . mark . parametrize ( \"input_id,prefix\" , test_citekey_infer_prefix_params ) def test_citekey_no_infer_prefix ( input_id , prefix ) : citekey = CiteKey ( input_id , infer_prefix = False ) assert citekey . prefix is None assert citekey . accession is None","title":"test_citekey_no_infer_prefix"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_standard_id","text":"def test_citekey_standard_id ( input_id , expected ) Test CiteKey.standard_id property for common prefixes. View Source @pytest . mark . parametrize ( \"input_id,expected\" , [ (\"doi:10.5061/DRYAD.q447c/1\", \"doi:10.5061/dryad.q447c/1\"), (\"doi:10.5061/dryad.q447c/1\", \"doi:10.5061/dryad.q447c/1\"), # infers by default (\"10.5061/dryad.q447c/1\", \"doi:10.5061/dryad.q447c/1\"), (\"doi:10/b6vnmd\", \"doi:10.1016/s0933-3657(96)00367-3\"), (\"doi:10/B6VNMD\", \"doi:10.1016/s0933-3657(96)00367-3\"), ( \"doi:10/xxxxxxxxxxxxxYY\", \"doi:10/xxxxxxxxxxxxxyy\", ), # passthrough non-existent shortDOI (\"pmid:24159271\", \"pubmed:24159271\"), (\"isbn:1339919885\", \"isbn:9781339919881\"), (\"isbn:1-339-91988-5\", \"isbn:9781339919881\"), (\"isbn:978-0-387-95069-3\", \"isbn:9780387950693\"), (\"isbn:9780387950938\", \"isbn:9780387950938\"), (\"isbn:1-55860-510-X\", \"isbn:9781558605107\"), (\"isbn:1-55860-510-x\", \"isbn:9781558605107\"), ] , ) def test_citekey_standard_id ( input_id , expected ) : \"\"\" Test CiteKey.standard_id property for common prefixes. \"\"\" citekey = CiteKey ( input_id ) assert citekey . standard_id == expected","title":"test_citekey_standard_id"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_arxiv","text":"def test_citekey_to_csl_item_arxiv ( ) View Source def test_citekey_to_csl_item_arxiv(): citekey = \"arxiv:cond-mat/0703470v2\" csl_item = citekey_to_csl_item(citekey) assert csl_item[\"id\"] == \"ES92tcdg\" assert csl_item[\"URL\"] == \"https://arxiv.org/abs/cond-mat/0703470v2\" assert csl_item[\"number\"] == \"cond-mat/0703470v2\" assert csl_item[\"version\"] == \"v2\" assert csl_item[\"type\"] == \"report\" assert csl_item[\"container-title\"] == \"arXiv\" assert csl_item[\"title\"] == \"Portraits of Complex Networks\" authors = csl_item[\"author\"] assert authors[0][\"literal\"] == \"J. P. Bagrow\" assert csl_item[\"DOI\"] == \"10.1209/0295-5075/81/68004\"","title":"test_citekey_to_csl_item_arxiv"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_clinical_trial","text":"def test_citekey_to_csl_item_clinical_trial ( ) Test clinicaltrials.gov citation support using CURIEs. https://github.com/manubot/manubot/issues/216 View Source def test_citekey_to_csl_item_clinical_trial () : \"\"\" Test clinicaltrials . gov citation support using CURIEs . https : // github . com / manubot / manubot / issues / 216 \"\"\" csl_item = citekey_to_csl_item ( \" clinicaltrials:NCT04292899 \" ) assert csl_item [ \" title \" ]. startswith ( \" A Phase 3 Randomized Study \" ) assert csl_item [ \" source \" ]. startswith ( \" clinicaltrials.gov \" ) assert csl_item [ \" URL \" ] == \" https://clinicaltrials.gov/ct2/show/NCT04292899 \"","title":"test_citekey_to_csl_item_clinical_trial"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_doi_datacite","text":"def test_citekey_to_csl_item_doi_datacite ( ) View Source @pytest . mark . xfail ( reason = \"https://twitter.com/dhimmel/status/950443969313419264\" ) def test_citekey_to_csl_item_doi_datacite () : citekey = \"doi:10.7287/peerj.preprints.3100v1\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"11cb5HXoY\" assert csl_item [ \"URL\" ] == \"https://doi.org/10.7287/peerj.preprints.3100v1\" assert csl_item [ \"DOI\" ] == \"10.7287/peerj.preprints.3100v1\" assert csl_item [ \"type\" ] == \"report\" assert ( csl_item [ \"title\" ] == \"Sci-Hub provides access to nearly all scholarly literature\" ) authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"family\" ] == \"Himmelstein\" assert authors [ -1 ][ \"family\" ] == \"Greene\"","title":"test_citekey_to_csl_item_doi_datacite"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_isbn","text":"def test_citekey_to_csl_item_isbn ( ) View Source def test_citekey_to_csl_item_isbn(): csl_item = citekey_to_csl_item(\"isbn:9780387950693\") assert csl_item[\"type\"] == \"book\" assert csl_item[\"title\"] == \"Complex analysis\"","title":"test_citekey_to_csl_item_isbn"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_pmc","text":"def test_citekey_to_csl_item_pmc ( ) https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/pmc/?format=csl&id=3041534 View Source def test_citekey_to_csl_item_pmc(): \"\"\" https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/pmc/?format=csl&id=3041534 \"\"\" citekey = f\"pmc:PMC3041534\" csl_item = citekey_to_csl_item(citekey) assert csl_item[\"id\"] == \"1CGP1eifE\" assert csl_item[\"URL\"] == \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041534/\" assert csl_item[\"container-title-short\"] == \"Summit Transl Bioinform\" assert ( csl_item[\"title\"] == \"Secondary Use of EHR: Data Quality Issues and Informatics Opportunities\" ) authors = csl_item[\"author\"] assert authors[0][\"family\"] == \"Botsis\" assert csl_item[\"PMID\"] == \"21347133\" assert csl_item[\"PMCID\"] == \"PMC3041534\" assert \"generated by Manubot\" in csl_item[\"note\"] assert \"standard_id: pmc:PMC3041534\" in csl_item[\"note\"]","title":"test_citekey_to_csl_item_pmc"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_pubmed_1","text":"def test_citekey_to_csl_item_pubmed_1 ( ) Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=21347133&rettype=full View Source def test_citekey_to_csl_item_pubmed_1(): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=21347133&rettype=full \"\"\" citekey = \"pubmed:21347133\" csl_item = citekey_to_csl_item(citekey) assert csl_item[\"id\"] == \"o7hs4FTC\" assert csl_item[\"type\"] == \"article-journal\" assert csl_item[\"URL\"] == \"https://www.ncbi.nlm.nih.gov/pubmed/21347133\" assert csl_item[\"container-title\"] == \"Summit on translational bioinformatics\" assert ( csl_item[\"title\"] == \"Secondary Use of EHR: Data Quality Issues and Informatics Opportunities.\" ) assert csl_item[\"issued\"][\"date-parts\"] == [[2010, 3, 1]] authors = csl_item[\"author\"] assert authors[0][\"given\"] == \"Taxiarchis\" assert authors[0][\"family\"] == \"Botsis\" assert csl_item[\"PMID\"] == \"21347133\" assert csl_item[\"PMCID\"] == \"PMC3041534\"","title":"test_citekey_to_csl_item_pubmed_1"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_pubmed_2","text":"def test_citekey_to_csl_item_pubmed_2 ( ) Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=27094199&rettype=full View Source def test_citekey_to_csl_item_pubmed_2 () : \"\"\" Generated from XML returned by https : // eutils . ncbi . nlm . nih . gov / entrez / eutils / efetch . fcgi ? db = pubmed & id = 27094199 & rettype = full \"\"\" citekey = \" pubmed:27094199 \" csl_item = citekey_to_csl_item ( citekey ) print ( csl_item ) assert csl_item [ \" id \" ] == \" 5v0vabZu \" assert csl_item [ \" type \" ] == \" article-journal \" assert csl_item [ \" URL \" ] == \" https://www.ncbi.nlm.nih.gov/pubmed/27094199 \" assert csl_item [ \" container-title \" ] == \" Circulation. Cardiovascular genetics \" assert csl_item [ \" container-title-short \" ] == \" Circ Cardiovasc Genet \" assert csl_item [ \" page \" ] == \" 179-84 \" assert ( csl_item [ \" title \" ] == \" Genetic Association-Guided Analysis of Gene Networks for the Study of Complex Traits. \" ) assert csl_item [ \" issued \" ][ \" date-parts \" ] == [[ 2016 , 4 ]] authors = csl_item [ \" author \" ] assert authors [ 0 ][ \" given \" ] == \" Casey S \" assert authors [ 0 ][ \" family \" ] == \" Greene \" assert csl_item [ \" PMID \" ] == \" 27094199 \" assert csl_item [ \" DOI \" ] == \" 10.1161/circgenetics.115.001181 \"","title":"test_citekey_to_csl_item_pubmed_2"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_pubmed_book","text":"def test_citekey_to_csl_item_pubmed_book ( caplog ) Extracting CSL metadata from books in PubMed is not supported. Logic not implemented to parse XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29227604&rettype=full View Source def test_citekey_to_csl_item_pubmed_book(caplog): \"\"\" Extracting CSL metadata from books in PubMed is not supported. Logic not implemented to parse XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29227604&rettype=full \"\"\" csl_item = citekey_to_csl_item(\"pmid:29227604\", log_level=\"ERROR\") assert csl_item is None assert ( \"Expected article to be an XML element with tag PubmedArticle, received tag 'PubmedBookArticle'\" in caplog.text )","title":"test_citekey_to_csl_item_pubmed_book"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_pubmed_with_numeric_month","text":"def test_citekey_to_csl_item_pubmed_with_numeric_month ( ) Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29028984&rettype=full See https://github.com/manubot/manubot/issues/69 View Source def test_citekey_to_csl_item_pubmed_with_numeric_month(): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29028984&rettype=full See https://github.com/manubot/manubot/issues/69 \"\"\" citekey = \"pmid:29028984\" csl_item = citekey_to_csl_item(citekey) print(csl_item) assert csl_item[\"issued\"][\"date-parts\"] == [[2018, 3, 15]]","title":"test_citekey_to_csl_item_pubmed_with_numeric_month"},{"location":"reference/manubot/cite/tests/test_citeproc/","text":"Module manubot.cite.tests.test_citeproc None None View Source import json import pathlib import pytest from manubot.cite.citeproc import remove_jsonschema_errors directory = pathlib . Path ( __file__ ) . parent csl_instances = [ x . name for x in directory . glob ( \"csl-json/*-csl\" )] def load_json ( path ): return json . loads ( path . read_text ( encoding = \"utf-8-sig\" )) def test_json_is_readable_on_windows_in_different_oem_encoding (): name = \"crossref-deep-review-csl\" path = directory / \"csl-json\" / name / \"raw.json\" content = path . read_text ( encoding = \"utf-8-sig\" ) assert content json1 = load_json ( path ) assert json1 @pytest . mark . parametrize ( \"name\" , csl_instances ) def test_remove_jsonschema_errors ( name ): \"\"\" Tests whether CSL pruning of raw.json produces the expected pruned.json. The fidelity of the remove_jsonschema_errors implementation is difficult to assess theoretically, therefore its performance should be evaluated empirically with thorough test coverage. It is recommended that all conceivable conformations of invalid CSL that citeproc methods may generate be tested. To create a new test case, derive pruned.json from raw.json, by manually deleting any invalid fields. Do not use `manubot cite` to directly generate pruned.json as that also relies on remove_jsonschema_errors for pruning. \"\"\" data_dir = directory / \"csl-json\" / name raw = load_json ( data_dir / \"raw.json\" ) expected = load_json ( data_dir / \"pruned.json\" ) pruned = remove_jsonschema_errors ( raw ) assert pruned == expected Variables csl_instances directory Functions load_json def load_json ( path ) View Source def load_json ( path ): return json . loads ( path . read_text ( encoding = \"utf-8-sig\" )) test_json_is_readable_on_windows_in_different_oem_encoding def test_json_is_readable_on_windows_in_different_oem_encoding ( ) View Source def test_json_is_readable_on_windows_in_different_oem_encoding (): name = \"crossref-deep-review-csl\" path = directory / \"csl-json\" / name / \"raw.json\" content = path . read_text ( encoding = \"utf-8-sig\" ) assert content json1 = load_json ( path ) assert json1 test_remove_jsonschema_errors def test_remove_jsonschema_errors ( name ) Tests whether CSL pruning of raw.json produces the expected pruned.json. The fidelity of the remove_jsonschema_errors implementation is difficult to assess theoretically, therefore its performance should be evaluated empirically with thorough test coverage. It is recommended that all conceivable conformations of invalid CSL that citeproc methods may generate be tested. To create a new test case, derive pruned.json from raw.json, by manually deleting any invalid fields. Do not use manubot cite to directly generate pruned.json as that also relies on remove_jsonschema_errors for pruning. View Source @ pytest . mark . parametrize ( \"name\" , csl_instances ) def test_remove_jsonschema_errors ( name ): \"\"\" Tests whether CSL pruning of raw.json produces the expected pruned.json. The fidelity of the remove_jsonschema_errors implementation is difficult to assess theoretically, therefore its performance should be evaluated empirically with thorough test coverage. It is recommended that all conceivable conformations of invalid CSL that citeproc methods may generate be tested. To create a new test case, derive pruned.json from raw.json, by manually deleting any invalid fields. Do not use `manubot cite` to directly generate pruned.json as that also relies on remove_jsonschema_errors for pruning. \"\"\" data_dir = directory / \"csl-json\" / name raw = load_json ( data_dir / \"raw.json\" ) expected = load_json ( data_dir / \"pruned.json\" ) pruned = remove_jsonschema_errors ( raw ) assert pruned == expected","title":"Test Citeproc"},{"location":"reference/manubot/cite/tests/test_citeproc/#module-manubotciteteststest_citeproc","text":"None None View Source import json import pathlib import pytest from manubot.cite.citeproc import remove_jsonschema_errors directory = pathlib . Path ( __file__ ) . parent csl_instances = [ x . name for x in directory . glob ( \"csl-json/*-csl\" )] def load_json ( path ): return json . loads ( path . read_text ( encoding = \"utf-8-sig\" )) def test_json_is_readable_on_windows_in_different_oem_encoding (): name = \"crossref-deep-review-csl\" path = directory / \"csl-json\" / name / \"raw.json\" content = path . read_text ( encoding = \"utf-8-sig\" ) assert content json1 = load_json ( path ) assert json1 @pytest . mark . parametrize ( \"name\" , csl_instances ) def test_remove_jsonschema_errors ( name ): \"\"\" Tests whether CSL pruning of raw.json produces the expected pruned.json. The fidelity of the remove_jsonschema_errors implementation is difficult to assess theoretically, therefore its performance should be evaluated empirically with thorough test coverage. It is recommended that all conceivable conformations of invalid CSL that citeproc methods may generate be tested. To create a new test case, derive pruned.json from raw.json, by manually deleting any invalid fields. Do not use `manubot cite` to directly generate pruned.json as that also relies on remove_jsonschema_errors for pruning. \"\"\" data_dir = directory / \"csl-json\" / name raw = load_json ( data_dir / \"raw.json\" ) expected = load_json ( data_dir / \"pruned.json\" ) pruned = remove_jsonschema_errors ( raw ) assert pruned == expected","title":"Module manubot.cite.tests.test_citeproc"},{"location":"reference/manubot/cite/tests/test_citeproc/#variables","text":"csl_instances directory","title":"Variables"},{"location":"reference/manubot/cite/tests/test_citeproc/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_citeproc/#load_json","text":"def load_json ( path ) View Source def load_json ( path ): return json . loads ( path . read_text ( encoding = \"utf-8-sig\" ))","title":"load_json"},{"location":"reference/manubot/cite/tests/test_citeproc/#test_json_is_readable_on_windows_in_different_oem_encoding","text":"def test_json_is_readable_on_windows_in_different_oem_encoding ( ) View Source def test_json_is_readable_on_windows_in_different_oem_encoding (): name = \"crossref-deep-review-csl\" path = directory / \"csl-json\" / name / \"raw.json\" content = path . read_text ( encoding = \"utf-8-sig\" ) assert content json1 = load_json ( path ) assert json1","title":"test_json_is_readable_on_windows_in_different_oem_encoding"},{"location":"reference/manubot/cite/tests/test_citeproc/#test_remove_jsonschema_errors","text":"def test_remove_jsonschema_errors ( name ) Tests whether CSL pruning of raw.json produces the expected pruned.json. The fidelity of the remove_jsonschema_errors implementation is difficult to assess theoretically, therefore its performance should be evaluated empirically with thorough test coverage. It is recommended that all conceivable conformations of invalid CSL that citeproc methods may generate be tested. To create a new test case, derive pruned.json from raw.json, by manually deleting any invalid fields. Do not use manubot cite to directly generate pruned.json as that also relies on remove_jsonschema_errors for pruning. View Source @ pytest . mark . parametrize ( \"name\" , csl_instances ) def test_remove_jsonschema_errors ( name ): \"\"\" Tests whether CSL pruning of raw.json produces the expected pruned.json. The fidelity of the remove_jsonschema_errors implementation is difficult to assess theoretically, therefore its performance should be evaluated empirically with thorough test coverage. It is recommended that all conceivable conformations of invalid CSL that citeproc methods may generate be tested. To create a new test case, derive pruned.json from raw.json, by manually deleting any invalid fields. Do not use `manubot cite` to directly generate pruned.json as that also relies on remove_jsonschema_errors for pruning. \"\"\" data_dir = directory / \"csl-json\" / name raw = load_json ( data_dir / \"raw.json\" ) expected = load_json ( data_dir / \"pruned.json\" ) pruned = remove_jsonschema_errors ( raw ) assert pruned == expected","title":"test_remove_jsonschema_errors"},{"location":"reference/manubot/cite/tests/test_csl_item/","text":"Module manubot.cite.tests.test_csl_item None None View Source import copy import datetime import pytest from ..csl_item import ( CSL_Item , assert_csl_item_type , date_parts_to_string , date_to_date_parts , ) class Test_CSL_Item : def test_constuctor_empty ( self ): assert CSL_Item () == {} def test_constuctor_by_dict ( self ): d = { \"title\" : \"My book\" } assert CSL_Item ( d ) == d def test_constuctor_by_keyword ( self ): assert CSL_Item ( type = \"journal-article\" ) == { \"type\" : \"journal-article\" } def test_constuctor_by_dict_keyword_combination ( self ): assert CSL_Item ({ \"title\" : \"My journal article\" }, type = \"journal-article\" ) == { \"title\" : \"My journal article\" , \"type\" : \"journal-article\" , } def test_recursive_constructor ( self ): assert CSL_Item ( CSL_Item ()) == {} assert CSL_Item ( CSL_Item ( abc = 1 )) == { \"abc\" : 1 } def test_constructor_leaves_no_inplace_effects ( self ): dict1 = { \"a\" : 1 } ci = CSL_Item ( dict1 , b = 2 ) assert ci == { \"a\" : 1 , \"b\" : 2 } assert dict1 == { \"a\" : 1 } def test_correct_invalid_type ( self ): assert CSL_Item ( type = \"journal-article\" ) . correct_invalid_type () == { \"type\" : \"article-journal\" } def test_set_default_type ( self ): assert CSL_Item () . set_default_type () == { \"type\" : \"entry\" } def test_no_change_of_type ( self ): assert CSL_Item ( type = \"book\" ) . correct_invalid_type () == { \"type\" : \"book\" } assert CSL_Item ( type = \"book\" ) . set_default_type () == { \"type\" : \"book\" } def test_clean ( self ): csl_item = CSL_Item ( type = \"chapter\" , id = \"abc\" ) csl_item . clean ( prune = True ) assert csl_item == { \"type\" : \"chapter\" , \"id\" : \"abc\" } def test_clean_set_id ( self ): csl_item = CSL_Item ( type = \"chapter\" ) csl_item . set_id ( \"abc\" ) csl_item . clean ( prune = True ) assert csl_item == { \"type\" : \"chapter\" , \"id\" : \"abc\" } def test_assert_csl_item_type_passes (): assert_csl_item_type ( CSL_Item ()) def test_assert_csl_item_type_raises_error_on_dict (): with pytest . raises ( TypeError ): assert_csl_item_type ({}) @pytest . mark . parametrize ( [ \"csl_item\" , \"standard_citation\" ], [ pytest . param ( { \"id\" : \"my-id\" , \"standard_citation\" : \"doi:10.7554/elife.32822\" }, \"doi:10.7554/elife.32822\" , id = \"from_standard_citation\" , ), pytest . param ( { \"id\" : \"doi:10.7554/elife.32822\" }, \"doi:10.7554/elife.32822\" , id = \"from_doi_id\" , ), pytest . param ( { \"id\" : \"DOI:10.7554/ELIFE.32822\" }, \"doi:10.7554/elife.32822\" , id = \"from_doi_id_standardize\" , ), pytest . param ({ \"id\" : \"my-id\" }, \"my-id\" , id = \"from_raw_id\" ), ], ) def test_csl_item_standardize_id ( csl_item , standard_citation ): csl_item = CSL_Item ( csl_item ) output = csl_item . standardize_id () assert output is csl_item assert output [ \"id\" ] == standard_citation def test_csl_item_standardize_id_repeated (): csl_item = CSL_Item ( id = \"pmid:1\" , type = \"article-journal\" ) csl_item_1 = copy . deepcopy ( csl_item . standardize_id ()) assert \"standard_citation\" not in \"csl_item\" csl_item_2 = copy . deepcopy ( csl_item . standardize_id ()) assert csl_item_1 == csl_item_2 def test_csl_item_standardize_id_note (): \"\"\" Test extracting standard_id from a note and setting additional note fields. \"\"\" csl_item = CSL_Item ( { \"id\" : \"original-id\" , \"type\" : \"article-journal\" , \"note\" : \"standard_id: doi:10.1371/journal.PPAT.1006256\" , } ) csl_item . standardize_id () assert csl_item [ \"id\" ] == \"doi:10.1371/journal.ppat.1006256\" note_dict = csl_item . note_dict assert note_dict [ \"original_id\" ] == \"original-id\" assert note_dict [ \"original_standard_id\" ] == \"doi:10.1371/journal.PPAT.1006256\" @pytest . mark . parametrize ( [ \"input_note\" , \"text\" , \"dictionary\" , \"expected_note\" ], [ ( \"\" , \"\" , {}, \"\" ), ( None , \"\" , {}, \"\" ), ( \"preexisting note\" , \"\" , {}, \"preexisting note\" ), ( \"preexisting note\" , \"\" , { \"key\" : \"the value\" }, \"preexisting note \\n key: the value\" , ), ( \"\" , \"\" , { \"KEYOKAY\" : \"the value\" }, \"KEYOKAY: the value\" ), ( \"preexisting note\" , \"\" , { \"KEY-NOT-OKAY\" : \"the value\" }, \"preexisting note\" ), ( \"\" , \"\" , { \"standard_citation\" : \"doi:10.7554/elife.32822\" }, \"standard_citation: doi:10.7554/elife.32822\" , ), ( \"This CSL Item was produced using Manubot.\" , \"\" , { \"standard_citation\" : \"doi:10.7554/elife.32822\" }, \"This CSL Item was produced using Manubot. \\n standard_citation: doi:10.7554/elife.32822\" , ), # do not append duplicate lines to a note # https://github.com/manubot/manubot/issues/258 ( \"Already exists.\" , \"Already exists.\" , {}, \"Already exists.\" ), ( \"\" , \"exists: yes\" , { \"exists\" : \"yes\" }, \"exists: yes\" ), ], ) def test_csl_item_note_append ( input_note , text , dictionary , expected_note ): csl_item = CSL_Item ({ \"id\" : \"test_csl_item\" , \"type\" : \"entry\" , \"note\" : input_note }) csl_item . note_append_text ( text ) csl_item . note_append_dict ( dictionary ) assert csl_item . note == expected_note @pytest . mark . parametrize ( [ \"note\" , \"dictionary\" ], [ ( \"This is a note \\n key_one: value \\n KEYTWO: value 2 \" , { \"key_one\" : \"value\" , \"KEYTWO\" : \"value 2\" }, ), ( \"BAD_KEY: good value \\n good-key: good value\" , { \"good-key\" : \"good value\" }), ( \"This is a note {:key_one: value} {:KEYTWO: value 2 } \" , { \"key_one\" : \"value\" , \"KEYTWO\" : \"value 2\" }, ), ( \"{:BAD_KEY: good value} \\n {:good-key: good value}\" , { \"good-key\" : \"good value\" }), ( \"Mixed line-entry and braced-entry syntax \\n GOODKEY: good value \\n {:good-key: good value}\" , { \"GOODKEY\" : \"good value\" , \"good-key\" : \"good value\" }, ), ( \"Note without any key-value pairs\" , {}), ( \"Other text \\n standard_citation: doi:10/ckcj \\n More other text\" , { \"standard_citation\" : \"doi:10/ckcj\" }, ), ], ) def test_csl_item_note_dict ( note , dictionary ): csl_item = CSL_Item ( note = note ) assert csl_item . note_dict == dictionary @pytest . mark . parametrize ( [ \"date\" , \"expected\" ], [ ( None , None ), ( \"\" , None ), ( \"2019\" , [ 2019 ]), ( \"2019-01\" , [ 2019 , 1 ]), ( \"2019-12\" , [ 2019 , 12 ]), ( \"2019-12-31\" , [ 2019 , 12 , 31 ]), ( \"2019-12-99\" , [ 2019 , 12 ]), ( \"2019-12-01\" , [ 2019 , 12 , 1 ]), ( \" 2019-12-01 \" , [ 2019 , 12 , 1 ]), ( \"2019-12-30T23:32:16Z\" , [ 2019 , 12 , 30 ]), ( datetime . date ( 2019 , 12 , 31 ), [ 2019 , 12 , 31 ]), ( datetime . datetime ( 2019 , 12 , 31 , 23 , 32 , 16 ), [ 2019 , 12 , 31 ]), ], ) def test_date_to_date_parts ( date , expected ): assert date_to_date_parts ( date ) == expected @pytest . mark . parametrize ( [ \"expected\" , \"date_parts\" , \"fill\" ], [ ( None , None , False ), ( None , [], True ), ( None , [], False ), ( None , None , True ), ( \"2019\" , [ 2019 ], False ), ( \"2019-01-01\" , [ 2019 ], True ), ( \"2019-01\" , [ 2019 , 1 ], False ), ( \"2019-12\" , [ 2019 , 12 ], False ), ( \"2019-12-01\" , [ 2019 , 12 ], True ), ( \"2019-12-31\" , [ 2019 , 12 , 31 ], False ), ( \"2019-12-31\" , [ 2019 , 12 , 31 ], True ), ( \"2019-12\" , [ 2019 , 12 , \"bad day\" ], False ), ( \"2019-12-01\" , [ 2019 , 12 , 1 ], False ), ( \"2019-12-01\" , [ \"2019\" , \"12\" , \"01\" ], False ), ( \"2019-02-01\" , [ \"2019\" , \"2\" , \"1\" ], False ), ( \"2019-12-31\" , [ 2019 , 12 , 31 , 23 , 32 , 16 ], False ), ( \"2019-12-31\" , [ 2019 , 12 , 31 , 23 , 32 , 16 ], True ), ( \"0080-07-14\" , [ 80 , 7 , 14 ], False ), ( \"0080-07-14\" , [ \"80\" , \"07\" , 14 ], False ), ], ) def test_date_parts_to_string ( expected , date_parts , fill ): assert expected == date_parts_to_string ( date_parts , fill = fill ) Functions test_assert_csl_item_type_passes def test_assert_csl_item_type_passes ( ) View Source def test_assert_csl_item_type_passes(): assert_csl_item_type(CSL_Item()) test_assert_csl_item_type_raises_error_on_dict def test_assert_csl_item_type_raises_error_on_dict ( ) View Source def test_assert_csl_item_type_raises_error_on_dict(): with pytest.raises(TypeError): assert_csl_item_type({}) test_csl_item_note_append def test_csl_item_note_append ( input_note , text , dictionary , expected_note ) View Source @pytest . mark . parametrize ( [ \"input_note\", \"text\", \"dictionary\", \"expected_note\" ] , [ (\"\", \"\", {}, \"\"), (None, \"\", {}, \"\"), (\"preexisting note\", \"\", {}, \"preexisting note\"), ( \"preexisting note\", \"\", {\"key\": \"the value\"}, \"preexisting note\\nkey: the value\", ), (\"\", \"\", {\"KEYOKAY\": \"the value\"}, \"KEYOKAY: the value\"), (\"preexisting note\", \"\", {\"KEY-NOT-OKAY\": \"the value\"}, \"preexisting note\"), ( \"\", \"\", {\"standard_citation\": \"doi:10.7554/elife.32822\"}, \"standard_citation: doi:10.7554/elife.32822\", ), ( \"This CSL Item was produced using Manubot.\", \"\", {\"standard_citation\": \"doi:10.7554/elife.32822\"}, \"This CSL Item was produced using Manubot.\\nstandard_citation: doi:10.7554/elife.32822\", ), # do not append duplicate lines to a note # https://github.com/manubot/manubot/issues/258 (\"Already exists.\", \"Already exists.\", {}, \"Already exists.\"), (\"\", \"exists: yes\", {\"exists\": \"yes\"}, \"exists: yes\"), ] , ) def test_csl_item_note_append ( input_note , text , dictionary , expected_note ) : csl_item = CSL_Item ( { \"id\" : \"test_csl_item\" , \"type\" : \"entry\" , \"note\" : input_note } ) csl_item . note_append_text ( text ) csl_item . note_append_dict ( dictionary ) assert csl_item . note == expected_note test_csl_item_note_dict def test_csl_item_note_dict ( note , dictionary ) View Source @pytest . mark . parametrize ( [ \"note\", \"dictionary\" ] , [ ( \"This is a note\\nkey_one: value\\nKEYTWO: value 2 \", {\"key_one\": \"value\", \"KEYTWO\": \"value 2\"}, ), (\"BAD_KEY: good value\\ngood-key: good value\", {\"good-key\": \"good value\"}), ( \"This is a note {:key_one: value} {:KEYTWO: value 2 } \", {\"key_one\": \"value\", \"KEYTWO\": \"value 2\"}, ), (\"{:BAD_KEY: good value}\\n{:good-key: good value}\", {\"good-key\": \"good value\"}), ( \"Mixed line-entry and braced-entry syntax\\nGOODKEY: good value\\n{:good-key: good value}\", {\"GOODKEY\": \"good value\", \"good-key\": \"good value\"}, ), (\"Note without any key-value pairs\", {}), ( \"Other text\\nstandard_citation: doi:10/ckcj\\nMore other text\", {\"standard_citation\": \"doi:10/ckcj\"}, ), ] , ) def test_csl_item_note_dict ( note , dictionary ) : csl_item = CSL_Item ( note = note ) assert csl_item . note_dict == dictionary test_csl_item_standardize_id def test_csl_item_standardize_id ( csl_item , standard_citation ) View Source @pytest . mark . parametrize ( [ \"csl_item\", \"standard_citation\" ] , [ pytest.param( {\"id\": \"my-id\", \"standard_citation\": \"doi:10.7554/elife.32822\"}, \"doi:10.7554/elife.32822\", id=\"from_standard_citation\", ), pytest.param( {\"id\": \"doi:10.7554/elife.32822\"}, \"doi:10.7554/elife.32822\", id=\"from_doi_id\", ), pytest.param( {\"id\": \"DOI:10.7554/ELIFE.32822\"}, \"doi:10.7554/elife.32822\", id=\"from_doi_id_standardize\", ), pytest.param({\"id\": \"my-id\"}, \"my-id\", id=\"from_raw_id\"), ] , ) def test_csl_item_standardize_id ( csl_item , standard_citation ) : csl_item = CSL_Item ( csl_item ) output = csl_item . standardize_id () assert output is csl_item assert output [ \"id\" ] == standard_citation test_csl_item_standardize_id_note def test_csl_item_standardize_id_note ( ) Test extracting standard_id from a note and setting additional note fields. View Source def test_csl_item_standardize_id_note(): \"\"\" Test extracting standard_id from a note and setting additional note fields. \"\"\" csl_item = CSL_Item( { \"id\": \"original-id\", \"type\": \"article-journal\", \"note\": \"standard_id: doi:10.1371/journal.PPAT.1006256\", } ) csl_item.standardize_id() assert csl_item[\"id\"] == \"doi:10.1371/journal.ppat.1006256\" note_dict = csl_item.note_dict assert note_dict[\"original_id\"] == \"original-id\" assert note_dict[\"original_standard_id\"] == \"doi:10.1371/journal.PPAT.1006256\" test_csl_item_standardize_id_repeated def test_csl_item_standardize_id_repeated ( ) View Source def test_csl_item_standardize_id_repeated(): csl_item = CSL_Item(id=\"pmid:1\", type=\"article-journal\") csl_item_1 = copy.deepcopy(csl_item.standardize_id()) assert \"standard_citation\" not in \"csl_item\" csl_item_2 = copy.deepcopy(csl_item.standardize_id()) assert csl_item_1 == csl_item_2 test_date_parts_to_string def test_date_parts_to_string ( expected , date_parts , fill ) View Source @pytest . mark . parametrize ( [ \"expected\", \"date_parts\", \"fill\" ] , [ (None, None, False), (None, [ ] , True ), ( None , [] , False ), ( None , None , True ), ( \"2019\" , [ 2019 ] , False ), ( \"2019-01-01\" , [ 2019 ] , True ), ( \"2019-01\" , [ 2019, 1 ] , False ), ( \"2019-12\" , [ 2019, 12 ] , False ), ( \"2019-12-01\" , [ 2019, 12 ] , True ), ( \"2019-12-31\" , [ 2019, 12, 31 ] , False ), ( \"2019-12-31\" , [ 2019, 12, 31 ] , True ), ( \"2019-12\" , [ 2019, 12, \"bad day\" ] , False ), ( \"2019-12-01\" , [ 2019, 12, 1 ] , False ), ( \"2019-12-01\" , [ \"2019\", \"12\", \"01\" ] , False ), ( \"2019-02-01\" , [ \"2019\", \"2\", \"1\" ] , False ), ( \"2019-12-31\" , [ 2019, 12, 31, 23, 32, 16 ] , False ), ( \"2019-12-31\" , [ 2019, 12, 31, 23, 32, 16 ] , True ), ( \"0080-07-14\" , [ 80, 7, 14 ] , False ), ( \"0080-07-14\" , [ \"80\", \"07\", 14 ] , False ), ] , ) def test_date_parts_to_string ( expected , date_parts , fill ) : assert expected == date_parts_to_string ( date_parts , fill = fill ) test_date_to_date_parts def test_date_to_date_parts ( date , expected ) View Source @pytest . mark . parametrize ( [ \"date\", \"expected\" ] , [ (None, None), (\"\", None), (\"2019\", [2019 ] ), ( \"2019-01\" , [ 2019, 1 ] ), ( \"2019-12\" , [ 2019, 12 ] ), ( \"2019-12-31\" , [ 2019, 12, 31 ] ), ( \"2019-12-99\" , [ 2019, 12 ] ), ( \"2019-12-01\" , [ 2019, 12, 1 ] ), ( \" 2019-12-01 \" , [ 2019, 12, 1 ] ), ( \"2019-12-30T23:32:16Z\" , [ 2019, 12, 30 ] ), ( datetime . date ( 2019 , 12 , 31 ), [ 2019, 12, 31 ] ), ( datetime . datetime ( 2019 , 12 , 31 , 23 , 32 , 16 ), [ 2019, 12, 31 ] ), ] , ) def test_date_to_date_parts ( date , expected ) : assert date_to_date_parts ( date ) == expected Classes Test_CSL_Item class Test_CSL_Item ( / , * args , ** kwargs ) View Source class Test_CSL_Item : def test_constuctor_empty ( self ): assert CSL_Item () == {} def test_constuctor_by_dict ( self ): d = { \"title\" : \"My book\" } assert CSL_Item ( d ) == d def test_constuctor_by_keyword ( self ): assert CSL_Item ( type = \"journal-article\" ) == { \"type\" : \"journal-article\" } def test_constuctor_by_dict_keyword_combination ( self ): assert CSL_Item ({ \"title\" : \"My journal article\" }, type = \"journal-article\" ) == { \"title\" : \"My journal article\" , \"type\" : \"journal-article\" , } def test_recursive_constructor ( self ): assert CSL_Item ( CSL_Item ()) == {} assert CSL_Item ( CSL_Item ( abc = 1 )) == { \"abc\" : 1 } def test_constructor_leaves_no_inplace_effects ( self ): dict1 = { \"a\" : 1 } ci = CSL_Item ( dict1 , b = 2 ) assert ci == { \"a\" : 1 , \"b\" : 2 } assert dict1 == { \"a\" : 1 } def test_correct_invalid_type ( self ): assert CSL_Item ( type = \"journal-article\" ) . correct_invalid_type () == { \"type\" : \"article-journal\" } def test_set_default_type ( self ): assert CSL_Item () . set_default_type () == { \"type\" : \"entry\" } def test_no_change_of_type ( self ): assert CSL_Item ( type = \"book\" ) . correct_invalid_type () == { \"type\" : \"book\" } assert CSL_Item ( type = \"book\" ) . set_default_type () == { \"type\" : \"book\" } def test_clean ( self ): csl_item = CSL_Item ( type = \"chapter\" , id = \"abc\" ) csl_item . clean ( prune = True ) assert csl_item == { \"type\" : \"chapter\" , \"id\" : \"abc\" } def test_clean_set_id ( self ): csl_item = CSL_Item ( type = \"chapter\" ) csl_item . set_id ( \"abc\" ) csl_item . clean ( prune = True ) assert csl_item == { \"type\" : \"chapter\" , \"id\" : \"abc\" } Methods test_clean def test_clean ( self ) View Source def test_clean(self): csl_item = CSL_Item(type=\"chapter\", id=\"abc\") csl_item.clean(prune=True) assert csl_item == {\"type\": \"chapter\", \"id\": \"abc\"} test_clean_set_id def test_clean_set_id ( self ) View Source def test_clean_set_id(self): csl_item = CSL_Item(type=\"chapter\") csl_item.set_id(\"abc\") csl_item.clean(prune=True) assert csl_item == {\"type\": \"chapter\", \"id\": \"abc\"} test_constructor_leaves_no_inplace_effects def test_constructor_leaves_no_inplace_effects ( self ) View Source def test_constructor_leaves_no_inplace_effects ( self ): dict1 = { \"a\" : 1 } ci = CSL_Item ( dict1 , b = 2 ) assert ci == { \"a\" : 1 , \"b\" : 2 } assert dict1 == { \"a\" : 1 } test_constuctor_by_dict def test_constuctor_by_dict ( self ) View Source def test_constuctor_by_dict ( self ): d = { \"title\" : \"My book\" } assert CSL_Item ( d ) == d test_constuctor_by_dict_keyword_combination def test_constuctor_by_dict_keyword_combination ( self ) View Source def test_constuctor_by_dict_keyword_combination ( self ): assert CSL_Item ({ \"title\" : \"My journal article\" }, type = \"journal-article\" ) == { \"title\" : \"My journal article\" , \"type\" : \"journal-article\" , } test_constuctor_by_keyword def test_constuctor_by_keyword ( self ) View Source def test_constuctor_by_keyword ( self ): assert CSL_Item ( type = \"journal-article\" ) == { \"type\" : \"journal-article\" } test_constuctor_empty def test_constuctor_empty ( self ) View Source def test_constuctor_empty ( self ): assert CSL_Item () == {} test_correct_invalid_type def test_correct_invalid_type ( self ) View Source def test_correct_invalid_type(self): assert CSL_Item(type=\"journal-article\").correct_invalid_type() == { \"type\": \"article-journal\" } test_no_change_of_type def test_no_change_of_type ( self ) View Source def test_no_change_of_type(self): assert CSL_Item(type=\"book\").correct_invalid_type() == {\"type\": \"book\"} assert CSL_Item(type=\"book\").set_default_type() == {\"type\": \"book\"} test_recursive_constructor def test_recursive_constructor ( self ) View Source def test_recursive_constructor ( self ): assert CSL_Item ( CSL_Item ()) == {} assert CSL_Item ( CSL_Item ( abc = 1 )) == { \"abc\" : 1 } test_set_default_type def test_set_default_type ( self ) View Source def test_set_default_type(self): assert CSL_Item().set_default_type() == {\"type\": \"entry\"}","title":"Test Csl Item"},{"location":"reference/manubot/cite/tests/test_csl_item/#module-manubotciteteststest_csl_item","text":"None None View Source import copy import datetime import pytest from ..csl_item import ( CSL_Item , assert_csl_item_type , date_parts_to_string , date_to_date_parts , ) class Test_CSL_Item : def test_constuctor_empty ( self ): assert CSL_Item () == {} def test_constuctor_by_dict ( self ): d = { \"title\" : \"My book\" } assert CSL_Item ( d ) == d def test_constuctor_by_keyword ( self ): assert CSL_Item ( type = \"journal-article\" ) == { \"type\" : \"journal-article\" } def test_constuctor_by_dict_keyword_combination ( self ): assert CSL_Item ({ \"title\" : \"My journal article\" }, type = \"journal-article\" ) == { \"title\" : \"My journal article\" , \"type\" : \"journal-article\" , } def test_recursive_constructor ( self ): assert CSL_Item ( CSL_Item ()) == {} assert CSL_Item ( CSL_Item ( abc = 1 )) == { \"abc\" : 1 } def test_constructor_leaves_no_inplace_effects ( self ): dict1 = { \"a\" : 1 } ci = CSL_Item ( dict1 , b = 2 ) assert ci == { \"a\" : 1 , \"b\" : 2 } assert dict1 == { \"a\" : 1 } def test_correct_invalid_type ( self ): assert CSL_Item ( type = \"journal-article\" ) . correct_invalid_type () == { \"type\" : \"article-journal\" } def test_set_default_type ( self ): assert CSL_Item () . set_default_type () == { \"type\" : \"entry\" } def test_no_change_of_type ( self ): assert CSL_Item ( type = \"book\" ) . correct_invalid_type () == { \"type\" : \"book\" } assert CSL_Item ( type = \"book\" ) . set_default_type () == { \"type\" : \"book\" } def test_clean ( self ): csl_item = CSL_Item ( type = \"chapter\" , id = \"abc\" ) csl_item . clean ( prune = True ) assert csl_item == { \"type\" : \"chapter\" , \"id\" : \"abc\" } def test_clean_set_id ( self ): csl_item = CSL_Item ( type = \"chapter\" ) csl_item . set_id ( \"abc\" ) csl_item . clean ( prune = True ) assert csl_item == { \"type\" : \"chapter\" , \"id\" : \"abc\" } def test_assert_csl_item_type_passes (): assert_csl_item_type ( CSL_Item ()) def test_assert_csl_item_type_raises_error_on_dict (): with pytest . raises ( TypeError ): assert_csl_item_type ({}) @pytest . mark . parametrize ( [ \"csl_item\" , \"standard_citation\" ], [ pytest . param ( { \"id\" : \"my-id\" , \"standard_citation\" : \"doi:10.7554/elife.32822\" }, \"doi:10.7554/elife.32822\" , id = \"from_standard_citation\" , ), pytest . param ( { \"id\" : \"doi:10.7554/elife.32822\" }, \"doi:10.7554/elife.32822\" , id = \"from_doi_id\" , ), pytest . param ( { \"id\" : \"DOI:10.7554/ELIFE.32822\" }, \"doi:10.7554/elife.32822\" , id = \"from_doi_id_standardize\" , ), pytest . param ({ \"id\" : \"my-id\" }, \"my-id\" , id = \"from_raw_id\" ), ], ) def test_csl_item_standardize_id ( csl_item , standard_citation ): csl_item = CSL_Item ( csl_item ) output = csl_item . standardize_id () assert output is csl_item assert output [ \"id\" ] == standard_citation def test_csl_item_standardize_id_repeated (): csl_item = CSL_Item ( id = \"pmid:1\" , type = \"article-journal\" ) csl_item_1 = copy . deepcopy ( csl_item . standardize_id ()) assert \"standard_citation\" not in \"csl_item\" csl_item_2 = copy . deepcopy ( csl_item . standardize_id ()) assert csl_item_1 == csl_item_2 def test_csl_item_standardize_id_note (): \"\"\" Test extracting standard_id from a note and setting additional note fields. \"\"\" csl_item = CSL_Item ( { \"id\" : \"original-id\" , \"type\" : \"article-journal\" , \"note\" : \"standard_id: doi:10.1371/journal.PPAT.1006256\" , } ) csl_item . standardize_id () assert csl_item [ \"id\" ] == \"doi:10.1371/journal.ppat.1006256\" note_dict = csl_item . note_dict assert note_dict [ \"original_id\" ] == \"original-id\" assert note_dict [ \"original_standard_id\" ] == \"doi:10.1371/journal.PPAT.1006256\" @pytest . mark . parametrize ( [ \"input_note\" , \"text\" , \"dictionary\" , \"expected_note\" ], [ ( \"\" , \"\" , {}, \"\" ), ( None , \"\" , {}, \"\" ), ( \"preexisting note\" , \"\" , {}, \"preexisting note\" ), ( \"preexisting note\" , \"\" , { \"key\" : \"the value\" }, \"preexisting note \\n key: the value\" , ), ( \"\" , \"\" , { \"KEYOKAY\" : \"the value\" }, \"KEYOKAY: the value\" ), ( \"preexisting note\" , \"\" , { \"KEY-NOT-OKAY\" : \"the value\" }, \"preexisting note\" ), ( \"\" , \"\" , { \"standard_citation\" : \"doi:10.7554/elife.32822\" }, \"standard_citation: doi:10.7554/elife.32822\" , ), ( \"This CSL Item was produced using Manubot.\" , \"\" , { \"standard_citation\" : \"doi:10.7554/elife.32822\" }, \"This CSL Item was produced using Manubot. \\n standard_citation: doi:10.7554/elife.32822\" , ), # do not append duplicate lines to a note # https://github.com/manubot/manubot/issues/258 ( \"Already exists.\" , \"Already exists.\" , {}, \"Already exists.\" ), ( \"\" , \"exists: yes\" , { \"exists\" : \"yes\" }, \"exists: yes\" ), ], ) def test_csl_item_note_append ( input_note , text , dictionary , expected_note ): csl_item = CSL_Item ({ \"id\" : \"test_csl_item\" , \"type\" : \"entry\" , \"note\" : input_note }) csl_item . note_append_text ( text ) csl_item . note_append_dict ( dictionary ) assert csl_item . note == expected_note @pytest . mark . parametrize ( [ \"note\" , \"dictionary\" ], [ ( \"This is a note \\n key_one: value \\n KEYTWO: value 2 \" , { \"key_one\" : \"value\" , \"KEYTWO\" : \"value 2\" }, ), ( \"BAD_KEY: good value \\n good-key: good value\" , { \"good-key\" : \"good value\" }), ( \"This is a note {:key_one: value} {:KEYTWO: value 2 } \" , { \"key_one\" : \"value\" , \"KEYTWO\" : \"value 2\" }, ), ( \"{:BAD_KEY: good value} \\n {:good-key: good value}\" , { \"good-key\" : \"good value\" }), ( \"Mixed line-entry and braced-entry syntax \\n GOODKEY: good value \\n {:good-key: good value}\" , { \"GOODKEY\" : \"good value\" , \"good-key\" : \"good value\" }, ), ( \"Note without any key-value pairs\" , {}), ( \"Other text \\n standard_citation: doi:10/ckcj \\n More other text\" , { \"standard_citation\" : \"doi:10/ckcj\" }, ), ], ) def test_csl_item_note_dict ( note , dictionary ): csl_item = CSL_Item ( note = note ) assert csl_item . note_dict == dictionary @pytest . mark . parametrize ( [ \"date\" , \"expected\" ], [ ( None , None ), ( \"\" , None ), ( \"2019\" , [ 2019 ]), ( \"2019-01\" , [ 2019 , 1 ]), ( \"2019-12\" , [ 2019 , 12 ]), ( \"2019-12-31\" , [ 2019 , 12 , 31 ]), ( \"2019-12-99\" , [ 2019 , 12 ]), ( \"2019-12-01\" , [ 2019 , 12 , 1 ]), ( \" 2019-12-01 \" , [ 2019 , 12 , 1 ]), ( \"2019-12-30T23:32:16Z\" , [ 2019 , 12 , 30 ]), ( datetime . date ( 2019 , 12 , 31 ), [ 2019 , 12 , 31 ]), ( datetime . datetime ( 2019 , 12 , 31 , 23 , 32 , 16 ), [ 2019 , 12 , 31 ]), ], ) def test_date_to_date_parts ( date , expected ): assert date_to_date_parts ( date ) == expected @pytest . mark . parametrize ( [ \"expected\" , \"date_parts\" , \"fill\" ], [ ( None , None , False ), ( None , [], True ), ( None , [], False ), ( None , None , True ), ( \"2019\" , [ 2019 ], False ), ( \"2019-01-01\" , [ 2019 ], True ), ( \"2019-01\" , [ 2019 , 1 ], False ), ( \"2019-12\" , [ 2019 , 12 ], False ), ( \"2019-12-01\" , [ 2019 , 12 ], True ), ( \"2019-12-31\" , [ 2019 , 12 , 31 ], False ), ( \"2019-12-31\" , [ 2019 , 12 , 31 ], True ), ( \"2019-12\" , [ 2019 , 12 , \"bad day\" ], False ), ( \"2019-12-01\" , [ 2019 , 12 , 1 ], False ), ( \"2019-12-01\" , [ \"2019\" , \"12\" , \"01\" ], False ), ( \"2019-02-01\" , [ \"2019\" , \"2\" , \"1\" ], False ), ( \"2019-12-31\" , [ 2019 , 12 , 31 , 23 , 32 , 16 ], False ), ( \"2019-12-31\" , [ 2019 , 12 , 31 , 23 , 32 , 16 ], True ), ( \"0080-07-14\" , [ 80 , 7 , 14 ], False ), ( \"0080-07-14\" , [ \"80\" , \"07\" , 14 ], False ), ], ) def test_date_parts_to_string ( expected , date_parts , fill ): assert expected == date_parts_to_string ( date_parts , fill = fill )","title":"Module manubot.cite.tests.test_csl_item"},{"location":"reference/manubot/cite/tests/test_csl_item/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_assert_csl_item_type_passes","text":"def test_assert_csl_item_type_passes ( ) View Source def test_assert_csl_item_type_passes(): assert_csl_item_type(CSL_Item())","title":"test_assert_csl_item_type_passes"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_assert_csl_item_type_raises_error_on_dict","text":"def test_assert_csl_item_type_raises_error_on_dict ( ) View Source def test_assert_csl_item_type_raises_error_on_dict(): with pytest.raises(TypeError): assert_csl_item_type({})","title":"test_assert_csl_item_type_raises_error_on_dict"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_csl_item_note_append","text":"def test_csl_item_note_append ( input_note , text , dictionary , expected_note ) View Source @pytest . mark . parametrize ( [ \"input_note\", \"text\", \"dictionary\", \"expected_note\" ] , [ (\"\", \"\", {}, \"\"), (None, \"\", {}, \"\"), (\"preexisting note\", \"\", {}, \"preexisting note\"), ( \"preexisting note\", \"\", {\"key\": \"the value\"}, \"preexisting note\\nkey: the value\", ), (\"\", \"\", {\"KEYOKAY\": \"the value\"}, \"KEYOKAY: the value\"), (\"preexisting note\", \"\", {\"KEY-NOT-OKAY\": \"the value\"}, \"preexisting note\"), ( \"\", \"\", {\"standard_citation\": \"doi:10.7554/elife.32822\"}, \"standard_citation: doi:10.7554/elife.32822\", ), ( \"This CSL Item was produced using Manubot.\", \"\", {\"standard_citation\": \"doi:10.7554/elife.32822\"}, \"This CSL Item was produced using Manubot.\\nstandard_citation: doi:10.7554/elife.32822\", ), # do not append duplicate lines to a note # https://github.com/manubot/manubot/issues/258 (\"Already exists.\", \"Already exists.\", {}, \"Already exists.\"), (\"\", \"exists: yes\", {\"exists\": \"yes\"}, \"exists: yes\"), ] , ) def test_csl_item_note_append ( input_note , text , dictionary , expected_note ) : csl_item = CSL_Item ( { \"id\" : \"test_csl_item\" , \"type\" : \"entry\" , \"note\" : input_note } ) csl_item . note_append_text ( text ) csl_item . note_append_dict ( dictionary ) assert csl_item . note == expected_note","title":"test_csl_item_note_append"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_csl_item_note_dict","text":"def test_csl_item_note_dict ( note , dictionary ) View Source @pytest . mark . parametrize ( [ \"note\", \"dictionary\" ] , [ ( \"This is a note\\nkey_one: value\\nKEYTWO: value 2 \", {\"key_one\": \"value\", \"KEYTWO\": \"value 2\"}, ), (\"BAD_KEY: good value\\ngood-key: good value\", {\"good-key\": \"good value\"}), ( \"This is a note {:key_one: value} {:KEYTWO: value 2 } \", {\"key_one\": \"value\", \"KEYTWO\": \"value 2\"}, ), (\"{:BAD_KEY: good value}\\n{:good-key: good value}\", {\"good-key\": \"good value\"}), ( \"Mixed line-entry and braced-entry syntax\\nGOODKEY: good value\\n{:good-key: good value}\", {\"GOODKEY\": \"good value\", \"good-key\": \"good value\"}, ), (\"Note without any key-value pairs\", {}), ( \"Other text\\nstandard_citation: doi:10/ckcj\\nMore other text\", {\"standard_citation\": \"doi:10/ckcj\"}, ), ] , ) def test_csl_item_note_dict ( note , dictionary ) : csl_item = CSL_Item ( note = note ) assert csl_item . note_dict == dictionary","title":"test_csl_item_note_dict"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_csl_item_standardize_id","text":"def test_csl_item_standardize_id ( csl_item , standard_citation ) View Source @pytest . mark . parametrize ( [ \"csl_item\", \"standard_citation\" ] , [ pytest.param( {\"id\": \"my-id\", \"standard_citation\": \"doi:10.7554/elife.32822\"}, \"doi:10.7554/elife.32822\", id=\"from_standard_citation\", ), pytest.param( {\"id\": \"doi:10.7554/elife.32822\"}, \"doi:10.7554/elife.32822\", id=\"from_doi_id\", ), pytest.param( {\"id\": \"DOI:10.7554/ELIFE.32822\"}, \"doi:10.7554/elife.32822\", id=\"from_doi_id_standardize\", ), pytest.param({\"id\": \"my-id\"}, \"my-id\", id=\"from_raw_id\"), ] , ) def test_csl_item_standardize_id ( csl_item , standard_citation ) : csl_item = CSL_Item ( csl_item ) output = csl_item . standardize_id () assert output is csl_item assert output [ \"id\" ] == standard_citation","title":"test_csl_item_standardize_id"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_csl_item_standardize_id_note","text":"def test_csl_item_standardize_id_note ( ) Test extracting standard_id from a note and setting additional note fields. View Source def test_csl_item_standardize_id_note(): \"\"\" Test extracting standard_id from a note and setting additional note fields. \"\"\" csl_item = CSL_Item( { \"id\": \"original-id\", \"type\": \"article-journal\", \"note\": \"standard_id: doi:10.1371/journal.PPAT.1006256\", } ) csl_item.standardize_id() assert csl_item[\"id\"] == \"doi:10.1371/journal.ppat.1006256\" note_dict = csl_item.note_dict assert note_dict[\"original_id\"] == \"original-id\" assert note_dict[\"original_standard_id\"] == \"doi:10.1371/journal.PPAT.1006256\"","title":"test_csl_item_standardize_id_note"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_csl_item_standardize_id_repeated","text":"def test_csl_item_standardize_id_repeated ( ) View Source def test_csl_item_standardize_id_repeated(): csl_item = CSL_Item(id=\"pmid:1\", type=\"article-journal\") csl_item_1 = copy.deepcopy(csl_item.standardize_id()) assert \"standard_citation\" not in \"csl_item\" csl_item_2 = copy.deepcopy(csl_item.standardize_id()) assert csl_item_1 == csl_item_2","title":"test_csl_item_standardize_id_repeated"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_date_parts_to_string","text":"def test_date_parts_to_string ( expected , date_parts , fill ) View Source @pytest . mark . parametrize ( [ \"expected\", \"date_parts\", \"fill\" ] , [ (None, None, False), (None, [ ] , True ), ( None , [] , False ), ( None , None , True ), ( \"2019\" , [ 2019 ] , False ), ( \"2019-01-01\" , [ 2019 ] , True ), ( \"2019-01\" , [ 2019, 1 ] , False ), ( \"2019-12\" , [ 2019, 12 ] , False ), ( \"2019-12-01\" , [ 2019, 12 ] , True ), ( \"2019-12-31\" , [ 2019, 12, 31 ] , False ), ( \"2019-12-31\" , [ 2019, 12, 31 ] , True ), ( \"2019-12\" , [ 2019, 12, \"bad day\" ] , False ), ( \"2019-12-01\" , [ 2019, 12, 1 ] , False ), ( \"2019-12-01\" , [ \"2019\", \"12\", \"01\" ] , False ), ( \"2019-02-01\" , [ \"2019\", \"2\", \"1\" ] , False ), ( \"2019-12-31\" , [ 2019, 12, 31, 23, 32, 16 ] , False ), ( \"2019-12-31\" , [ 2019, 12, 31, 23, 32, 16 ] , True ), ( \"0080-07-14\" , [ 80, 7, 14 ] , False ), ( \"0080-07-14\" , [ \"80\", \"07\", 14 ] , False ), ] , ) def test_date_parts_to_string ( expected , date_parts , fill ) : assert expected == date_parts_to_string ( date_parts , fill = fill )","title":"test_date_parts_to_string"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_date_to_date_parts","text":"def test_date_to_date_parts ( date , expected ) View Source @pytest . mark . parametrize ( [ \"date\", \"expected\" ] , [ (None, None), (\"\", None), (\"2019\", [2019 ] ), ( \"2019-01\" , [ 2019, 1 ] ), ( \"2019-12\" , [ 2019, 12 ] ), ( \"2019-12-31\" , [ 2019, 12, 31 ] ), ( \"2019-12-99\" , [ 2019, 12 ] ), ( \"2019-12-01\" , [ 2019, 12, 1 ] ), ( \" 2019-12-01 \" , [ 2019, 12, 1 ] ), ( \"2019-12-30T23:32:16Z\" , [ 2019, 12, 30 ] ), ( datetime . date ( 2019 , 12 , 31 ), [ 2019, 12, 31 ] ), ( datetime . datetime ( 2019 , 12 , 31 , 23 , 32 , 16 ), [ 2019, 12, 31 ] ), ] , ) def test_date_to_date_parts ( date , expected ) : assert date_to_date_parts ( date ) == expected","title":"test_date_to_date_parts"},{"location":"reference/manubot/cite/tests/test_csl_item/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_csl_item","text":"class Test_CSL_Item ( / , * args , ** kwargs ) View Source class Test_CSL_Item : def test_constuctor_empty ( self ): assert CSL_Item () == {} def test_constuctor_by_dict ( self ): d = { \"title\" : \"My book\" } assert CSL_Item ( d ) == d def test_constuctor_by_keyword ( self ): assert CSL_Item ( type = \"journal-article\" ) == { \"type\" : \"journal-article\" } def test_constuctor_by_dict_keyword_combination ( self ): assert CSL_Item ({ \"title\" : \"My journal article\" }, type = \"journal-article\" ) == { \"title\" : \"My journal article\" , \"type\" : \"journal-article\" , } def test_recursive_constructor ( self ): assert CSL_Item ( CSL_Item ()) == {} assert CSL_Item ( CSL_Item ( abc = 1 )) == { \"abc\" : 1 } def test_constructor_leaves_no_inplace_effects ( self ): dict1 = { \"a\" : 1 } ci = CSL_Item ( dict1 , b = 2 ) assert ci == { \"a\" : 1 , \"b\" : 2 } assert dict1 == { \"a\" : 1 } def test_correct_invalid_type ( self ): assert CSL_Item ( type = \"journal-article\" ) . correct_invalid_type () == { \"type\" : \"article-journal\" } def test_set_default_type ( self ): assert CSL_Item () . set_default_type () == { \"type\" : \"entry\" } def test_no_change_of_type ( self ): assert CSL_Item ( type = \"book\" ) . correct_invalid_type () == { \"type\" : \"book\" } assert CSL_Item ( type = \"book\" ) . set_default_type () == { \"type\" : \"book\" } def test_clean ( self ): csl_item = CSL_Item ( type = \"chapter\" , id = \"abc\" ) csl_item . clean ( prune = True ) assert csl_item == { \"type\" : \"chapter\" , \"id\" : \"abc\" } def test_clean_set_id ( self ): csl_item = CSL_Item ( type = \"chapter\" ) csl_item . set_id ( \"abc\" ) csl_item . clean ( prune = True ) assert csl_item == { \"type\" : \"chapter\" , \"id\" : \"abc\" }","title":"Test_CSL_Item"},{"location":"reference/manubot/cite/tests/test_csl_item/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_clean","text":"def test_clean ( self ) View Source def test_clean(self): csl_item = CSL_Item(type=\"chapter\", id=\"abc\") csl_item.clean(prune=True) assert csl_item == {\"type\": \"chapter\", \"id\": \"abc\"}","title":"test_clean"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_clean_set_id","text":"def test_clean_set_id ( self ) View Source def test_clean_set_id(self): csl_item = CSL_Item(type=\"chapter\") csl_item.set_id(\"abc\") csl_item.clean(prune=True) assert csl_item == {\"type\": \"chapter\", \"id\": \"abc\"}","title":"test_clean_set_id"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_constructor_leaves_no_inplace_effects","text":"def test_constructor_leaves_no_inplace_effects ( self ) View Source def test_constructor_leaves_no_inplace_effects ( self ): dict1 = { \"a\" : 1 } ci = CSL_Item ( dict1 , b = 2 ) assert ci == { \"a\" : 1 , \"b\" : 2 } assert dict1 == { \"a\" : 1 }","title":"test_constructor_leaves_no_inplace_effects"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_constuctor_by_dict","text":"def test_constuctor_by_dict ( self ) View Source def test_constuctor_by_dict ( self ): d = { \"title\" : \"My book\" } assert CSL_Item ( d ) == d","title":"test_constuctor_by_dict"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_constuctor_by_dict_keyword_combination","text":"def test_constuctor_by_dict_keyword_combination ( self ) View Source def test_constuctor_by_dict_keyword_combination ( self ): assert CSL_Item ({ \"title\" : \"My journal article\" }, type = \"journal-article\" ) == { \"title\" : \"My journal article\" , \"type\" : \"journal-article\" , }","title":"test_constuctor_by_dict_keyword_combination"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_constuctor_by_keyword","text":"def test_constuctor_by_keyword ( self ) View Source def test_constuctor_by_keyword ( self ): assert CSL_Item ( type = \"journal-article\" ) == { \"type\" : \"journal-article\" }","title":"test_constuctor_by_keyword"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_constuctor_empty","text":"def test_constuctor_empty ( self ) View Source def test_constuctor_empty ( self ): assert CSL_Item () == {}","title":"test_constuctor_empty"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_correct_invalid_type","text":"def test_correct_invalid_type ( self ) View Source def test_correct_invalid_type(self): assert CSL_Item(type=\"journal-article\").correct_invalid_type() == { \"type\": \"article-journal\" }","title":"test_correct_invalid_type"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_no_change_of_type","text":"def test_no_change_of_type ( self ) View Source def test_no_change_of_type(self): assert CSL_Item(type=\"book\").correct_invalid_type() == {\"type\": \"book\"} assert CSL_Item(type=\"book\").set_default_type() == {\"type\": \"book\"}","title":"test_no_change_of_type"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_recursive_constructor","text":"def test_recursive_constructor ( self ) View Source def test_recursive_constructor ( self ): assert CSL_Item ( CSL_Item ()) == {} assert CSL_Item ( CSL_Item ( abc = 1 )) == { \"abc\" : 1 }","title":"test_recursive_constructor"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_set_default_type","text":"def test_set_default_type ( self ) View Source def test_set_default_type(self): assert CSL_Item().set_default_type() == {\"type\": \"entry\"}","title":"test_set_default_type"},{"location":"reference/manubot/cite/tests/test_curie/","text":"Module manubot.cite.tests.test_curie None None View Source import pytest from ..curie import curie_to_url , get_namespaces , get_prefix_to_namespace def test_get_namespaces_with_compile_patterns (): \"\"\" To see printed output when this test passes, run: ```shell # show regexes that do not match the sampleId pytest --capture=no --verbose manubot/cite/tests/test_curie.py ``` \"\"\" namespaces = get_namespaces ( compile_patterns = True ) assert isinstance ( namespaces , list ) for namespace in namespaces : # ensure prefix field exists assert namespace [ \"prefix\" ] # check whether compiled pattern matches example identifier # do not fail when no match, since this is an upstream issue # https://github.com/identifiers-org/identifiers-org.github.io/issues/99 compact_id = namespace [ \"sampleId\" ] if namespace [ \"namespaceEmbeddedInLui\" ]: compact_id = f \" { namespace [ 'curiePrefix' ] } : { compact_id } \" match = namespace [ \"compiled_pattern\" ] . fullmatch ( compact_id ) if not match : print ( f \" { namespace [ 'prefix' ] } regex \" f \" { namespace [ 'compiled_pattern' ] . pattern } \" f \"does not match { compact_id } \" ) if namespace [ \"prefix\" ] != namespace [ \"curiePrefix\" ] . lower (): print ( f \" { namespace [ 'prefix' ] } identifiers use \" f \"curiePrefix { namespace [ 'curiePrefix' ] } \" ) def test_get_prefix_to_namespace (): prefix_to_namespace = get_prefix_to_namespace () assert isinstance ( prefix_to_namespace , dict ) assert \"doid\" in prefix_to_namespace namespace = prefix_to_namespace [ \"doid\" ] namespace [ \"curiePrefix\" ] = \"DOID\" @pytest . mark . parametrize ( \"curie, expected\" , [ ( \"doi:10.1038/nbt1156\" , \"https://identifiers.org/doi:10.1038/nbt1156\" ), ( \"DOI:10.1038/nbt1156\" , \"https://identifiers.org/doi:10.1038/nbt1156\" ), ( \"arXiv:0807.4956v1\" , \"https://identifiers.org/arxiv:0807.4956v1\" ), ( \"taxonomy:9606\" , \"https://identifiers.org/taxonomy:9606\" ), ( \"CHEBI:36927\" , \"https://identifiers.org/CHEBI:36927\" ), ( \"ChEBI:36927\" , \"https://identifiers.org/CHEBI:36927\" ), ( \"DOID:11337\" , \"https://identifiers.org/DOID:11337\" ), ( \"doid:11337\" , \"https://identifiers.org/DOID:11337\" ), ( \"clinicaltrials:NCT00222573\" , \"https://identifiers.org/clinicaltrials:NCT00222573\" , ), # https://github.com/identifiers-org/identifiers-org.github.io/issues/99#issuecomment-614690283 pytest . param ( \"GRO:0007133\" , \"https://identifiers.org/GRO:0007133\" , id = \"gramene.growthstage\" , ), ], ) def test_curie_to_url ( curie , expected ): url = curie_to_url ( curie ) assert url == expected def test_curie_to_url_bad_curie (): with pytest . raises ( ValueError ): curie_to_url ( \"this.is.not:a_curie\" ) Functions test_curie_to_url def test_curie_to_url ( curie , expected ) View Source @pytest . mark . parametrize ( \"curie, expected\" , [ (\"doi:10.1038/nbt1156\", \"https://identifiers.org/doi:10.1038/nbt1156\"), (\"DOI:10.1038/nbt1156\", \"https://identifiers.org/doi:10.1038/nbt1156\"), (\"arXiv:0807.4956v1\", \"https://identifiers.org/arxiv:0807.4956v1\"), (\"taxonomy:9606\", \"https://identifiers.org/taxonomy:9606\"), (\"CHEBI:36927\", \"https://identifiers.org/CHEBI:36927\"), (\"ChEBI:36927\", \"https://identifiers.org/CHEBI:36927\"), (\"DOID:11337\", \"https://identifiers.org/DOID:11337\"), (\"doid:11337\", \"https://identifiers.org/DOID:11337\"), ( \"clinicaltrials:NCT00222573\", \"https://identifiers.org/clinicaltrials:NCT00222573\", ), # https://github.com/identifiers-org/identifiers-org.github.io/issues/99#issuecomment-614690283 pytest.param( \"GRO:0007133\", \"https://identifiers.org/GRO:0007133\", id=\"gramene.growthstage\", ), ] , ) def test_curie_to_url ( curie , expected ) : url = curie_to_url ( curie ) assert url == expected test_curie_to_url_bad_curie def test_curie_to_url_bad_curie ( ) View Source def test_curie_to_url_bad_curie(): with pytest.raises(ValueError): curie_to_url(\"this.is.not:a_curie\") test_get_namespaces_with_compile_patterns def test_get_namespaces_with_compile_patterns ( ) To see printed output when this test passes, run: # show regexes that do not match the sampleId pytest --capture = no --verbose manubot/cite/tests/test_curie.py View Source def test_get_namespaces_with_compile_patterns () : \"\"\" To see printed output when this test passes , run: `` `shell # show regexes that do not match the sampleId pytest -- capture = no -- verbose manubot / cite / tests / test_curie . py ``` \"\"\" namespaces = get_namespaces ( compile_patterns = True ) assert isinstance ( namespaces , list ) for namespace in namespaces: # ensure prefix field exists assert namespace [ \"prefix\" ] # check whether compiled pattern matches example identifier # do not fail when no match , since this is an upstream issue # https: //github.com/identifiers-org/identifiers-org.github.io/issues/99 compact_id = namespace [ \"sampleId\" ] if namespace [ \"namespaceEmbeddedInLui\" ] : compact_id = f \"{namespace['curiePrefix']}:{compact_id}\" match = namespace [ \"compiled_pattern\" ]. fullmatch ( compact_id ) if not match: print ( f \"{namespace['prefix']} regex \" f \"{namespace['compiled_pattern'].pattern} \" f \"does not match {compact_id}\" ) if namespace [ \"prefix\" ] != namespace [ \"curiePrefix\" ]. lower () : print ( f \"{namespace['prefix']} identifiers use \" f \"curiePrefix {namespace['curiePrefix']}\" ) test_get_prefix_to_namespace def test_get_prefix_to_namespace ( ) View Source def test_get_prefix_to_namespace(): prefix_to_namespace = get_prefix_to_namespace() assert isinstance(prefix_to_namespace, dict) assert \"doid\" in prefix_to_namespace namespace = prefix_to_namespace[\"doid\"] namespace[\"curiePrefix\"] = \"DOID\"","title":"Test Curie"},{"location":"reference/manubot/cite/tests/test_curie/#module-manubotciteteststest_curie","text":"None None View Source import pytest from ..curie import curie_to_url , get_namespaces , get_prefix_to_namespace def test_get_namespaces_with_compile_patterns (): \"\"\" To see printed output when this test passes, run: ```shell # show regexes that do not match the sampleId pytest --capture=no --verbose manubot/cite/tests/test_curie.py ``` \"\"\" namespaces = get_namespaces ( compile_patterns = True ) assert isinstance ( namespaces , list ) for namespace in namespaces : # ensure prefix field exists assert namespace [ \"prefix\" ] # check whether compiled pattern matches example identifier # do not fail when no match, since this is an upstream issue # https://github.com/identifiers-org/identifiers-org.github.io/issues/99 compact_id = namespace [ \"sampleId\" ] if namespace [ \"namespaceEmbeddedInLui\" ]: compact_id = f \" { namespace [ 'curiePrefix' ] } : { compact_id } \" match = namespace [ \"compiled_pattern\" ] . fullmatch ( compact_id ) if not match : print ( f \" { namespace [ 'prefix' ] } regex \" f \" { namespace [ 'compiled_pattern' ] . pattern } \" f \"does not match { compact_id } \" ) if namespace [ \"prefix\" ] != namespace [ \"curiePrefix\" ] . lower (): print ( f \" { namespace [ 'prefix' ] } identifiers use \" f \"curiePrefix { namespace [ 'curiePrefix' ] } \" ) def test_get_prefix_to_namespace (): prefix_to_namespace = get_prefix_to_namespace () assert isinstance ( prefix_to_namespace , dict ) assert \"doid\" in prefix_to_namespace namespace = prefix_to_namespace [ \"doid\" ] namespace [ \"curiePrefix\" ] = \"DOID\" @pytest . mark . parametrize ( \"curie, expected\" , [ ( \"doi:10.1038/nbt1156\" , \"https://identifiers.org/doi:10.1038/nbt1156\" ), ( \"DOI:10.1038/nbt1156\" , \"https://identifiers.org/doi:10.1038/nbt1156\" ), ( \"arXiv:0807.4956v1\" , \"https://identifiers.org/arxiv:0807.4956v1\" ), ( \"taxonomy:9606\" , \"https://identifiers.org/taxonomy:9606\" ), ( \"CHEBI:36927\" , \"https://identifiers.org/CHEBI:36927\" ), ( \"ChEBI:36927\" , \"https://identifiers.org/CHEBI:36927\" ), ( \"DOID:11337\" , \"https://identifiers.org/DOID:11337\" ), ( \"doid:11337\" , \"https://identifiers.org/DOID:11337\" ), ( \"clinicaltrials:NCT00222573\" , \"https://identifiers.org/clinicaltrials:NCT00222573\" , ), # https://github.com/identifiers-org/identifiers-org.github.io/issues/99#issuecomment-614690283 pytest . param ( \"GRO:0007133\" , \"https://identifiers.org/GRO:0007133\" , id = \"gramene.growthstage\" , ), ], ) def test_curie_to_url ( curie , expected ): url = curie_to_url ( curie ) assert url == expected def test_curie_to_url_bad_curie (): with pytest . raises ( ValueError ): curie_to_url ( \"this.is.not:a_curie\" )","title":"Module manubot.cite.tests.test_curie"},{"location":"reference/manubot/cite/tests/test_curie/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_curie/#test_curie_to_url","text":"def test_curie_to_url ( curie , expected ) View Source @pytest . mark . parametrize ( \"curie, expected\" , [ (\"doi:10.1038/nbt1156\", \"https://identifiers.org/doi:10.1038/nbt1156\"), (\"DOI:10.1038/nbt1156\", \"https://identifiers.org/doi:10.1038/nbt1156\"), (\"arXiv:0807.4956v1\", \"https://identifiers.org/arxiv:0807.4956v1\"), (\"taxonomy:9606\", \"https://identifiers.org/taxonomy:9606\"), (\"CHEBI:36927\", \"https://identifiers.org/CHEBI:36927\"), (\"ChEBI:36927\", \"https://identifiers.org/CHEBI:36927\"), (\"DOID:11337\", \"https://identifiers.org/DOID:11337\"), (\"doid:11337\", \"https://identifiers.org/DOID:11337\"), ( \"clinicaltrials:NCT00222573\", \"https://identifiers.org/clinicaltrials:NCT00222573\", ), # https://github.com/identifiers-org/identifiers-org.github.io/issues/99#issuecomment-614690283 pytest.param( \"GRO:0007133\", \"https://identifiers.org/GRO:0007133\", id=\"gramene.growthstage\", ), ] , ) def test_curie_to_url ( curie , expected ) : url = curie_to_url ( curie ) assert url == expected","title":"test_curie_to_url"},{"location":"reference/manubot/cite/tests/test_curie/#test_curie_to_url_bad_curie","text":"def test_curie_to_url_bad_curie ( ) View Source def test_curie_to_url_bad_curie(): with pytest.raises(ValueError): curie_to_url(\"this.is.not:a_curie\")","title":"test_curie_to_url_bad_curie"},{"location":"reference/manubot/cite/tests/test_curie/#test_get_namespaces_with_compile_patterns","text":"def test_get_namespaces_with_compile_patterns ( ) To see printed output when this test passes, run: # show regexes that do not match the sampleId pytest --capture = no --verbose manubot/cite/tests/test_curie.py View Source def test_get_namespaces_with_compile_patterns () : \"\"\" To see printed output when this test passes , run: `` `shell # show regexes that do not match the sampleId pytest -- capture = no -- verbose manubot / cite / tests / test_curie . py ``` \"\"\" namespaces = get_namespaces ( compile_patterns = True ) assert isinstance ( namespaces , list ) for namespace in namespaces: # ensure prefix field exists assert namespace [ \"prefix\" ] # check whether compiled pattern matches example identifier # do not fail when no match , since this is an upstream issue # https: //github.com/identifiers-org/identifiers-org.github.io/issues/99 compact_id = namespace [ \"sampleId\" ] if namespace [ \"namespaceEmbeddedInLui\" ] : compact_id = f \"{namespace['curiePrefix']}:{compact_id}\" match = namespace [ \"compiled_pattern\" ]. fullmatch ( compact_id ) if not match: print ( f \"{namespace['prefix']} regex \" f \"{namespace['compiled_pattern'].pattern} \" f \"does not match {compact_id}\" ) if namespace [ \"prefix\" ] != namespace [ \"curiePrefix\" ]. lower () : print ( f \"{namespace['prefix']} identifiers use \" f \"curiePrefix {namespace['curiePrefix']}\" )","title":"test_get_namespaces_with_compile_patterns"},{"location":"reference/manubot/cite/tests/test_curie/#test_get_prefix_to_namespace","text":"def test_get_prefix_to_namespace ( ) View Source def test_get_prefix_to_namespace(): prefix_to_namespace = get_prefix_to_namespace() assert isinstance(prefix_to_namespace, dict) assert \"doid\" in prefix_to_namespace namespace = prefix_to_namespace[\"doid\"] namespace[\"curiePrefix\"] = \"DOID\"","title":"test_get_prefix_to_namespace"},{"location":"reference/manubot/cite/tests/test_doi/","text":"Module manubot.cite.tests.test_doi None None View Source import pytest from manubot.cite.doi import ( expand_short_doi , get_doi_csl_item , get_doi_csl_item_crosscite , get_doi_csl_item_zotero , ) def test_expand_short_doi (): doi = expand_short_doi ( \"10/b6vnmd\" ) assert doi == \"10.1016/s0933-3657(96)00367-3\" def test_expand_short_doi_invalid (): with pytest . raises ( ValueError , match = \"Handle not found. Double check short_doi\" ): expand_short_doi ( \"10/b6vnmdxxxxxx\" ) def test_expand_short_doi_not_short (): with pytest . raises ( ValueError , match = \"shortDOIs start with `10/`\" ): expand_short_doi ( \"10.1016/S0933-3657(96)00367-3\" ) def test_get_doi_csl_item_crosscite (): doi = \"10.1101/142760\" csl_item = get_doi_csl_item_crosscite ( doi ) assert isinstance ( csl_item , dict ) csl_item [ \"publisher\" ] == \"Cold Spring Harbor Laboratory\" def test_get_doi_csl_item_zotero (): \"\"\" As of 2019-10-25, DOI Content Negotiation (i.e. crosscite) encodes the consortium in author.name rather than author.literal. Zotero translation-server encodes the consortium in author.family, which is better than nothing since it's a valid CSL JSON field. https://github.com/manubot/manubot/issues/158 \"\"\" doi = \"10.1038/ng.3834\" csl_item = get_doi_csl_item_zotero ( doi ) assert isinstance ( csl_item , dict ) csl_item [ \"author\" ][ 9 ][ \"family\" ] == \"GTEx Consortium\" def test_get_doi_csl_item (): \"\"\" Test URL is set with shortDOI when calling get_doi_csl_item. \"\"\" doi = \"10.1101/142760\" csl_item = get_doi_csl_item ( doi ) assert isinstance ( csl_item , dict ) assert csl_item [ \"URL\" ] == \"https://doi.org/gbpvh5\" def test_get_doi_crosscite_with_consortium_author (): \"\"\" Make sure the author \"GTEx Consortium\" is properly encoded using the `author.literal` CSL JSON field. References: - <https://github.com/manubot/manubot/issues/158> - <https://github.com/crosscite/content-negotiation/issues/92> \"\"\" doi = \"10.1038/ng.3834\" csl_item = get_doi_csl_item_crosscite ( doi ) assert isinstance ( csl_item , dict ) assert any ( author . get ( \"literal\" ) == \"GTEx Consortium\" for author in csl_item [ \"author\" ] ) Functions test_expand_short_doi def test_expand_short_doi ( ) View Source def test_expand_short_doi(): doi = expand_short_doi(\"10/b6vnmd\") assert doi == \"10.1016/s0933-3657(96)00367-3\" test_expand_short_doi_invalid def test_expand_short_doi_invalid ( ) View Source def test_expand_short_doi_invalid(): with pytest.raises(ValueError, match=\"Handle not found. Double check short_doi\"): expand_short_doi(\"10/b6vnmdxxxxxx\") test_expand_short_doi_not_short def test_expand_short_doi_not_short ( ) View Source def test_expand_short_doi_not_short(): with pytest.raises(ValueError, match=\"shortDOIs start with `10/`\"): expand_short_doi(\"10.1016/S0933-3657(96)00367-3\") test_get_doi_crosscite_with_consortium_author def test_get_doi_crosscite_with_consortium_author ( ) Make sure the author \"GTEx Consortium\" is properly encoded using the author.literal CSL JSON field. References: https://github.com/manubot/manubot/issues/158 https://github.com/crosscite/content-negotiation/issues/92 View Source def test_get_doi_crosscite_with_consortium_author () : \"\"\" Make sure the author \" GTEx Consortium \" is properly encoded using the ` author . literal ` CSL JSON field . References : - < https : // github . com / manubot / manubot / issues / 158 > - < https : // github . com / crosscite / content - negotiation / issues / 92 > \"\"\" doi = \" 10.1038/ng.3834 \" csl_item = get_doi_csl_item_crosscite ( doi ) assert isinstance ( csl_item , dict ) assert any ( author . get ( \" literal \" ) == \" GTEx Consortium \" for author in csl_item [ \" author \" ] ) test_get_doi_csl_item def test_get_doi_csl_item ( ) Test URL is set with shortDOI when calling get_doi_csl_item. View Source def test_get_doi_csl_item(): \"\"\" Test URL is set with shortDOI when calling get_doi_csl_item. \"\"\" doi = \"10.1101/142760\" csl_item = get_doi_csl_item(doi) assert isinstance(csl_item, dict) assert csl_item[\"URL\"] == \"https://doi.org/gbpvh5\" test_get_doi_csl_item_crosscite def test_get_doi_csl_item_crosscite ( ) View Source def test_get_doi_csl_item_crosscite(): doi = \"10.1101/142760\" csl_item = get_doi_csl_item_crosscite(doi) assert isinstance(csl_item, dict) csl_item[\"publisher\"] == \"Cold Spring Harbor Laboratory\" test_get_doi_csl_item_zotero def test_get_doi_csl_item_zotero ( ) As of 2019-10-25, DOI Content Negotiation (i.e. crosscite) encodes the consortium in author.name rather than author.literal. Zotero translation-server encodes the consortium in author.family, which is better than nothing since it's a valid CSL JSON field. https://github.com/manubot/manubot/issues/158 View Source def test_get_doi_csl_item_zotero(): \"\"\" As of 2019-10-25, DOI Content Negotiation (i.e. crosscite) encodes the consortium in author.name rather than author.literal. Zotero translation-server encodes the consortium in author.family, which is better than nothing since it's a valid CSL JSON field. https://github.com/manubot/manubot/issues/158 \"\"\" doi = \"10.1038/ng.3834\" csl_item = get_doi_csl_item_zotero(doi) assert isinstance(csl_item, dict) csl_item[\"author\"][9][\"family\"] == \"GTEx Consortium\"","title":"Test Doi"},{"location":"reference/manubot/cite/tests/test_doi/#module-manubotciteteststest_doi","text":"None None View Source import pytest from manubot.cite.doi import ( expand_short_doi , get_doi_csl_item , get_doi_csl_item_crosscite , get_doi_csl_item_zotero , ) def test_expand_short_doi (): doi = expand_short_doi ( \"10/b6vnmd\" ) assert doi == \"10.1016/s0933-3657(96)00367-3\" def test_expand_short_doi_invalid (): with pytest . raises ( ValueError , match = \"Handle not found. Double check short_doi\" ): expand_short_doi ( \"10/b6vnmdxxxxxx\" ) def test_expand_short_doi_not_short (): with pytest . raises ( ValueError , match = \"shortDOIs start with `10/`\" ): expand_short_doi ( \"10.1016/S0933-3657(96)00367-3\" ) def test_get_doi_csl_item_crosscite (): doi = \"10.1101/142760\" csl_item = get_doi_csl_item_crosscite ( doi ) assert isinstance ( csl_item , dict ) csl_item [ \"publisher\" ] == \"Cold Spring Harbor Laboratory\" def test_get_doi_csl_item_zotero (): \"\"\" As of 2019-10-25, DOI Content Negotiation (i.e. crosscite) encodes the consortium in author.name rather than author.literal. Zotero translation-server encodes the consortium in author.family, which is better than nothing since it's a valid CSL JSON field. https://github.com/manubot/manubot/issues/158 \"\"\" doi = \"10.1038/ng.3834\" csl_item = get_doi_csl_item_zotero ( doi ) assert isinstance ( csl_item , dict ) csl_item [ \"author\" ][ 9 ][ \"family\" ] == \"GTEx Consortium\" def test_get_doi_csl_item (): \"\"\" Test URL is set with shortDOI when calling get_doi_csl_item. \"\"\" doi = \"10.1101/142760\" csl_item = get_doi_csl_item ( doi ) assert isinstance ( csl_item , dict ) assert csl_item [ \"URL\" ] == \"https://doi.org/gbpvh5\" def test_get_doi_crosscite_with_consortium_author (): \"\"\" Make sure the author \"GTEx Consortium\" is properly encoded using the `author.literal` CSL JSON field. References: - <https://github.com/manubot/manubot/issues/158> - <https://github.com/crosscite/content-negotiation/issues/92> \"\"\" doi = \"10.1038/ng.3834\" csl_item = get_doi_csl_item_crosscite ( doi ) assert isinstance ( csl_item , dict ) assert any ( author . get ( \"literal\" ) == \"GTEx Consortium\" for author in csl_item [ \"author\" ] )","title":"Module manubot.cite.tests.test_doi"},{"location":"reference/manubot/cite/tests/test_doi/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_doi/#test_expand_short_doi","text":"def test_expand_short_doi ( ) View Source def test_expand_short_doi(): doi = expand_short_doi(\"10/b6vnmd\") assert doi == \"10.1016/s0933-3657(96)00367-3\"","title":"test_expand_short_doi"},{"location":"reference/manubot/cite/tests/test_doi/#test_expand_short_doi_invalid","text":"def test_expand_short_doi_invalid ( ) View Source def test_expand_short_doi_invalid(): with pytest.raises(ValueError, match=\"Handle not found. Double check short_doi\"): expand_short_doi(\"10/b6vnmdxxxxxx\")","title":"test_expand_short_doi_invalid"},{"location":"reference/manubot/cite/tests/test_doi/#test_expand_short_doi_not_short","text":"def test_expand_short_doi_not_short ( ) View Source def test_expand_short_doi_not_short(): with pytest.raises(ValueError, match=\"shortDOIs start with `10/`\"): expand_short_doi(\"10.1016/S0933-3657(96)00367-3\")","title":"test_expand_short_doi_not_short"},{"location":"reference/manubot/cite/tests/test_doi/#test_get_doi_crosscite_with_consortium_author","text":"def test_get_doi_crosscite_with_consortium_author ( ) Make sure the author \"GTEx Consortium\" is properly encoded using the author.literal CSL JSON field. References: https://github.com/manubot/manubot/issues/158 https://github.com/crosscite/content-negotiation/issues/92 View Source def test_get_doi_crosscite_with_consortium_author () : \"\"\" Make sure the author \" GTEx Consortium \" is properly encoded using the ` author . literal ` CSL JSON field . References : - < https : // github . com / manubot / manubot / issues / 158 > - < https : // github . com / crosscite / content - negotiation / issues / 92 > \"\"\" doi = \" 10.1038/ng.3834 \" csl_item = get_doi_csl_item_crosscite ( doi ) assert isinstance ( csl_item , dict ) assert any ( author . get ( \" literal \" ) == \" GTEx Consortium \" for author in csl_item [ \" author \" ] )","title":"test_get_doi_crosscite_with_consortium_author"},{"location":"reference/manubot/cite/tests/test_doi/#test_get_doi_csl_item","text":"def test_get_doi_csl_item ( ) Test URL is set with shortDOI when calling get_doi_csl_item. View Source def test_get_doi_csl_item(): \"\"\" Test URL is set with shortDOI when calling get_doi_csl_item. \"\"\" doi = \"10.1101/142760\" csl_item = get_doi_csl_item(doi) assert isinstance(csl_item, dict) assert csl_item[\"URL\"] == \"https://doi.org/gbpvh5\"","title":"test_get_doi_csl_item"},{"location":"reference/manubot/cite/tests/test_doi/#test_get_doi_csl_item_crosscite","text":"def test_get_doi_csl_item_crosscite ( ) View Source def test_get_doi_csl_item_crosscite(): doi = \"10.1101/142760\" csl_item = get_doi_csl_item_crosscite(doi) assert isinstance(csl_item, dict) csl_item[\"publisher\"] == \"Cold Spring Harbor Laboratory\"","title":"test_get_doi_csl_item_crosscite"},{"location":"reference/manubot/cite/tests/test_doi/#test_get_doi_csl_item_zotero","text":"def test_get_doi_csl_item_zotero ( ) As of 2019-10-25, DOI Content Negotiation (i.e. crosscite) encodes the consortium in author.name rather than author.literal. Zotero translation-server encodes the consortium in author.family, which is better than nothing since it's a valid CSL JSON field. https://github.com/manubot/manubot/issues/158 View Source def test_get_doi_csl_item_zotero(): \"\"\" As of 2019-10-25, DOI Content Negotiation (i.e. crosscite) encodes the consortium in author.name rather than author.literal. Zotero translation-server encodes the consortium in author.family, which is better than nothing since it's a valid CSL JSON field. https://github.com/manubot/manubot/issues/158 \"\"\" doi = \"10.1038/ng.3834\" csl_item = get_doi_csl_item_zotero(doi) assert isinstance(csl_item, dict) csl_item[\"author\"][9][\"family\"] == \"GTEx Consortium\"","title":"test_get_doi_csl_item_zotero"},{"location":"reference/manubot/cite/tests/test_handlers/","text":"Module manubot.cite.tests.test_handlers None None View Source import pytest from ..handlers import _generate_prefix_to_handler , prefix_to_handler def test_prefix_to_handler (): \"\"\" If this test fails, copy the output from `print(expected)` to use as the value for `handlers.prefix_to_handler`. \"\"\" expected = _generate_prefix_to_handler () print ( expected ) assert prefix_to_handler == expected @pytest . mark . parametrize ( \"prefix\" , [ \"raw\" , \"tag\" ]) def test_legacy_prefixes_are_unhandled ( prefix ): \"\"\" For backwards compatability, these prefixes should be unhandled. In the past, these prefixes referred to citekeys that were not resolvable identifiers by themselves. \"\"\" assert prefix not in prefix_to_handler Variables prefix_to_handler Functions test_legacy_prefixes_are_unhandled def test_legacy_prefixes_are_unhandled ( prefix ) For backwards compatability, these prefixes should be unhandled. In the past, these prefixes referred to citekeys that were not resolvable identifiers by themselves. View Source @pytest . mark . parametrize ( \"prefix\" , [ \"raw\", \"tag\" ] ) def test_legacy_prefixes_are_unhandled ( prefix ) : \"\"\" For backwards compatability, these prefixes should be unhandled. In the past, these prefixes referred to citekeys that were not resolvable identifiers by themselves. \"\"\" assert prefix not in prefix_to_handler test_prefix_to_handler def test_prefix_to_handler ( ) If this test fails, copy the output from print(expected) to use as the value for handlers.prefix_to_handler . View Source def test_prefix_to_handler () : \"\"\" If this test fails , copy the output from ` print ( expected ) ` to use as the value for ` handlers . prefix_to_handler `. \"\"\" expected = _generate_prefix_to_handler () print ( expected ) assert prefix_to_handler == expected","title":"Test Handlers"},{"location":"reference/manubot/cite/tests/test_handlers/#module-manubotciteteststest_handlers","text":"None None View Source import pytest from ..handlers import _generate_prefix_to_handler , prefix_to_handler def test_prefix_to_handler (): \"\"\" If this test fails, copy the output from `print(expected)` to use as the value for `handlers.prefix_to_handler`. \"\"\" expected = _generate_prefix_to_handler () print ( expected ) assert prefix_to_handler == expected @pytest . mark . parametrize ( \"prefix\" , [ \"raw\" , \"tag\" ]) def test_legacy_prefixes_are_unhandled ( prefix ): \"\"\" For backwards compatability, these prefixes should be unhandled. In the past, these prefixes referred to citekeys that were not resolvable identifiers by themselves. \"\"\" assert prefix not in prefix_to_handler","title":"Module manubot.cite.tests.test_handlers"},{"location":"reference/manubot/cite/tests/test_handlers/#variables","text":"prefix_to_handler","title":"Variables"},{"location":"reference/manubot/cite/tests/test_handlers/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_handlers/#test_legacy_prefixes_are_unhandled","text":"def test_legacy_prefixes_are_unhandled ( prefix ) For backwards compatability, these prefixes should be unhandled. In the past, these prefixes referred to citekeys that were not resolvable identifiers by themselves. View Source @pytest . mark . parametrize ( \"prefix\" , [ \"raw\", \"tag\" ] ) def test_legacy_prefixes_are_unhandled ( prefix ) : \"\"\" For backwards compatability, these prefixes should be unhandled. In the past, these prefixes referred to citekeys that were not resolvable identifiers by themselves. \"\"\" assert prefix not in prefix_to_handler","title":"test_legacy_prefixes_are_unhandled"},{"location":"reference/manubot/cite/tests/test_handlers/#test_prefix_to_handler","text":"def test_prefix_to_handler ( ) If this test fails, copy the output from print(expected) to use as the value for handlers.prefix_to_handler . View Source def test_prefix_to_handler () : \"\"\" If this test fails , copy the output from ` print ( expected ) ` to use as the value for ` handlers . prefix_to_handler `. \"\"\" expected = _generate_prefix_to_handler () print ( expected ) assert prefix_to_handler == expected","title":"test_prefix_to_handler"},{"location":"reference/manubot/cite/tests/test_isbn/","text":"Module manubot.cite.tests.test_isbn None None View Source import pytest from manubot.cite.isbn import ( get_isbn_csl_item_citoid , get_isbn_csl_item_isbnlib , get_isbn_csl_item_zotero , ) @pytest . mark . xfail ( reason = \"Quotation in title removed at some upstream point\" ) def test_citekey_to_csl_item_isbnlib_title_with_quotation_mark (): csl_item = get_isbn_csl_item_isbnlib ( \"9780312353780\" ) assert csl_item [ \"type\" ] == \"book\" assert csl_item [ \"title\" ] . startswith ( '\"F\" is for Fugitive' ) def test_get_isbn_csl_item_citoid_weird_date (): \"\"\" isbn:9780719561023 has a date value of \"(2004 printing)\" https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9780719561023 \"\"\" csl_item = get_isbn_csl_item_citoid ( \"9780719561023\" ) assert csl_item [ \"issued\" ][ \"date-parts\" ] == [[ 2004 ]] assert csl_item [ \"ISBN\" ] == \"9780719561023\" def test_get_isbn_csl_item_citoid_not_found (): \"\"\" isbn:9781439566039 is not found by Citoid: https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9781439566039 \"\"\" with pytest . raises ( KeyError , match = r \"Metadata for ISBN [0-9]{10,13} not found\" ): get_isbn_csl_item_citoid ( \"9781439566039\" ) def test_get_isbn_csl_item_zotero_with_note_issue (): \"\"\" translation-server returns two metadata records for this ISBN. The second has itemType=note and previously caused CSL export to fail. https://github.com/zotero/translation-server/issues/67 \"\"\" isbn = \"9780262517638\" csl_item = get_isbn_csl_item_zotero ( isbn ) assert csl_item [ \"author\" ][ 0 ][ \"family\" ] == \"Suber\" Functions test_citekey_to_csl_item_isbnlib_title_with_quotation_mark def test_citekey_to_csl_item_isbnlib_title_with_quotation_mark ( ) View Source @pytest . mark . xfail ( reason = \"Quotation in title removed at some upstream point\" ) def test_citekey_to_csl_item_isbnlib_title_with_quotation_mark () : csl_item = get_isbn_csl_item_isbnlib ( \"9780312353780\" ) assert csl_item [ \"type\" ] == \"book\" assert csl_item [ \"title\" ] . startswith ( '\"F\" is for Fugitive' ) test_get_isbn_csl_item_citoid_not_found def test_get_isbn_csl_item_citoid_not_found ( ) isbn:9781439566039 is not found by Citoid: https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9781439566039 View Source def test_get_isbn_csl_item_citoid_not_found () : \"\"\" isbn : 9781439566039 is not found by Citoid : https : // en . wikipedia . org / api / rest_v1 / data / citation / mediawiki / 9781439566039 \"\"\" with pytest . raises ( KeyError , match = r \" Metadata for ISBN [0-9]{10,13} not found \" ) : get_isbn_csl_item_citoid ( \" 9781439566039 \" ) test_get_isbn_csl_item_citoid_weird_date def test_get_isbn_csl_item_citoid_weird_date ( ) isbn:9780719561023 has a date value of \"(2004 printing)\" https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9780719561023 View Source def test_get_isbn_csl_item_citoid_weird_date(): \"\"\" isbn:9780719561023 has a date value of \"(2004 printing)\" https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9780719561023 \"\"\" csl_item = get_isbn_csl_item_citoid(\"9780719561023\") assert csl_item[\"issued\"][\"date-parts\"] == [[2004]] assert csl_item[\"ISBN\"] == \"9780719561023\" test_get_isbn_csl_item_zotero_with_note_issue def test_get_isbn_csl_item_zotero_with_note_issue ( ) translation-server returns two metadata records for this ISBN. The second has itemType=note and previously caused CSL export to fail. https://github.com/zotero/translation-server/issues/67 View Source def test_get_isbn_csl_item_zotero_with_note_issue (): \"\"\" translation-server returns two metadata records for this ISBN. The second has itemType=note and previously caused CSL export to fail. https://github.com/zotero/translation-server/issues/67 \"\"\" isbn = \"9780262517638\" csl_item = get_isbn_csl_item_zotero ( isbn ) assert csl_item [ \"author\" ][ 0 ][ \"family\" ] == \"Suber\"","title":"Test Isbn"},{"location":"reference/manubot/cite/tests/test_isbn/#module-manubotciteteststest_isbn","text":"None None View Source import pytest from manubot.cite.isbn import ( get_isbn_csl_item_citoid , get_isbn_csl_item_isbnlib , get_isbn_csl_item_zotero , ) @pytest . mark . xfail ( reason = \"Quotation in title removed at some upstream point\" ) def test_citekey_to_csl_item_isbnlib_title_with_quotation_mark (): csl_item = get_isbn_csl_item_isbnlib ( \"9780312353780\" ) assert csl_item [ \"type\" ] == \"book\" assert csl_item [ \"title\" ] . startswith ( '\"F\" is for Fugitive' ) def test_get_isbn_csl_item_citoid_weird_date (): \"\"\" isbn:9780719561023 has a date value of \"(2004 printing)\" https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9780719561023 \"\"\" csl_item = get_isbn_csl_item_citoid ( \"9780719561023\" ) assert csl_item [ \"issued\" ][ \"date-parts\" ] == [[ 2004 ]] assert csl_item [ \"ISBN\" ] == \"9780719561023\" def test_get_isbn_csl_item_citoid_not_found (): \"\"\" isbn:9781439566039 is not found by Citoid: https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9781439566039 \"\"\" with pytest . raises ( KeyError , match = r \"Metadata for ISBN [0-9]{10,13} not found\" ): get_isbn_csl_item_citoid ( \"9781439566039\" ) def test_get_isbn_csl_item_zotero_with_note_issue (): \"\"\" translation-server returns two metadata records for this ISBN. The second has itemType=note and previously caused CSL export to fail. https://github.com/zotero/translation-server/issues/67 \"\"\" isbn = \"9780262517638\" csl_item = get_isbn_csl_item_zotero ( isbn ) assert csl_item [ \"author\" ][ 0 ][ \"family\" ] == \"Suber\"","title":"Module manubot.cite.tests.test_isbn"},{"location":"reference/manubot/cite/tests/test_isbn/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_isbn/#test_citekey_to_csl_item_isbnlib_title_with_quotation_mark","text":"def test_citekey_to_csl_item_isbnlib_title_with_quotation_mark ( ) View Source @pytest . mark . xfail ( reason = \"Quotation in title removed at some upstream point\" ) def test_citekey_to_csl_item_isbnlib_title_with_quotation_mark () : csl_item = get_isbn_csl_item_isbnlib ( \"9780312353780\" ) assert csl_item [ \"type\" ] == \"book\" assert csl_item [ \"title\" ] . startswith ( '\"F\" is for Fugitive' )","title":"test_citekey_to_csl_item_isbnlib_title_with_quotation_mark"},{"location":"reference/manubot/cite/tests/test_isbn/#test_get_isbn_csl_item_citoid_not_found","text":"def test_get_isbn_csl_item_citoid_not_found ( ) isbn:9781439566039 is not found by Citoid: https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9781439566039 View Source def test_get_isbn_csl_item_citoid_not_found () : \"\"\" isbn : 9781439566039 is not found by Citoid : https : // en . wikipedia . org / api / rest_v1 / data / citation / mediawiki / 9781439566039 \"\"\" with pytest . raises ( KeyError , match = r \" Metadata for ISBN [0-9]{10,13} not found \" ) : get_isbn_csl_item_citoid ( \" 9781439566039 \" )","title":"test_get_isbn_csl_item_citoid_not_found"},{"location":"reference/manubot/cite/tests/test_isbn/#test_get_isbn_csl_item_citoid_weird_date","text":"def test_get_isbn_csl_item_citoid_weird_date ( ) isbn:9780719561023 has a date value of \"(2004 printing)\" https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9780719561023 View Source def test_get_isbn_csl_item_citoid_weird_date(): \"\"\" isbn:9780719561023 has a date value of \"(2004 printing)\" https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9780719561023 \"\"\" csl_item = get_isbn_csl_item_citoid(\"9780719561023\") assert csl_item[\"issued\"][\"date-parts\"] == [[2004]] assert csl_item[\"ISBN\"] == \"9780719561023\"","title":"test_get_isbn_csl_item_citoid_weird_date"},{"location":"reference/manubot/cite/tests/test_isbn/#test_get_isbn_csl_item_zotero_with_note_issue","text":"def test_get_isbn_csl_item_zotero_with_note_issue ( ) translation-server returns two metadata records for this ISBN. The second has itemType=note and previously caused CSL export to fail. https://github.com/zotero/translation-server/issues/67 View Source def test_get_isbn_csl_item_zotero_with_note_issue (): \"\"\" translation-server returns two metadata records for this ISBN. The second has itemType=note and previously caused CSL export to fail. https://github.com/zotero/translation-server/issues/67 \"\"\" isbn = \"9780262517638\" csl_item = get_isbn_csl_item_zotero ( isbn ) assert csl_item [ \"author\" ][ 0 ][ \"family\" ] == \"Suber\"","title":"test_get_isbn_csl_item_zotero_with_note_issue"},{"location":"reference/manubot/cite/tests/test_pubmed/","text":"Module manubot.cite.tests.test_pubmed None None View Source import pytest from manubot.cite.pubmed import ( get_pmcid_and_pmid_for_doi , get_pmid_for_doi , get_pubmed_ids_for_doi , ) @pytest . mark . parametrize ( ( \"doi\" , \"pmid\" ), [ ( \"10.1098/rsif.2017.0387\" , \"29618526\" ), # in PubMed and PMC ( \"10.1161/CIRCGENETICS.115.001181\" , \"27094199\" ), # in PubMed but not PMC ( \"10.7717/peerj-cs.134\" , None ), # DOI in journal not indexed by PubMed ( \"10.1161/CIRC\" , None ), # invalid DOI ], ) def test_get_pmid_for_doi ( doi , pmid ): output = get_pmid_for_doi ( doi ) assert pmid == output @pytest . mark . parametrize ( ( \"doi\" , \"id_dict\" ), [ ( \"10.1098/rsif.2017.0387\" , { \"PMCID\" : \"PMC5938574\" , \"PMID\" : \"29618526\" }), ( \"10.7554/ELIFE.32822\" , { \"PMCID\" : \"PMC5832410\" , \"PMID\" : \"29424689\" }), ( \"10.1161/CIRCGENETICS.115.001181\" , {}), # only in PubMed, not in PMC ( \"10.7717/peerj.000\" , {}), # Non-existent DOI ( \"10.peerj.000\" , {}), # malformed DOI ], ) def test_get_pmcid_and_pmid_for_doi ( doi , id_dict ): output = get_pmcid_and_pmid_for_doi ( doi ) assert id_dict == output @pytest . mark . parametrize ( ( \"doi\" , \"id_dict\" ), [ ( \"10.1098/rsif.2017.0387\" , { \"PMCID\" : \"PMC5938574\" , \"PMID\" : \"29618526\" }), ( \"10.7554/ELIFE.32822\" , { \"PMCID\" : \"PMC5832410\" , \"PMID\" : \"29424689\" }), ( \"10.1161/CIRCGENETICS.115.001181\" , { \"PMID\" : \"27094199\" }, ), # only in PubMed, not in PMC ( \"10.7717/peerj.000\" , {}), # Non-existent DOI ], ) def test_get_pubmed_ids_for_doi ( doi , id_dict ): output = get_pubmed_ids_for_doi ( doi ) assert id_dict == output Functions test_get_pmcid_and_pmid_for_doi def test_get_pmcid_and_pmid_for_doi ( doi , id_dict ) View Source @pytest . mark . parametrize ( ( \"doi\" , \"id_dict\" ), [ (\"10.1098/rsif.2017.0387\", {\"PMCID\": \"PMC5938574\", \"PMID\": \"29618526\"}), (\"10.7554/ELIFE.32822\", {\"PMCID\": \"PMC5832410\", \"PMID\": \"29424689\"}), (\"10.1161/CIRCGENETICS.115.001181\", {}), # only in PubMed, not in PMC (\"10.7717/peerj.000\", {}), # Non-existent DOI (\"10.peerj.000\", {}), # malformed DOI ] , ) def test_get_pmcid_and_pmid_for_doi ( doi , id_dict ) : output = get_pmcid_and_pmid_for_doi ( doi ) assert id_dict == output test_get_pmid_for_doi def test_get_pmid_for_doi ( doi , pmid ) View Source @pytest . mark . parametrize ( ( \"doi\" , \"pmid\" ), [ (\"10.1098/rsif.2017.0387\", \"29618526\"), # in PubMed and PMC (\"10.1161/CIRCGENETICS.115.001181\", \"27094199\"), # in PubMed but not PMC (\"10.7717/peerj-cs.134\", None), # DOI in journal not indexed by PubMed (\"10.1161/CIRC\", None), # invalid DOI ] , ) def test_get_pmid_for_doi ( doi , pmid ) : output = get_pmid_for_doi ( doi ) assert pmid == output test_get_pubmed_ids_for_doi def test_get_pubmed_ids_for_doi ( doi , id_dict ) View Source @pytest . mark . parametrize ( ( \"doi\" , \"id_dict\" ), [ (\"10.1098/rsif.2017.0387\", {\"PMCID\": \"PMC5938574\", \"PMID\": \"29618526\"}), (\"10.7554/ELIFE.32822\", {\"PMCID\": \"PMC5832410\", \"PMID\": \"29424689\"}), ( \"10.1161/CIRCGENETICS.115.001181\", {\"PMID\": \"27094199\"}, ), # only in PubMed, not in PMC (\"10.7717/peerj.000\", {}), # Non-existent DOI ] , ) def test_get_pubmed_ids_for_doi ( doi , id_dict ) : output = get_pubmed_ids_for_doi ( doi ) assert id_dict == output","title":"Test Pubmed"},{"location":"reference/manubot/cite/tests/test_pubmed/#module-manubotciteteststest_pubmed","text":"None None View Source import pytest from manubot.cite.pubmed import ( get_pmcid_and_pmid_for_doi , get_pmid_for_doi , get_pubmed_ids_for_doi , ) @pytest . mark . parametrize ( ( \"doi\" , \"pmid\" ), [ ( \"10.1098/rsif.2017.0387\" , \"29618526\" ), # in PubMed and PMC ( \"10.1161/CIRCGENETICS.115.001181\" , \"27094199\" ), # in PubMed but not PMC ( \"10.7717/peerj-cs.134\" , None ), # DOI in journal not indexed by PubMed ( \"10.1161/CIRC\" , None ), # invalid DOI ], ) def test_get_pmid_for_doi ( doi , pmid ): output = get_pmid_for_doi ( doi ) assert pmid == output @pytest . mark . parametrize ( ( \"doi\" , \"id_dict\" ), [ ( \"10.1098/rsif.2017.0387\" , { \"PMCID\" : \"PMC5938574\" , \"PMID\" : \"29618526\" }), ( \"10.7554/ELIFE.32822\" , { \"PMCID\" : \"PMC5832410\" , \"PMID\" : \"29424689\" }), ( \"10.1161/CIRCGENETICS.115.001181\" , {}), # only in PubMed, not in PMC ( \"10.7717/peerj.000\" , {}), # Non-existent DOI ( \"10.peerj.000\" , {}), # malformed DOI ], ) def test_get_pmcid_and_pmid_for_doi ( doi , id_dict ): output = get_pmcid_and_pmid_for_doi ( doi ) assert id_dict == output @pytest . mark . parametrize ( ( \"doi\" , \"id_dict\" ), [ ( \"10.1098/rsif.2017.0387\" , { \"PMCID\" : \"PMC5938574\" , \"PMID\" : \"29618526\" }), ( \"10.7554/ELIFE.32822\" , { \"PMCID\" : \"PMC5832410\" , \"PMID\" : \"29424689\" }), ( \"10.1161/CIRCGENETICS.115.001181\" , { \"PMID\" : \"27094199\" }, ), # only in PubMed, not in PMC ( \"10.7717/peerj.000\" , {}), # Non-existent DOI ], ) def test_get_pubmed_ids_for_doi ( doi , id_dict ): output = get_pubmed_ids_for_doi ( doi ) assert id_dict == output","title":"Module manubot.cite.tests.test_pubmed"},{"location":"reference/manubot/cite/tests/test_pubmed/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_pubmed/#test_get_pmcid_and_pmid_for_doi","text":"def test_get_pmcid_and_pmid_for_doi ( doi , id_dict ) View Source @pytest . mark . parametrize ( ( \"doi\" , \"id_dict\" ), [ (\"10.1098/rsif.2017.0387\", {\"PMCID\": \"PMC5938574\", \"PMID\": \"29618526\"}), (\"10.7554/ELIFE.32822\", {\"PMCID\": \"PMC5832410\", \"PMID\": \"29424689\"}), (\"10.1161/CIRCGENETICS.115.001181\", {}), # only in PubMed, not in PMC (\"10.7717/peerj.000\", {}), # Non-existent DOI (\"10.peerj.000\", {}), # malformed DOI ] , ) def test_get_pmcid_and_pmid_for_doi ( doi , id_dict ) : output = get_pmcid_and_pmid_for_doi ( doi ) assert id_dict == output","title":"test_get_pmcid_and_pmid_for_doi"},{"location":"reference/manubot/cite/tests/test_pubmed/#test_get_pmid_for_doi","text":"def test_get_pmid_for_doi ( doi , pmid ) View Source @pytest . mark . parametrize ( ( \"doi\" , \"pmid\" ), [ (\"10.1098/rsif.2017.0387\", \"29618526\"), # in PubMed and PMC (\"10.1161/CIRCGENETICS.115.001181\", \"27094199\"), # in PubMed but not PMC (\"10.7717/peerj-cs.134\", None), # DOI in journal not indexed by PubMed (\"10.1161/CIRC\", None), # invalid DOI ] , ) def test_get_pmid_for_doi ( doi , pmid ) : output = get_pmid_for_doi ( doi ) assert pmid == output","title":"test_get_pmid_for_doi"},{"location":"reference/manubot/cite/tests/test_pubmed/#test_get_pubmed_ids_for_doi","text":"def test_get_pubmed_ids_for_doi ( doi , id_dict ) View Source @pytest . mark . parametrize ( ( \"doi\" , \"id_dict\" ), [ (\"10.1098/rsif.2017.0387\", {\"PMCID\": \"PMC5938574\", \"PMID\": \"29618526\"}), (\"10.7554/ELIFE.32822\", {\"PMCID\": \"PMC5832410\", \"PMID\": \"29424689\"}), ( \"10.1161/CIRCGENETICS.115.001181\", {\"PMID\": \"27094199\"}, ), # only in PubMed, not in PMC (\"10.7717/peerj.000\", {}), # Non-existent DOI ] , ) def test_get_pubmed_ids_for_doi ( doi , id_dict ) : output = get_pubmed_ids_for_doi ( doi ) assert id_dict == output","title":"test_get_pubmed_ids_for_doi"},{"location":"reference/manubot/cite/tests/test_unpaywall/","text":"Module manubot.cite.tests.test_unpaywall None None View Source from ..unpaywall import Unpaywall , Unpaywall_arXiv , Unpaywall_DOI def test_unpaywall_doi (): doi = \"10.1371/journal.pcbi.1007250\" unpaywall = Unpaywall_DOI ( doi ) assert isinstance ( unpaywall . oa_locations , list ) assert unpaywall . best_pdf . has_creative_commons_license def test_unpaywall_arxiv (): arxiv_id = \"1912.04616\" unpaywall = Unpaywall_arXiv ( arxiv_id , use_doi = False ) assert isinstance ( unpaywall . oa_locations , list ) best_pdf = unpaywall . best_pdf assert isinstance ( best_pdf , dict ) assert best_pdf [ \"url\" ] == \"https://arxiv.org/pdf/1912.04616.pdf\" assert best_pdf [ \"url_for_landing_page\" ] == \"https://arxiv.org/abs/1912.04616\" assert best_pdf [ \"license\" ] == \"cc-by-sa\" def test_unpaywall_from_citekey (): \"\"\" https://arxiv.org/abs/1906.11964 is now published in https://doi.org/10.1162/qss_a_00023. Therefore, locations are coming from Unpaywall_DOI since defaulting to use_doi=True. \"\"\" unpaywall = Unpaywall . from_citekey ( \"arxiv:1906.11964v3\" ) assert isinstance ( unpaywall , Unpaywall_arXiv ) best_pdf = unpaywall . best_pdf assert best_pdf [ \"url_for_landing_page\" ] == \"https://doi.org/10.1162/qss_a_00023\" def test_unpaywall_from_csl_item (): csl_item = { \"id\" : \"ijxfHyzg\" , \"URL\" : \"https://arxiv.org/abs/1908.11459\" , \"title\" : \"Introducing: The Game Jam License\" , \"note\" : \"license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/ \\n standard_id: arxiv:1908.11459\" , } unpaywall = Unpaywall . from_csl_item ( csl_item ) assert isinstance ( unpaywall , Unpaywall_arXiv ) best_pdf = unpaywall . best_pdf assert best_pdf [ \"url_for_landing_page\" ] == \"https://arxiv.org/abs/1908.11459\" def test_unpaywall_from_csl_item_with_doi (): csl_item = { \"id\" : \"ijxfHyzg\" , \"URL\" : \"https://arxiv.org/abs/1908.11459\" , \"title\" : \"Introducing: The Game Jam License\" , \"note\" : \"license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/ \\n standard_id: arxiv:1908.11459\" , \"DOI\" : \"10.1145/3337722.3341844\" , } unpaywall = Unpaywall . from_csl_item ( csl_item ) # Unpaywall.from_csl_item uses DOI lookup when available assert isinstance ( unpaywall , Unpaywall_DOI ) assert unpaywall . best_pdf [ \"url_for_pdf\" ] Functions test_unpaywall_arxiv def test_unpaywall_arxiv ( ) View Source def test_unpaywall_arxiv(): arxiv_id = \"1912.04616\" unpaywall = Unpaywall_arXiv(arxiv_id, use_doi=False) assert isinstance(unpaywall.oa_locations, list) best_pdf = unpaywall.best_pdf assert isinstance(best_pdf, dict) assert best_pdf[\"url\"] == \"https://arxiv.org/pdf/1912.04616.pdf\" assert best_pdf[\"url_for_landing_page\"] == \"https://arxiv.org/abs/1912.04616\" assert best_pdf[\"license\"] == \"cc-by-sa\" test_unpaywall_doi def test_unpaywall_doi ( ) View Source def test_unpaywall_doi(): doi = \"10.1371/journal.pcbi.1007250\" unpaywall = Unpaywall_DOI(doi) assert isinstance(unpaywall.oa_locations, list) assert unpaywall.best_pdf.has_creative_commons_license test_unpaywall_from_citekey def test_unpaywall_from_citekey ( ) https://arxiv.org/abs/1906.11964 is now published in https://doi.org/10.1162/qss_a_00023. Therefore, locations are coming from Unpaywall_DOI since defaulting to use_doi=True. View Source def test_unpaywall_from_citekey(): \"\"\" https://arxiv.org/abs/1906.11964 is now published in https://doi.org/10.1162/qss_a_00023. Therefore, locations are coming from Unpaywall_DOI since defaulting to use_doi=True. \"\"\" unpaywall = Unpaywall.from_citekey(\"arxiv:1906.11964v3\") assert isinstance(unpaywall, Unpaywall_arXiv) best_pdf = unpaywall.best_pdf assert best_pdf[\"url_for_landing_page\"] == \"https://doi.org/10.1162/qss_a_00023\" test_unpaywall_from_csl_item def test_unpaywall_from_csl_item ( ) View Source def test_unpaywall_from_csl_item(): csl_item = { \"id\": \"ijxfHyzg\", \"URL\": \"https://arxiv.org/abs/1908.11459\", \"title\": \"Introducing: The Game Jam License\", \"note\": \"license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\\nstandard_id: arxiv:1908.11459\", } unpaywall = Unpaywall.from_csl_item(csl_item) assert isinstance(unpaywall, Unpaywall_arXiv) best_pdf = unpaywall.best_pdf assert best_pdf[\"url_for_landing_page\"] == \"https://arxiv.org/abs/1908.11459\" test_unpaywall_from_csl_item_with_doi def test_unpaywall_from_csl_item_with_doi ( ) View Source def test_unpaywall_from_csl_item_with_doi(): csl_item = { \"id\": \"ijxfHyzg\", \"URL\": \"https://arxiv.org/abs/1908.11459\", \"title\": \"Introducing: The Game Jam License\", \"note\": \"license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\\nstandard_id: arxiv:1908.11459\", \"DOI\": \"10.1145/3337722.3341844\", } unpaywall = Unpaywall.from_csl_item(csl_item) # Unpaywall.from_csl_item uses DOI lookup when available assert isinstance(unpaywall, Unpaywall_DOI) assert unpaywall.best_pdf[\"url_for_pdf\"]","title":"Test Unpaywall"},{"location":"reference/manubot/cite/tests/test_unpaywall/#module-manubotciteteststest_unpaywall","text":"None None View Source from ..unpaywall import Unpaywall , Unpaywall_arXiv , Unpaywall_DOI def test_unpaywall_doi (): doi = \"10.1371/journal.pcbi.1007250\" unpaywall = Unpaywall_DOI ( doi ) assert isinstance ( unpaywall . oa_locations , list ) assert unpaywall . best_pdf . has_creative_commons_license def test_unpaywall_arxiv (): arxiv_id = \"1912.04616\" unpaywall = Unpaywall_arXiv ( arxiv_id , use_doi = False ) assert isinstance ( unpaywall . oa_locations , list ) best_pdf = unpaywall . best_pdf assert isinstance ( best_pdf , dict ) assert best_pdf [ \"url\" ] == \"https://arxiv.org/pdf/1912.04616.pdf\" assert best_pdf [ \"url_for_landing_page\" ] == \"https://arxiv.org/abs/1912.04616\" assert best_pdf [ \"license\" ] == \"cc-by-sa\" def test_unpaywall_from_citekey (): \"\"\" https://arxiv.org/abs/1906.11964 is now published in https://doi.org/10.1162/qss_a_00023. Therefore, locations are coming from Unpaywall_DOI since defaulting to use_doi=True. \"\"\" unpaywall = Unpaywall . from_citekey ( \"arxiv:1906.11964v3\" ) assert isinstance ( unpaywall , Unpaywall_arXiv ) best_pdf = unpaywall . best_pdf assert best_pdf [ \"url_for_landing_page\" ] == \"https://doi.org/10.1162/qss_a_00023\" def test_unpaywall_from_csl_item (): csl_item = { \"id\" : \"ijxfHyzg\" , \"URL\" : \"https://arxiv.org/abs/1908.11459\" , \"title\" : \"Introducing: The Game Jam License\" , \"note\" : \"license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/ \\n standard_id: arxiv:1908.11459\" , } unpaywall = Unpaywall . from_csl_item ( csl_item ) assert isinstance ( unpaywall , Unpaywall_arXiv ) best_pdf = unpaywall . best_pdf assert best_pdf [ \"url_for_landing_page\" ] == \"https://arxiv.org/abs/1908.11459\" def test_unpaywall_from_csl_item_with_doi (): csl_item = { \"id\" : \"ijxfHyzg\" , \"URL\" : \"https://arxiv.org/abs/1908.11459\" , \"title\" : \"Introducing: The Game Jam License\" , \"note\" : \"license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/ \\n standard_id: arxiv:1908.11459\" , \"DOI\" : \"10.1145/3337722.3341844\" , } unpaywall = Unpaywall . from_csl_item ( csl_item ) # Unpaywall.from_csl_item uses DOI lookup when available assert isinstance ( unpaywall , Unpaywall_DOI ) assert unpaywall . best_pdf [ \"url_for_pdf\" ]","title":"Module manubot.cite.tests.test_unpaywall"},{"location":"reference/manubot/cite/tests/test_unpaywall/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_unpaywall/#test_unpaywall_arxiv","text":"def test_unpaywall_arxiv ( ) View Source def test_unpaywall_arxiv(): arxiv_id = \"1912.04616\" unpaywall = Unpaywall_arXiv(arxiv_id, use_doi=False) assert isinstance(unpaywall.oa_locations, list) best_pdf = unpaywall.best_pdf assert isinstance(best_pdf, dict) assert best_pdf[\"url\"] == \"https://arxiv.org/pdf/1912.04616.pdf\" assert best_pdf[\"url_for_landing_page\"] == \"https://arxiv.org/abs/1912.04616\" assert best_pdf[\"license\"] == \"cc-by-sa\"","title":"test_unpaywall_arxiv"},{"location":"reference/manubot/cite/tests/test_unpaywall/#test_unpaywall_doi","text":"def test_unpaywall_doi ( ) View Source def test_unpaywall_doi(): doi = \"10.1371/journal.pcbi.1007250\" unpaywall = Unpaywall_DOI(doi) assert isinstance(unpaywall.oa_locations, list) assert unpaywall.best_pdf.has_creative_commons_license","title":"test_unpaywall_doi"},{"location":"reference/manubot/cite/tests/test_unpaywall/#test_unpaywall_from_citekey","text":"def test_unpaywall_from_citekey ( ) https://arxiv.org/abs/1906.11964 is now published in https://doi.org/10.1162/qss_a_00023. Therefore, locations are coming from Unpaywall_DOI since defaulting to use_doi=True. View Source def test_unpaywall_from_citekey(): \"\"\" https://arxiv.org/abs/1906.11964 is now published in https://doi.org/10.1162/qss_a_00023. Therefore, locations are coming from Unpaywall_DOI since defaulting to use_doi=True. \"\"\" unpaywall = Unpaywall.from_citekey(\"arxiv:1906.11964v3\") assert isinstance(unpaywall, Unpaywall_arXiv) best_pdf = unpaywall.best_pdf assert best_pdf[\"url_for_landing_page\"] == \"https://doi.org/10.1162/qss_a_00023\"","title":"test_unpaywall_from_citekey"},{"location":"reference/manubot/cite/tests/test_unpaywall/#test_unpaywall_from_csl_item","text":"def test_unpaywall_from_csl_item ( ) View Source def test_unpaywall_from_csl_item(): csl_item = { \"id\": \"ijxfHyzg\", \"URL\": \"https://arxiv.org/abs/1908.11459\", \"title\": \"Introducing: The Game Jam License\", \"note\": \"license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\\nstandard_id: arxiv:1908.11459\", } unpaywall = Unpaywall.from_csl_item(csl_item) assert isinstance(unpaywall, Unpaywall_arXiv) best_pdf = unpaywall.best_pdf assert best_pdf[\"url_for_landing_page\"] == \"https://arxiv.org/abs/1908.11459\"","title":"test_unpaywall_from_csl_item"},{"location":"reference/manubot/cite/tests/test_unpaywall/#test_unpaywall_from_csl_item_with_doi","text":"def test_unpaywall_from_csl_item_with_doi ( ) View Source def test_unpaywall_from_csl_item_with_doi(): csl_item = { \"id\": \"ijxfHyzg\", \"URL\": \"https://arxiv.org/abs/1908.11459\", \"title\": \"Introducing: The Game Jam License\", \"note\": \"license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\\nstandard_id: arxiv:1908.11459\", \"DOI\": \"10.1145/3337722.3341844\", } unpaywall = Unpaywall.from_csl_item(csl_item) # Unpaywall.from_csl_item uses DOI lookup when available assert isinstance(unpaywall, Unpaywall_DOI) assert unpaywall.best_pdf[\"url_for_pdf\"]","title":"test_unpaywall_from_csl_item_with_doi"},{"location":"reference/manubot/cite/tests/test_url/","text":"Module manubot.cite.tests.test_url None None View Source import pytest from manubot.cite.url import get_url_csl_item_zotero def test_get_url_csl_item_zotero_nyt (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'https://nyti.ms/1NuB0WJ' \\ 'https://translate.manubot.org/web' ``` \"\"\" url = \"https://nyti.ms/1NuB0WJ\" csl_item = get_url_csl_item_zotero ( url ) assert csl_item [ \"title\" ] . startswith ( \"Unraveling the Ties of Altitude, Oxygen and Lung Cancer\" ) assert csl_item [ \"author\" ][ 0 ][ \"family\" ] == \"Johnson\" def test_get_url_csl_item_zotero_manubot (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/' \\ 'https://translate.manubot.org/web' ``` \"\"\" url = \"https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/\" csl_item = get_url_csl_item_zotero ( url ) assert csl_item [ \"title\" ] == \"Open collaborative writing with Manubot\" assert csl_item [ \"author\" ][ 1 ][ \"family\" ] == \"Slochower\" # Zotero CSL exporter returns mixed string/int date-parts # https://github.com/zotero/zotero/issues/1603 assert [ int ( x ) for x in csl_item [ \"issued\" ][ \"date-parts\" ][ 0 ]] == [ 2018 , 12 , 18 ] @pytest . mark . skip ( reason = \"test intermittently fails as metadata varies between two states\" ) def test_get_url_csl_item_zotero_github (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927' \\ 'https://translate.manubot.org/web' ``` Note: this test may have temporary failures, due to performance of translation-server. It seems that sometimes translation-server returns a different title for the same URL. A real mystery. See also: https://github.com/manubot/manubot/pull/139#discussion_r328703233 Proposed action: Probably should inquire upstream or change the test. \"\"\" url = \"https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927\" csl_item = get_url_csl_item_zotero ( url ) # FIXME: arbitrarily, csl_item['abstract'], and not csl_item['title'] contains the title. assert csl_item [ \"title\" ] . startswith ( \"Flexible and powerful data analysis\" ) assert csl_item [ \"source\" ] == \"GitHub\" def test_get_url_csl_item_zotero_no_url ( monkeypatch ): \"\"\" Ensure get_url_csl_item_zotero sets URL to the query URL, when the Zotero translator does not return it. https://github.com/manubot/manubot/issues/244 \"\"\" query_url = \"http://icbo2016.cgrb.oregonstate.edu/node/251\" def mock_web_query ( url : str ): assert url == query_url return [ { \"key\" : \"J86G3MS7\" , \"version\" : 0 , \"itemType\" : \"webpage\" , \"creators\" : [ { \"firstName\" : \"Senay\" , \"lastName\" : \"Kafkas\" , \"creatorType\" : \"author\" , }, { \"firstName\" : \"Ian\" , \"lastName\" : \"Dunham\" , \"creatorType\" : \"author\" }, { \"firstName\" : \"Helen\" , \"lastName\" : \"Parkinson\" , \"creatorType\" : \"author\" , }, { \"firstName\" : \"Johanna\" , \"lastName\" : \"Mcentyre\" , \"creatorType\" : \"author\" , }, ], \"tags\" : [], \"title\" : \"BIT106: Use of text mining for Experimental Factor Ontology coverage expansion in the scope of target validation\" , \"date\" : \"2016\" , \"shortTitle\" : \"BIT106\" , } ] monkeypatch . setattr ( \"manubot.cite.zotero.web_query\" , mock_web_query ) csl_item = get_url_csl_item_zotero ( query_url ) assert \"URL\" in csl_item assert csl_item [ \"URL\" ] == query_url Functions test_get_url_csl_item_zotero_github def test_get_url_csl_item_zotero_github ( ) This command creates two translation-server queries. The first query is equivalent to: curl --verbose --header \"Content-Type: text/plain\" --data 'https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927' 'https://translate.manubot.org/web' Note: this test may have temporary failures, due to performance of translation-server. It seems that sometimes translation-server returns a different title for the same URL. A real mystery. See also: https://github.com/manubot/manubot/pull/139#discussion_r328703233 Proposed action: Probably should inquire upstream or change the test. View Source @ pytest . mark . skip ( reason = \"test intermittently fails as metadata varies between two states\" ) def test_get_url_csl_item_zotero_github (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927' \\ 'https://translate.manubot.org/web' ``` Note: this test may have temporary failures, due to performance of translation-server. It seems that sometimes translation-server returns a different title for the same URL. A real mystery. See also: https://github.com/manubot/manubot/pull/139#discussion_r328703233 Proposed action: Probably should inquire upstream or change the test. \"\"\" url = \"https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927\" csl_item = get_url_csl_item_zotero ( url ) # FIXME: arbitrarily, csl_item['abstract'], and not csl_item['title'] contains the title. assert csl_item [ \"title\" ] . startswith ( \"Flexible and powerful data analysis\" ) assert csl_item [ \"source\" ] == \"GitHub\" test_get_url_csl_item_zotero_manubot def test_get_url_csl_item_zotero_manubot ( ) This command creates two translation-server queries. The first query is equivalent to: curl --verbose --header \"Content-Type: text/plain\" --data 'https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/' 'https://translate.manubot.org/web' View Source def test_get_url_csl_item_zotero_manubot (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/' \\ 'https://translate.manubot.org/web' ``` \"\"\" url = \"https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/\" csl_item = get_url_csl_item_zotero ( url ) assert csl_item [ \"title\" ] == \"Open collaborative writing with Manubot\" assert csl_item [ \"author\" ][ 1 ][ \"family\" ] == \"Slochower\" # Zotero CSL exporter returns mixed string/int date-parts # https://github.com/zotero/zotero/issues/1603 assert [ int ( x ) for x in csl_item [ \"issued\" ][ \"date-parts\" ][ 0 ]] == [ 2018 , 12 , 18 ] test_get_url_csl_item_zotero_no_url def test_get_url_csl_item_zotero_no_url ( monkeypatch ) Ensure get_url_csl_item_zotero sets URL to the query URL, when the Zotero translator does not return it. https://github.com/manubot/manubot/issues/244 View Source def test_get_url_csl_item_zotero_no_url ( monkeypatch ) : \"\"\" Ensure get_url_csl_item_zotero sets URL to the query URL , when the Zotero translator does not return it . https: //github.com/manubot/manubot/issues/244 \"\"\" query_url = \"http://icbo2016.cgrb.oregonstate.edu/node/251\" def mock_web_query ( url: str ) : assert url == query_url return [ { \"key\" : \"J86G3MS7\" , \"version\" : 0 , \"itemType\" : \"webpage\" , \"creators\" : [ { \"firstName\" : \"Senay\" , \"lastName\" : \"Kafkas\" , \"creatorType\" : \"author\" , }, { \"firstName\" : \"Ian\" , \"lastName\" : \"Dunham\" , \"creatorType\" : \"author\" }, { \"firstName\" : \"Helen\" , \"lastName\" : \"Parkinson\" , \"creatorType\" : \"author\" , }, { \"firstName\" : \"Johanna\" , \"lastName\" : \"Mcentyre\" , \"creatorType\" : \"author\" , }, ], \"tags\" : [], \"title\" : \"BIT106: Use of text mining for Experimental Factor Ontology coverage expansion in the scope of target validation\" , \"date\" : \"2016\" , \"shortTitle\" : \"BIT106\" , } ] monkeypatch . setattr ( \"manubot.cite.zotero.web_query\" , mock_web_query ) csl_item = get_url_csl_item_zotero ( query_url ) assert \"URL\" in csl_item assert csl_item [ \"URL\" ] == query_url test_get_url_csl_item_zotero_nyt def test_get_url_csl_item_zotero_nyt ( ) This command creates two translation-server queries. The first query is equivalent to: curl --verbose --header \"Content-Type: text/plain\" --data 'https://nyti.ms/1NuB0WJ' 'https://translate.manubot.org/web' View Source def test_get_url_csl_item_zotero_nyt(): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'https://nyti.ms/1NuB0WJ' \\ 'https://translate.manubot.org/web' ``` \"\"\" url = \"https://nyti.ms/1NuB0WJ\" csl_item = get_url_csl_item_zotero(url) assert csl_item[\"title\"].startswith( \"Unraveling the Ties of Altitude, Oxygen and Lung Cancer\" ) assert csl_item[\"author\"][0][\"family\"] == \"Johnson\"","title":"Test Url"},{"location":"reference/manubot/cite/tests/test_url/#module-manubotciteteststest_url","text":"None None View Source import pytest from manubot.cite.url import get_url_csl_item_zotero def test_get_url_csl_item_zotero_nyt (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'https://nyti.ms/1NuB0WJ' \\ 'https://translate.manubot.org/web' ``` \"\"\" url = \"https://nyti.ms/1NuB0WJ\" csl_item = get_url_csl_item_zotero ( url ) assert csl_item [ \"title\" ] . startswith ( \"Unraveling the Ties of Altitude, Oxygen and Lung Cancer\" ) assert csl_item [ \"author\" ][ 0 ][ \"family\" ] == \"Johnson\" def test_get_url_csl_item_zotero_manubot (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/' \\ 'https://translate.manubot.org/web' ``` \"\"\" url = \"https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/\" csl_item = get_url_csl_item_zotero ( url ) assert csl_item [ \"title\" ] == \"Open collaborative writing with Manubot\" assert csl_item [ \"author\" ][ 1 ][ \"family\" ] == \"Slochower\" # Zotero CSL exporter returns mixed string/int date-parts # https://github.com/zotero/zotero/issues/1603 assert [ int ( x ) for x in csl_item [ \"issued\" ][ \"date-parts\" ][ 0 ]] == [ 2018 , 12 , 18 ] @pytest . mark . skip ( reason = \"test intermittently fails as metadata varies between two states\" ) def test_get_url_csl_item_zotero_github (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927' \\ 'https://translate.manubot.org/web' ``` Note: this test may have temporary failures, due to performance of translation-server. It seems that sometimes translation-server returns a different title for the same URL. A real mystery. See also: https://github.com/manubot/manubot/pull/139#discussion_r328703233 Proposed action: Probably should inquire upstream or change the test. \"\"\" url = \"https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927\" csl_item = get_url_csl_item_zotero ( url ) # FIXME: arbitrarily, csl_item['abstract'], and not csl_item['title'] contains the title. assert csl_item [ \"title\" ] . startswith ( \"Flexible and powerful data analysis\" ) assert csl_item [ \"source\" ] == \"GitHub\" def test_get_url_csl_item_zotero_no_url ( monkeypatch ): \"\"\" Ensure get_url_csl_item_zotero sets URL to the query URL, when the Zotero translator does not return it. https://github.com/manubot/manubot/issues/244 \"\"\" query_url = \"http://icbo2016.cgrb.oregonstate.edu/node/251\" def mock_web_query ( url : str ): assert url == query_url return [ { \"key\" : \"J86G3MS7\" , \"version\" : 0 , \"itemType\" : \"webpage\" , \"creators\" : [ { \"firstName\" : \"Senay\" , \"lastName\" : \"Kafkas\" , \"creatorType\" : \"author\" , }, { \"firstName\" : \"Ian\" , \"lastName\" : \"Dunham\" , \"creatorType\" : \"author\" }, { \"firstName\" : \"Helen\" , \"lastName\" : \"Parkinson\" , \"creatorType\" : \"author\" , }, { \"firstName\" : \"Johanna\" , \"lastName\" : \"Mcentyre\" , \"creatorType\" : \"author\" , }, ], \"tags\" : [], \"title\" : \"BIT106: Use of text mining for Experimental Factor Ontology coverage expansion in the scope of target validation\" , \"date\" : \"2016\" , \"shortTitle\" : \"BIT106\" , } ] monkeypatch . setattr ( \"manubot.cite.zotero.web_query\" , mock_web_query ) csl_item = get_url_csl_item_zotero ( query_url ) assert \"URL\" in csl_item assert csl_item [ \"URL\" ] == query_url","title":"Module manubot.cite.tests.test_url"},{"location":"reference/manubot/cite/tests/test_url/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_url/#test_get_url_csl_item_zotero_github","text":"def test_get_url_csl_item_zotero_github ( ) This command creates two translation-server queries. The first query is equivalent to: curl --verbose --header \"Content-Type: text/plain\" --data 'https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927' 'https://translate.manubot.org/web' Note: this test may have temporary failures, due to performance of translation-server. It seems that sometimes translation-server returns a different title for the same URL. A real mystery. See also: https://github.com/manubot/manubot/pull/139#discussion_r328703233 Proposed action: Probably should inquire upstream or change the test. View Source @ pytest . mark . skip ( reason = \"test intermittently fails as metadata varies between two states\" ) def test_get_url_csl_item_zotero_github (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927' \\ 'https://translate.manubot.org/web' ``` Note: this test may have temporary failures, due to performance of translation-server. It seems that sometimes translation-server returns a different title for the same URL. A real mystery. See also: https://github.com/manubot/manubot/pull/139#discussion_r328703233 Proposed action: Probably should inquire upstream or change the test. \"\"\" url = \"https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927\" csl_item = get_url_csl_item_zotero ( url ) # FIXME: arbitrarily, csl_item['abstract'], and not csl_item['title'] contains the title. assert csl_item [ \"title\" ] . startswith ( \"Flexible and powerful data analysis\" ) assert csl_item [ \"source\" ] == \"GitHub\"","title":"test_get_url_csl_item_zotero_github"},{"location":"reference/manubot/cite/tests/test_url/#test_get_url_csl_item_zotero_manubot","text":"def test_get_url_csl_item_zotero_manubot ( ) This command creates two translation-server queries. The first query is equivalent to: curl --verbose --header \"Content-Type: text/plain\" --data 'https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/' 'https://translate.manubot.org/web' View Source def test_get_url_csl_item_zotero_manubot (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/' \\ 'https://translate.manubot.org/web' ``` \"\"\" url = \"https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/\" csl_item = get_url_csl_item_zotero ( url ) assert csl_item [ \"title\" ] == \"Open collaborative writing with Manubot\" assert csl_item [ \"author\" ][ 1 ][ \"family\" ] == \"Slochower\" # Zotero CSL exporter returns mixed string/int date-parts # https://github.com/zotero/zotero/issues/1603 assert [ int ( x ) for x in csl_item [ \"issued\" ][ \"date-parts\" ][ 0 ]] == [ 2018 , 12 , 18 ]","title":"test_get_url_csl_item_zotero_manubot"},{"location":"reference/manubot/cite/tests/test_url/#test_get_url_csl_item_zotero_no_url","text":"def test_get_url_csl_item_zotero_no_url ( monkeypatch ) Ensure get_url_csl_item_zotero sets URL to the query URL, when the Zotero translator does not return it. https://github.com/manubot/manubot/issues/244 View Source def test_get_url_csl_item_zotero_no_url ( monkeypatch ) : \"\"\" Ensure get_url_csl_item_zotero sets URL to the query URL , when the Zotero translator does not return it . https: //github.com/manubot/manubot/issues/244 \"\"\" query_url = \"http://icbo2016.cgrb.oregonstate.edu/node/251\" def mock_web_query ( url: str ) : assert url == query_url return [ { \"key\" : \"J86G3MS7\" , \"version\" : 0 , \"itemType\" : \"webpage\" , \"creators\" : [ { \"firstName\" : \"Senay\" , \"lastName\" : \"Kafkas\" , \"creatorType\" : \"author\" , }, { \"firstName\" : \"Ian\" , \"lastName\" : \"Dunham\" , \"creatorType\" : \"author\" }, { \"firstName\" : \"Helen\" , \"lastName\" : \"Parkinson\" , \"creatorType\" : \"author\" , }, { \"firstName\" : \"Johanna\" , \"lastName\" : \"Mcentyre\" , \"creatorType\" : \"author\" , }, ], \"tags\" : [], \"title\" : \"BIT106: Use of text mining for Experimental Factor Ontology coverage expansion in the scope of target validation\" , \"date\" : \"2016\" , \"shortTitle\" : \"BIT106\" , } ] monkeypatch . setattr ( \"manubot.cite.zotero.web_query\" , mock_web_query ) csl_item = get_url_csl_item_zotero ( query_url ) assert \"URL\" in csl_item assert csl_item [ \"URL\" ] == query_url","title":"test_get_url_csl_item_zotero_no_url"},{"location":"reference/manubot/cite/tests/test_url/#test_get_url_csl_item_zotero_nyt","text":"def test_get_url_csl_item_zotero_nyt ( ) This command creates two translation-server queries. The first query is equivalent to: curl --verbose --header \"Content-Type: text/plain\" --data 'https://nyti.ms/1NuB0WJ' 'https://translate.manubot.org/web' View Source def test_get_url_csl_item_zotero_nyt(): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'https://nyti.ms/1NuB0WJ' \\ 'https://translate.manubot.org/web' ``` \"\"\" url = \"https://nyti.ms/1NuB0WJ\" csl_item = get_url_csl_item_zotero(url) assert csl_item[\"title\"].startswith( \"Unraveling the Ties of Altitude, Oxygen and Lung Cancer\" ) assert csl_item[\"author\"][0][\"family\"] == \"Johnson\"","title":"test_get_url_csl_item_zotero_nyt"},{"location":"reference/manubot/cite/tests/test_wikidata/","text":"Module manubot.cite.tests.test_wikidata None None View Source from manubot.cite.wikidata import get_wikidata_csl_item def test_get_wikidata_csl_item (): \"\"\" Test metadata extraction from https://www.wikidata.org/wiki/Q50051684 \"\"\" wikidata_id = \"Q50051684\" csl_item = get_wikidata_csl_item ( wikidata_id ) assert ( \"Sci-Hub provides access to nearly all scholarly literature\" in csl_item [ \"title\" ] ) assert csl_item [ \"container-title\" ] == \"eLife\" assert csl_item [ \"DOI\" ] == \"10.7554/elife.32822\" def test_get_wikidata_csl_item_author_ordering (): \"\"\" Test extraction of author ordering from https://www.wikidata.org/wiki/Q50051684. Wikidata uses a \"series ordinal\" qualifier that must be considered or else author ordering may be wrong. Author ordering was previously not properly set by the Wikidata translator https://github.com/zotero/translators/issues/1790 \"\"\" wikidata_id = \"Q50051684\" csl_item = get_wikidata_csl_item ( wikidata_id ) family_names = [ author [ \"family\" ] for author in csl_item [ \"author\" ]] print ( family_names ) assert family_names == [ \"Himmelstein\" , \"Romero\" , \"Levernier\" , \"Munro\" , \"McLaughlin\" , \"Greshake\" , # actually should be Greshake Tzovaras \"Greene\" , ] Functions test_get_wikidata_csl_item def test_get_wikidata_csl_item ( ) Test metadata extraction from https://www.wikidata.org/wiki/Q50051684 View Source def test_get_wikidata_csl_item(): \"\"\" Test metadata extraction from https://www.wikidata.org/wiki/Q50051684 \"\"\" wikidata_id = \"Q50051684\" csl_item = get_wikidata_csl_item(wikidata_id) assert ( \"Sci-Hub provides access to nearly all scholarly literature\" in csl_item[\"title\"] ) assert csl_item[\"container-title\"] == \"eLife\" assert csl_item[\"DOI\"] == \"10.7554/elife.32822\" test_get_wikidata_csl_item_author_ordering def test_get_wikidata_csl_item_author_ordering ( ) Test extraction of author ordering from https://www.wikidata.org/wiki/Q50051684. Wikidata uses a \"series ordinal\" qualifier that must be considered or else author ordering may be wrong. Author ordering was previously not properly set by the Wikidata translator https://github.com/zotero/translators/issues/1790 View Source def test_get_wikidata_csl_item_author_ordering (): \"\"\" Test extraction of author ordering from https://www.wikidata.org/wiki/Q50051684. Wikidata uses a \"series ordinal\" qualifier that must be considered or else author ordering may be wrong. Author ordering was previously not properly set by the Wikidata translator https://github.com/zotero/translators/issues/1790 \"\"\" wikidata_id = \"Q50051684\" csl_item = get_wikidata_csl_item ( wikidata_id ) family_names = [ author [ \"family\" ] for author in csl_item [ \"author\" ]] print ( family_names ) assert family_names == [ \"Himmelstein\" , \"Romero\" , \"Levernier\" , \"Munro\" , \"McLaughlin\" , \"Greshake\" , # actually should be Greshake Tzovaras \"Greene\" , ]","title":"Test Wikidata"},{"location":"reference/manubot/cite/tests/test_wikidata/#module-manubotciteteststest_wikidata","text":"None None View Source from manubot.cite.wikidata import get_wikidata_csl_item def test_get_wikidata_csl_item (): \"\"\" Test metadata extraction from https://www.wikidata.org/wiki/Q50051684 \"\"\" wikidata_id = \"Q50051684\" csl_item = get_wikidata_csl_item ( wikidata_id ) assert ( \"Sci-Hub provides access to nearly all scholarly literature\" in csl_item [ \"title\" ] ) assert csl_item [ \"container-title\" ] == \"eLife\" assert csl_item [ \"DOI\" ] == \"10.7554/elife.32822\" def test_get_wikidata_csl_item_author_ordering (): \"\"\" Test extraction of author ordering from https://www.wikidata.org/wiki/Q50051684. Wikidata uses a \"series ordinal\" qualifier that must be considered or else author ordering may be wrong. Author ordering was previously not properly set by the Wikidata translator https://github.com/zotero/translators/issues/1790 \"\"\" wikidata_id = \"Q50051684\" csl_item = get_wikidata_csl_item ( wikidata_id ) family_names = [ author [ \"family\" ] for author in csl_item [ \"author\" ]] print ( family_names ) assert family_names == [ \"Himmelstein\" , \"Romero\" , \"Levernier\" , \"Munro\" , \"McLaughlin\" , \"Greshake\" , # actually should be Greshake Tzovaras \"Greene\" , ]","title":"Module manubot.cite.tests.test_wikidata"},{"location":"reference/manubot/cite/tests/test_wikidata/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_wikidata/#test_get_wikidata_csl_item","text":"def test_get_wikidata_csl_item ( ) Test metadata extraction from https://www.wikidata.org/wiki/Q50051684 View Source def test_get_wikidata_csl_item(): \"\"\" Test metadata extraction from https://www.wikidata.org/wiki/Q50051684 \"\"\" wikidata_id = \"Q50051684\" csl_item = get_wikidata_csl_item(wikidata_id) assert ( \"Sci-Hub provides access to nearly all scholarly literature\" in csl_item[\"title\"] ) assert csl_item[\"container-title\"] == \"eLife\" assert csl_item[\"DOI\"] == \"10.7554/elife.32822\"","title":"test_get_wikidata_csl_item"},{"location":"reference/manubot/cite/tests/test_wikidata/#test_get_wikidata_csl_item_author_ordering","text":"def test_get_wikidata_csl_item_author_ordering ( ) Test extraction of author ordering from https://www.wikidata.org/wiki/Q50051684. Wikidata uses a \"series ordinal\" qualifier that must be considered or else author ordering may be wrong. Author ordering was previously not properly set by the Wikidata translator https://github.com/zotero/translators/issues/1790 View Source def test_get_wikidata_csl_item_author_ordering (): \"\"\" Test extraction of author ordering from https://www.wikidata.org/wiki/Q50051684. Wikidata uses a \"series ordinal\" qualifier that must be considered or else author ordering may be wrong. Author ordering was previously not properly set by the Wikidata translator https://github.com/zotero/translators/issues/1790 \"\"\" wikidata_id = \"Q50051684\" csl_item = get_wikidata_csl_item ( wikidata_id ) family_names = [ author [ \"family\" ] for author in csl_item [ \"author\" ]] print ( family_names ) assert family_names == [ \"Himmelstein\" , \"Romero\" , \"Levernier\" , \"Munro\" , \"McLaughlin\" , \"Greshake\" , # actually should be Greshake Tzovaras \"Greene\" , ]","title":"test_get_wikidata_csl_item_author_ordering"},{"location":"reference/manubot/cite/tests/test_zotero/","text":"Module manubot.cite.tests.test_zotero None None View Source import pytest from manubot.cite.zotero import export_as_csl , search_query , web_query def test_web_query (): \"\"\" The translation-server web endpoint can be tested via curl: ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'https://bigthink.com/neurobonkers/a-pirate-bay-for-science' \\ 'https://translate.manubot.org/web' ``` An outdated installation of translation-server caused the web query for this URL to be extraordinarily slow but has now been fixed. See https://github.com/zotero/translation-server/issues/63 \"\"\" url = \"https://bigthink.com/neurobonkers/a-pirate-bay-for-science\" zotero_data = web_query ( url ) assert isinstance ( zotero_data , list ) assert len ( zotero_data ) == 1 assert zotero_data [ 0 ][ \"title\" ] . startswith ( \"Meet the Robin Hood of Science\" ) def test_export_as_csl (): \"\"\" CSL export can be tested via curl: ``` curl \\ --header \"Content-Type: application/json\" \\ --data '[{\"key\": \"IN22XN53\", \"itemType\": \"webpage\", \"date\": \"2016-02-09T20:12:00\"}]' \\ 'https://translate.manubot.org/export?format=csljson' ``` \"\"\" zotero_data = [ { \"key\" : \"IN22XN53\" , \"version\" : 0 , \"itemType\" : \"webpage\" , \"creators\" : [], \"tags\" : [], \"title\" : \"Meet the Robin Hood of Science\" , \"websiteTitle\" : \"Big Think\" , \"date\" : \"2016-02-09T20:12:00\" , \"url\" : \"https://bigthink.com/neurobonkers/a-pirate-bay-for-science\" , \"abstractNote\" : \"How one researcher created a pirate bay for science more powerful than even libraries at top universities.\" , \"language\" : \"en\" , \"accessDate\" : \"2018-12-06T20:10:14Z\" , } ] csl_item = export_as_csl ( zotero_data )[ 0 ] assert csl_item [ \"title\" ] == \"Meet the Robin Hood of Science\" assert csl_item [ \"container-title\" ] == \"Big Think\" def test_web_query_returns_single_result_legacy_manubot_url (): \"\"\" Check that single=1 is specified for web queries. Without this, Zotero can prefer translators that return multiple choices. This occurs with legacy Manubot mansucripts, which get assigned the DOI translator as top priority. https://github.com/zotero/translation-server/issues/65 ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/' \\ 'https://translate.manubot.org/web?single=1' ``` \"\"\" url = \"https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/\" zotero_metadata = web_query ( url ) assert isinstance ( zotero_metadata , list ) assert len ( zotero_metadata ) == 1 ( zotero_metadata ,) = zotero_metadata assert ( zotero_metadata [ \"title\" ] == \"Sci-Hub provides access to nearly all scholarly literature\" ) def test_web_query_returns_single_result_pubmed_url (): \"\"\" See test_web_query_returns_single_result_legacy_manubot_url docstring. ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D' \\ 'https://translate.manubot.org/web?single=1' ``` \"\"\" url = \"https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D\" zotero_metadata = web_query ( url ) assert isinstance ( zotero_metadata , list ) assert len ( zotero_metadata ) == 1 ( zotero_metadata ,) = zotero_metadata assert zotero_metadata [ \"title\" ] . startswith ( \"sci-hub[title]\" ) def test_search_query_isbn (): \"\"\" The translation-server search endpoint can be tested via curl: ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'isbn:9781339919881' \\ 'https://translate.manubot.org/search' ``` \"\"\" identifier = \"isbn:9781339919881\" zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ \"title\" ] . startswith ( \"The hetnet awakens\" ) def test_search_query_arxiv (): \"\"\" Test citing https://arxiv.org/abs/1604.05363v1 The translation-server search endpoint can be tested via curl: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'arxiv:1604.05363v1' \\ 'https://translate.manubot.org/search' ``` \"\"\" identifier = \"arxiv:1604.05363v1\" zotero_data = search_query ( identifier ) assert ( zotero_data [ 0 ][ \"title\" ] == \"Comparing Published Scientific Journal Articles to Their Pre-print Versions\" ) assert zotero_data [ 0 ][ \"creators\" ][ - 1 ][ \"firstName\" ] == \"Todd\" assert zotero_data [ 0 ][ \"date\" ] == \"2016-04-18\" @pytest . mark . parametrize ( \"identifier\" , [ \"30571677\" , # https://www.ncbi.nlm.nih.gov/pubmed/30571677 \"doi:10.1371/journal.pcbi.1006561\" , # https://doi.org/10.1371/journal.pcbi.1006561 ], ) def test_search_query ( identifier ): \"\"\" The translation-server search endpoint can be tested via curl: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data '30571677' \\ 'https://translate.manubot.org/search' ``` translation-server does not support PMIDs with a `pmid:` prefix. https://github.com/zotero/translation-server/issues/71 \"\"\" zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ \"title\" ] . startswith ( \"Ten simple rules for documenting scientific software\" ) assert zotero_data [ 0 ][ \"creators\" ][ 0 ][ \"lastName\" ] == \"Lee\" Functions test_export_as_csl def test_export_as_csl ( ) CSL export can be tested via curl: curl -- header \"Content-Type: application/json\" -- data '[{\"key\": \"IN22XN53\", \"itemType\": \"webpage\", \"date\": \"2016-02-09T20:12:00\"}]' 'https://translate.manubot.org/export?format=csljson' View Source def test_export_as_csl (): \"\"\" CSL export can be tested via curl: ``` curl \\ --header \"Content-Type: application/json\" \\ --data '[{\"key\": \"IN22XN53\", \"itemType\": \"webpage\", \"date\": \"2016-02-09T20:12:00\"}]' \\ 'https://translate.manubot.org/export?format=csljson' ``` \"\"\" zotero_data = [ { \"key\" : \"IN22XN53\" , \"version\" : 0 , \"itemType\" : \"webpage\" , \"creators\" : [], \"tags\" : [], \"title\" : \"Meet the Robin Hood of Science\" , \"websiteTitle\" : \"Big Think\" , \"date\" : \"2016-02-09T20:12:00\" , \"url\" : \"https://bigthink.com/neurobonkers/a-pirate-bay-for-science\" , \"abstractNote\" : \"How one researcher created a pirate bay for science more powerful than even libraries at top universities.\" , \"language\" : \"en\" , \"accessDate\" : \"2018-12-06T20:10:14Z\" , } ] csl_item = export_as_csl ( zotero_data )[ 0 ] assert csl_item [ \"title\" ] == \"Meet the Robin Hood of Science\" assert csl_item [ \"container-title\" ] == \"Big Think\" test_search_query def test_search_query ( identifier ) The translation-server search endpoint can be tested via curl: curl --verbose --header \"Content-Type: text/plain\" --data '30571677' 'https://translate.manubot.org/search' translation-server does not support PMIDs with a pmid: prefix. https://github.com/zotero/translation-server/issues/71 View Source @pytest . mark . parametrize ( \"identifier\" , [ \"30571677\", # https://www.ncbi.nlm.nih.gov/pubmed/30571677 \"doi:10.1371/journal.pcbi.1006561\", # https://doi.org/10.1371/journal.pcbi.1006561 ] , ) def test_search_query ( identifier ) : \"\"\" The translation-server search endpoint can be tested via curl: ``` curl --verbose \\ --header \" Content - Type : text / plain \" \\ --data '30571677' \\ 'https://translate.manubot.org/search' ``` translation-server does not support PMIDs with a `pmid:` prefix. https://github.com/zotero/translation-server/issues/71 \"\"\" zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ \"title\" ] . startswith ( \"Ten simple rules for documenting scientific software\" ) assert zotero_data [ 0 ][ \"creators\" ][ 0 ][ \"lastName\" ] == \"Lee\" test_search_query_arxiv def test_search_query_arxiv ( ) Test citing https://arxiv.org/abs/1604.05363v1 The translation-server search endpoint can be tested via curl: curl --verbose --header \"Content-Type: text/plain\" --data 'arxiv:1604.05363v1' 'https://translate.manubot.org/search' View Source def test_search_query_arxiv(): \"\"\" Test citing https://arxiv.org/abs/1604.05363v1 The translation-server search endpoint can be tested via curl: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'arxiv:1604.05363v1' \\ 'https://translate.manubot.org/search' ``` \"\"\" identifier = \"arxiv:1604.05363v1\" zotero_data = search_query(identifier) assert ( zotero_data[0][\"title\"] == \"Comparing Published Scientific Journal Articles to Their Pre-print Versions\" ) assert zotero_data[0][\"creators\"][-1][\"firstName\"] == \"Todd\" assert zotero_data[0][\"date\"] == \"2016-04-18\" test_search_query_isbn def test_search_query_isbn ( ) The translation-server search endpoint can be tested via curl: curl --header \"Content-Type: text/plain\" --data 'isbn:9781339919881' 'https://translate.manubot.org/search' View Source def test_search_query_isbn(): \"\"\" The translation-server search endpoint can be tested via curl: ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'isbn:9781339919881' \\ 'https://translate.manubot.org/search' ``` \"\"\" identifier = \"isbn:9781339919881\" zotero_data = search_query(identifier) assert zotero_data[0][\"title\"].startswith(\"The hetnet awakens\") test_web_query def test_web_query ( ) The translation-server web endpoint can be tested via curl: curl -- header \" Content-Type: text/plain \" -- data ' https://bigthink.com/neurobonkers/a-pirate-bay-for-science ' ' https://translate.manubot.org/web ' An outdated installation of translation-server caused the web query for this URL to be extraordinarily slow but has now been fixed. See https://github.com/zotero/translation-server/issues/63 View Source def test_web_query () : \"\"\" The translation - server web endpoint can be tested via curl : ``` curl \\ -- header \" Content-Type: text/plain \" \\ -- data ' https://bigthink.com/neurobonkers/a-pirate-bay-for-science ' \\ ' https://translate.manubot.org/web ' ``` An outdated installation of translation - server caused the web query for this URL to be extraordinarily slow but has now been fixed . See https : // github . com / zotero / translation - server / issues / 63 \"\"\" url = \" https://bigthink.com/neurobonkers/a-pirate-bay-for-science \" zotero_data = web_query ( url ) assert isinstance ( zotero_data , list ) assert len ( zotero_data ) == 1 assert zotero_data [ 0 ][ \" title \" ]. startswith ( \" Meet the Robin Hood of Science \" ) test_web_query_returns_single_result_legacy_manubot_url def test_web_query_returns_single_result_legacy_manubot_url ( ) Check that single=1 is specified for web queries. Without this, Zotero can prefer translators that return multiple choices. This occurs with legacy Manubot mansucripts, which get assigned the DOI translator as top priority. https://github.com/zotero/translation-server/issues/65 curl --header \"Content-Type: text/plain\" --data 'https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/' 'https://translate.manubot.org/web?single=1' View Source def test_web_query_returns_single_result_legacy_manubot_url () : \"\"\" Check that single = 1 is specified for web queries . Without this , Zotero can prefer translators that return multiple choices . This occurs with legacy Manubot mansucripts , which get assigned the DOI translator as top priority . https: //github.com/zotero/translation-server/issues/65 ``` curl \\ -- header \"Content-Type: text/plain\" \\ -- data ' https: //greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/' \\ ' https: //translate.manubot.org/web?single=1' ``` \"\"\" url = \"https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/\" zotero_metadata = web_query ( url ) assert isinstance ( zotero_metadata , list ) assert len ( zotero_metadata ) == 1 ( zotero_metadata ,) = zotero_metadata assert ( zotero_metadata [ \"title\" ] == \"Sci-Hub provides access to nearly all scholarly literature\" ) test_web_query_returns_single_result_pubmed_url def test_web_query_returns_single_result_pubmed_url ( ) See test_web_query_returns_single_result_legacy_manubot_url docstring. curl --header \"Content-Type: text/plain\" --data 'https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D' 'https://translate.manubot.org/web?single=1' View Source def test_web_query_returns_single_result_pubmed_url () : \"\"\" See test_web_query_returns_single_result_legacy_manubot_url docstring. ``` curl \\ --header \" Content - Type : text / plain \" \\ --data 'https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D' \\ 'https://translate.manubot.org/web?single=1' ``` \"\"\" url = \"https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D\" zotero_metadata = web_query ( url ) assert isinstance ( zotero_metadata , list ) assert len ( zotero_metadata ) == 1 ( zotero_metadata ,) = zotero_metadata assert zotero_metadata [ \"title\" ] . startswith ( \"sci-hub[title]\" )","title":"Test Zotero"},{"location":"reference/manubot/cite/tests/test_zotero/#module-manubotciteteststest_zotero","text":"None None View Source import pytest from manubot.cite.zotero import export_as_csl , search_query , web_query def test_web_query (): \"\"\" The translation-server web endpoint can be tested via curl: ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'https://bigthink.com/neurobonkers/a-pirate-bay-for-science' \\ 'https://translate.manubot.org/web' ``` An outdated installation of translation-server caused the web query for this URL to be extraordinarily slow but has now been fixed. See https://github.com/zotero/translation-server/issues/63 \"\"\" url = \"https://bigthink.com/neurobonkers/a-pirate-bay-for-science\" zotero_data = web_query ( url ) assert isinstance ( zotero_data , list ) assert len ( zotero_data ) == 1 assert zotero_data [ 0 ][ \"title\" ] . startswith ( \"Meet the Robin Hood of Science\" ) def test_export_as_csl (): \"\"\" CSL export can be tested via curl: ``` curl \\ --header \"Content-Type: application/json\" \\ --data '[{\"key\": \"IN22XN53\", \"itemType\": \"webpage\", \"date\": \"2016-02-09T20:12:00\"}]' \\ 'https://translate.manubot.org/export?format=csljson' ``` \"\"\" zotero_data = [ { \"key\" : \"IN22XN53\" , \"version\" : 0 , \"itemType\" : \"webpage\" , \"creators\" : [], \"tags\" : [], \"title\" : \"Meet the Robin Hood of Science\" , \"websiteTitle\" : \"Big Think\" , \"date\" : \"2016-02-09T20:12:00\" , \"url\" : \"https://bigthink.com/neurobonkers/a-pirate-bay-for-science\" , \"abstractNote\" : \"How one researcher created a pirate bay for science more powerful than even libraries at top universities.\" , \"language\" : \"en\" , \"accessDate\" : \"2018-12-06T20:10:14Z\" , } ] csl_item = export_as_csl ( zotero_data )[ 0 ] assert csl_item [ \"title\" ] == \"Meet the Robin Hood of Science\" assert csl_item [ \"container-title\" ] == \"Big Think\" def test_web_query_returns_single_result_legacy_manubot_url (): \"\"\" Check that single=1 is specified for web queries. Without this, Zotero can prefer translators that return multiple choices. This occurs with legacy Manubot mansucripts, which get assigned the DOI translator as top priority. https://github.com/zotero/translation-server/issues/65 ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/' \\ 'https://translate.manubot.org/web?single=1' ``` \"\"\" url = \"https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/\" zotero_metadata = web_query ( url ) assert isinstance ( zotero_metadata , list ) assert len ( zotero_metadata ) == 1 ( zotero_metadata ,) = zotero_metadata assert ( zotero_metadata [ \"title\" ] == \"Sci-Hub provides access to nearly all scholarly literature\" ) def test_web_query_returns_single_result_pubmed_url (): \"\"\" See test_web_query_returns_single_result_legacy_manubot_url docstring. ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D' \\ 'https://translate.manubot.org/web?single=1' ``` \"\"\" url = \"https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D\" zotero_metadata = web_query ( url ) assert isinstance ( zotero_metadata , list ) assert len ( zotero_metadata ) == 1 ( zotero_metadata ,) = zotero_metadata assert zotero_metadata [ \"title\" ] . startswith ( \"sci-hub[title]\" ) def test_search_query_isbn (): \"\"\" The translation-server search endpoint can be tested via curl: ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'isbn:9781339919881' \\ 'https://translate.manubot.org/search' ``` \"\"\" identifier = \"isbn:9781339919881\" zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ \"title\" ] . startswith ( \"The hetnet awakens\" ) def test_search_query_arxiv (): \"\"\" Test citing https://arxiv.org/abs/1604.05363v1 The translation-server search endpoint can be tested via curl: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'arxiv:1604.05363v1' \\ 'https://translate.manubot.org/search' ``` \"\"\" identifier = \"arxiv:1604.05363v1\" zotero_data = search_query ( identifier ) assert ( zotero_data [ 0 ][ \"title\" ] == \"Comparing Published Scientific Journal Articles to Their Pre-print Versions\" ) assert zotero_data [ 0 ][ \"creators\" ][ - 1 ][ \"firstName\" ] == \"Todd\" assert zotero_data [ 0 ][ \"date\" ] == \"2016-04-18\" @pytest . mark . parametrize ( \"identifier\" , [ \"30571677\" , # https://www.ncbi.nlm.nih.gov/pubmed/30571677 \"doi:10.1371/journal.pcbi.1006561\" , # https://doi.org/10.1371/journal.pcbi.1006561 ], ) def test_search_query ( identifier ): \"\"\" The translation-server search endpoint can be tested via curl: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data '30571677' \\ 'https://translate.manubot.org/search' ``` translation-server does not support PMIDs with a `pmid:` prefix. https://github.com/zotero/translation-server/issues/71 \"\"\" zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ \"title\" ] . startswith ( \"Ten simple rules for documenting scientific software\" ) assert zotero_data [ 0 ][ \"creators\" ][ 0 ][ \"lastName\" ] == \"Lee\"","title":"Module manubot.cite.tests.test_zotero"},{"location":"reference/manubot/cite/tests/test_zotero/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_zotero/#test_export_as_csl","text":"def test_export_as_csl ( ) CSL export can be tested via curl: curl -- header \"Content-Type: application/json\" -- data '[{\"key\": \"IN22XN53\", \"itemType\": \"webpage\", \"date\": \"2016-02-09T20:12:00\"}]' 'https://translate.manubot.org/export?format=csljson' View Source def test_export_as_csl (): \"\"\" CSL export can be tested via curl: ``` curl \\ --header \"Content-Type: application/json\" \\ --data '[{\"key\": \"IN22XN53\", \"itemType\": \"webpage\", \"date\": \"2016-02-09T20:12:00\"}]' \\ 'https://translate.manubot.org/export?format=csljson' ``` \"\"\" zotero_data = [ { \"key\" : \"IN22XN53\" , \"version\" : 0 , \"itemType\" : \"webpage\" , \"creators\" : [], \"tags\" : [], \"title\" : \"Meet the Robin Hood of Science\" , \"websiteTitle\" : \"Big Think\" , \"date\" : \"2016-02-09T20:12:00\" , \"url\" : \"https://bigthink.com/neurobonkers/a-pirate-bay-for-science\" , \"abstractNote\" : \"How one researcher created a pirate bay for science more powerful than even libraries at top universities.\" , \"language\" : \"en\" , \"accessDate\" : \"2018-12-06T20:10:14Z\" , } ] csl_item = export_as_csl ( zotero_data )[ 0 ] assert csl_item [ \"title\" ] == \"Meet the Robin Hood of Science\" assert csl_item [ \"container-title\" ] == \"Big Think\"","title":"test_export_as_csl"},{"location":"reference/manubot/cite/tests/test_zotero/#test_search_query","text":"def test_search_query ( identifier ) The translation-server search endpoint can be tested via curl: curl --verbose --header \"Content-Type: text/plain\" --data '30571677' 'https://translate.manubot.org/search' translation-server does not support PMIDs with a pmid: prefix. https://github.com/zotero/translation-server/issues/71 View Source @pytest . mark . parametrize ( \"identifier\" , [ \"30571677\", # https://www.ncbi.nlm.nih.gov/pubmed/30571677 \"doi:10.1371/journal.pcbi.1006561\", # https://doi.org/10.1371/journal.pcbi.1006561 ] , ) def test_search_query ( identifier ) : \"\"\" The translation-server search endpoint can be tested via curl: ``` curl --verbose \\ --header \" Content - Type : text / plain \" \\ --data '30571677' \\ 'https://translate.manubot.org/search' ``` translation-server does not support PMIDs with a `pmid:` prefix. https://github.com/zotero/translation-server/issues/71 \"\"\" zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ \"title\" ] . startswith ( \"Ten simple rules for documenting scientific software\" ) assert zotero_data [ 0 ][ \"creators\" ][ 0 ][ \"lastName\" ] == \"Lee\"","title":"test_search_query"},{"location":"reference/manubot/cite/tests/test_zotero/#test_search_query_arxiv","text":"def test_search_query_arxiv ( ) Test citing https://arxiv.org/abs/1604.05363v1 The translation-server search endpoint can be tested via curl: curl --verbose --header \"Content-Type: text/plain\" --data 'arxiv:1604.05363v1' 'https://translate.manubot.org/search' View Source def test_search_query_arxiv(): \"\"\" Test citing https://arxiv.org/abs/1604.05363v1 The translation-server search endpoint can be tested via curl: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'arxiv:1604.05363v1' \\ 'https://translate.manubot.org/search' ``` \"\"\" identifier = \"arxiv:1604.05363v1\" zotero_data = search_query(identifier) assert ( zotero_data[0][\"title\"] == \"Comparing Published Scientific Journal Articles to Their Pre-print Versions\" ) assert zotero_data[0][\"creators\"][-1][\"firstName\"] == \"Todd\" assert zotero_data[0][\"date\"] == \"2016-04-18\"","title":"test_search_query_arxiv"},{"location":"reference/manubot/cite/tests/test_zotero/#test_search_query_isbn","text":"def test_search_query_isbn ( ) The translation-server search endpoint can be tested via curl: curl --header \"Content-Type: text/plain\" --data 'isbn:9781339919881' 'https://translate.manubot.org/search' View Source def test_search_query_isbn(): \"\"\" The translation-server search endpoint can be tested via curl: ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'isbn:9781339919881' \\ 'https://translate.manubot.org/search' ``` \"\"\" identifier = \"isbn:9781339919881\" zotero_data = search_query(identifier) assert zotero_data[0][\"title\"].startswith(\"The hetnet awakens\")","title":"test_search_query_isbn"},{"location":"reference/manubot/cite/tests/test_zotero/#test_web_query","text":"def test_web_query ( ) The translation-server web endpoint can be tested via curl: curl -- header \" Content-Type: text/plain \" -- data ' https://bigthink.com/neurobonkers/a-pirate-bay-for-science ' ' https://translate.manubot.org/web ' An outdated installation of translation-server caused the web query for this URL to be extraordinarily slow but has now been fixed. See https://github.com/zotero/translation-server/issues/63 View Source def test_web_query () : \"\"\" The translation - server web endpoint can be tested via curl : ``` curl \\ -- header \" Content-Type: text/plain \" \\ -- data ' https://bigthink.com/neurobonkers/a-pirate-bay-for-science ' \\ ' https://translate.manubot.org/web ' ``` An outdated installation of translation - server caused the web query for this URL to be extraordinarily slow but has now been fixed . See https : // github . com / zotero / translation - server / issues / 63 \"\"\" url = \" https://bigthink.com/neurobonkers/a-pirate-bay-for-science \" zotero_data = web_query ( url ) assert isinstance ( zotero_data , list ) assert len ( zotero_data ) == 1 assert zotero_data [ 0 ][ \" title \" ]. startswith ( \" Meet the Robin Hood of Science \" )","title":"test_web_query"},{"location":"reference/manubot/cite/tests/test_zotero/#test_web_query_returns_single_result_legacy_manubot_url","text":"def test_web_query_returns_single_result_legacy_manubot_url ( ) Check that single=1 is specified for web queries. Without this, Zotero can prefer translators that return multiple choices. This occurs with legacy Manubot mansucripts, which get assigned the DOI translator as top priority. https://github.com/zotero/translation-server/issues/65 curl --header \"Content-Type: text/plain\" --data 'https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/' 'https://translate.manubot.org/web?single=1' View Source def test_web_query_returns_single_result_legacy_manubot_url () : \"\"\" Check that single = 1 is specified for web queries . Without this , Zotero can prefer translators that return multiple choices . This occurs with legacy Manubot mansucripts , which get assigned the DOI translator as top priority . https: //github.com/zotero/translation-server/issues/65 ``` curl \\ -- header \"Content-Type: text/plain\" \\ -- data ' https: //greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/' \\ ' https: //translate.manubot.org/web?single=1' ``` \"\"\" url = \"https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/\" zotero_metadata = web_query ( url ) assert isinstance ( zotero_metadata , list ) assert len ( zotero_metadata ) == 1 ( zotero_metadata ,) = zotero_metadata assert ( zotero_metadata [ \"title\" ] == \"Sci-Hub provides access to nearly all scholarly literature\" )","title":"test_web_query_returns_single_result_legacy_manubot_url"},{"location":"reference/manubot/cite/tests/test_zotero/#test_web_query_returns_single_result_pubmed_url","text":"def test_web_query_returns_single_result_pubmed_url ( ) See test_web_query_returns_single_result_legacy_manubot_url docstring. curl --header \"Content-Type: text/plain\" --data 'https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D' 'https://translate.manubot.org/web?single=1' View Source def test_web_query_returns_single_result_pubmed_url () : \"\"\" See test_web_query_returns_single_result_legacy_manubot_url docstring. ``` curl \\ --header \" Content - Type : text / plain \" \\ --data 'https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D' \\ 'https://translate.manubot.org/web?single=1' ``` \"\"\" url = \"https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D\" zotero_metadata = web_query ( url ) assert isinstance ( zotero_metadata , list ) assert len ( zotero_metadata ) == 1 ( zotero_metadata ,) = zotero_metadata assert zotero_metadata [ \"title\" ] . startswith ( \"sci-hub[title]\" )","title":"test_web_query_returns_single_result_pubmed_url"},{"location":"reference/manubot/pandoc/","text":"Module manubot.pandoc None None Sub-modules manubot.pandoc.bibliography manubot.pandoc.cite_filter manubot.pandoc.util","title":"Index"},{"location":"reference/manubot/pandoc/#module-manubotpandoc","text":"None None","title":"Module manubot.pandoc"},{"location":"reference/manubot/pandoc/#sub-modules","text":"manubot.pandoc.bibliography manubot.pandoc.cite_filter manubot.pandoc.util","title":"Sub-modules"},{"location":"reference/manubot/pandoc/bibliography/","text":"Module manubot.pandoc.bibliography None None View Source import json import logging import os import subprocess from typing import Any , Dict , List , Optional from manubot.pandoc.util import get_pandoc_info from manubot.util import shlex_join def load_bibliography ( path : Optional [ str ] = None , text : Optional [ str ] = None , input_format : Optional [ str ] = None , ) -> List [ Dict [ str , Any ]]: \"\"\" Convert a bibliography to CSL JSON using either `pandoc-citeproc --bib2json` or `pandoc --to=csljson`, depending on availability of pandoc commands on the system. Accepts either a bibliography path or text (string). If supplying text, pandoc-citeproc will likely require input_format be specified. The CSL JSON is returned as Python objects. If loading fails, log an error and return an empty list. Parameters ---------- path : str, pathlike, or None Path to a bibliography file. Extension is used by pandoc-citeproc to infer the format of the input. text : str or None Text representation of the bibliography, such as a JSON-formatted string. `input_format` should be specified if providing text input. input_format : str or None Manually specified input formatted that is supported by pandoc-citeproc: https://github.com/jgm/pandoc-citeproc/blob/master/man/pandoc-citeproc.1.md#options Use 'bib' for BibLaTeX. Use 'json' for CSL JSON. Returns ------- csl_json : JSON-like object CSL JSON Data for the references encoded by the input bibliography. \"\"\" use_text = path is None use_path = text is None if use_path : path = os . fspath ( path ) if not ( use_text ^ use_path ): raise ValueError ( \"load_bibliography: specify either path or text but not both.\" ) pdoc_info = get_pandoc_info () if pdoc_info [ \"pandoc-citeproc\" ]: return _load_bibliography_pandoc_citeproc ( path , text , input_format ) if input_format == \"bib\" or ( use_path and path . endswith ( \".bib\" )): return _load_bibliography_pandoc ( path , text ) logging . error ( \"pandoc-citeproc not found on system, but is required to convert any format besides 'bib': \" \"manubot.pandoc.bibliography.load_bibliography returning empty CSL JSON\" ) return [] def _load_bibliography_pandoc_citeproc ( path : Optional [ str ] = None , text : Optional [ str ] = None , input_format : Optional [ str ] = None , ) -> List [ Dict [ str , Any ]]: \"\"\" Convert a bibliography to CSL JSON using `pandoc-citeproc --bib2json`. Accepts either a bibliography path or text (string). If supplying text, pandoc-citeproc will likely require input_format be specified. The CSL JSON is returned as Python objects. If loading fails, log an error and return an empty list. Parameters ---------- path : str, pathlike, or None Path to a bibliography file. Extension is used by pandoc-citeproc to infer the format of the input. text : str or None Text representation of the bibliography, such as a JSON-formatted string. `input_format` should be specified if providing text input. input_format : str or None Manually specified input formatted that is supported by pandoc-citeproc: https://github.com/jgm/pandoc-citeproc/blob/master/man/pandoc-citeproc.1.md#options Returns ------- csl_json : JSON-like object CSL JSON Data for the references encoded by the input bibliography. \"\"\" command_args = [ \"pandoc-citeproc\" , \"--bib2json\" ] if input_format : command_args . extend ([ \"--format\" , input_format ]) return _pandoc_system_call ( command_args , path , text ) def _load_bibliography_pandoc ( path : Optional [ str ] = None , text : Optional [ str ] = None , ) -> List [ Dict [ str , Any ]]: \"\"\" Convert a biblatex (.bib) bibliography to CSL JSON data using pandoc directly. Pandoc support for csljson output requires pandoc >= 2.11. \"\"\" pdoc_info = get_pandoc_info () if not pdoc_info [ \"pandoc\" ]: logging . error ( \"pandoc not found on system: \" \"manubot.pandoc.bibliography.load_bibliography returning empty CSL JSON\" ) return [] if pdoc_info [ \"pandoc version\" ] < ( 2 , 11 ): logging . error ( \"pandoc >= version 2.11 required for biblatex to csljson conversion. \" \"manubot.pandoc.bibliography.load_bibliography returning empty CSL JSON\" ) return [] command_args = \"pandoc --from=biblatex --to=csljson\" . split () return _pandoc_system_call ( command_args , path , text ) def _pandoc_system_call ( command_args : List [ str ], path : Optional [ str ], text : Optional [ str ] ) -> List [ Dict [ str , Any ]]: \"\"\" Call \"pandoc citeproc\" or \"pandoc\" using input from a path or text. Return dict representing CSL JSON. \"\"\" assert command_args [ 0 ] . startswith ( \"pandoc\" ) run_kwargs = {} if path : command_args . append ( os . fspath ( path )) else : run_kwargs [ \"input\" ] = text logging . info ( \"load_bibliography subprocess args: \\n >>> \" + shlex_join ( command_args )) process = subprocess . run ( command_args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ** run_kwargs , ) logging . info ( f \"captured stderr: \\n { process . stderr } \" ) if process . returncode : logging . error ( f \"Pandoc call returned nonzero exit code. \\n \" f \" { shlex_join ( process . args ) } \\n { process . stderr } \" ) return [] try : csl_json = json . loads ( process . stdout ) except ( TypeError , json . decoder . JSONDecodeError ): logging . error ( f \"Error parsing bib2json output as JSON: \\n { process . stdout } \" ) csl_json = [] return csl_json Functions load_bibliography def load_bibliography ( path : Optional [ str ] = None , text : Optional [ str ] = None , input_format : Optional [ str ] = None ) -> List [ Dict [ str , Any ]] Convert a bibliography to CSL JSON using either pandoc-citeproc --bib2json or pandoc --to=csljson , depending on availability of pandoc commands on the system. Accepts either a bibliography path or text (string). If supplying text, pandoc-citeproc will likely require input_format be specified. The CSL JSON is returned as Python objects. If loading fails, log an error and return an empty list. Parameters: Name Type Description Default path str, pathlike, or None Path to a bibliography file. Extension is used by pandoc-citeproc to infer the format of the input. None text str or None Text representation of the bibliography, such as a JSON-formatted string. input_format should be specified if providing text input. None input_format str or None Manually specified input formatted that is supported by pandoc-citeproc: https://github.com/jgm/pandoc-citeproc/blob/master/man/pandoc-citeproc.1.md#options Use 'bib' for BibLaTeX. Use 'json' for CSL JSON. None Returns: Type Description JSON-like object CSL JSON Data for the references encoded by the input bibliography. View Source def load_bibliography ( path : Optional [ str ] = None , text : Optional [ str ] = None , input_format : Optional [ str ] = None , ) -> List [ Dict[str, Any ] ]: \"\"\" Convert a bibliography to CSL JSON using either `pandoc-citeproc --bib2json` or `pandoc --to=csljson`, depending on availability of pandoc commands on the system. Accepts either a bibliography path or text (string). If supplying text, pandoc-citeproc will likely require input_format be specified. The CSL JSON is returned as Python objects. If loading fails, log an error and return an empty list. Parameters ---------- path : str, pathlike, or None Path to a bibliography file. Extension is used by pandoc-citeproc to infer the format of the input. text : str or None Text representation of the bibliography, such as a JSON-formatted string. `input_format` should be specified if providing text input. input_format : str or None Manually specified input formatted that is supported by pandoc-citeproc: https://github.com/jgm/pandoc-citeproc/blob/master/man/pandoc-citeproc.1.md#options Use 'bib' for BibLaTeX. Use 'json' for CSL JSON. Returns ------- csl_json : JSON-like object CSL JSON Data for the references encoded by the input bibliography. \"\"\" use_text = path is None use_path = text is None if use_path : path = os . fspath ( path ) if not ( use_text ^ use_path ) : raise ValueError ( \"load_bibliography: specify either path or text but not both.\" ) pdoc_info = get_pandoc_info () if pdoc_info [ \"pandoc-citeproc\" ] : return _load_bibliography_pandoc_citeproc ( path , text , input_format ) if input_format == \"bib\" or ( use_path and path . endswith ( \".bib\" )) : return _load_bibliography_pandoc ( path , text ) logging . error ( \"pandoc-citeproc not found on system, but is required to convert any format besides 'bib': \" \"manubot.pandoc.bibliography.load_bibliography returning empty CSL JSON\" ) return []","title":"Bibliography"},{"location":"reference/manubot/pandoc/bibliography/#module-manubotpandocbibliography","text":"None None View Source import json import logging import os import subprocess from typing import Any , Dict , List , Optional from manubot.pandoc.util import get_pandoc_info from manubot.util import shlex_join def load_bibliography ( path : Optional [ str ] = None , text : Optional [ str ] = None , input_format : Optional [ str ] = None , ) -> List [ Dict [ str , Any ]]: \"\"\" Convert a bibliography to CSL JSON using either `pandoc-citeproc --bib2json` or `pandoc --to=csljson`, depending on availability of pandoc commands on the system. Accepts either a bibliography path or text (string). If supplying text, pandoc-citeproc will likely require input_format be specified. The CSL JSON is returned as Python objects. If loading fails, log an error and return an empty list. Parameters ---------- path : str, pathlike, or None Path to a bibliography file. Extension is used by pandoc-citeproc to infer the format of the input. text : str or None Text representation of the bibliography, such as a JSON-formatted string. `input_format` should be specified if providing text input. input_format : str or None Manually specified input formatted that is supported by pandoc-citeproc: https://github.com/jgm/pandoc-citeproc/blob/master/man/pandoc-citeproc.1.md#options Use 'bib' for BibLaTeX. Use 'json' for CSL JSON. Returns ------- csl_json : JSON-like object CSL JSON Data for the references encoded by the input bibliography. \"\"\" use_text = path is None use_path = text is None if use_path : path = os . fspath ( path ) if not ( use_text ^ use_path ): raise ValueError ( \"load_bibliography: specify either path or text but not both.\" ) pdoc_info = get_pandoc_info () if pdoc_info [ \"pandoc-citeproc\" ]: return _load_bibliography_pandoc_citeproc ( path , text , input_format ) if input_format == \"bib\" or ( use_path and path . endswith ( \".bib\" )): return _load_bibliography_pandoc ( path , text ) logging . error ( \"pandoc-citeproc not found on system, but is required to convert any format besides 'bib': \" \"manubot.pandoc.bibliography.load_bibliography returning empty CSL JSON\" ) return [] def _load_bibliography_pandoc_citeproc ( path : Optional [ str ] = None , text : Optional [ str ] = None , input_format : Optional [ str ] = None , ) -> List [ Dict [ str , Any ]]: \"\"\" Convert a bibliography to CSL JSON using `pandoc-citeproc --bib2json`. Accepts either a bibliography path or text (string). If supplying text, pandoc-citeproc will likely require input_format be specified. The CSL JSON is returned as Python objects. If loading fails, log an error and return an empty list. Parameters ---------- path : str, pathlike, or None Path to a bibliography file. Extension is used by pandoc-citeproc to infer the format of the input. text : str or None Text representation of the bibliography, such as a JSON-formatted string. `input_format` should be specified if providing text input. input_format : str or None Manually specified input formatted that is supported by pandoc-citeproc: https://github.com/jgm/pandoc-citeproc/blob/master/man/pandoc-citeproc.1.md#options Returns ------- csl_json : JSON-like object CSL JSON Data for the references encoded by the input bibliography. \"\"\" command_args = [ \"pandoc-citeproc\" , \"--bib2json\" ] if input_format : command_args . extend ([ \"--format\" , input_format ]) return _pandoc_system_call ( command_args , path , text ) def _load_bibliography_pandoc ( path : Optional [ str ] = None , text : Optional [ str ] = None , ) -> List [ Dict [ str , Any ]]: \"\"\" Convert a biblatex (.bib) bibliography to CSL JSON data using pandoc directly. Pandoc support for csljson output requires pandoc >= 2.11. \"\"\" pdoc_info = get_pandoc_info () if not pdoc_info [ \"pandoc\" ]: logging . error ( \"pandoc not found on system: \" \"manubot.pandoc.bibliography.load_bibliography returning empty CSL JSON\" ) return [] if pdoc_info [ \"pandoc version\" ] < ( 2 , 11 ): logging . error ( \"pandoc >= version 2.11 required for biblatex to csljson conversion. \" \"manubot.pandoc.bibliography.load_bibliography returning empty CSL JSON\" ) return [] command_args = \"pandoc --from=biblatex --to=csljson\" . split () return _pandoc_system_call ( command_args , path , text ) def _pandoc_system_call ( command_args : List [ str ], path : Optional [ str ], text : Optional [ str ] ) -> List [ Dict [ str , Any ]]: \"\"\" Call \"pandoc citeproc\" or \"pandoc\" using input from a path or text. Return dict representing CSL JSON. \"\"\" assert command_args [ 0 ] . startswith ( \"pandoc\" ) run_kwargs = {} if path : command_args . append ( os . fspath ( path )) else : run_kwargs [ \"input\" ] = text logging . info ( \"load_bibliography subprocess args: \\n >>> \" + shlex_join ( command_args )) process = subprocess . run ( command_args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ** run_kwargs , ) logging . info ( f \"captured stderr: \\n { process . stderr } \" ) if process . returncode : logging . error ( f \"Pandoc call returned nonzero exit code. \\n \" f \" { shlex_join ( process . args ) } \\n { process . stderr } \" ) return [] try : csl_json = json . loads ( process . stdout ) except ( TypeError , json . decoder . JSONDecodeError ): logging . error ( f \"Error parsing bib2json output as JSON: \\n { process . stdout } \" ) csl_json = [] return csl_json","title":"Module manubot.pandoc.bibliography"},{"location":"reference/manubot/pandoc/bibliography/#functions","text":"","title":"Functions"},{"location":"reference/manubot/pandoc/bibliography/#load_bibliography","text":"def load_bibliography ( path : Optional [ str ] = None , text : Optional [ str ] = None , input_format : Optional [ str ] = None ) -> List [ Dict [ str , Any ]] Convert a bibliography to CSL JSON using either pandoc-citeproc --bib2json or pandoc --to=csljson , depending on availability of pandoc commands on the system. Accepts either a bibliography path or text (string). If supplying text, pandoc-citeproc will likely require input_format be specified. The CSL JSON is returned as Python objects. If loading fails, log an error and return an empty list. Parameters: Name Type Description Default path str, pathlike, or None Path to a bibliography file. Extension is used by pandoc-citeproc to infer the format of the input. None text str or None Text representation of the bibliography, such as a JSON-formatted string. input_format should be specified if providing text input. None input_format str or None Manually specified input formatted that is supported by pandoc-citeproc: https://github.com/jgm/pandoc-citeproc/blob/master/man/pandoc-citeproc.1.md#options Use 'bib' for BibLaTeX. Use 'json' for CSL JSON. None Returns: Type Description JSON-like object CSL JSON Data for the references encoded by the input bibliography. View Source def load_bibliography ( path : Optional [ str ] = None , text : Optional [ str ] = None , input_format : Optional [ str ] = None , ) -> List [ Dict[str, Any ] ]: \"\"\" Convert a bibliography to CSL JSON using either `pandoc-citeproc --bib2json` or `pandoc --to=csljson`, depending on availability of pandoc commands on the system. Accepts either a bibliography path or text (string). If supplying text, pandoc-citeproc will likely require input_format be specified. The CSL JSON is returned as Python objects. If loading fails, log an error and return an empty list. Parameters ---------- path : str, pathlike, or None Path to a bibliography file. Extension is used by pandoc-citeproc to infer the format of the input. text : str or None Text representation of the bibliography, such as a JSON-formatted string. `input_format` should be specified if providing text input. input_format : str or None Manually specified input formatted that is supported by pandoc-citeproc: https://github.com/jgm/pandoc-citeproc/blob/master/man/pandoc-citeproc.1.md#options Use 'bib' for BibLaTeX. Use 'json' for CSL JSON. Returns ------- csl_json : JSON-like object CSL JSON Data for the references encoded by the input bibliography. \"\"\" use_text = path is None use_path = text is None if use_path : path = os . fspath ( path ) if not ( use_text ^ use_path ) : raise ValueError ( \"load_bibliography: specify either path or text but not both.\" ) pdoc_info = get_pandoc_info () if pdoc_info [ \"pandoc-citeproc\" ] : return _load_bibliography_pandoc_citeproc ( path , text , input_format ) if input_format == \"bib\" or ( use_path and path . endswith ( \".bib\" )) : return _load_bibliography_pandoc ( path , text ) logging . error ( \"pandoc-citeproc not found on system, but is required to convert any format besides 'bib': \" \"manubot.pandoc.bibliography.load_bibliography returning empty CSL JSON\" ) return []","title":"load_bibliography"},{"location":"reference/manubot/pandoc/cite_filter/","text":"Module manubot.pandoc.cite_filter This module defines a pandoc filter for manubot cite functionality. The filter can be called with the pandoc-manubot-cite command. Options Configuration is provided via Pandoc metadata fields. bibliography (sequence of strings): Use to define reference metadata manually. Pandoc supports specifying multiple external bibliography files. When bibliography files are specified, this filter will read them instead of pandoc. Behavior should be similar to Pandoc, with format inferred by extension: .json for CSL JSON, .yaml for CSL YAML, .bib for BibLaTeX. references (sequence of CSL-Item mappings): Same as Pandoc's references metadata field. citekey-aliases (mapping: string -> string): Used to define aliases (tags) for cite-by-id citations. Useful when a citation is used many times or contains invalid characters. Aliases can also be defined in markdown with link reference syntax. manubot-infer-citekey-prefixes (boolean): Attempt to infer the prefix for citekeys without a known prefix. For example, allow '@10.1371/journal.pcbi.1007128' with a 'doi:' prefix. Default is true. manubot-output-citekeys (string): path to write TSV table of citekeys showing their transformation from input_id to short_id. manubot-bibliography-cache (string): Path to read and write bibliographic metadata as CSL JSON/YAML. Intended as a human-editable cache of the bibliography data, for situations where this filter is run multiple times. This is similar to specifying bibliography=FILE and manubot-output-bibliography=FILE in a single argument, but will not error trying to read a bibliography file that does not yet exist. manubot-output-bibliography (string): path to write generated CSL JSON bibliography. If specified in addition to manubot-bibliography-cache , two output bibliographies will be written (with the same references). manubot-requests-cache-path (string): Enable caching HTTP requests to this path (minus the extension) using requests-cache . For example, setting to .cache/requests-cache will cache requests to .cache/requests-cache.sqlite . manubot-clear-requests-cache (boolean): If true, clear the requests cache at manubot-requests-cache-path . manubot-fail-on-errors (boolean): If true, return a nonzero exit status if any errors are logged. Default is false, which allows Pandoc to proceed when some citations could not be processed. development commands # export to plain text (with pandoc < 2.11) pandoc --to = plain --standalone --filter = pandoc-manubot-cite --filter = pandoc-citeproc manubot/pandoc/tests/test_cite_filter/input.md # call the filter manually using pandoc JSON output pandoc --to = json manubot/pandoc/tests/test_cite_filter/input.md | python manubot/pandoc/test_cite.py markdown Related resources on pandoc filters: python pandocfilters package python panflute package panflute Citation class View Source \"\"\" This module defines a pandoc filter for manubot cite functionality . The filter can be called with the ` pandoc - manubot - cite ` command . ## Options Configuration is provided via Pandoc metadata fields . - ` bibliography ` ( sequence of strings ) : Use to define reference metadata manually . Pandoc supports specifying multiple external bibliography files . When bibliography files are specified , this filter will read them instead of pandoc . Behavior should be similar to Pandoc , with format inferred by extension : . json for CSL JSON , . yaml for CSL YAML , . bib for BibLaTeX . - ` references ` ( sequence of CSL - Item mappings ) : Same as Pandoc ' s references metadata field . - ` citekey - aliases ` ( mapping : string -> string ) : Used to define aliases ( tags ) for cite - by - id citations . Useful when a citation is used many times or contains invalid characters . Aliases can also be defined in markdown with link reference syntax . - ` manubot - infer - citekey - prefixes ` ( boolean ) : Attempt to infer the prefix for citekeys without a known prefix . For example , allow ' @10.1371 / journal . pcbi .1007128 ' with a ' doi : ' prefix . Default is true . - ` manubot - output - citekeys ` ( string ) : path to write TSV table of citekeys showing their transformation from input_id to short_id . - ` manubot - bibliography - cache ` ( string ) : Path to read and write bibliographic metadata as CSL JSON / YAML . Intended as a human - editable cache of the bibliography data , for situations where this filter is run multiple times . This is similar to specifying bibliography = FILE and manubot - output - bibliography = FILE in a single argument , but will not error trying to read a bibliography file that does not yet exist . - ` manubot - output - bibliography ` ( string ) : path to write generated CSL JSON bibliography . If specified in addition to ` manubot - bibliography - cache ` , two output bibliographies will be written ( with the same references ). - ` manubot - requests - cache - path ` ( string ) : Enable caching HTTP requests to this path ( minus the extension ) using [ requests - cache ]( https : //github.com/reclosedev/requests-cache). For example , setting to ` . cache / requests - cache ` will cache requests to ` . cache / requests - cache . sqlite ` . - ` manubot - clear - requests - cache ` ( boolean ) : If true , clear the requests cache at ` manubot - requests - cache - path ` . - ` manubot - fail - on - errors ` ( boolean ) : If true , return a nonzero exit status if any errors are logged . Default is false , which allows Pandoc to proceed when some citations could not be processed . ## development commands ``` shell # export to plain text (with pandoc < 2.11) pandoc \\ -- to = plain \\ -- standalone \\ -- filter = pandoc - manubot - cite \\ -- filter = pandoc - citeproc \\ manubot / pandoc / tests / test_cite_filter / input . md # call the filter manually using pandoc JSON output pandoc \\ -- to = json \\ manubot / pandoc / tests / test_cite_filter / input . md \\ | python manubot / pandoc / test_cite . py markdown ``` Related resources on pandoc filters : - [ python pandocfilters package ]( https : //github.com/jgm/pandocfilters) - [ python panflute package ]( https : //github.com/sergiocorreia/panflute) - [ panflute Citation class ]( http : //scorreia.com/software/panflute/code.html#panflute.elements.Citation) \"\"\" import argparse import logging import os from typing import Any , Dict import panflute as pf from manubot . cite . citations import Citations def parse_args () -> argparse . Namespace : \"\"\" Read command line arguments \"\"\" parser = argparse . ArgumentParser ( description = \"Pandoc filter for citation by persistent identifier. \" \"Filters are command-line programs that read and write a JSON-encoded abstract syntax tree for Pandoc. \" \"Unless you are debugging, run this filter as part of a pandoc command by specifying --filter=pandoc-manubot-cite.\" ) parser . add_argument ( \"target_format\" , help = \"output format of the pandoc command, as per Pandoc's --to option\" , ) parser . add_argument ( \"--input\" , nargs = \"?\" , type = argparse . FileType ( \"r\" , encoding = \"utf-8\" ), help = \"path read JSON input (defaults to stdin)\" , ) parser . add_argument ( \"--output\" , nargs = \"?\" , type = argparse . FileType ( \"w\" , encoding = \"utf-8\" ), help = \"path to write JSON output (defaults to stdout)\" , ) args = parser . parse_args () return args def _get_citekeys_action ( elem : pf . Element , doc : pf . Doc ) -> None : \"\"\" Panflute action to extract citationId from all Citations in the AST . \"\"\" if not isinstance ( elem , pf . Citation ) : return None manuscript_citekeys = doc . manubot [ \"manuscript_citekeys\" ] manuscript_citekeys . append ( elem . id ) return None def _citation_to_id_action ( elem : pf . Element , doc : pf . Doc ) -> None : \"\"\" Panflute action to update the citationId of Citations in the AST with their manubot - created keys . \"\"\" if not isinstance ( elem , pf . Citation ) : return None mapper = doc . manubot [ \"citekey_shortener\" ] if elem . id in mapper : elem . id = mapper [ elem . id ] return None def _get_reference_link_citekey_aliases ( elem : pf . Element , doc : pf . Doc ) -> None : \"\"\" Extract citekey aliases from the document that were defined using markdown ' s link reference syntax . https : //spec.commonmark.org/0.29/#link-reference-definitions Based on pandoc - url2cite implementation by phiresky at https : //github.com/phiresky/pandoc-url2cite/blob/b28374a9a037a5ce1747b8567160d8dffd64177e/index.ts#L118-L152 \"\"\" if type ( elem ) != pf . Para : # require link reference definitions to be in their own paragraph return while ( len ( elem . content ) >= 3 and type ( elem . content [ 0 ]) == pf . Cite and len ( elem . content [ 0 ]. citations ) == 1 and type ( elem . content [ 1 ]) == pf . Str and elem . content [ 1 ]. text == \":\" ) : # paragraph consists of at least a Cite (with one Citation), # a Str (equal to \":\"), and additional elements, such as a # link destination and possibly more link-reference definitions. space_index = 3 if type ( elem . content [ 2 ]) == pf . Space else 2 destination = elem . content [ space_index ] if type ( destination ) == pf . Str : # paragraph starts with `[@something]: something` # save info to citekeys and remove from paragraph citekey = elem . content [ 0 ]. citations [ 0 ]. id citekey_aliases = doc . manubot [ \"citekey_aliases\" ] if ( citekey in citekey_aliases and citekey_aliases [ citekey ] != destination . text ) : logging . warning ( f \"multiple aliases defined for @ { citekey } \") citekey_aliases [ citekey ] = destination . text # found citation, add it to citekeys and remove it from document elem . content = elem . content [ space_index + 1 : ] # remove leading SoftBreak, before continuing if len ( elem . content ) > 0 and type ( elem . content [ 0 ]) == pf . SoftBreak : elem . content . pop ( 0 ) def _get_load_manual_references_kwargs ( doc : pf . Doc ) -> Dict [ str , Any ] : \"\"\" Return keyword arguments for Citations . load_manual_references . \"\"\" manual_refs = doc . get_metadata ( \"references\" , default = []) bibliography_paths = doc . get_metadata ( \"bibliography\" , default = []) if not isinstance ( bibliography_paths , list ) : bibliography_paths = [ bibliography_paths ] bibliography_cache_path = doc . manubot [ \"bibliography_cache\" ] if ( bibliography_cache_path and bibliography_cache_path not in bibliography_paths and os . path . exists ( bibliography_cache_path ) ) : bibliography_paths . append ( bibliography_cache_path ) return dict ( paths = bibliography_paths , extra_csl_items = manual_refs , ) def process_citations ( doc : pf . Doc ) -> None : \"\"\" Apply citation - by - identifier to a Python object representation of Pandoc ' s Abstract Syntax Tree . \"\"\" # process metadata.manubot-bibliography-cache bib_cache = doc . get_metadata ( key = \"manubot-bibliography-cache\" ) if not ( bib_cache is None or isinstance ( bib_cache , str )) : logging . warning ( f \"Expected metadata.manubot-bibliography-cache to be a string or null (None), \" f \"but received a {bib_cache.__class__.__name__}. Setting to None.\" ) bib_cache = None doc . manubot [ \"bibliography_cache\" ] = bib_cache # process metadata.citekey-aliases citekey_aliases = doc . get_metadata ( \"citekey-aliases\" , default = {}) if not isinstance ( citekey_aliases , dict ) : logging . warning ( f \"Expected metadata.citekey-aliases to be a dict, \" f \"but received a {citekey_aliases.__class__.__name__}. Disregarding.\" ) citekey_aliases = dict () doc . manubot [ \"citekey_aliases\" ] = citekey_aliases doc . walk ( _get_reference_link_citekey_aliases ) doc . walk ( _get_citekeys_action ) manuscript_citekeys = doc . manubot [ \"manuscript_citekeys\" ] citations = Citations ( input_ids = manuscript_citekeys , aliases = citekey_aliases , infer_citekey_prefixes = doc . get_metadata ( \"manubot-infer-citekey-prefixes\" , default = True ), ) citations . csl_item_failure_log_level = \"ERROR\" requests_cache_path = doc . get_metadata ( \"manubot-requests-cache-path\" ) if requests_cache_path : from manubot . process . requests_cache import RequestsCache req_cache = RequestsCache ( requests_cache_path ) req_cache . mkdir () req_cache . install () if doc . get_metadata ( \"manubot-clear-requests-cache\" , default = False ) : req_cache . clear () citations . filter_pandoc_xnos () citations . load_manual_references ( ** _get_load_manual_references_kwargs ( doc )) citations . inspect ( log_level = \"WARNING\" ) citations . get_csl_items () doc . manubot [ \"citekey_shortener\" ] = citations . input_to_csl_id doc . walk ( _citation_to_id_action ) if requests_cache_path : req_cache . close () citations . write_citekeys_tsv ( path = doc . get_metadata ( \"manubot-output-citekeys\" )) citations . write_csl_items ( path = doc . get_metadata ( \"manubot-output-bibliography\" )) citations . write_csl_items ( path = doc . manubot [ \"bibliography_cache\" ]) # Update pandoc metadata with fields that this filter # has either consumed, created, or modified. doc . metadata [ \"bibliography\" ] = [] doc . metadata [ \"references\" ] = citations . csl_items doc . metadata [ \"citekey_aliases\" ] = citekey_aliases def main () -> None : from manubot . command import exit_if_error_handler_fired , setup_logging_and_errors diagnostics = setup_logging_and_errors () args = parse_args () # Let panflute handle io to sys.stdout / sys.stdin to set utf-8 encoding. # args.input=None for stdin, args.output=None for stdout doc = pf . load ( input_stream = args . input ) log_level = doc . get_metadata ( \"manubot-log-level\" , \"WARNING\" ) diagnostics [ \"logger\" ]. setLevel ( getattr ( logging , log_level )) logging . debug ( f \"Input Pandoc metadata: \\n {doc.get_metadata()}\" ) doc . manubot = { \"manuscript_citekeys\" : []} process_citations ( doc ) pf . dump ( doc , output_stream = args . output ) if doc . get_metadata ( \"manubot-fail-on-errors\" , False ) : exit_if_error_handler_fired ( diagnostics [ \"error_handler\" ]) if __name__ == \"__main__\" : main () Functions main def main ( ) -> None View Source def main () -> None : from manubot.command import exit_if_error_handler_fired , setup_logging_and_errors diagnostics = setup_logging_and_errors () args = parse_args () # Let panflute handle io to sys.stdout / sys.stdin to set utf-8 encoding. # args.input=None for stdin, args.output=None for stdout doc = pf . load ( input_stream = args . input ) log_level = doc . get_metadata ( \"manubot-log-level\" , \"WARNING\" ) diagnostics [ \"logger\" ] . setLevel ( getattr ( logging , log_level )) logging . debug ( f \"Input Pandoc metadata: \\n { doc . get_metadata () } \" ) doc . manubot = { \"manuscript_citekeys\" : []} process_citations ( doc ) pf . dump ( doc , output_stream = args . output ) if doc . get_metadata ( \"manubot-fail-on-errors\" , False ): exit_if_error_handler_fired ( diagnostics [ \"error_handler\" ]) parse_args def parse_args ( ) -> argparse . Namespace Read command line arguments View Source def parse_args () -> argparse . Namespace : \"\"\" Read command line arguments \"\"\" parser = argparse . ArgumentParser ( description = \"Pandoc filter for citation by persistent identifier. \" \"Filters are command-line programs that read and write a JSON-encoded abstract syntax tree for Pandoc. \" \"Unless you are debugging, run this filter as part of a pandoc command by specifying --filter=pandoc-manubot-cite.\" ) parser . add_argument ( \"target_format\" , help = \"output format of the pandoc command, as per Pandoc' s -- to option \", ) parser.add_argument( \" -- input \", nargs=\" ? \", type=argparse.FileType(\" r \", encoding=\" utf - 8 \"), help=\" path read JSON input ( defaults to stdin ) \", ) parser.add_argument( \" -- output \", nargs=\" ? \", type=argparse.FileType(\" w \", encoding=\" utf - 8 \"), help=\" path to write JSON output ( defaults to stdout ) \", ) args = parser.parse_args() return args process_citations def process_citations ( doc : panflute . elements . Doc ) -> None Apply citation-by-identifier to a Python object representation of Pandoc's Abstract Syntax Tree. View Source def process_citations ( doc : pf . Doc ) -> None : \"\"\" Apply citation-by-identifier to a Python object representation of Pandoc's Abstract Syntax Tree. \"\"\" # process metadata.manubot-bibliography-cache bib_cache = doc . get_metadata ( key = \"manubot-bibliography-cache\" ) if not ( bib_cache is None or isinstance ( bib_cache , str )): logging . warning ( f \"Expected metadata.manubot-bibliography-cache to be a string or null (None), \" f \"but received a {bib_cache.__class__.__name__}. Setting to None.\" ) bib_cache = None doc . manubot [ \"bibliography_cache\" ] = bib_cache # process metadata.citekey-aliases citekey_aliases = doc . get_metadata ( \"citekey-aliases\" , default = {}) if not isinstance ( citekey_aliases , dict ): logging . warning ( f \"Expected metadata.citekey-aliases to be a dict, \" f \"but received a {citekey_aliases.__class__.__name__}. Disregarding.\" ) citekey_aliases = dict () doc . manubot [ \"citekey_aliases\" ] = citekey_aliases doc . walk ( _get_reference_link_citekey_aliases ) doc . walk ( _get_citekeys_action ) manuscript_citekeys = doc . manubot [ \"manuscript_citekeys\" ] citations = Citations ( input_ids = manuscript_citekeys , aliases = citekey_aliases , infer_citekey_prefixes = doc . get_metadata ( \"manubot-infer-citekey-prefixes\" , default = True ), ) citations . csl_item_failure_log_level = \"ERROR\" requests_cache_path = doc . get_metadata ( \"manubot-requests-cache-path\" ) if requests_cache_path : from manubot . process . requests_cache import RequestsCache req_cache = RequestsCache ( requests_cache_path ) req_cache . mkdir () req_cache . install () if doc . get_metadata ( \"manubot-clear-requests-cache\" , default = False ): req_cache . clear () citations . filter_pandoc_xnos () citations . load_manual_references ( ** _get_load_manual_references_kwargs ( doc )) citations . inspect ( log_level = \"WARNING\" ) citations . get_csl_items () doc . manubot [ \"citekey_shortener\" ] = citations . input_to_csl_id doc . walk ( _citation_to_id_action ) if requests_cache_path : req_cache . close () citations . write_citekeys_tsv ( path = doc . get_metadata ( \"manubot-output-citekeys\" )) citations . write_csl_items ( path = doc . get_metadata ( \"manubot-output-bibliography\" )) citations . write_csl_items ( path = doc . manubot [ \"bibliography_cache\" ]) # Update pandoc metadata with fields that this filter # has either consumed, created, or modified. doc . metadata [ \"bibliography\" ] = [] doc . metadata [ \"references\" ] = citations . csl_items doc . metadata [ \"citekey_aliases\" ] = citekey_aliases","title":"Cite Filter"},{"location":"reference/manubot/pandoc/cite_filter/#module-manubotpandoccite_filter","text":"This module defines a pandoc filter for manubot cite functionality. The filter can be called with the pandoc-manubot-cite command.","title":"Module manubot.pandoc.cite_filter"},{"location":"reference/manubot/pandoc/cite_filter/#options","text":"Configuration is provided via Pandoc metadata fields. bibliography (sequence of strings): Use to define reference metadata manually. Pandoc supports specifying multiple external bibliography files. When bibliography files are specified, this filter will read them instead of pandoc. Behavior should be similar to Pandoc, with format inferred by extension: .json for CSL JSON, .yaml for CSL YAML, .bib for BibLaTeX. references (sequence of CSL-Item mappings): Same as Pandoc's references metadata field. citekey-aliases (mapping: string -> string): Used to define aliases (tags) for cite-by-id citations. Useful when a citation is used many times or contains invalid characters. Aliases can also be defined in markdown with link reference syntax. manubot-infer-citekey-prefixes (boolean): Attempt to infer the prefix for citekeys without a known prefix. For example, allow '@10.1371/journal.pcbi.1007128' with a 'doi:' prefix. Default is true. manubot-output-citekeys (string): path to write TSV table of citekeys showing their transformation from input_id to short_id. manubot-bibliography-cache (string): Path to read and write bibliographic metadata as CSL JSON/YAML. Intended as a human-editable cache of the bibliography data, for situations where this filter is run multiple times. This is similar to specifying bibliography=FILE and manubot-output-bibliography=FILE in a single argument, but will not error trying to read a bibliography file that does not yet exist. manubot-output-bibliography (string): path to write generated CSL JSON bibliography. If specified in addition to manubot-bibliography-cache , two output bibliographies will be written (with the same references). manubot-requests-cache-path (string): Enable caching HTTP requests to this path (minus the extension) using requests-cache . For example, setting to .cache/requests-cache will cache requests to .cache/requests-cache.sqlite . manubot-clear-requests-cache (boolean): If true, clear the requests cache at manubot-requests-cache-path . manubot-fail-on-errors (boolean): If true, return a nonzero exit status if any errors are logged. Default is false, which allows Pandoc to proceed when some citations could not be processed.","title":"Options"},{"location":"reference/manubot/pandoc/cite_filter/#development-commands","text":"# export to plain text (with pandoc < 2.11) pandoc --to = plain --standalone --filter = pandoc-manubot-cite --filter = pandoc-citeproc manubot/pandoc/tests/test_cite_filter/input.md # call the filter manually using pandoc JSON output pandoc --to = json manubot/pandoc/tests/test_cite_filter/input.md | python manubot/pandoc/test_cite.py markdown Related resources on pandoc filters: python pandocfilters package python panflute package panflute Citation class View Source \"\"\" This module defines a pandoc filter for manubot cite functionality . The filter can be called with the ` pandoc - manubot - cite ` command . ## Options Configuration is provided via Pandoc metadata fields . - ` bibliography ` ( sequence of strings ) : Use to define reference metadata manually . Pandoc supports specifying multiple external bibliography files . When bibliography files are specified , this filter will read them instead of pandoc . Behavior should be similar to Pandoc , with format inferred by extension : . json for CSL JSON , . yaml for CSL YAML , . bib for BibLaTeX . - ` references ` ( sequence of CSL - Item mappings ) : Same as Pandoc ' s references metadata field . - ` citekey - aliases ` ( mapping : string -> string ) : Used to define aliases ( tags ) for cite - by - id citations . Useful when a citation is used many times or contains invalid characters . Aliases can also be defined in markdown with link reference syntax . - ` manubot - infer - citekey - prefixes ` ( boolean ) : Attempt to infer the prefix for citekeys without a known prefix . For example , allow ' @10.1371 / journal . pcbi .1007128 ' with a ' doi : ' prefix . Default is true . - ` manubot - output - citekeys ` ( string ) : path to write TSV table of citekeys showing their transformation from input_id to short_id . - ` manubot - bibliography - cache ` ( string ) : Path to read and write bibliographic metadata as CSL JSON / YAML . Intended as a human - editable cache of the bibliography data , for situations where this filter is run multiple times . This is similar to specifying bibliography = FILE and manubot - output - bibliography = FILE in a single argument , but will not error trying to read a bibliography file that does not yet exist . - ` manubot - output - bibliography ` ( string ) : path to write generated CSL JSON bibliography . If specified in addition to ` manubot - bibliography - cache ` , two output bibliographies will be written ( with the same references ). - ` manubot - requests - cache - path ` ( string ) : Enable caching HTTP requests to this path ( minus the extension ) using [ requests - cache ]( https : //github.com/reclosedev/requests-cache). For example , setting to ` . cache / requests - cache ` will cache requests to ` . cache / requests - cache . sqlite ` . - ` manubot - clear - requests - cache ` ( boolean ) : If true , clear the requests cache at ` manubot - requests - cache - path ` . - ` manubot - fail - on - errors ` ( boolean ) : If true , return a nonzero exit status if any errors are logged . Default is false , which allows Pandoc to proceed when some citations could not be processed . ## development commands ``` shell # export to plain text (with pandoc < 2.11) pandoc \\ -- to = plain \\ -- standalone \\ -- filter = pandoc - manubot - cite \\ -- filter = pandoc - citeproc \\ manubot / pandoc / tests / test_cite_filter / input . md # call the filter manually using pandoc JSON output pandoc \\ -- to = json \\ manubot / pandoc / tests / test_cite_filter / input . md \\ | python manubot / pandoc / test_cite . py markdown ``` Related resources on pandoc filters : - [ python pandocfilters package ]( https : //github.com/jgm/pandocfilters) - [ python panflute package ]( https : //github.com/sergiocorreia/panflute) - [ panflute Citation class ]( http : //scorreia.com/software/panflute/code.html#panflute.elements.Citation) \"\"\" import argparse import logging import os from typing import Any , Dict import panflute as pf from manubot . cite . citations import Citations def parse_args () -> argparse . Namespace : \"\"\" Read command line arguments \"\"\" parser = argparse . ArgumentParser ( description = \"Pandoc filter for citation by persistent identifier. \" \"Filters are command-line programs that read and write a JSON-encoded abstract syntax tree for Pandoc. \" \"Unless you are debugging, run this filter as part of a pandoc command by specifying --filter=pandoc-manubot-cite.\" ) parser . add_argument ( \"target_format\" , help = \"output format of the pandoc command, as per Pandoc's --to option\" , ) parser . add_argument ( \"--input\" , nargs = \"?\" , type = argparse . FileType ( \"r\" , encoding = \"utf-8\" ), help = \"path read JSON input (defaults to stdin)\" , ) parser . add_argument ( \"--output\" , nargs = \"?\" , type = argparse . FileType ( \"w\" , encoding = \"utf-8\" ), help = \"path to write JSON output (defaults to stdout)\" , ) args = parser . parse_args () return args def _get_citekeys_action ( elem : pf . Element , doc : pf . Doc ) -> None : \"\"\" Panflute action to extract citationId from all Citations in the AST . \"\"\" if not isinstance ( elem , pf . Citation ) : return None manuscript_citekeys = doc . manubot [ \"manuscript_citekeys\" ] manuscript_citekeys . append ( elem . id ) return None def _citation_to_id_action ( elem : pf . Element , doc : pf . Doc ) -> None : \"\"\" Panflute action to update the citationId of Citations in the AST with their manubot - created keys . \"\"\" if not isinstance ( elem , pf . Citation ) : return None mapper = doc . manubot [ \"citekey_shortener\" ] if elem . id in mapper : elem . id = mapper [ elem . id ] return None def _get_reference_link_citekey_aliases ( elem : pf . Element , doc : pf . Doc ) -> None : \"\"\" Extract citekey aliases from the document that were defined using markdown ' s link reference syntax . https : //spec.commonmark.org/0.29/#link-reference-definitions Based on pandoc - url2cite implementation by phiresky at https : //github.com/phiresky/pandoc-url2cite/blob/b28374a9a037a5ce1747b8567160d8dffd64177e/index.ts#L118-L152 \"\"\" if type ( elem ) != pf . Para : # require link reference definitions to be in their own paragraph return while ( len ( elem . content ) >= 3 and type ( elem . content [ 0 ]) == pf . Cite and len ( elem . content [ 0 ]. citations ) == 1 and type ( elem . content [ 1 ]) == pf . Str and elem . content [ 1 ]. text == \":\" ) : # paragraph consists of at least a Cite (with one Citation), # a Str (equal to \":\"), and additional elements, such as a # link destination and possibly more link-reference definitions. space_index = 3 if type ( elem . content [ 2 ]) == pf . Space else 2 destination = elem . content [ space_index ] if type ( destination ) == pf . Str : # paragraph starts with `[@something]: something` # save info to citekeys and remove from paragraph citekey = elem . content [ 0 ]. citations [ 0 ]. id citekey_aliases = doc . manubot [ \"citekey_aliases\" ] if ( citekey in citekey_aliases and citekey_aliases [ citekey ] != destination . text ) : logging . warning ( f \"multiple aliases defined for @ { citekey } \") citekey_aliases [ citekey ] = destination . text # found citation, add it to citekeys and remove it from document elem . content = elem . content [ space_index + 1 : ] # remove leading SoftBreak, before continuing if len ( elem . content ) > 0 and type ( elem . content [ 0 ]) == pf . SoftBreak : elem . content . pop ( 0 ) def _get_load_manual_references_kwargs ( doc : pf . Doc ) -> Dict [ str , Any ] : \"\"\" Return keyword arguments for Citations . load_manual_references . \"\"\" manual_refs = doc . get_metadata ( \"references\" , default = []) bibliography_paths = doc . get_metadata ( \"bibliography\" , default = []) if not isinstance ( bibliography_paths , list ) : bibliography_paths = [ bibliography_paths ] bibliography_cache_path = doc . manubot [ \"bibliography_cache\" ] if ( bibliography_cache_path and bibliography_cache_path not in bibliography_paths and os . path . exists ( bibliography_cache_path ) ) : bibliography_paths . append ( bibliography_cache_path ) return dict ( paths = bibliography_paths , extra_csl_items = manual_refs , ) def process_citations ( doc : pf . Doc ) -> None : \"\"\" Apply citation - by - identifier to a Python object representation of Pandoc ' s Abstract Syntax Tree . \"\"\" # process metadata.manubot-bibliography-cache bib_cache = doc . get_metadata ( key = \"manubot-bibliography-cache\" ) if not ( bib_cache is None or isinstance ( bib_cache , str )) : logging . warning ( f \"Expected metadata.manubot-bibliography-cache to be a string or null (None), \" f \"but received a {bib_cache.__class__.__name__}. Setting to None.\" ) bib_cache = None doc . manubot [ \"bibliography_cache\" ] = bib_cache # process metadata.citekey-aliases citekey_aliases = doc . get_metadata ( \"citekey-aliases\" , default = {}) if not isinstance ( citekey_aliases , dict ) : logging . warning ( f \"Expected metadata.citekey-aliases to be a dict, \" f \"but received a {citekey_aliases.__class__.__name__}. Disregarding.\" ) citekey_aliases = dict () doc . manubot [ \"citekey_aliases\" ] = citekey_aliases doc . walk ( _get_reference_link_citekey_aliases ) doc . walk ( _get_citekeys_action ) manuscript_citekeys = doc . manubot [ \"manuscript_citekeys\" ] citations = Citations ( input_ids = manuscript_citekeys , aliases = citekey_aliases , infer_citekey_prefixes = doc . get_metadata ( \"manubot-infer-citekey-prefixes\" , default = True ), ) citations . csl_item_failure_log_level = \"ERROR\" requests_cache_path = doc . get_metadata ( \"manubot-requests-cache-path\" ) if requests_cache_path : from manubot . process . requests_cache import RequestsCache req_cache = RequestsCache ( requests_cache_path ) req_cache . mkdir () req_cache . install () if doc . get_metadata ( \"manubot-clear-requests-cache\" , default = False ) : req_cache . clear () citations . filter_pandoc_xnos () citations . load_manual_references ( ** _get_load_manual_references_kwargs ( doc )) citations . inspect ( log_level = \"WARNING\" ) citations . get_csl_items () doc . manubot [ \"citekey_shortener\" ] = citations . input_to_csl_id doc . walk ( _citation_to_id_action ) if requests_cache_path : req_cache . close () citations . write_citekeys_tsv ( path = doc . get_metadata ( \"manubot-output-citekeys\" )) citations . write_csl_items ( path = doc . get_metadata ( \"manubot-output-bibliography\" )) citations . write_csl_items ( path = doc . manubot [ \"bibliography_cache\" ]) # Update pandoc metadata with fields that this filter # has either consumed, created, or modified. doc . metadata [ \"bibliography\" ] = [] doc . metadata [ \"references\" ] = citations . csl_items doc . metadata [ \"citekey_aliases\" ] = citekey_aliases def main () -> None : from manubot . command import exit_if_error_handler_fired , setup_logging_and_errors diagnostics = setup_logging_and_errors () args = parse_args () # Let panflute handle io to sys.stdout / sys.stdin to set utf-8 encoding. # args.input=None for stdin, args.output=None for stdout doc = pf . load ( input_stream = args . input ) log_level = doc . get_metadata ( \"manubot-log-level\" , \"WARNING\" ) diagnostics [ \"logger\" ]. setLevel ( getattr ( logging , log_level )) logging . debug ( f \"Input Pandoc metadata: \\n {doc.get_metadata()}\" ) doc . manubot = { \"manuscript_citekeys\" : []} process_citations ( doc ) pf . dump ( doc , output_stream = args . output ) if doc . get_metadata ( \"manubot-fail-on-errors\" , False ) : exit_if_error_handler_fired ( diagnostics [ \"error_handler\" ]) if __name__ == \"__main__\" : main ()","title":"development commands"},{"location":"reference/manubot/pandoc/cite_filter/#functions","text":"","title":"Functions"},{"location":"reference/manubot/pandoc/cite_filter/#main","text":"def main ( ) -> None View Source def main () -> None : from manubot.command import exit_if_error_handler_fired , setup_logging_and_errors diagnostics = setup_logging_and_errors () args = parse_args () # Let panflute handle io to sys.stdout / sys.stdin to set utf-8 encoding. # args.input=None for stdin, args.output=None for stdout doc = pf . load ( input_stream = args . input ) log_level = doc . get_metadata ( \"manubot-log-level\" , \"WARNING\" ) diagnostics [ \"logger\" ] . setLevel ( getattr ( logging , log_level )) logging . debug ( f \"Input Pandoc metadata: \\n { doc . get_metadata () } \" ) doc . manubot = { \"manuscript_citekeys\" : []} process_citations ( doc ) pf . dump ( doc , output_stream = args . output ) if doc . get_metadata ( \"manubot-fail-on-errors\" , False ): exit_if_error_handler_fired ( diagnostics [ \"error_handler\" ])","title":"main"},{"location":"reference/manubot/pandoc/cite_filter/#parse_args","text":"def parse_args ( ) -> argparse . Namespace Read command line arguments View Source def parse_args () -> argparse . Namespace : \"\"\" Read command line arguments \"\"\" parser = argparse . ArgumentParser ( description = \"Pandoc filter for citation by persistent identifier. \" \"Filters are command-line programs that read and write a JSON-encoded abstract syntax tree for Pandoc. \" \"Unless you are debugging, run this filter as part of a pandoc command by specifying --filter=pandoc-manubot-cite.\" ) parser . add_argument ( \"target_format\" , help = \"output format of the pandoc command, as per Pandoc' s -- to option \", ) parser.add_argument( \" -- input \", nargs=\" ? \", type=argparse.FileType(\" r \", encoding=\" utf - 8 \"), help=\" path read JSON input ( defaults to stdin ) \", ) parser.add_argument( \" -- output \", nargs=\" ? \", type=argparse.FileType(\" w \", encoding=\" utf - 8 \"), help=\" path to write JSON output ( defaults to stdout ) \", ) args = parser.parse_args() return args","title":"parse_args"},{"location":"reference/manubot/pandoc/cite_filter/#process_citations","text":"def process_citations ( doc : panflute . elements . Doc ) -> None Apply citation-by-identifier to a Python object representation of Pandoc's Abstract Syntax Tree. View Source def process_citations ( doc : pf . Doc ) -> None : \"\"\" Apply citation-by-identifier to a Python object representation of Pandoc's Abstract Syntax Tree. \"\"\" # process metadata.manubot-bibliography-cache bib_cache = doc . get_metadata ( key = \"manubot-bibliography-cache\" ) if not ( bib_cache is None or isinstance ( bib_cache , str )): logging . warning ( f \"Expected metadata.manubot-bibliography-cache to be a string or null (None), \" f \"but received a {bib_cache.__class__.__name__}. Setting to None.\" ) bib_cache = None doc . manubot [ \"bibliography_cache\" ] = bib_cache # process metadata.citekey-aliases citekey_aliases = doc . get_metadata ( \"citekey-aliases\" , default = {}) if not isinstance ( citekey_aliases , dict ): logging . warning ( f \"Expected metadata.citekey-aliases to be a dict, \" f \"but received a {citekey_aliases.__class__.__name__}. Disregarding.\" ) citekey_aliases = dict () doc . manubot [ \"citekey_aliases\" ] = citekey_aliases doc . walk ( _get_reference_link_citekey_aliases ) doc . walk ( _get_citekeys_action ) manuscript_citekeys = doc . manubot [ \"manuscript_citekeys\" ] citations = Citations ( input_ids = manuscript_citekeys , aliases = citekey_aliases , infer_citekey_prefixes = doc . get_metadata ( \"manubot-infer-citekey-prefixes\" , default = True ), ) citations . csl_item_failure_log_level = \"ERROR\" requests_cache_path = doc . get_metadata ( \"manubot-requests-cache-path\" ) if requests_cache_path : from manubot . process . requests_cache import RequestsCache req_cache = RequestsCache ( requests_cache_path ) req_cache . mkdir () req_cache . install () if doc . get_metadata ( \"manubot-clear-requests-cache\" , default = False ): req_cache . clear () citations . filter_pandoc_xnos () citations . load_manual_references ( ** _get_load_manual_references_kwargs ( doc )) citations . inspect ( log_level = \"WARNING\" ) citations . get_csl_items () doc . manubot [ \"citekey_shortener\" ] = citations . input_to_csl_id doc . walk ( _citation_to_id_action ) if requests_cache_path : req_cache . close () citations . write_citekeys_tsv ( path = doc . get_metadata ( \"manubot-output-citekeys\" )) citations . write_csl_items ( path = doc . get_metadata ( \"manubot-output-bibliography\" )) citations . write_csl_items ( path = doc . manubot [ \"bibliography_cache\" ]) # Update pandoc metadata with fields that this filter # has either consumed, created, or modified. doc . metadata [ \"bibliography\" ] = [] doc . metadata [ \"references\" ] = citations . csl_items doc . metadata [ \"citekey_aliases\" ] = citekey_aliases","title":"process_citations"},{"location":"reference/manubot/pandoc/util/","text":"Module manubot.pandoc.util None None View Source import functools import logging import shutil import subprocess from typing import Any , Dict , Tuple @functools . lru_cache () def get_pandoc_info () -> Dict [ str , Any ]: \"\"\" Return path and version information for the system's pandoc and pandoc-citeproc commands. When Pandoc is installed, the output will look like: ```python { 'pandoc': True, 'pandoc path': '/PATH_TO_EXECUTABLES/pandoc', 'pandoc version': (2, 5), 'pandoc-citeproc': True, 'pandoc-citeproc path': '/PATH_TO_EXECUTABLES/pandoc-citeproc', 'pandoc-citeproc version': (0, 15), } ``` If the executables are missing, the output will be like: ```python { 'pandoc': False, 'pandoc-citeproc': False, } ``` \"\"\" stats = get_command_info ( \"pandoc\" ) if stats [ \"pandoc version\" ] < ( 2 , 11 ): stats . update ( get_command_info ( \"pandoc-citeproc\" )) else : stats [ \"pandoc-citeproc\" ] = False logging . info ( \" \\n \" . join ( f \" { k } : { v } \" for k , v in stats . items ())) return stats def get_pandoc_version () -> Tuple [ int , ... ]: \"\"\" Return pandoc version as tuple of major and minor version numbers, for example: (2, 7, 2) \"\"\" pandoc_info = get_pandoc_info () if not pandoc_info [ \"pandoc\" ]: # https://twitter.com/dhimmel/status/1327082301994496000 raise ImportError ( \"missing pandoc command on system.\" ) return pandoc_info [ \"pandoc version\" ] def get_command_info ( command : str ) -> dict : \"\"\" Returns a dictionary containing some information about a command \"\"\" command_info_dict = dict () path = shutil . which ( command ) command_info_dict [ command ] = bool ( path ) if not path : return command_info_dict version = subprocess . check_output ( args = [ command , \"--version\" ], encoding = \"utf-8\" ) logging . debug ( version ) version , * _discard = version . splitlines () _discard , version = version . strip () . split () from packaging.version import parse as parse_version version = parse_version ( version ) . release command_info_dict [ f \" { command } version\" ] = version command_info_dict [ f \" { command } path\" ] = path return command_info_dict Functions get_command_info def get_command_info ( command : str ) -> dict Returns a dictionary containing some information about a command View Source def get_command_info ( command : str ) -> dict : \"\"\" Returns a dictionary containing some information about a command \"\"\" command_info_dict = dict () path = shutil . which ( command ) command_info_dict [ command ] = bool ( path ) if not path : return command_info_dict version = subprocess . check_output ( args = [ command , \"--version\" ], encoding = \"utf-8\" ) logging . debug ( version ) version , * _discard = version . splitlines () _discard , version = version . strip () . split () from packaging.version import parse as parse_version version = parse_version ( version ) . release command_info_dict [ f \" { command } version\" ] = version command_info_dict [ f \" { command } path\" ] = path return command_info_dict get_pandoc_info def get_pandoc_info ( ) -> Dict [ str , Any ] Return path and version information for the system's pandoc and pandoc-citeproc commands. When Pandoc is installed, the output will look like: { 'pandoc' : True , 'pandoc path' : '/PATH_TO_EXECUTABLES/pandoc' , 'pandoc version' : ( 2 , 5 ), 'pandoc-citeproc' : True , 'pandoc-citeproc path' : '/PATH_TO_EXECUTABLES/pandoc-citeproc' , 'pandoc-citeproc version' : ( 0 , 15 ), } If the executables are missing, the output will be like: { 'pandoc' : False , 'pandoc-citeproc' : False , } View Source @ functools . lru_cache () def get_pandoc_info () -> Dict [ str , Any ]: \"\"\" Return path and version information for the system's pandoc and pandoc-citeproc commands. When Pandoc is installed, the output will look like: ```python { 'pandoc': True, 'pandoc path': '/PATH_TO_EXECUTABLES/pandoc', 'pandoc version': (2, 5), 'pandoc-citeproc': True, 'pandoc-citeproc path': '/PATH_TO_EXECUTABLES/pandoc-citeproc', 'pandoc-citeproc version': (0, 15), } ``` If the executables are missing, the output will be like: ```python { 'pandoc': False, 'pandoc-citeproc': False, } ``` \"\"\" stats = get_command_info ( \"pandoc\" ) if stats [ \"pandoc version\" ] < ( 2 , 11 ): stats . update ( get_command_info ( \"pandoc-citeproc\" )) else : stats [ \"pandoc-citeproc\" ] = False logging . info ( \" \\n \" . join ( f \"{k}: {v}\" for k , v in stats . items ())) return stats get_pandoc_version def get_pandoc_version ( ) -> Tuple [ int , ... ] Return pandoc version as tuple of major and minor version numbers, for example: (2, 7, 2) View Source def get_pandoc_version () -> Tuple [ int , ...] : \"\"\" Return pandoc version as tuple of major and minor version numbers, for example: (2, 7, 2) \"\"\" pandoc_info = get_pandoc_info () if not pandoc_info [ \"pandoc\" ] : # https :// twitter . com / dhimmel / status / 1327082301994496000 raise ImportError ( \"missing pandoc command on system.\" ) return pandoc_info [ \"pandoc version\" ]","title":"Util"},{"location":"reference/manubot/pandoc/util/#module-manubotpandocutil","text":"None None View Source import functools import logging import shutil import subprocess from typing import Any , Dict , Tuple @functools . lru_cache () def get_pandoc_info () -> Dict [ str , Any ]: \"\"\" Return path and version information for the system's pandoc and pandoc-citeproc commands. When Pandoc is installed, the output will look like: ```python { 'pandoc': True, 'pandoc path': '/PATH_TO_EXECUTABLES/pandoc', 'pandoc version': (2, 5), 'pandoc-citeproc': True, 'pandoc-citeproc path': '/PATH_TO_EXECUTABLES/pandoc-citeproc', 'pandoc-citeproc version': (0, 15), } ``` If the executables are missing, the output will be like: ```python { 'pandoc': False, 'pandoc-citeproc': False, } ``` \"\"\" stats = get_command_info ( \"pandoc\" ) if stats [ \"pandoc version\" ] < ( 2 , 11 ): stats . update ( get_command_info ( \"pandoc-citeproc\" )) else : stats [ \"pandoc-citeproc\" ] = False logging . info ( \" \\n \" . join ( f \" { k } : { v } \" for k , v in stats . items ())) return stats def get_pandoc_version () -> Tuple [ int , ... ]: \"\"\" Return pandoc version as tuple of major and minor version numbers, for example: (2, 7, 2) \"\"\" pandoc_info = get_pandoc_info () if not pandoc_info [ \"pandoc\" ]: # https://twitter.com/dhimmel/status/1327082301994496000 raise ImportError ( \"missing pandoc command on system.\" ) return pandoc_info [ \"pandoc version\" ] def get_command_info ( command : str ) -> dict : \"\"\" Returns a dictionary containing some information about a command \"\"\" command_info_dict = dict () path = shutil . which ( command ) command_info_dict [ command ] = bool ( path ) if not path : return command_info_dict version = subprocess . check_output ( args = [ command , \"--version\" ], encoding = \"utf-8\" ) logging . debug ( version ) version , * _discard = version . splitlines () _discard , version = version . strip () . split () from packaging.version import parse as parse_version version = parse_version ( version ) . release command_info_dict [ f \" { command } version\" ] = version command_info_dict [ f \" { command } path\" ] = path return command_info_dict","title":"Module manubot.pandoc.util"},{"location":"reference/manubot/pandoc/util/#functions","text":"","title":"Functions"},{"location":"reference/manubot/pandoc/util/#get_command_info","text":"def get_command_info ( command : str ) -> dict Returns a dictionary containing some information about a command View Source def get_command_info ( command : str ) -> dict : \"\"\" Returns a dictionary containing some information about a command \"\"\" command_info_dict = dict () path = shutil . which ( command ) command_info_dict [ command ] = bool ( path ) if not path : return command_info_dict version = subprocess . check_output ( args = [ command , \"--version\" ], encoding = \"utf-8\" ) logging . debug ( version ) version , * _discard = version . splitlines () _discard , version = version . strip () . split () from packaging.version import parse as parse_version version = parse_version ( version ) . release command_info_dict [ f \" { command } version\" ] = version command_info_dict [ f \" { command } path\" ] = path return command_info_dict","title":"get_command_info"},{"location":"reference/manubot/pandoc/util/#get_pandoc_info","text":"def get_pandoc_info ( ) -> Dict [ str , Any ] Return path and version information for the system's pandoc and pandoc-citeproc commands. When Pandoc is installed, the output will look like: { 'pandoc' : True , 'pandoc path' : '/PATH_TO_EXECUTABLES/pandoc' , 'pandoc version' : ( 2 , 5 ), 'pandoc-citeproc' : True , 'pandoc-citeproc path' : '/PATH_TO_EXECUTABLES/pandoc-citeproc' , 'pandoc-citeproc version' : ( 0 , 15 ), } If the executables are missing, the output will be like: { 'pandoc' : False , 'pandoc-citeproc' : False , } View Source @ functools . lru_cache () def get_pandoc_info () -> Dict [ str , Any ]: \"\"\" Return path and version information for the system's pandoc and pandoc-citeproc commands. When Pandoc is installed, the output will look like: ```python { 'pandoc': True, 'pandoc path': '/PATH_TO_EXECUTABLES/pandoc', 'pandoc version': (2, 5), 'pandoc-citeproc': True, 'pandoc-citeproc path': '/PATH_TO_EXECUTABLES/pandoc-citeproc', 'pandoc-citeproc version': (0, 15), } ``` If the executables are missing, the output will be like: ```python { 'pandoc': False, 'pandoc-citeproc': False, } ``` \"\"\" stats = get_command_info ( \"pandoc\" ) if stats [ \"pandoc version\" ] < ( 2 , 11 ): stats . update ( get_command_info ( \"pandoc-citeproc\" )) else : stats [ \"pandoc-citeproc\" ] = False logging . info ( \" \\n \" . join ( f \"{k}: {v}\" for k , v in stats . items ())) return stats","title":"get_pandoc_info"},{"location":"reference/manubot/pandoc/util/#get_pandoc_version","text":"def get_pandoc_version ( ) -> Tuple [ int , ... ] Return pandoc version as tuple of major and minor version numbers, for example: (2, 7, 2) View Source def get_pandoc_version () -> Tuple [ int , ...] : \"\"\" Return pandoc version as tuple of major and minor version numbers, for example: (2, 7, 2) \"\"\" pandoc_info = get_pandoc_info () if not pandoc_info [ \"pandoc\" ] : # https :// twitter . com / dhimmel / status / 1327082301994496000 raise ImportError ( \"missing pandoc command on system.\" ) return pandoc_info [ \"pandoc version\" ]","title":"get_pandoc_version"},{"location":"reference/manubot/process/","text":"Module manubot.process None None Sub-modules manubot.process.bibliography manubot.process.ci manubot.process.manuscript manubot.process.metadata manubot.process.process_command manubot.process.requests_cache manubot.process.tests manubot.process.util","title":"Index"},{"location":"reference/manubot/process/#module-manubotprocess","text":"None None","title":"Module manubot.process"},{"location":"reference/manubot/process/#sub-modules","text":"manubot.process.bibliography manubot.process.ci manubot.process.manuscript manubot.process.metadata manubot.process.process_command manubot.process.requests_cache manubot.process.tests manubot.process.util","title":"Sub-modules"},{"location":"reference/manubot/process/bibliography/","text":"Module manubot.process.bibliography None None View Source import json import logging import os import pathlib from manubot.cite.citekey import shorten_citekey from manubot.util import read_serialized_data def load_bibliography ( path : str ) -> list : \"\"\" Load a bibliography as CSL Items (a CSL JSON Python object). For paths that already contain CSL Items (inferred from a .json or .yaml extension), parse these files directly (URLs supported). Otherwise, delegate conversion to CSL Items to pandoc-citeproc (URLs not supported). If loading fails, log an error and return an empty list. \"\"\" path_obj = pathlib . Path ( path ) if path_obj . suffix in { \".json\" , \".yaml\" }: try : csl_items = read_serialized_data ( path ) except Exception as error : logging . error ( f \"load_bibliography: error reading { path !r} . \\n { error } \" ) logging . info ( \"load_bibliography exception info\" , exc_info = True ) csl_items = [] else : from manubot.pandoc.bibliography import ( load_bibliography as load_bibliography_pandoc , ) csl_items = load_bibliography_pandoc ( path ) if not isinstance ( csl_items , list ): logging . error ( f \"process.load_bibliography: csl_items read from { path } are of type { type ( csl_items ) } . \" \"Setting csl_items to an empty list.\" ) csl_items = [] from manubot.cite.csl_item import CSL_Item csl_items = [ CSL_Item ( csl_item ) for csl_item in csl_items ] return csl_items def load_manual_references ( paths = [], extra_csl_items = []) -> dict : \"\"\" Read manual references from bibliography text files specified by a list of paths. Returns a standard_citation to CSL Item dictionary. `extra_csl_items` specifies CSL Items stored as a Python object, to be used in addition to the CSL Items stored as text in the files specified by `paths`. Set `paths=[]` to only use extra_csl_items. When multiple references have the same standard_id, precedence is given to reference defined last. References in `extra_csl_items` take precedence over those from `paths`. \"\"\" from manubot.cite.csl_item import CSL_Item csl_items = [] paths = list ( dict . fromkeys ( paths )) # remove duplicates for path in paths : path = os . fspath ( path ) path_obj = pathlib . Path ( path ) bibliography = load_bibliography ( path ) for csl_item in bibliography : csl_item . note_append_text ( f \"Loaded from an external bibliography file by Manubot.\" ) csl_item . note_append_dict ({ \"source_bibliography\" : path_obj . name }) csl_items . append ( csl_item ) csl_items . extend ( map ( CSL_Item , extra_csl_items )) manual_refs = dict () for csl_item in csl_items : try : csl_item . standardize_id () except Exception : csl_item_str = json . dumps ( csl_item , indent = 2 ) logging . info ( f \"Skipping csl_item where setting standard_id failed: \\n { csl_item_str } \" , exc_info = True , ) continue standard_id = csl_item [ \"id\" ] csl_item . set_id ( shorten_citekey ( standard_id )) csl_item . clean () manual_refs [ standard_id ] = csl_item return manual_refs Functions load_bibliography def load_bibliography ( path : str ) -> list Load a bibliography as CSL Items (a CSL JSON Python object). For paths that already contain CSL Items (inferred from a .json or .yaml extension), parse these files directly (URLs supported). Otherwise, delegate conversion to CSL Items to pandoc-citeproc (URLs not supported). If loading fails, log an error and return an empty list. View Source def load_bibliography ( path : str ) -> list : \"\"\" Load a bibliography as CSL Items (a CSL JSON Python object). For paths that already contain CSL Items (inferred from a .json or .yaml extension), parse these files directly (URLs supported). Otherwise, delegate conversion to CSL Items to pandoc-citeproc (URLs not supported). If loading fails, log an error and return an empty list. \"\"\" path_obj = pathlib . Path ( path ) if path_obj . suffix in { \".json\" , \".yaml\" }: try : csl_items = read_serialized_data ( path ) except Exception as error : logging . error ( f \"load_bibliography: error reading { path !r} . \\n { error } \" ) logging . info ( \"load_bibliography exception info\" , exc_info = True ) csl_items = [] else : from manubot.pandoc.bibliography import ( load_bibliography as load_bibliography_pandoc , ) csl_items = load_bibliography_pandoc ( path ) if not isinstance ( csl_items , list ): logging . error ( f \"process.load_bibliography: csl_items read from { path } are of type { type ( csl_items ) } . \" \"Setting csl_items to an empty list.\" ) csl_items = [] from manubot.cite.csl_item import CSL_Item csl_items = [ CSL_Item ( csl_item ) for csl_item in csl_items ] return csl_items load_manual_references def load_manual_references ( paths = [], extra_csl_items = [] ) -> dict Read manual references from bibliography text files specified by a list of paths. Returns a standard_citation to CSL Item dictionary. extra_csl_items specifies CSL Items stored as a Python object, to be used in addition to the CSL Items stored as text in the files specified by paths . Set paths=[] to only use extra_csl_items. When multiple references have the same standard_id, precedence is given to reference defined last. References in extra_csl_items take precedence over those from paths . View Source def load_manual_references ( paths = [], extra_csl_items = []) -> dict : \"\"\" Read manual references from bibliography text files specified by a list of paths. Returns a standard_citation to CSL Item dictionary. `extra_csl_items` specifies CSL Items stored as a Python object, to be used in addition to the CSL Items stored as text in the files specified by `paths`. Set `paths=[]` to only use extra_csl_items. When multiple references have the same standard_id, precedence is given to reference defined last. References in `extra_csl_items` take precedence over those from `paths`. \"\"\" from manubot.cite.csl_item import CSL_Item csl_items = [] paths = list ( dict . fromkeys ( paths )) # remove duplicates for path in paths : path = os . fspath ( path ) path_obj = pathlib . Path ( path ) bibliography = load_bibliography ( path ) for csl_item in bibliography : csl_item . note_append_text ( f \"Loaded from an external bibliography file by Manubot.\" ) csl_item . note_append_dict ({ \"source_bibliography\" : path_obj . name }) csl_items . append ( csl_item ) csl_items . extend ( map ( CSL_Item , extra_csl_items )) manual_refs = dict () for csl_item in csl_items : try : csl_item . standardize_id () except Exception : csl_item_str = json . dumps ( csl_item , indent = 2 ) logging . info ( f \"Skipping csl_item where setting standard_id failed: \\n { csl_item_str } \" , exc_info = True , ) continue standard_id = csl_item [ \"id\" ] csl_item . set_id ( shorten_citekey ( standard_id )) csl_item . clean () manual_refs [ standard_id ] = csl_item return manual_refs","title":"Bibliography"},{"location":"reference/manubot/process/bibliography/#module-manubotprocessbibliography","text":"None None View Source import json import logging import os import pathlib from manubot.cite.citekey import shorten_citekey from manubot.util import read_serialized_data def load_bibliography ( path : str ) -> list : \"\"\" Load a bibliography as CSL Items (a CSL JSON Python object). For paths that already contain CSL Items (inferred from a .json or .yaml extension), parse these files directly (URLs supported). Otherwise, delegate conversion to CSL Items to pandoc-citeproc (URLs not supported). If loading fails, log an error and return an empty list. \"\"\" path_obj = pathlib . Path ( path ) if path_obj . suffix in { \".json\" , \".yaml\" }: try : csl_items = read_serialized_data ( path ) except Exception as error : logging . error ( f \"load_bibliography: error reading { path !r} . \\n { error } \" ) logging . info ( \"load_bibliography exception info\" , exc_info = True ) csl_items = [] else : from manubot.pandoc.bibliography import ( load_bibliography as load_bibliography_pandoc , ) csl_items = load_bibliography_pandoc ( path ) if not isinstance ( csl_items , list ): logging . error ( f \"process.load_bibliography: csl_items read from { path } are of type { type ( csl_items ) } . \" \"Setting csl_items to an empty list.\" ) csl_items = [] from manubot.cite.csl_item import CSL_Item csl_items = [ CSL_Item ( csl_item ) for csl_item in csl_items ] return csl_items def load_manual_references ( paths = [], extra_csl_items = []) -> dict : \"\"\" Read manual references from bibliography text files specified by a list of paths. Returns a standard_citation to CSL Item dictionary. `extra_csl_items` specifies CSL Items stored as a Python object, to be used in addition to the CSL Items stored as text in the files specified by `paths`. Set `paths=[]` to only use extra_csl_items. When multiple references have the same standard_id, precedence is given to reference defined last. References in `extra_csl_items` take precedence over those from `paths`. \"\"\" from manubot.cite.csl_item import CSL_Item csl_items = [] paths = list ( dict . fromkeys ( paths )) # remove duplicates for path in paths : path = os . fspath ( path ) path_obj = pathlib . Path ( path ) bibliography = load_bibliography ( path ) for csl_item in bibliography : csl_item . note_append_text ( f \"Loaded from an external bibliography file by Manubot.\" ) csl_item . note_append_dict ({ \"source_bibliography\" : path_obj . name }) csl_items . append ( csl_item ) csl_items . extend ( map ( CSL_Item , extra_csl_items )) manual_refs = dict () for csl_item in csl_items : try : csl_item . standardize_id () except Exception : csl_item_str = json . dumps ( csl_item , indent = 2 ) logging . info ( f \"Skipping csl_item where setting standard_id failed: \\n { csl_item_str } \" , exc_info = True , ) continue standard_id = csl_item [ \"id\" ] csl_item . set_id ( shorten_citekey ( standard_id )) csl_item . clean () manual_refs [ standard_id ] = csl_item return manual_refs","title":"Module manubot.process.bibliography"},{"location":"reference/manubot/process/bibliography/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/bibliography/#load_bibliography","text":"def load_bibliography ( path : str ) -> list Load a bibliography as CSL Items (a CSL JSON Python object). For paths that already contain CSL Items (inferred from a .json or .yaml extension), parse these files directly (URLs supported). Otherwise, delegate conversion to CSL Items to pandoc-citeproc (URLs not supported). If loading fails, log an error and return an empty list. View Source def load_bibliography ( path : str ) -> list : \"\"\" Load a bibliography as CSL Items (a CSL JSON Python object). For paths that already contain CSL Items (inferred from a .json or .yaml extension), parse these files directly (URLs supported). Otherwise, delegate conversion to CSL Items to pandoc-citeproc (URLs not supported). If loading fails, log an error and return an empty list. \"\"\" path_obj = pathlib . Path ( path ) if path_obj . suffix in { \".json\" , \".yaml\" }: try : csl_items = read_serialized_data ( path ) except Exception as error : logging . error ( f \"load_bibliography: error reading { path !r} . \\n { error } \" ) logging . info ( \"load_bibliography exception info\" , exc_info = True ) csl_items = [] else : from manubot.pandoc.bibliography import ( load_bibliography as load_bibliography_pandoc , ) csl_items = load_bibliography_pandoc ( path ) if not isinstance ( csl_items , list ): logging . error ( f \"process.load_bibliography: csl_items read from { path } are of type { type ( csl_items ) } . \" \"Setting csl_items to an empty list.\" ) csl_items = [] from manubot.cite.csl_item import CSL_Item csl_items = [ CSL_Item ( csl_item ) for csl_item in csl_items ] return csl_items","title":"load_bibliography"},{"location":"reference/manubot/process/bibliography/#load_manual_references","text":"def load_manual_references ( paths = [], extra_csl_items = [] ) -> dict Read manual references from bibliography text files specified by a list of paths. Returns a standard_citation to CSL Item dictionary. extra_csl_items specifies CSL Items stored as a Python object, to be used in addition to the CSL Items stored as text in the files specified by paths . Set paths=[] to only use extra_csl_items. When multiple references have the same standard_id, precedence is given to reference defined last. References in extra_csl_items take precedence over those from paths . View Source def load_manual_references ( paths = [], extra_csl_items = []) -> dict : \"\"\" Read manual references from bibliography text files specified by a list of paths. Returns a standard_citation to CSL Item dictionary. `extra_csl_items` specifies CSL Items stored as a Python object, to be used in addition to the CSL Items stored as text in the files specified by `paths`. Set `paths=[]` to only use extra_csl_items. When multiple references have the same standard_id, precedence is given to reference defined last. References in `extra_csl_items` take precedence over those from `paths`. \"\"\" from manubot.cite.csl_item import CSL_Item csl_items = [] paths = list ( dict . fromkeys ( paths )) # remove duplicates for path in paths : path = os . fspath ( path ) path_obj = pathlib . Path ( path ) bibliography = load_bibliography ( path ) for csl_item in bibliography : csl_item . note_append_text ( f \"Loaded from an external bibliography file by Manubot.\" ) csl_item . note_append_dict ({ \"source_bibliography\" : path_obj . name }) csl_items . append ( csl_item ) csl_items . extend ( map ( CSL_Item , extra_csl_items )) manual_refs = dict () for csl_item in csl_items : try : csl_item . standardize_id () except Exception : csl_item_str = json . dumps ( csl_item , indent = 2 ) logging . info ( f \"Skipping csl_item where setting standard_id failed: \\n { csl_item_str } \" , exc_info = True , ) continue standard_id = csl_item [ \"id\" ] csl_item . set_id ( shorten_citekey ( standard_id )) csl_item . clean () manual_refs [ standard_id ] = csl_item return manual_refs","title":"load_manual_references"},{"location":"reference/manubot/process/ci/","text":"Module manubot.process.ci None None View Source import logging import os supported_providers = [ \"github\" , \"travis\" , \"appveyor\" ] def get_continuous_integration_parameters (): \"\"\" Return a dictionary with information on a continuous integration build inferred from environment variables. The following dictionary keys are set (all values are also strings): - provider: name of CI provider, such as \"travis\" or \"appveyor\". - repo_slug: owner/name of the source code repository, i.e. \"manubot/rootstock\" - repo_owner: owner from repo_slug, i.e. \"manubot\" - repo_name: name from repo_slug, i.e. \"rootstock\" - commit: git commit being evaluated by the CI build For pull requests, CI services often build a merge commit with the default branch rather than the commit that was added to the pull request. In these cases, commit refers to this merge commit. - triggering_commit: git commit that triggered the CI build. - build_url: URL for the webpage with build details - job_url: URL for the webpage with job details GitHub Actions does not set an environment variable with triggering_commit for pull requests. Therefore, set the following environment variable in your workflow: ```yaml env: GITHUB_PULL_REQUEST_SHA: ${{ github.event.pull_request.head.sha }} ``` \"\"\" if os . getenv ( \"GITHUB_ACTIONS\" , \"false\" ) == \"true\" : # https://git.io/JvUf7 repo_slug = os . environ [ \"GITHUB_REPOSITORY\" ] repo_owner , repo_name = repo_slug . split ( \"/\" ) run_id = os . environ [ \"GITHUB_RUN_ID\" ] # GITHUB_SHA for pull_request event: Last merge commit on the GITHUB_REF branch # GITHUB_SHA for push event: Commit pushed, unless deleting a branch (when it's the default branch) # https://git.io/JvUfd github_sha = os . environ [ \"GITHUB_SHA\" ] ci_params = { \"provider\" : \"github\" , \"repo_slug\" : repo_slug , \"repo_owner\" : repo_owner , \"repo_name\" : repo_name , \"commit\" : github_sha , \"triggering_commit\" : os . getenv ( \"GITHUB_PULL_REQUEST_SHA\" ) or github_sha , \"build_url\" : f \"https://github.com/ { repo_slug } /commit/ { github_sha } /checks\" , \"job_url\" : f \"https://github.com/ { repo_slug } /actions/runs/ { run_id } \" , } return ci_params if os . getenv ( \"TRAVIS\" , \"false\" ) == \"true\" : # https://docs.travis-ci.com/user/environment-variables/ repo_slug = os . environ [ \"TRAVIS_REPO_SLUG\" ] repo_owner , repo_name = repo_slug . split ( \"/\" ) return { \"provider\" : \"travis\" , \"repo_slug\" : repo_slug , \"repo_owner\" : repo_owner , \"repo_name\" : repo_name , \"commit\" : os . environ [ \"TRAVIS_COMMIT\" ], \"triggering_commit\" : os . getenv ( \"TRAVIS_PULL_REQUEST_SHA\" ) or os . environ [ \"TRAVIS_COMMIT\" ], \"build_url\" : os . environ [ \"TRAVIS_BUILD_WEB_URL\" ], \"job_url\" : os . environ [ \"TRAVIS_JOB_WEB_URL\" ], } if os . getenv ( \"APPVEYOR\" , \"false\" ) . lower () == \"true\" : # https://www.appveyor.com/docs/environment-variables/ repo_slug = os . environ [ \"APPVEYOR_REPO_NAME\" ] repo_owner , repo_name = repo_slug . split ( \"/\" ) provider_url = \" {APPVEYOR_URL} /project/ {APPVEYOR_ACCOUNT_NAME} / {APPVEYOR_PROJECT_SLUG} \" . format ( ** os . environ ) build_url = f \" { provider_url } /builds/ { os . environ [ 'APPVEYOR_BUILD_ID' ] } \" return { \"provider\" : \"appveyor\" , \"provider_account\" : os . environ [ \"APPVEYOR_ACCOUNT_NAME\" ], \"repo_slug\" : repo_slug , \"repo_owner\" : repo_owner , \"repo_name\" : repo_name , \"commit\" : os . environ [ \"APPVEYOR_REPO_COMMIT\" ], \"triggering_commit\" : os . getenv ( \"APPVEYOR_PULL_REQUEST_HEAD_COMMIT\" ) or os . environ [ \"APPVEYOR_REPO_COMMIT\" ], \"build_url\" : build_url , \"job_url\" : f \" { provider_url } /build/job/ { os . environ [ 'APPVEYOR_JOB_ID' ] } \" , \"artifact_url\" : f \" { build_url } /artifacts\" , } if os . getenv ( \"CI\" , \"false\" ) . lower () == \"true\" : logging . warning ( \"Detected CI environment variable, but get_continuous_integration_parameters \" \"did not detect environment variables for a supported CI provider. \" \"Supported providers are: {} \" . format ( \", \" . join ( supported_providers )) ) return None Variables supported_providers Functions get_continuous_integration_parameters def get_continuous_integration_parameters ( ) Return a dictionary with information on a continuous integration build inferred from environment variables. The following dictionary keys are set (all values are also strings): - provider: name of CI provider, such as \"travis\" or \"appveyor\". - repo_slug: owner/name of the source code repository, i.e. \"manubot/rootstock\" - repo_owner: owner from repo_slug, i.e. \"manubot\" - repo_name: name from repo_slug, i.e. \"rootstock\" - commit: git commit being evaluated by the CI build For pull requests, CI services often build a merge commit with the default branch rather than the commit that was added to the pull request. In these cases, commit refers to this merge commit. - triggering_commit: git commit that triggered the CI build. - build_url: URL for the webpage with build details - job_url: URL for the webpage with job details GitHub Actions does not set an environment variable with triggering_commit for pull requests. Therefore, set the following environment variable in your workflow: env : GITHUB_PULL_REQUEST_SHA : ${{ github.event.pull_request.head.sha }} View Source def get_continuous_integration_parameters (): \"\"\" Return a dictionary with information on a continuous integration build inferred from environment variables. The following dictionary keys are set (all values are also strings): - provider: name of CI provider, such as \"travis\" or \"appveyor\". - repo_slug: owner/name of the source code repository, i.e. \"manubot/rootstock\" - repo_owner: owner from repo_slug, i.e. \"manubot\" - repo_name: name from repo_slug, i.e. \"rootstock\" - commit: git commit being evaluated by the CI build For pull requests, CI services often build a merge commit with the default branch rather than the commit that was added to the pull request. In these cases, commit refers to this merge commit. - triggering_commit: git commit that triggered the CI build. - build_url: URL for the webpage with build details - job_url: URL for the webpage with job details GitHub Actions does not set an environment variable with triggering_commit for pull requests. Therefore, set the following environment variable in your workflow: ```yaml env: GITHUB_PULL_REQUEST_SHA: ${{ github.event.pull_request.head.sha }} ``` \"\"\" if os . getenv ( \"GITHUB_ACTIONS\" , \"false\" ) == \"true\" : # https://git.io/JvUf7 repo_slug = os . environ [ \"GITHUB_REPOSITORY\" ] repo_owner , repo_name = repo_slug . split ( \"/\" ) run_id = os . environ [ \"GITHUB_RUN_ID\" ] # GITHUB_SHA for pull_request event: Last merge commit on the GITHUB_REF branch # GITHUB_SHA for push event: Commit pushed, unless deleting a branch (when it's the default branch) # https://git.io/JvUfd github_sha = os . environ [ \"GITHUB_SHA\" ] ci_params = { \"provider\" : \"github\" , \"repo_slug\" : repo_slug , \"repo_owner\" : repo_owner , \"repo_name\" : repo_name , \"commit\" : github_sha , \"triggering_commit\" : os . getenv ( \"GITHUB_PULL_REQUEST_SHA\" ) or github_sha , \"build_url\" : f \"https://github.com/{repo_slug}/commit/{github_sha}/checks\" , \"job_url\" : f \"https://github.com/{repo_slug}/actions/runs/{run_id}\" , } return ci_params if os . getenv ( \"TRAVIS\" , \"false\" ) == \"true\" : # https://docs.travis-ci.com/user/environment-variables/ repo_slug = os . environ [ \"TRAVIS_REPO_SLUG\" ] repo_owner , repo_name = repo_slug . split ( \"/\" ) return { \"provider\" : \"travis\" , \"repo_slug\" : repo_slug , \"repo_owner\" : repo_owner , \"repo_name\" : repo_name , \"commit\" : os . environ [ \"TRAVIS_COMMIT\" ], \"triggering_commit\" : os . getenv ( \"TRAVIS_PULL_REQUEST_SHA\" ) or os . environ [ \"TRAVIS_COMMIT\" ], \"build_url\" : os . environ [ \"TRAVIS_BUILD_WEB_URL\" ], \"job_url\" : os . environ [ \"TRAVIS_JOB_WEB_URL\" ], } if os . getenv ( \"APPVEYOR\" , \"false\" ) . lower () == \"true\" : # https://www.appveyor.com/docs/environment-variables/ repo_slug = os . environ [ \"APPVEYOR_REPO_NAME\" ] repo_owner , repo_name = repo_slug . split ( \"/\" ) provider_url = \"{APPVEYOR_URL}/project/{APPVEYOR_ACCOUNT_NAME}/{APPVEYOR_PROJECT_SLUG}\" . format ( ** os . environ ) build_url = f \"{provider_url}/builds/{os.environ['APPVEYOR_BUILD_ID']}\" return { \"provider\" : \"appveyor\" , \"provider_account\" : os . environ [ \"APPVEYOR_ACCOUNT_NAME\" ], \"repo_slug\" : repo_slug , \"repo_owner\" : repo_owner , \"repo_name\" : repo_name , \"commit\" : os . environ [ \"APPVEYOR_REPO_COMMIT\" ], \"triggering_commit\" : os . getenv ( \"APPVEYOR_PULL_REQUEST_HEAD_COMMIT\" ) or os . environ [ \"APPVEYOR_REPO_COMMIT\" ], \"build_url\" : build_url , \"job_url\" : f \"{provider_url}/build/job/{os.environ['APPVEYOR_JOB_ID']}\" , \"artifact_url\" : f \"{build_url}/artifacts\" , } if os . getenv ( \"CI\" , \"false\" ) . lower () == \"true\" : logging . warning ( \"Detected CI environment variable, but get_continuous_integration_parameters \" \"did not detect environment variables for a supported CI provider. \" \"Supported providers are: {}\" . format ( \", \" . join ( supported_providers )) ) return None","title":"Ci"},{"location":"reference/manubot/process/ci/#module-manubotprocessci","text":"None None View Source import logging import os supported_providers = [ \"github\" , \"travis\" , \"appveyor\" ] def get_continuous_integration_parameters (): \"\"\" Return a dictionary with information on a continuous integration build inferred from environment variables. The following dictionary keys are set (all values are also strings): - provider: name of CI provider, such as \"travis\" or \"appveyor\". - repo_slug: owner/name of the source code repository, i.e. \"manubot/rootstock\" - repo_owner: owner from repo_slug, i.e. \"manubot\" - repo_name: name from repo_slug, i.e. \"rootstock\" - commit: git commit being evaluated by the CI build For pull requests, CI services often build a merge commit with the default branch rather than the commit that was added to the pull request. In these cases, commit refers to this merge commit. - triggering_commit: git commit that triggered the CI build. - build_url: URL for the webpage with build details - job_url: URL for the webpage with job details GitHub Actions does not set an environment variable with triggering_commit for pull requests. Therefore, set the following environment variable in your workflow: ```yaml env: GITHUB_PULL_REQUEST_SHA: ${{ github.event.pull_request.head.sha }} ``` \"\"\" if os . getenv ( \"GITHUB_ACTIONS\" , \"false\" ) == \"true\" : # https://git.io/JvUf7 repo_slug = os . environ [ \"GITHUB_REPOSITORY\" ] repo_owner , repo_name = repo_slug . split ( \"/\" ) run_id = os . environ [ \"GITHUB_RUN_ID\" ] # GITHUB_SHA for pull_request event: Last merge commit on the GITHUB_REF branch # GITHUB_SHA for push event: Commit pushed, unless deleting a branch (when it's the default branch) # https://git.io/JvUfd github_sha = os . environ [ \"GITHUB_SHA\" ] ci_params = { \"provider\" : \"github\" , \"repo_slug\" : repo_slug , \"repo_owner\" : repo_owner , \"repo_name\" : repo_name , \"commit\" : github_sha , \"triggering_commit\" : os . getenv ( \"GITHUB_PULL_REQUEST_SHA\" ) or github_sha , \"build_url\" : f \"https://github.com/ { repo_slug } /commit/ { github_sha } /checks\" , \"job_url\" : f \"https://github.com/ { repo_slug } /actions/runs/ { run_id } \" , } return ci_params if os . getenv ( \"TRAVIS\" , \"false\" ) == \"true\" : # https://docs.travis-ci.com/user/environment-variables/ repo_slug = os . environ [ \"TRAVIS_REPO_SLUG\" ] repo_owner , repo_name = repo_slug . split ( \"/\" ) return { \"provider\" : \"travis\" , \"repo_slug\" : repo_slug , \"repo_owner\" : repo_owner , \"repo_name\" : repo_name , \"commit\" : os . environ [ \"TRAVIS_COMMIT\" ], \"triggering_commit\" : os . getenv ( \"TRAVIS_PULL_REQUEST_SHA\" ) or os . environ [ \"TRAVIS_COMMIT\" ], \"build_url\" : os . environ [ \"TRAVIS_BUILD_WEB_URL\" ], \"job_url\" : os . environ [ \"TRAVIS_JOB_WEB_URL\" ], } if os . getenv ( \"APPVEYOR\" , \"false\" ) . lower () == \"true\" : # https://www.appveyor.com/docs/environment-variables/ repo_slug = os . environ [ \"APPVEYOR_REPO_NAME\" ] repo_owner , repo_name = repo_slug . split ( \"/\" ) provider_url = \" {APPVEYOR_URL} /project/ {APPVEYOR_ACCOUNT_NAME} / {APPVEYOR_PROJECT_SLUG} \" . format ( ** os . environ ) build_url = f \" { provider_url } /builds/ { os . environ [ 'APPVEYOR_BUILD_ID' ] } \" return { \"provider\" : \"appveyor\" , \"provider_account\" : os . environ [ \"APPVEYOR_ACCOUNT_NAME\" ], \"repo_slug\" : repo_slug , \"repo_owner\" : repo_owner , \"repo_name\" : repo_name , \"commit\" : os . environ [ \"APPVEYOR_REPO_COMMIT\" ], \"triggering_commit\" : os . getenv ( \"APPVEYOR_PULL_REQUEST_HEAD_COMMIT\" ) or os . environ [ \"APPVEYOR_REPO_COMMIT\" ], \"build_url\" : build_url , \"job_url\" : f \" { provider_url } /build/job/ { os . environ [ 'APPVEYOR_JOB_ID' ] } \" , \"artifact_url\" : f \" { build_url } /artifacts\" , } if os . getenv ( \"CI\" , \"false\" ) . lower () == \"true\" : logging . warning ( \"Detected CI environment variable, but get_continuous_integration_parameters \" \"did not detect environment variables for a supported CI provider. \" \"Supported providers are: {} \" . format ( \", \" . join ( supported_providers )) ) return None","title":"Module manubot.process.ci"},{"location":"reference/manubot/process/ci/#variables","text":"supported_providers","title":"Variables"},{"location":"reference/manubot/process/ci/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/ci/#get_continuous_integration_parameters","text":"def get_continuous_integration_parameters ( ) Return a dictionary with information on a continuous integration build inferred from environment variables. The following dictionary keys are set (all values are also strings): - provider: name of CI provider, such as \"travis\" or \"appveyor\". - repo_slug: owner/name of the source code repository, i.e. \"manubot/rootstock\" - repo_owner: owner from repo_slug, i.e. \"manubot\" - repo_name: name from repo_slug, i.e. \"rootstock\" - commit: git commit being evaluated by the CI build For pull requests, CI services often build a merge commit with the default branch rather than the commit that was added to the pull request. In these cases, commit refers to this merge commit. - triggering_commit: git commit that triggered the CI build. - build_url: URL for the webpage with build details - job_url: URL for the webpage with job details GitHub Actions does not set an environment variable with triggering_commit for pull requests. Therefore, set the following environment variable in your workflow: env : GITHUB_PULL_REQUEST_SHA : ${{ github.event.pull_request.head.sha }} View Source def get_continuous_integration_parameters (): \"\"\" Return a dictionary with information on a continuous integration build inferred from environment variables. The following dictionary keys are set (all values are also strings): - provider: name of CI provider, such as \"travis\" or \"appveyor\". - repo_slug: owner/name of the source code repository, i.e. \"manubot/rootstock\" - repo_owner: owner from repo_slug, i.e. \"manubot\" - repo_name: name from repo_slug, i.e. \"rootstock\" - commit: git commit being evaluated by the CI build For pull requests, CI services often build a merge commit with the default branch rather than the commit that was added to the pull request. In these cases, commit refers to this merge commit. - triggering_commit: git commit that triggered the CI build. - build_url: URL for the webpage with build details - job_url: URL for the webpage with job details GitHub Actions does not set an environment variable with triggering_commit for pull requests. Therefore, set the following environment variable in your workflow: ```yaml env: GITHUB_PULL_REQUEST_SHA: ${{ github.event.pull_request.head.sha }} ``` \"\"\" if os . getenv ( \"GITHUB_ACTIONS\" , \"false\" ) == \"true\" : # https://git.io/JvUf7 repo_slug = os . environ [ \"GITHUB_REPOSITORY\" ] repo_owner , repo_name = repo_slug . split ( \"/\" ) run_id = os . environ [ \"GITHUB_RUN_ID\" ] # GITHUB_SHA for pull_request event: Last merge commit on the GITHUB_REF branch # GITHUB_SHA for push event: Commit pushed, unless deleting a branch (when it's the default branch) # https://git.io/JvUfd github_sha = os . environ [ \"GITHUB_SHA\" ] ci_params = { \"provider\" : \"github\" , \"repo_slug\" : repo_slug , \"repo_owner\" : repo_owner , \"repo_name\" : repo_name , \"commit\" : github_sha , \"triggering_commit\" : os . getenv ( \"GITHUB_PULL_REQUEST_SHA\" ) or github_sha , \"build_url\" : f \"https://github.com/{repo_slug}/commit/{github_sha}/checks\" , \"job_url\" : f \"https://github.com/{repo_slug}/actions/runs/{run_id}\" , } return ci_params if os . getenv ( \"TRAVIS\" , \"false\" ) == \"true\" : # https://docs.travis-ci.com/user/environment-variables/ repo_slug = os . environ [ \"TRAVIS_REPO_SLUG\" ] repo_owner , repo_name = repo_slug . split ( \"/\" ) return { \"provider\" : \"travis\" , \"repo_slug\" : repo_slug , \"repo_owner\" : repo_owner , \"repo_name\" : repo_name , \"commit\" : os . environ [ \"TRAVIS_COMMIT\" ], \"triggering_commit\" : os . getenv ( \"TRAVIS_PULL_REQUEST_SHA\" ) or os . environ [ \"TRAVIS_COMMIT\" ], \"build_url\" : os . environ [ \"TRAVIS_BUILD_WEB_URL\" ], \"job_url\" : os . environ [ \"TRAVIS_JOB_WEB_URL\" ], } if os . getenv ( \"APPVEYOR\" , \"false\" ) . lower () == \"true\" : # https://www.appveyor.com/docs/environment-variables/ repo_slug = os . environ [ \"APPVEYOR_REPO_NAME\" ] repo_owner , repo_name = repo_slug . split ( \"/\" ) provider_url = \"{APPVEYOR_URL}/project/{APPVEYOR_ACCOUNT_NAME}/{APPVEYOR_PROJECT_SLUG}\" . format ( ** os . environ ) build_url = f \"{provider_url}/builds/{os.environ['APPVEYOR_BUILD_ID']}\" return { \"provider\" : \"appveyor\" , \"provider_account\" : os . environ [ \"APPVEYOR_ACCOUNT_NAME\" ], \"repo_slug\" : repo_slug , \"repo_owner\" : repo_owner , \"repo_name\" : repo_name , \"commit\" : os . environ [ \"APPVEYOR_REPO_COMMIT\" ], \"triggering_commit\" : os . getenv ( \"APPVEYOR_PULL_REQUEST_HEAD_COMMIT\" ) or os . environ [ \"APPVEYOR_REPO_COMMIT\" ], \"build_url\" : build_url , \"job_url\" : f \"{provider_url}/build/job/{os.environ['APPVEYOR_JOB_ID']}\" , \"artifact_url\" : f \"{build_url}/artifacts\" , } if os . getenv ( \"CI\" , \"false\" ) . lower () == \"true\" : logging . warning ( \"Detected CI environment variable, but get_continuous_integration_parameters \" \"did not detect environment variables for a supported CI provider. \" \"Supported providers are: {}\" . format ( \", \" . join ( supported_providers )) ) return None","title":"get_continuous_integration_parameters"},{"location":"reference/manubot/process/manuscript/","text":"Module manubot.process.manuscript None None View Source import datetime import json import logging import pathlib def get_text ( directory ): \"\"\" Return a concatenated string of section texts from the specified directory. Text files should be UTF-8 encoded. \"\"\" section_dir = pathlib . Path ( directory ) paths = sorted ( section_dir . glob ( \"[0-9]*.md\" )) name_to_text = dict () for path in paths : name_to_text [ path . stem ] = path . read_text ( encoding = \"utf-8-sig\" ) logging . info ( \"Manuscript content parts: \\n \" + \" \\n \" . join ( name_to_text )) return \" \\n\\n \" . join ( name_to_text . values ()) + \" \\n \" def get_manuscript_stats ( text ): \"\"\" Compute manuscript statistics. \"\"\" stats = dict () stats [ \"word_count\" ] = len ( text . split ()) logging . info ( f \"Generated manscript stats: \\n { json . dumps ( stats , indent = 2 ) } \" ) return stats def datetime_now (): \"\"\" Return the current datetime, with timezone awareness https://stackoverflow.com/a/39079819/4651668 \"\"\" tzinfo = datetime . datetime . now ( datetime . timezone . utc ) . astimezone () . tzinfo return datetime . datetime . now ( tzinfo ) Functions datetime_now def datetime_now ( ) Return the current datetime, with timezone awareness https://stackoverflow.com/a/39079819/4651668 View Source def datetime_now () : \"\"\" Return the current datetime , with timezone awareness https : // stackoverflow . com / a / 39079819 / 4651668 \"\"\" tzinfo = datetime . datetime . now ( datetime . timezone . utc ) . astimezone () . tzinfo return datetime . datetime . now ( tzinfo ) get_manuscript_stats def get_manuscript_stats ( text ) Compute manuscript statistics. View Source def get_manuscript_stats ( text ) : \"\"\" Compute manuscript statistics . \"\"\" stats = dict () stats [ \" word_count \" ] = len ( text . split ()) logging . info ( f \" Generated manscript stats: \\n {json.dumps(stats, indent=2)} \" ) return stats get_text def get_text ( directory ) Return a concatenated string of section texts from the specified directory. Text files should be UTF-8 encoded. View Source def get_text ( directory ) : \"\"\" Return a concatenated string of section texts from the specified directory . Text files should be UTF - 8 encoded . \"\"\" section_dir = pathlib . Path ( directory ) paths = sorted ( section_dir . glob ( \" [0-9]*.md \" )) name_to_text = dict () for path in paths : name_to_text [ path . stem ] = path . read_text ( encoding = \" utf-8-sig \" ) logging . info ( \" Manuscript content parts: \\n \" + \" \\n \" . join ( name_to_text )) return \" \\n\\n \" . join ( name_to_text . values ()) + \" \\n \"","title":"Manuscript"},{"location":"reference/manubot/process/manuscript/#module-manubotprocessmanuscript","text":"None None View Source import datetime import json import logging import pathlib def get_text ( directory ): \"\"\" Return a concatenated string of section texts from the specified directory. Text files should be UTF-8 encoded. \"\"\" section_dir = pathlib . Path ( directory ) paths = sorted ( section_dir . glob ( \"[0-9]*.md\" )) name_to_text = dict () for path in paths : name_to_text [ path . stem ] = path . read_text ( encoding = \"utf-8-sig\" ) logging . info ( \"Manuscript content parts: \\n \" + \" \\n \" . join ( name_to_text )) return \" \\n\\n \" . join ( name_to_text . values ()) + \" \\n \" def get_manuscript_stats ( text ): \"\"\" Compute manuscript statistics. \"\"\" stats = dict () stats [ \"word_count\" ] = len ( text . split ()) logging . info ( f \"Generated manscript stats: \\n { json . dumps ( stats , indent = 2 ) } \" ) return stats def datetime_now (): \"\"\" Return the current datetime, with timezone awareness https://stackoverflow.com/a/39079819/4651668 \"\"\" tzinfo = datetime . datetime . now ( datetime . timezone . utc ) . astimezone () . tzinfo return datetime . datetime . now ( tzinfo )","title":"Module manubot.process.manuscript"},{"location":"reference/manubot/process/manuscript/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/manuscript/#datetime_now","text":"def datetime_now ( ) Return the current datetime, with timezone awareness https://stackoverflow.com/a/39079819/4651668 View Source def datetime_now () : \"\"\" Return the current datetime , with timezone awareness https : // stackoverflow . com / a / 39079819 / 4651668 \"\"\" tzinfo = datetime . datetime . now ( datetime . timezone . utc ) . astimezone () . tzinfo return datetime . datetime . now ( tzinfo )","title":"datetime_now"},{"location":"reference/manubot/process/manuscript/#get_manuscript_stats","text":"def get_manuscript_stats ( text ) Compute manuscript statistics. View Source def get_manuscript_stats ( text ) : \"\"\" Compute manuscript statistics . \"\"\" stats = dict () stats [ \" word_count \" ] = len ( text . split ()) logging . info ( f \" Generated manscript stats: \\n {json.dumps(stats, indent=2)} \" ) return stats","title":"get_manuscript_stats"},{"location":"reference/manubot/process/manuscript/#get_text","text":"def get_text ( directory ) Return a concatenated string of section texts from the specified directory. Text files should be UTF-8 encoded. View Source def get_text ( directory ) : \"\"\" Return a concatenated string of section texts from the specified directory . Text files should be UTF - 8 encoded . \"\"\" section_dir = pathlib . Path ( directory ) paths = sorted ( section_dir . glob ( \" [0-9]*.md \" )) name_to_text = dict () for path in paths : name_to_text [ path . stem ] = path . read_text ( encoding = \" utf-8-sig \" ) logging . info ( \" Manuscript content parts: \\n \" + \" \\n \" . join ( name_to_text )) return \" \\n\\n \" . join ( name_to_text . values ()) + \" \\n \"","title":"get_text"},{"location":"reference/manubot/process/metadata/","text":"Module manubot.process.metadata Tools for manuscript metadata processing including thumbnail detection and processing. None View Source \"\"\" Tools for manuscript metadata processing including thumbnail detection and processing. \"\"\" import functools import logging import pathlib import subprocess from typing import Optional from urllib.parse import urljoin def get_header_includes ( variables : dict ) -> str : \"\"\" Render `header-includes-template.html` using information from `variables`. \"\"\" from .util import template_with_jinja2 path = pathlib . Path ( __file__ ) . parent . joinpath ( \"header-includes-template.html\" ) try : template = path . read_text ( encoding = \"utf-8-sig\" ) return template_with_jinja2 ( template , variables ) except Exception : logging . exception ( f \"Error generating header-includes.\" ) return \"\" def get_thumbnail_url ( thumbnail = None ): \"\"\" Starting with a user-specified `thumbnail` as either a path, URL, or None, return an absolute URL pointing to the thumbnail image. If the provided `thumbnail` is a URL, return this URL unmodified. If `thumbnail` is None, search for `thumbnail.png` within the git repository from which this function is executed. If `thumbnail` is a local path, the path should be relative to root directory of the git repository it is located in. If a local path is provided or detected, it is converted to a GitHub raw URL. \"\"\" from manubot.util import is_http_url if not thumbnail : message = \"get_thumbnail_url: thumbnail location not explicitly provided. \" thumbnail = _find_thumbnail_path () message += ( f \"Thumbnail detected at { thumbnail !r} \" if thumbnail else \"No local thumbnail detected\" ) logging . debug ( message ) elif is_http_url ( thumbnail ): logging . debug ( \"provided thumbnail is a URL. Pass it through.\" ) return thumbnail return _thumbnail_path_to_url ( thumbnail ) def _find_thumbnail_path (): \"\"\" If this this function is executed with a working directory that is inside a git repository, return the path to a `thumbnail.png` file located anywhere in that repository. Otherwise, return `None`. \"\"\" directory = git_repository_root () if not directory : return None paths = directory . glob ( \"**/thumbnail.png\" ) paths = [ path . relative_to ( directory ) for path in paths ] paths = sorted ( paths , key = lambda x : ( len ( x . parents ), x )) if not paths : return None return paths [ 0 ] . as_posix () def _thumbnail_path_to_url ( path ): \"\"\" Convert a local thumbnail path (string) to an absolute URL using the GitHub repository location detected using `get_continuous_integration_parameters`. \"\"\" if not path : return None from .ci import get_continuous_integration_parameters info = get_continuous_integration_parameters () try : url = f \"https://github.com/ { info [ 'repo_slug' ] } /raw/ { info [ 'triggering_commit' ] } / { path } \" except ( TypeError , KeyError ): return None return url @functools . lru_cache () def git_repository_root (): \"\"\" Return the path to repository root directory or `None` if indeterminate. \"\"\" for cmd in ( [ \"git\" , \"rev-parse\" , \"--show-superproject-working-tree\" ], [ \"git\" , \"rev-parse\" , \"--show-toplevel\" ], ): try : path = subprocess . check_output ( cmd , universal_newlines = True ) . rstrip ( \" \\r\\n \" ) if path : return pathlib . Path ( path ) except ( subprocess . CalledProcessError , OSError ): pass return None def get_manuscript_urls ( html_url : Optional [ str ] = None ) -> dict : \"\"\" Return a dictionary with URLs for a manuscript. An example for a manuscript where all URLs get set, inferred from continuous integration environment variables, is: ```python { \"html_url\": \"https://manubot.github.io/rootstock/\", \"pdf_url\": \"https://manubot.github.io/rootstock/manuscript.pdf\", \"html_url_versioned\": \"https://manubot.github.io/rootstock/v/7cf9071212ce33116ad09cf2237a370b180a3c35/\", \"pdf_url_versioned\": \"https://manubot.github.io/rootstock/v/7cf9071212ce33116ad09cf2237a370b180a3c35/manuscript.pdf\", } ``` Provide `html_url` to set a custom domain. If `html_url=\"https://git.dhimmel.com/bitcoin-whitepaper/\"`, the return dictionary will be like: ```python { \"html_url\": \"https://git.dhimmel.com/bitcoin-whitepaper/\", \"pdf_url\": \"https://git.dhimmel.com/bitcoin-whitepaper/manuscript.pdf\", \"html_url_versioned\": \"https://git.dhimmel.com/bitcoin-whitepaper/v/cb1f2c12eec8b56db9ef5f641ec805e2d449d319/\", \"pdf_url_versioned\": \"https://git.dhimmel.com/bitcoin-whitepaper/v/cb1f2c12eec8b56db9ef5f641ec805e2d449d319/manuscript.pdf\", } ``` Note the trailing `/` in `html_url`, which is required for proper functioning. \"\"\" import requests from .ci import get_continuous_integration_parameters urls = dict () ci_params = get_continuous_integration_parameters () if html_url is None : if not ci_params : return urls html_url = \"https:// {repo_owner} .github.io/ {repo_name} /\" . format ( ** ci_params ) urls [ \"html_url\" ] = html_url urls [ \"pdf_url\" ] = urljoin ( html_url , \"manuscript.pdf\" ) if not ci_params : return urls urls [ \"html_url_versioned\" ] = urljoin ( html_url , \"v/ {commit} /\" . format ( ** ci_params )) urls [ \"pdf_url_versioned\" ] = urljoin ( urls [ \"html_url_versioned\" ], \"manuscript.pdf\" ) response = requests . head ( html_url , allow_redirects = True ) if not response . ok : logging . warning ( \"html_url is not web accessible. \" f \" { html_url } returned status code { response . status_code } . \" \"Ignore this warning if the manuscript has not yet been deployed for the first time. \" ) if response . history : logging . info ( \"html_url includes redirects. In order of oldest to most recent: \\n \" + \" \\n \" . join ( x . url for x in response . history + [ response ]) ) return urls def get_software_versions ( rootstock : bool = True ) -> dict : \"\"\" Return a dictionary of software versions for softwares components: - manubot_version: the semantic version number of the manubot python package. - rootstock_commit: the version of the rootstock repository, as a commit hash, included in the manuscript repository. Values whose detection fails are set to None. The `rootstock` parameter controls whether to fetch the rootstock commit id. The rootstock git remote will be added to the local git repository if true. \"\"\" from manubot import __version__ as manubot_version return { \"manubot_version\" : manubot_version , \"rootstock_commit\" : get_rootstock_commit () if rootstock else None , } def get_rootstock_commit () -> Optional [ str ]: \"\"\" Return the most recent commit in common between the git repository this function is run within (usually a Manubot manuscript repository) and the `main` branch of the `rootstock` remote. WARNING: This function may modify the git repository its executed within: - if the repository has not set the `roostock` remote, it is set to point to the default Rootstock repository of <https://github.com/manubot/rootstock>. - fetches the latest commits in the `main` branch of the `rootstock` remote \"\"\" from manubot.util import shlex_join # add rootstock remote if remote is not already set rootstock_remote = \"https://github.com/manubot/rootstock.git\" args = [ \"git\" , \"remote\" , \"add\" , \"rootstock\" , rootstock_remote ] process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) if process . returncode == 0 : logging . info ( \"get_rootstock_commit added a `rootstock` remote to the git repository.\" ) # find most recent common ancestor commit try : args = [ \"git\" , \"fetch\" , \"rootstock\" , \"main\" ] subprocess . check_output ( args , stderr = subprocess . PIPE , universal_newlines = True ) args = [ \"git\" , \"merge-base\" , \"HEAD\" , \"rootstock/main\" ] output = subprocess . check_output ( args , stderr = subprocess . PIPE , universal_newlines = True ) except subprocess . CalledProcessError as error : logging . warning ( f \"get_rootstock_commit: { shlex_join ( error . cmd ) !r} returned exit code { error . returncode } \" f \"with the following stdout: \\n { error . stdout } \\n \" f \"And the following stderr: \\n { error . stderr } \" ) return None rootstock_commit = output . strip () return rootstock_commit Functions get_header_includes def get_header_includes ( variables : dict ) -> str Render header-includes-template.html using information from variables . View Source def get_header_includes ( variables : dict ) -> str : \"\"\" Render `header-includes-template.html` using information from `variables`. \"\"\" from .util import template_with_jinja2 path = pathlib . Path ( __file__ ) . parent . joinpath ( \"header-includes-template.html\" ) try : template = path . read_text ( encoding = \"utf-8-sig\" ) return template_with_jinja2 ( template , variables ) except Exception : logging . exception ( f \"Error generating header-includes.\" ) return \"\" get_manuscript_urls def get_manuscript_urls ( html_url : Optional [ str ] = None ) -> dict Return a dictionary with URLs for a manuscript. An example for a manuscript where all URLs get set, inferred from continuous integration environment variables, is: { \"html_url\" : \"https://manubot.github.io/rootstock/\" , \"pdf_url\" : \"https://manubot.github.io/rootstock/manuscript.pdf\" , \"html_url_versioned\" : \"https://manubot.github.io/rootstock/v/7cf9071212ce33116ad09cf2237a370b180a3c35/\" , \"pdf_url_versioned\" : \"https://manubot.github.io/rootstock/v/7cf9071212ce33116ad09cf2237a370b180a3c35/manuscript.pdf\" , } Provide html_url to set a custom domain. If html_url=\"https://git.dhimmel.com/bitcoin-whitepaper/\" , the return dictionary will be like: { \"html_url\" : \"https://git.dhimmel.com/bitcoin-whitepaper/\" , \"pdf_url\" : \"https://git.dhimmel.com/bitcoin-whitepaper/manuscript.pdf\" , \"html_url_versioned\" : \"https://git.dhimmel.com/bitcoin-whitepaper/v/cb1f2c12eec8b56db9ef5f641ec805e2d449d319/\" , \"pdf_url_versioned\" : \"https://git.dhimmel.com/bitcoin-whitepaper/v/cb1f2c12eec8b56db9ef5f641ec805e2d449d319/manuscript.pdf\" , } Note the trailing / in html_url , which is required for proper functioning. View Source def get_manuscript_urls ( html_url : Optional [ str ] = None ) -> dict : \"\"\" Return a dictionary with URLs for a manuscript. An example for a manuscript where all URLs get set, inferred from continuous integration environment variables, is: ```python { \"html_url\": \"https://manubot.github.io/rootstock/\", \"pdf_url\": \"https://manubot.github.io/rootstock/manuscript.pdf\", \"html_url_versioned\": \"https://manubot.github.io/rootstock/v/7cf9071212ce33116ad09cf2237a370b180a3c35/\", \"pdf_url_versioned\": \"https://manubot.github.io/rootstock/v/7cf9071212ce33116ad09cf2237a370b180a3c35/manuscript.pdf\", } ``` Provide `html_url` to set a custom domain. If `html_url=\"https://git.dhimmel.com/bitcoin-whitepaper/\"`, the return dictionary will be like: ```python { \"html_url\": \"https://git.dhimmel.com/bitcoin-whitepaper/\", \"pdf_url\": \"https://git.dhimmel.com/bitcoin-whitepaper/manuscript.pdf\", \"html_url_versioned\": \"https://git.dhimmel.com/bitcoin-whitepaper/v/cb1f2c12eec8b56db9ef5f641ec805e2d449d319/\", \"pdf_url_versioned\": \"https://git.dhimmel.com/bitcoin-whitepaper/v/cb1f2c12eec8b56db9ef5f641ec805e2d449d319/manuscript.pdf\", } ``` Note the trailing `/` in `html_url`, which is required for proper functioning. \"\"\" import requests from . ci import get_continuous_integration_parameters urls = dict () ci_params = get_continuous_integration_parameters () if html_url is None : if not ci_params : return urls html_url = \"https://{repo_owner}.github.io/{repo_name}/\" . format ( ** ci_params ) urls [ \"html_url\" ] = html_url urls [ \"pdf_url\" ] = urljoin ( html_url , \"manuscript.pdf\" ) if not ci_params : return urls urls [ \"html_url_versioned\" ] = urljoin ( html_url , \"v/{commit}/\" . format ( ** ci_params )) urls [ \"pdf_url_versioned\" ] = urljoin ( urls [ \"html_url_versioned\" ], \"manuscript.pdf\" ) response = requests . head ( html_url , allow_redirects = True ) if not response . ok : logging . warning ( \"html_url is not web accessible. \" f \"{html_url} returned status code {response.status_code}. \" \"Ignore this warning if the manuscript has not yet been deployed for the first time. \" ) if response . history : logging . info ( \"html_url includes redirects. In order of oldest to most recent: \\n \" + \" \\n \" . join ( x . url for x in response . history + [ response ]) ) return urls get_rootstock_commit def get_rootstock_commit ( ) -> Optional [ str ] Return the most recent commit in common between the git repository this function is run within (usually a Manubot manuscript repository) and the main branch of the rootstock remote. WARNING: This function may modify the git repository its executed within: if the repository has not set the roostock remote, it is set to point to the default Rootstock repository of https://github.com/manubot/rootstock . fetches the latest commits in the main branch of the rootstock remote View Source def get_rootstock_commit () -> Optional [ str ]: \"\"\" Return the most recent commit in common between the git repository this function is run within (usually a Manubot manuscript repository) and the `main` branch of the `rootstock` remote. WARNING: This function may modify the git repository its executed within: - if the repository has not set the `roostock` remote, it is set to point to the default Rootstock repository of <https://github.com/manubot/rootstock>. - fetches the latest commits in the `main` branch of the `rootstock` remote \"\"\" from manubot.util import shlex_join # add rootstock remote if remote is not already set rootstock_remote = \"https://github.com/manubot/rootstock.git\" args = [ \"git\" , \"remote\" , \"add\" , \"rootstock\" , rootstock_remote ] process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) if process . returncode == 0 : logging . info ( \"get_rootstock_commit added a `rootstock` remote to the git repository.\" ) # find most recent common ancestor commit try : args = [ \"git\" , \"fetch\" , \"rootstock\" , \"main\" ] subprocess . check_output ( args , stderr = subprocess . PIPE , universal_newlines = True ) args = [ \"git\" , \"merge-base\" , \"HEAD\" , \"rootstock/main\" ] output = subprocess . check_output ( args , stderr = subprocess . PIPE , universal_newlines = True ) except subprocess . CalledProcessError as error : logging . warning ( f \"get_rootstock_commit: { shlex_join ( error . cmd ) !r} returned exit code { error . returncode } \" f \"with the following stdout: \\n { error . stdout } \\n \" f \"And the following stderr: \\n { error . stderr } \" ) return None rootstock_commit = output . strip () return rootstock_commit get_software_versions def get_software_versions ( rootstock : bool = True ) -> dict Return a dictionary of software versions for softwares components: manubot_version: the semantic version number of the manubot python package. rootstock_commit: the version of the rootstock repository, as a commit hash, included in the manuscript repository. Values whose detection fails are set to None. The rootstock parameter controls whether to fetch the rootstock commit id. The rootstock git remote will be added to the local git repository if true. View Source def get_software_versions ( rootstock : bool = True ) -> dict : \"\"\" Return a dictionary of software versions for softwares components: - manubot_version: the semantic version number of the manubot python package. - rootstock_commit: the version of the rootstock repository, as a commit hash, included in the manuscript repository. Values whose detection fails are set to None. The `rootstock` parameter controls whether to fetch the rootstock commit id. The rootstock git remote will be added to the local git repository if true. \"\"\" from manubot import __version__ as manubot_version return { \"manubot_version\" : manubot_version , \"rootstock_commit\" : get_rootstock_commit () if rootstock else None , } get_thumbnail_url def get_thumbnail_url ( thumbnail = None ) Starting with a user-specified thumbnail as either a path, URL, or None, return an absolute URL pointing to the thumbnail image. If the provided thumbnail is a URL, return this URL unmodified. If thumbnail is None, search for thumbnail.png within the git repository from which this function is executed. If thumbnail is a local path, the path should be relative to root directory of the git repository it is located in. If a local path is provided or detected, it is converted to a GitHub raw URL. View Source def get_thumbnail_url ( thumbnail = None ): \"\"\" Starting with a user-specified `thumbnail` as either a path, URL, or None, return an absolute URL pointing to the thumbnail image. If the provided `thumbnail` is a URL, return this URL unmodified. If `thumbnail` is None, search for `thumbnail.png` within the git repository from which this function is executed. If `thumbnail` is a local path, the path should be relative to root directory of the git repository it is located in. If a local path is provided or detected, it is converted to a GitHub raw URL. \"\"\" from manubot.util import is_http_url if not thumbnail : message = \"get_thumbnail_url: thumbnail location not explicitly provided. \" thumbnail = _find_thumbnail_path () message += ( f \"Thumbnail detected at { thumbnail !r} \" if thumbnail else \"No local thumbnail detected\" ) logging . debug ( message ) elif is_http_url ( thumbnail ): logging . debug ( \"provided thumbnail is a URL. Pass it through.\" ) return thumbnail return _thumbnail_path_to_url ( thumbnail ) git_repository_root def git_repository_root ( ) Return the path to repository root directory or None if indeterminate. View Source @functools.lru_cache () def git_repository_root () : \" \"\" Return the path to repository root directory or `None` if indeterminate. \"\" \" for cmd in ( [ \"git\" , \"rev-parse\" , \"--show-superproject-working-tree\" ] , [ \"git\" , \"rev-parse\" , \"--show-toplevel\" ] , ) : try : path = subprocess . check_output ( cmd , universal_newlines = True ). rstrip ( \" \\r\\n \" ) if path : return pathlib . Path ( path ) except ( subprocess . CalledProcessError , OSError ) : pass return None","title":"Metadata"},{"location":"reference/manubot/process/metadata/#module-manubotprocessmetadata","text":"Tools for manuscript metadata processing including thumbnail detection and processing. None View Source \"\"\" Tools for manuscript metadata processing including thumbnail detection and processing. \"\"\" import functools import logging import pathlib import subprocess from typing import Optional from urllib.parse import urljoin def get_header_includes ( variables : dict ) -> str : \"\"\" Render `header-includes-template.html` using information from `variables`. \"\"\" from .util import template_with_jinja2 path = pathlib . Path ( __file__ ) . parent . joinpath ( \"header-includes-template.html\" ) try : template = path . read_text ( encoding = \"utf-8-sig\" ) return template_with_jinja2 ( template , variables ) except Exception : logging . exception ( f \"Error generating header-includes.\" ) return \"\" def get_thumbnail_url ( thumbnail = None ): \"\"\" Starting with a user-specified `thumbnail` as either a path, URL, or None, return an absolute URL pointing to the thumbnail image. If the provided `thumbnail` is a URL, return this URL unmodified. If `thumbnail` is None, search for `thumbnail.png` within the git repository from which this function is executed. If `thumbnail` is a local path, the path should be relative to root directory of the git repository it is located in. If a local path is provided or detected, it is converted to a GitHub raw URL. \"\"\" from manubot.util import is_http_url if not thumbnail : message = \"get_thumbnail_url: thumbnail location not explicitly provided. \" thumbnail = _find_thumbnail_path () message += ( f \"Thumbnail detected at { thumbnail !r} \" if thumbnail else \"No local thumbnail detected\" ) logging . debug ( message ) elif is_http_url ( thumbnail ): logging . debug ( \"provided thumbnail is a URL. Pass it through.\" ) return thumbnail return _thumbnail_path_to_url ( thumbnail ) def _find_thumbnail_path (): \"\"\" If this this function is executed with a working directory that is inside a git repository, return the path to a `thumbnail.png` file located anywhere in that repository. Otherwise, return `None`. \"\"\" directory = git_repository_root () if not directory : return None paths = directory . glob ( \"**/thumbnail.png\" ) paths = [ path . relative_to ( directory ) for path in paths ] paths = sorted ( paths , key = lambda x : ( len ( x . parents ), x )) if not paths : return None return paths [ 0 ] . as_posix () def _thumbnail_path_to_url ( path ): \"\"\" Convert a local thumbnail path (string) to an absolute URL using the GitHub repository location detected using `get_continuous_integration_parameters`. \"\"\" if not path : return None from .ci import get_continuous_integration_parameters info = get_continuous_integration_parameters () try : url = f \"https://github.com/ { info [ 'repo_slug' ] } /raw/ { info [ 'triggering_commit' ] } / { path } \" except ( TypeError , KeyError ): return None return url @functools . lru_cache () def git_repository_root (): \"\"\" Return the path to repository root directory or `None` if indeterminate. \"\"\" for cmd in ( [ \"git\" , \"rev-parse\" , \"--show-superproject-working-tree\" ], [ \"git\" , \"rev-parse\" , \"--show-toplevel\" ], ): try : path = subprocess . check_output ( cmd , universal_newlines = True ) . rstrip ( \" \\r\\n \" ) if path : return pathlib . Path ( path ) except ( subprocess . CalledProcessError , OSError ): pass return None def get_manuscript_urls ( html_url : Optional [ str ] = None ) -> dict : \"\"\" Return a dictionary with URLs for a manuscript. An example for a manuscript where all URLs get set, inferred from continuous integration environment variables, is: ```python { \"html_url\": \"https://manubot.github.io/rootstock/\", \"pdf_url\": \"https://manubot.github.io/rootstock/manuscript.pdf\", \"html_url_versioned\": \"https://manubot.github.io/rootstock/v/7cf9071212ce33116ad09cf2237a370b180a3c35/\", \"pdf_url_versioned\": \"https://manubot.github.io/rootstock/v/7cf9071212ce33116ad09cf2237a370b180a3c35/manuscript.pdf\", } ``` Provide `html_url` to set a custom domain. If `html_url=\"https://git.dhimmel.com/bitcoin-whitepaper/\"`, the return dictionary will be like: ```python { \"html_url\": \"https://git.dhimmel.com/bitcoin-whitepaper/\", \"pdf_url\": \"https://git.dhimmel.com/bitcoin-whitepaper/manuscript.pdf\", \"html_url_versioned\": \"https://git.dhimmel.com/bitcoin-whitepaper/v/cb1f2c12eec8b56db9ef5f641ec805e2d449d319/\", \"pdf_url_versioned\": \"https://git.dhimmel.com/bitcoin-whitepaper/v/cb1f2c12eec8b56db9ef5f641ec805e2d449d319/manuscript.pdf\", } ``` Note the trailing `/` in `html_url`, which is required for proper functioning. \"\"\" import requests from .ci import get_continuous_integration_parameters urls = dict () ci_params = get_continuous_integration_parameters () if html_url is None : if not ci_params : return urls html_url = \"https:// {repo_owner} .github.io/ {repo_name} /\" . format ( ** ci_params ) urls [ \"html_url\" ] = html_url urls [ \"pdf_url\" ] = urljoin ( html_url , \"manuscript.pdf\" ) if not ci_params : return urls urls [ \"html_url_versioned\" ] = urljoin ( html_url , \"v/ {commit} /\" . format ( ** ci_params )) urls [ \"pdf_url_versioned\" ] = urljoin ( urls [ \"html_url_versioned\" ], \"manuscript.pdf\" ) response = requests . head ( html_url , allow_redirects = True ) if not response . ok : logging . warning ( \"html_url is not web accessible. \" f \" { html_url } returned status code { response . status_code } . \" \"Ignore this warning if the manuscript has not yet been deployed for the first time. \" ) if response . history : logging . info ( \"html_url includes redirects. In order of oldest to most recent: \\n \" + \" \\n \" . join ( x . url for x in response . history + [ response ]) ) return urls def get_software_versions ( rootstock : bool = True ) -> dict : \"\"\" Return a dictionary of software versions for softwares components: - manubot_version: the semantic version number of the manubot python package. - rootstock_commit: the version of the rootstock repository, as a commit hash, included in the manuscript repository. Values whose detection fails are set to None. The `rootstock` parameter controls whether to fetch the rootstock commit id. The rootstock git remote will be added to the local git repository if true. \"\"\" from manubot import __version__ as manubot_version return { \"manubot_version\" : manubot_version , \"rootstock_commit\" : get_rootstock_commit () if rootstock else None , } def get_rootstock_commit () -> Optional [ str ]: \"\"\" Return the most recent commit in common between the git repository this function is run within (usually a Manubot manuscript repository) and the `main` branch of the `rootstock` remote. WARNING: This function may modify the git repository its executed within: - if the repository has not set the `roostock` remote, it is set to point to the default Rootstock repository of <https://github.com/manubot/rootstock>. - fetches the latest commits in the `main` branch of the `rootstock` remote \"\"\" from manubot.util import shlex_join # add rootstock remote if remote is not already set rootstock_remote = \"https://github.com/manubot/rootstock.git\" args = [ \"git\" , \"remote\" , \"add\" , \"rootstock\" , rootstock_remote ] process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) if process . returncode == 0 : logging . info ( \"get_rootstock_commit added a `rootstock` remote to the git repository.\" ) # find most recent common ancestor commit try : args = [ \"git\" , \"fetch\" , \"rootstock\" , \"main\" ] subprocess . check_output ( args , stderr = subprocess . PIPE , universal_newlines = True ) args = [ \"git\" , \"merge-base\" , \"HEAD\" , \"rootstock/main\" ] output = subprocess . check_output ( args , stderr = subprocess . PIPE , universal_newlines = True ) except subprocess . CalledProcessError as error : logging . warning ( f \"get_rootstock_commit: { shlex_join ( error . cmd ) !r} returned exit code { error . returncode } \" f \"with the following stdout: \\n { error . stdout } \\n \" f \"And the following stderr: \\n { error . stderr } \" ) return None rootstock_commit = output . strip () return rootstock_commit","title":"Module manubot.process.metadata"},{"location":"reference/manubot/process/metadata/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/metadata/#get_header_includes","text":"def get_header_includes ( variables : dict ) -> str Render header-includes-template.html using information from variables . View Source def get_header_includes ( variables : dict ) -> str : \"\"\" Render `header-includes-template.html` using information from `variables`. \"\"\" from .util import template_with_jinja2 path = pathlib . Path ( __file__ ) . parent . joinpath ( \"header-includes-template.html\" ) try : template = path . read_text ( encoding = \"utf-8-sig\" ) return template_with_jinja2 ( template , variables ) except Exception : logging . exception ( f \"Error generating header-includes.\" ) return \"\"","title":"get_header_includes"},{"location":"reference/manubot/process/metadata/#get_manuscript_urls","text":"def get_manuscript_urls ( html_url : Optional [ str ] = None ) -> dict Return a dictionary with URLs for a manuscript. An example for a manuscript where all URLs get set, inferred from continuous integration environment variables, is: { \"html_url\" : \"https://manubot.github.io/rootstock/\" , \"pdf_url\" : \"https://manubot.github.io/rootstock/manuscript.pdf\" , \"html_url_versioned\" : \"https://manubot.github.io/rootstock/v/7cf9071212ce33116ad09cf2237a370b180a3c35/\" , \"pdf_url_versioned\" : \"https://manubot.github.io/rootstock/v/7cf9071212ce33116ad09cf2237a370b180a3c35/manuscript.pdf\" , } Provide html_url to set a custom domain. If html_url=\"https://git.dhimmel.com/bitcoin-whitepaper/\" , the return dictionary will be like: { \"html_url\" : \"https://git.dhimmel.com/bitcoin-whitepaper/\" , \"pdf_url\" : \"https://git.dhimmel.com/bitcoin-whitepaper/manuscript.pdf\" , \"html_url_versioned\" : \"https://git.dhimmel.com/bitcoin-whitepaper/v/cb1f2c12eec8b56db9ef5f641ec805e2d449d319/\" , \"pdf_url_versioned\" : \"https://git.dhimmel.com/bitcoin-whitepaper/v/cb1f2c12eec8b56db9ef5f641ec805e2d449d319/manuscript.pdf\" , } Note the trailing / in html_url , which is required for proper functioning. View Source def get_manuscript_urls ( html_url : Optional [ str ] = None ) -> dict : \"\"\" Return a dictionary with URLs for a manuscript. An example for a manuscript where all URLs get set, inferred from continuous integration environment variables, is: ```python { \"html_url\": \"https://manubot.github.io/rootstock/\", \"pdf_url\": \"https://manubot.github.io/rootstock/manuscript.pdf\", \"html_url_versioned\": \"https://manubot.github.io/rootstock/v/7cf9071212ce33116ad09cf2237a370b180a3c35/\", \"pdf_url_versioned\": \"https://manubot.github.io/rootstock/v/7cf9071212ce33116ad09cf2237a370b180a3c35/manuscript.pdf\", } ``` Provide `html_url` to set a custom domain. If `html_url=\"https://git.dhimmel.com/bitcoin-whitepaper/\"`, the return dictionary will be like: ```python { \"html_url\": \"https://git.dhimmel.com/bitcoin-whitepaper/\", \"pdf_url\": \"https://git.dhimmel.com/bitcoin-whitepaper/manuscript.pdf\", \"html_url_versioned\": \"https://git.dhimmel.com/bitcoin-whitepaper/v/cb1f2c12eec8b56db9ef5f641ec805e2d449d319/\", \"pdf_url_versioned\": \"https://git.dhimmel.com/bitcoin-whitepaper/v/cb1f2c12eec8b56db9ef5f641ec805e2d449d319/manuscript.pdf\", } ``` Note the trailing `/` in `html_url`, which is required for proper functioning. \"\"\" import requests from . ci import get_continuous_integration_parameters urls = dict () ci_params = get_continuous_integration_parameters () if html_url is None : if not ci_params : return urls html_url = \"https://{repo_owner}.github.io/{repo_name}/\" . format ( ** ci_params ) urls [ \"html_url\" ] = html_url urls [ \"pdf_url\" ] = urljoin ( html_url , \"manuscript.pdf\" ) if not ci_params : return urls urls [ \"html_url_versioned\" ] = urljoin ( html_url , \"v/{commit}/\" . format ( ** ci_params )) urls [ \"pdf_url_versioned\" ] = urljoin ( urls [ \"html_url_versioned\" ], \"manuscript.pdf\" ) response = requests . head ( html_url , allow_redirects = True ) if not response . ok : logging . warning ( \"html_url is not web accessible. \" f \"{html_url} returned status code {response.status_code}. \" \"Ignore this warning if the manuscript has not yet been deployed for the first time. \" ) if response . history : logging . info ( \"html_url includes redirects. In order of oldest to most recent: \\n \" + \" \\n \" . join ( x . url for x in response . history + [ response ]) ) return urls","title":"get_manuscript_urls"},{"location":"reference/manubot/process/metadata/#get_rootstock_commit","text":"def get_rootstock_commit ( ) -> Optional [ str ] Return the most recent commit in common between the git repository this function is run within (usually a Manubot manuscript repository) and the main branch of the rootstock remote. WARNING: This function may modify the git repository its executed within: if the repository has not set the roostock remote, it is set to point to the default Rootstock repository of https://github.com/manubot/rootstock . fetches the latest commits in the main branch of the rootstock remote View Source def get_rootstock_commit () -> Optional [ str ]: \"\"\" Return the most recent commit in common between the git repository this function is run within (usually a Manubot manuscript repository) and the `main` branch of the `rootstock` remote. WARNING: This function may modify the git repository its executed within: - if the repository has not set the `roostock` remote, it is set to point to the default Rootstock repository of <https://github.com/manubot/rootstock>. - fetches the latest commits in the `main` branch of the `rootstock` remote \"\"\" from manubot.util import shlex_join # add rootstock remote if remote is not already set rootstock_remote = \"https://github.com/manubot/rootstock.git\" args = [ \"git\" , \"remote\" , \"add\" , \"rootstock\" , rootstock_remote ] process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) if process . returncode == 0 : logging . info ( \"get_rootstock_commit added a `rootstock` remote to the git repository.\" ) # find most recent common ancestor commit try : args = [ \"git\" , \"fetch\" , \"rootstock\" , \"main\" ] subprocess . check_output ( args , stderr = subprocess . PIPE , universal_newlines = True ) args = [ \"git\" , \"merge-base\" , \"HEAD\" , \"rootstock/main\" ] output = subprocess . check_output ( args , stderr = subprocess . PIPE , universal_newlines = True ) except subprocess . CalledProcessError as error : logging . warning ( f \"get_rootstock_commit: { shlex_join ( error . cmd ) !r} returned exit code { error . returncode } \" f \"with the following stdout: \\n { error . stdout } \\n \" f \"And the following stderr: \\n { error . stderr } \" ) return None rootstock_commit = output . strip () return rootstock_commit","title":"get_rootstock_commit"},{"location":"reference/manubot/process/metadata/#get_software_versions","text":"def get_software_versions ( rootstock : bool = True ) -> dict Return a dictionary of software versions for softwares components: manubot_version: the semantic version number of the manubot python package. rootstock_commit: the version of the rootstock repository, as a commit hash, included in the manuscript repository. Values whose detection fails are set to None. The rootstock parameter controls whether to fetch the rootstock commit id. The rootstock git remote will be added to the local git repository if true. View Source def get_software_versions ( rootstock : bool = True ) -> dict : \"\"\" Return a dictionary of software versions for softwares components: - manubot_version: the semantic version number of the manubot python package. - rootstock_commit: the version of the rootstock repository, as a commit hash, included in the manuscript repository. Values whose detection fails are set to None. The `rootstock` parameter controls whether to fetch the rootstock commit id. The rootstock git remote will be added to the local git repository if true. \"\"\" from manubot import __version__ as manubot_version return { \"manubot_version\" : manubot_version , \"rootstock_commit\" : get_rootstock_commit () if rootstock else None , }","title":"get_software_versions"},{"location":"reference/manubot/process/metadata/#get_thumbnail_url","text":"def get_thumbnail_url ( thumbnail = None ) Starting with a user-specified thumbnail as either a path, URL, or None, return an absolute URL pointing to the thumbnail image. If the provided thumbnail is a URL, return this URL unmodified. If thumbnail is None, search for thumbnail.png within the git repository from which this function is executed. If thumbnail is a local path, the path should be relative to root directory of the git repository it is located in. If a local path is provided or detected, it is converted to a GitHub raw URL. View Source def get_thumbnail_url ( thumbnail = None ): \"\"\" Starting with a user-specified `thumbnail` as either a path, URL, or None, return an absolute URL pointing to the thumbnail image. If the provided `thumbnail` is a URL, return this URL unmodified. If `thumbnail` is None, search for `thumbnail.png` within the git repository from which this function is executed. If `thumbnail` is a local path, the path should be relative to root directory of the git repository it is located in. If a local path is provided or detected, it is converted to a GitHub raw URL. \"\"\" from manubot.util import is_http_url if not thumbnail : message = \"get_thumbnail_url: thumbnail location not explicitly provided. \" thumbnail = _find_thumbnail_path () message += ( f \"Thumbnail detected at { thumbnail !r} \" if thumbnail else \"No local thumbnail detected\" ) logging . debug ( message ) elif is_http_url ( thumbnail ): logging . debug ( \"provided thumbnail is a URL. Pass it through.\" ) return thumbnail return _thumbnail_path_to_url ( thumbnail )","title":"get_thumbnail_url"},{"location":"reference/manubot/process/metadata/#git_repository_root","text":"def git_repository_root ( ) Return the path to repository root directory or None if indeterminate. View Source @functools.lru_cache () def git_repository_root () : \" \"\" Return the path to repository root directory or `None` if indeterminate. \"\" \" for cmd in ( [ \"git\" , \"rev-parse\" , \"--show-superproject-working-tree\" ] , [ \"git\" , \"rev-parse\" , \"--show-toplevel\" ] , ) : try : path = subprocess . check_output ( cmd , universal_newlines = True ). rstrip ( \" \\r\\n \" ) if path : return pathlib . Path ( path ) except ( subprocess . CalledProcessError , OSError ) : pass return None","title":"git_repository_root"},{"location":"reference/manubot/process/process_command/","text":"Module manubot.process.process_command None None View Source import logging def cli_process ( args ): args_dict = vars ( args ) # Set paths for content content_dir = args . content_directory if not content_dir . is_dir (): logging . warning ( f \"content directory does not exist: { content_dir } \" ) args_dict [ \"citation_tags_path\" ] = content_dir . joinpath ( \"citation-tags.tsv\" ) args_dict [ \"meta_yaml_path\" ] = content_dir . joinpath ( \"metadata.yaml\" ) args_dict [ \"manual_references_paths\" ] = sorted ( content_dir . rglob ( \"manual-references*.*\" ) ) # Set paths for output output_dir = args . output_directory output_dir . mkdir ( parents = True , exist_ok = True ) args_dict [ \"manuscript_path\" ] = output_dir . joinpath ( \"manuscript.md\" ) args_dict [ \"citations_path\" ] = output_dir . joinpath ( \"citations.tsv\" ) args_dict [ \"references_path\" ] = output_dir . joinpath ( \"references.json\" ) args_dict [ \"variables_path\" ] = output_dir . joinpath ( \"variables.json\" ) # Set paths for caching args_dict [ \"cache_directory\" ] = args . cache_directory or output_dir args . cache_directory . mkdir ( parents = True , exist_ok = True ) args_dict [ \"requests_cache_path\" ] = str ( args . cache_directory . joinpath ( \"requests-cache\" ) ) from manubot.process.util import prepare_manuscript prepare_manuscript ( args ) Functions cli_process def cli_process ( args ) View Source def cli_process ( args ): args_dict = vars ( args ) # Set paths for content content_dir = args . content_directory if not content_dir . is_dir (): logging . warning ( f \"content directory does not exist: {content_dir}\" ) args_dict [ \"citation_tags_path\" ] = content_dir . joinpath ( \"citation-tags.tsv\" ) args_dict [ \"meta_yaml_path\" ] = content_dir . joinpath ( \"metadata.yaml\" ) args_dict [ \"manual_references_paths\" ] = sorted ( content_dir . rglob ( \"manual-references*.*\" ) ) # Set paths for output output_dir = args . output_directory output_dir . mkdir ( parents = True , exist_ok = True ) args_dict [ \"manuscript_path\" ] = output_dir . joinpath ( \"manuscript.md\" ) args_dict [ \"citations_path\" ] = output_dir . joinpath ( \"citations.tsv\" ) args_dict [ \"references_path\" ] = output_dir . joinpath ( \"references.json\" ) args_dict [ \"variables_path\" ] = output_dir . joinpath ( \"variables.json\" ) # Set paths for caching args_dict [ \"cache_directory\" ] = args . cache_directory or output_dir args . cache_directory . mkdir ( parents = True , exist_ok = True ) args_dict [ \"requests_cache_path\" ] = str ( args . cache_directory . joinpath ( \"requests-cache\" ) ) from manubot . process . util import prepare_manuscript prepare_manuscript ( args )","title":"Process Command"},{"location":"reference/manubot/process/process_command/#module-manubotprocessprocess_command","text":"None None View Source import logging def cli_process ( args ): args_dict = vars ( args ) # Set paths for content content_dir = args . content_directory if not content_dir . is_dir (): logging . warning ( f \"content directory does not exist: { content_dir } \" ) args_dict [ \"citation_tags_path\" ] = content_dir . joinpath ( \"citation-tags.tsv\" ) args_dict [ \"meta_yaml_path\" ] = content_dir . joinpath ( \"metadata.yaml\" ) args_dict [ \"manual_references_paths\" ] = sorted ( content_dir . rglob ( \"manual-references*.*\" ) ) # Set paths for output output_dir = args . output_directory output_dir . mkdir ( parents = True , exist_ok = True ) args_dict [ \"manuscript_path\" ] = output_dir . joinpath ( \"manuscript.md\" ) args_dict [ \"citations_path\" ] = output_dir . joinpath ( \"citations.tsv\" ) args_dict [ \"references_path\" ] = output_dir . joinpath ( \"references.json\" ) args_dict [ \"variables_path\" ] = output_dir . joinpath ( \"variables.json\" ) # Set paths for caching args_dict [ \"cache_directory\" ] = args . cache_directory or output_dir args . cache_directory . mkdir ( parents = True , exist_ok = True ) args_dict [ \"requests_cache_path\" ] = str ( args . cache_directory . joinpath ( \"requests-cache\" ) ) from manubot.process.util import prepare_manuscript prepare_manuscript ( args )","title":"Module manubot.process.process_command"},{"location":"reference/manubot/process/process_command/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/process_command/#cli_process","text":"def cli_process ( args ) View Source def cli_process ( args ): args_dict = vars ( args ) # Set paths for content content_dir = args . content_directory if not content_dir . is_dir (): logging . warning ( f \"content directory does not exist: {content_dir}\" ) args_dict [ \"citation_tags_path\" ] = content_dir . joinpath ( \"citation-tags.tsv\" ) args_dict [ \"meta_yaml_path\" ] = content_dir . joinpath ( \"metadata.yaml\" ) args_dict [ \"manual_references_paths\" ] = sorted ( content_dir . rglob ( \"manual-references*.*\" ) ) # Set paths for output output_dir = args . output_directory output_dir . mkdir ( parents = True , exist_ok = True ) args_dict [ \"manuscript_path\" ] = output_dir . joinpath ( \"manuscript.md\" ) args_dict [ \"citations_path\" ] = output_dir . joinpath ( \"citations.tsv\" ) args_dict [ \"references_path\" ] = output_dir . joinpath ( \"references.json\" ) args_dict [ \"variables_path\" ] = output_dir . joinpath ( \"variables.json\" ) # Set paths for caching args_dict [ \"cache_directory\" ] = args . cache_directory or output_dir args . cache_directory . mkdir ( parents = True , exist_ok = True ) args_dict [ \"requests_cache_path\" ] = str ( args . cache_directory . joinpath ( \"requests-cache\" ) ) from manubot . process . util import prepare_manuscript prepare_manuscript ( args )","title":"cli_process"},{"location":"reference/manubot/process/requests_cache/","text":"Module manubot.process.requests_cache None None View Source import logging import os import pathlib import requests import requests_cache class RequestsCache : def __init__ ( self , path ): self . path = os . fspath ( path ) def mkdir ( self ): \"\"\"make directory containing cache file if it doesn't exist\"\"\" directory = pathlib . Path ( self . path ) . parent directory . mkdir ( parents = True , exist_ok = True ) def install ( self ): \"\"\"install cache\"\"\" requests # require `import requests` in case this is essential for monkey patching by requests_cache. requests_cache . install_cache ( self . path , include_get_headers = True ) self . cache = requests_cache . get_cache () logging . info ( f \"requests-cache starting with { len ( self . cache . responses ) } cached responses\" ) def clear ( self ): \"\"\"clear cache\"\"\" logging . info ( \"Clearing requests-cache\" ) requests_cache . clear () def close ( self ): \"\"\"uninstall cache\"\"\" logging . info ( f \"requests-cache finished with { len ( self . cache . responses ) } cached responses\" ) requests_cache . uninstall_cache () Classes RequestsCache class RequestsCache ( path ) View Source class RequestsCache : def __init__ ( self , path ): self . path = os . fspath ( path ) def mkdir ( self ): \"\"\"make directory containing cache file if it doesn't exist\"\"\" directory = pathlib . Path ( self . path ) . parent directory . mkdir ( parents = True , exist_ok = True ) def install ( self ): \"\"\"install cache\"\"\" requests # require `import requests` in case this is essential for monkey patching by requests_cache. requests_cache . install_cache ( self . path , include_get_headers = True ) self . cache = requests_cache . get_cache () logging . info ( f \"requests-cache starting with { len ( self . cache . responses ) } cached responses\" ) def clear ( self ): \"\"\"clear cache\"\"\" logging . info ( \"Clearing requests-cache\" ) requests_cache . clear () def close ( self ): \"\"\"uninstall cache\"\"\" logging . info ( f \"requests-cache finished with { len ( self . cache . responses ) } cached responses\" ) requests_cache . uninstall_cache () Methods clear def clear ( self ) clear cache View Source def clear(self): \"\"\"clear cache\"\"\" logging.info(\"Clearing requests-cache\") requests_cache.clear() close def close ( self ) uninstall cache View Source def close(self): \"\"\"uninstall cache\"\"\" logging.info( f\"requests-cache finished with {len(self.cache.responses)} cached responses\" ) requests_cache.uninstall_cache() install def install ( self ) install cache View Source def install ( self ): \"\"\"install cache\"\"\" requests # require `import requests` in case this is essential for monkey patching by requests_cache. requests_cache . install_cache ( self . path , include_get_headers = True ) self . cache = requests_cache . get_cache () logging . info ( f \"requests-cache starting with { len ( self . cache . responses ) } cached responses\" ) mkdir def mkdir ( self ) make directory containing cache file if it doesn't exist View Source def mkdir ( self ) : \"\"\" make directory containing cache file if it doesn't exist \"\"\" directory = pathlib . Path ( self . path ) . parent directory . mkdir ( parents = True , exist_ok = True )","title":"Requests Cache"},{"location":"reference/manubot/process/requests_cache/#module-manubotprocessrequests_cache","text":"None None View Source import logging import os import pathlib import requests import requests_cache class RequestsCache : def __init__ ( self , path ): self . path = os . fspath ( path ) def mkdir ( self ): \"\"\"make directory containing cache file if it doesn't exist\"\"\" directory = pathlib . Path ( self . path ) . parent directory . mkdir ( parents = True , exist_ok = True ) def install ( self ): \"\"\"install cache\"\"\" requests # require `import requests` in case this is essential for monkey patching by requests_cache. requests_cache . install_cache ( self . path , include_get_headers = True ) self . cache = requests_cache . get_cache () logging . info ( f \"requests-cache starting with { len ( self . cache . responses ) } cached responses\" ) def clear ( self ): \"\"\"clear cache\"\"\" logging . info ( \"Clearing requests-cache\" ) requests_cache . clear () def close ( self ): \"\"\"uninstall cache\"\"\" logging . info ( f \"requests-cache finished with { len ( self . cache . responses ) } cached responses\" ) requests_cache . uninstall_cache ()","title":"Module manubot.process.requests_cache"},{"location":"reference/manubot/process/requests_cache/#classes","text":"","title":"Classes"},{"location":"reference/manubot/process/requests_cache/#requestscache","text":"class RequestsCache ( path ) View Source class RequestsCache : def __init__ ( self , path ): self . path = os . fspath ( path ) def mkdir ( self ): \"\"\"make directory containing cache file if it doesn't exist\"\"\" directory = pathlib . Path ( self . path ) . parent directory . mkdir ( parents = True , exist_ok = True ) def install ( self ): \"\"\"install cache\"\"\" requests # require `import requests` in case this is essential for monkey patching by requests_cache. requests_cache . install_cache ( self . path , include_get_headers = True ) self . cache = requests_cache . get_cache () logging . info ( f \"requests-cache starting with { len ( self . cache . responses ) } cached responses\" ) def clear ( self ): \"\"\"clear cache\"\"\" logging . info ( \"Clearing requests-cache\" ) requests_cache . clear () def close ( self ): \"\"\"uninstall cache\"\"\" logging . info ( f \"requests-cache finished with { len ( self . cache . responses ) } cached responses\" ) requests_cache . uninstall_cache ()","title":"RequestsCache"},{"location":"reference/manubot/process/requests_cache/#methods","text":"","title":"Methods"},{"location":"reference/manubot/process/requests_cache/#clear","text":"def clear ( self ) clear cache View Source def clear(self): \"\"\"clear cache\"\"\" logging.info(\"Clearing requests-cache\") requests_cache.clear()","title":"clear"},{"location":"reference/manubot/process/requests_cache/#close","text":"def close ( self ) uninstall cache View Source def close(self): \"\"\"uninstall cache\"\"\" logging.info( f\"requests-cache finished with {len(self.cache.responses)} cached responses\" ) requests_cache.uninstall_cache()","title":"close"},{"location":"reference/manubot/process/requests_cache/#install","text":"def install ( self ) install cache View Source def install ( self ): \"\"\"install cache\"\"\" requests # require `import requests` in case this is essential for monkey patching by requests_cache. requests_cache . install_cache ( self . path , include_get_headers = True ) self . cache = requests_cache . get_cache () logging . info ( f \"requests-cache starting with { len ( self . cache . responses ) } cached responses\" )","title":"install"},{"location":"reference/manubot/process/requests_cache/#mkdir","text":"def mkdir ( self ) make directory containing cache file if it doesn't exist View Source def mkdir ( self ) : \"\"\" make directory containing cache file if it doesn't exist \"\"\" directory = pathlib . Path ( self . path ) . parent directory . mkdir ( parents = True , exist_ok = True )","title":"mkdir"},{"location":"reference/manubot/process/util/","text":"Module manubot.process.util None None View Source import json import logging import os import re import warnings from typing import List , Optional import jinja2 from manubot.process.ci import get_continuous_integration_parameters from manubot.process.manuscript import datetime_now , get_manuscript_stats , get_text from manubot.process.metadata import ( get_header_includes , get_manuscript_urls , get_software_versions , get_thumbnail_url , ) from manubot.util import get_configured_yaml , read_serialized_data , read_serialized_dict def read_variable_files ( paths : List [ str ], variables : Optional [ dict ] = None ) -> dict : \"\"\" Read multiple serialized data files into a user_variables dictionary. Provide `paths` (a list of URLs or local file paths). Paths can optionally have a namespace prepended. For example: ```python paths = [ 'https://git.io/vbkqm', # update the dictionary's top-level 'namespace_1=https://git.io/vbkqm', # store under 'namespace_1' key 'namespace_2=some_local_path.json', # store under 'namespace_2' key ] ``` If a namespace is not provided, the JSON must contain a dictionary as its top level. Namespaces should consist only of ASCII alphanumeric characters (includes underscores, first character cannot be numeric). Pass a dictionary to `variables` to update an existing dictionary rather than create a new dictionary. \"\"\" if variables is None : variables = {} for path in paths : logging . info ( f \"Reading user-provided templating variables at { path !r} \" ) # Match only namespaces that are valid jinja2 variable names # http://jinja.pocoo.org/docs/2.10/api/#identifier-naming match = re . match ( r \"([a-zA-Z_][a-zA-Z0-9_]*)=(.+)\" , path ) if match : namespace , path = match . groups () logging . info ( f \"Using the { namespace !r} namespace for template variables from { path !r} \" ) try : if match : obj = { namespace : read_serialized_data ( path )} else : obj = read_serialized_dict ( path ) except Exception : logging . exception ( f \"Error reading template variables from { path !r} \" ) continue assert isinstance ( obj , dict ) conflicts = variables . keys () & obj . keys () if conflicts : logging . warning ( f \"Template variables in { path !r} overwrite existing \" \"values for the following keys: \\n \" + \" \\n \" . join ( conflicts ) ) variables . update ( obj ) logging . debug ( f \"Reading user-provided templating variables complete: \\n \" f \" { json . dumps ( variables , indent = 2 , ensure_ascii = False ) } \" ) return variables def _convert_field_to_list ( dictionary , field , separator = False , deprecation_warning_key = None ): \"\"\" Convert `dictionary[field]` to a list. If value is a string and `separator` is specified, split by `separator`. If `deprecation_warning_key` is provided, warn when `dictionary[field]` is a string. \"\"\" if field not in dictionary : return dictionary value = dictionary [ field ] if isinstance ( value , list ): return dictionary if isinstance ( value , str ): if separator is False : dictionary [ field ] = [ value ] else : dictionary [ field ] = value . split ( separator ) if deprecation_warning_key : warnings . warn ( f \"Expected list for { dictionary . get ( deprecation_warning_key ) } 's { field } . \" + ( f \"Assuming multiple { field } are ` { separator } ` separated. \" if separator else \"\" ) + f \"Please switch { field } to a list.\" , category = DeprecationWarning , ) return dictionary raise ValueError ( \"Unsupported value type {value.__class__.__name__} \" ) def add_author_affiliations ( variables : dict ) -> dict : \"\"\" Edit variables to contain numbered author affiliations. Specifically, add a list of affiliation_numbers for each author and add a list of affiliations to the top-level of variables. If no authors have any affiliations, variables is left unmodified. \"\"\" affiliations = list () for author in variables [ \"authors\" ]: _convert_field_to_list ( dictionary = author , field = \"affiliations\" , separator = \"; \" , deprecation_warning_key = \"name\" , ) _convert_field_to_list ( dictionary = author , field = \"funders\" , ) affiliations . extend ( author . get ( \"affiliations\" , [])) if not affiliations : return variables affiliations = list ( dict . fromkeys ( affiliations )) # deduplicate affil_to_number = { affil : i for i , affil in enumerate ( affiliations , start = 1 )} for author in variables [ \"authors\" ]: numbers = [ affil_to_number [ affil ] for affil in author . get ( \"affiliations\" , [])] author [ \"affiliation_numbers\" ] = sorted ( numbers ) variables [ \"affiliations\" ] = [ dict ( affiliation = affil , affiliation_number = i ) for affil , i in affil_to_number . items () ] return variables def load_variables ( args ) -> dict : \"\"\" Read `metadata.yaml` and files specified by `--template-variables-path` to generate manuscript variables available for jinja2 templating. Returns a dictionary, refered to as `variables`, with the following keys: - `pandoc`: a dictionary for passing options to Pandoc via the `yaml_metadata_block`. Fields in `pandoc` are either generated by Manubot or hard-coded by the user if `metadata.yaml` includes a `pandoc` dictionary. - `manubot`: a dictionary for manubot-related information and metadata. Fields in `manubot` are either generated by Manubot or hard-coded by the user if `metadata.yaml` includes a `manubot` dictionary. - All fields from a manuscript's `metadata.yaml` that are not interpreted by Manubot are copied to `variables`. Interpreted fields include `pandoc`, `manubot`, `title`, `keywords`, `authors` (formerly `author_info`, now deprecated), `lang`, and `thumbnail`. - User-specified fields inserted according to the `--template-variables-path` option. User-specified variables take highest precedence and can overwrite values for existing keys like `pandoc` or `manubot` (dangerous). \"\"\" # Generated manuscript variables variables = { \"pandoc\" : {}, \"manubot\" : {}} # Read metadata which contains pandoc_yaml_metadata # as well as authors information. if args . meta_yaml_path . is_file (): metadata = read_serialized_dict ( args . meta_yaml_path ) else : metadata = {} logging . warning ( f \"missing { args . meta_yaml_path } file with yaml_metadata_block for pandoc\" ) # Interpreted keys that are intended for pandoc move_to_pandoc = \"title\" , \"keywords\" , \"lang\" for key in move_to_pandoc : if key in metadata : variables [ \"pandoc\" ][ key ] = metadata . pop ( key ) # Add date to metadata now = datetime_now () logging . info ( f \"Using { now : %Z } timezone. \\n \" f \"Dating manuscript with the current datetime: { now . isoformat () } \" ) variables [ \"pandoc\" ][ \"date-meta\" ] = now . date () . isoformat () variables [ \"manubot\" ][ \"date\" ] = f \" { now : %B } { now . day } , { now . year } \" # Process authors metadata if \"author_info\" in metadata : authors = metadata . pop ( \"author_info\" , []) warnings . warn ( \"metadata.yaml: 'author_info' is deprecated. Use 'authors' instead.\" , category = DeprecationWarning , ) else : authors = metadata . pop ( \"authors\" , []) if authors is None : authors = [] variables [ \"pandoc\" ][ \"author-meta\" ] = [ author [ \"name\" ] for author in authors ] variables [ \"manubot\" ][ \"authors\" ] = authors add_author_affiliations ( variables [ \"manubot\" ]) # Set repository version metadata for CI builds ci_params = get_continuous_integration_parameters () if ci_params : variables [ \"manubot\" ][ \"ci_source\" ] = ci_params # Add manuscript URLs variables [ \"manubot\" ] . update ( get_manuscript_urls ( metadata . pop ( \"html_url\" , None ))) # Add software versions variables [ \"manubot\" ] . update ( get_software_versions ( rootstock = not args . skip_remote )) # Add thumbnail URL if present thumbnail_url = get_thumbnail_url ( metadata . pop ( \"thumbnail\" , None )) if thumbnail_url : variables [ \"manubot\" ][ \"thumbnail_url\" ] = thumbnail_url # Update variables with metadata.yaml pandoc/manubot dicts for key in \"pandoc\" , \"manubot\" : dict_ = metadata . pop ( key , {}) if not isinstance ( dict_ , dict ): logging . warning ( f \"load_variables expected metadata.yaml field { key !r} to be a dict.\" f \"Received a { dict_ . __class__ . __name__ !r} instead.\" ) continue variables [ key ] . update ( dict_ ) # Update variables with uninterpreted metadata.yaml fields variables . update ( metadata ) # Update variables with user-provided variables here variables = read_variable_files ( args . template_variables_path , variables ) # Add header-includes metadata with <meta> information for the HTML output's <head> variables [ \"pandoc\" ][ \"header-includes\" ] = get_header_includes ( variables ) assert args . skip_citations # Extend Pandoc's metadata.bibliography field with manual references paths bibliographies = variables [ \"pandoc\" ] . get ( \"bibliography\" , []) if isinstance ( bibliographies , str ): bibliographies = [ bibliographies ] assert isinstance ( bibliographies , list ) bibliographies . extend ( args . manual_references_paths ) bibliographies = list ( map ( os . fspath , bibliographies )) variables [ \"pandoc\" ][ \"bibliography\" ] = bibliographies # enable pandoc-manubot-cite option to write bibliography to a file variables [ \"pandoc\" ][ \"manubot-output-bibliography\" ] = os . fspath ( args . references_path ) variables [ \"pandoc\" ][ \"manubot-output-citekeys\" ] = os . fspath ( args . citations_path ) variables [ \"pandoc\" ][ \"manubot-requests-cache-path\" ] = os . fspath ( args . requests_cache_path ) variables [ \"pandoc\" ][ \"manubot-clear-requests-cache\" ] = args . clear_requests_cache return variables def template_with_jinja2 ( text , variables ): \"\"\" Template using jinja2 with the variables dictionary unpacked as keyword arguments. \"\"\" jinja_environment = jinja2 . Environment ( loader = jinja2 . BaseLoader (), undefined = jinja2 . make_logging_undefined ( logging . getLogger ()), autoescape = False , comment_start_string = \"{##\" , comment_end_string = \"##}\" , extensions = [ \"jinja2.ext.do\" , \"jinja2.ext.loopcontrols\" ], ) template = jinja_environment . from_string ( text ) return template . render ( ** variables ) def prepare_manuscript ( args ): \"\"\" Compile manuscript, creating manuscript.md and references.json as inputs for pandoc. \"\"\" text = get_text ( args . content_directory ) assert args . skip_citations variables = load_variables ( args ) variables [ \"manubot\" ][ \"manuscript_stats\" ] = get_manuscript_stats ( text ) with args . variables_path . open ( \"w\" , encoding = \"utf-8\" ) as write_file : json . dump ( variables , write_file , ensure_ascii = False , indent = 2 ) write_file . write ( \" \\n \" ) text = template_with_jinja2 ( text , variables ) # Write manuscript for pandoc yaml = get_configured_yaml () with args . manuscript_path . open ( \"w\" , encoding = \"utf-8\" ) as write_file : yaml . dump ( variables [ \"pandoc\" ], write_file , default_flow_style = False , explicit_start = True , explicit_end = True , width = float ( \"inf\" ), allow_unicode = True , sort_keys = False , ) write_file . write ( \" \\n \" ) write_file . write ( text ) Functions add_author_affiliations def add_author_affiliations ( variables : dict ) -> dict Edit variables to contain numbered author affiliations. Specifically, add a list of affiliation_numbers for each author and add a list of affiliations to the top-level of variables. If no authors have any affiliations, variables is left unmodified. View Source def add_author_affiliations ( variables : dict ) -> dict : \"\"\" Edit variables to contain numbered author affiliations. Specifically, add a list of affiliation_numbers for each author and add a list of affiliations to the top-level of variables. If no authors have any affiliations, variables is left unmodified. \"\"\" affiliations = list () for author in variables [ \"authors\" ] : _convert_field_to_list ( dictionary = author , field = \"affiliations\" , separator = \"; \" , deprecation_warning_key = \"name\" , ) _convert_field_to_list ( dictionary = author , field = \"funders\" , ) affiliations . extend ( author . get ( \"affiliations\" , [] )) if not affiliations : return variables affiliations = list ( dict . fromkeys ( affiliations )) # deduplicate affil_to_number = { affil : i for i , affil in enumerate ( affiliations , start = 1 ) } for author in variables [ \"authors\" ] : numbers = [ affil_to_number[affil ] for affil in author . get ( \"affiliations\" , [] ) ] author [ \"affiliation_numbers\" ] = sorted ( numbers ) variables [ \"affiliations\" ] = [ dict(affiliation=affil, affiliation_number=i) for affil, i in affil_to_number.items() ] return variables load_variables def load_variables ( args ) -> dict Read metadata.yaml and files specified by --template-variables-path to generate manuscript variables available for jinja2 templating. Returns a dictionary, refered to as variables , with the following keys: pandoc : a dictionary for passing options to Pandoc via the yaml_metadata_block . Fields in pandoc are either generated by Manubot or hard-coded by the user if metadata.yaml includes a pandoc dictionary. manubot : a dictionary for manubot-related information and metadata. Fields in manubot are either generated by Manubot or hard-coded by the user if metadata.yaml includes a manubot dictionary. All fields from a manuscript's metadata.yaml that are not interpreted by Manubot are copied to variables . Interpreted fields include pandoc , manubot , title , keywords , authors (formerly author_info , now deprecated), lang , and thumbnail . User-specified fields inserted according to the --template-variables-path option. User-specified variables take highest precedence and can overwrite values for existing keys like pandoc or manubot (dangerous). View Source def load_variables ( args ) -> dict : \"\"\" Read `metadata.yaml` and files specified by `--template-variables-path` to generate manuscript variables available for jinja2 templating. Returns a dictionary, refered to as `variables`, with the following keys: - `pandoc`: a dictionary for passing options to Pandoc via the `yaml_metadata_block`. Fields in `pandoc` are either generated by Manubot or hard-coded by the user if `metadata.yaml` includes a `pandoc` dictionary. - `manubot`: a dictionary for manubot-related information and metadata. Fields in `manubot` are either generated by Manubot or hard-coded by the user if `metadata.yaml` includes a `manubot` dictionary. - All fields from a manuscript's `metadata.yaml` that are not interpreted by Manubot are copied to `variables`. Interpreted fields include `pandoc`, `manubot`, `title`, `keywords`, `authors` (formerly `author_info`, now deprecated), `lang`, and `thumbnail`. - User-specified fields inserted according to the `--template-variables-path` option. User-specified variables take highest precedence and can overwrite values for existing keys like `pandoc` or `manubot` (dangerous). \"\"\" # Generated manuscript variables variables = { \"pandoc\" : {}, \"manubot\" : {}} # Read metadata which contains pandoc_yaml_metadata # as well as authors information. if args . meta_yaml_path . is_file (): metadata = read_serialized_dict ( args . meta_yaml_path ) else : metadata = {} logging . warning ( f \"missing {args.meta_yaml_path} file with yaml_metadata_block for pandoc\" ) # Interpreted keys that are intended for pandoc move_to_pandoc = \"title\" , \"keywords\" , \"lang\" for key in move_to_pandoc : if key in metadata : variables [ \"pandoc\" ][ key ] = metadata . pop ( key ) # Add date to metadata now = datetime_now () logging . info ( f \"Using {now:%Z} timezone. \\n \" f \"Dating manuscript with the current datetime: {now.isoformat()}\" ) variables [ \"pandoc\" ][ \"date-meta\" ] = now . date () . isoformat () variables [ \"manubot\" ][ \"date\" ] = f \"{now:%B} {now.day}, {now.year}\" # Process authors metadata if \"author_info\" in metadata : authors = metadata . pop ( \"author_info\" , []) warnings . warn ( \"metadata.yaml: 'author_info' is deprecated. Use 'authors' instead.\" , category = DeprecationWarning , ) else : authors = metadata . pop ( \"authors\" , []) if authors is None : authors = [] variables [ \"pandoc\" ][ \"author-meta\" ] = [ author [ \"name\" ] for author in authors ] variables [ \"manubot\" ][ \"authors\" ] = authors add_author_affiliations ( variables [ \"manubot\" ]) # Set repository version metadata for CI builds ci_params = get_continuous_integration_parameters () if ci_params : variables [ \"manubot\" ][ \"ci_source\" ] = ci_params # Add manuscript URLs variables [ \"manubot\" ] . update ( get_manuscript_urls ( metadata . pop ( \"html_url\" , None ))) # Add software versions variables [ \"manubot\" ] . update ( get_software_versions ( rootstock = not args . skip_remote )) # Add thumbnail URL if present thumbnail_url = get_thumbnail_url ( metadata . pop ( \"thumbnail\" , None )) if thumbnail_url : variables [ \"manubot\" ][ \"thumbnail_url\" ] = thumbnail_url # Update variables with metadata.yaml pandoc/manubot dicts for key in \"pandoc\" , \"manubot\" : dict_ = metadata . pop ( key , {}) if not isinstance ( dict_ , dict ): logging . warning ( f \"load_variables expected metadata.yaml field {key!r} to be a dict.\" f \"Received a {dict_.__class__.__name__!r} instead.\" ) continue variables [ key ] . update ( dict_ ) # Update variables with uninterpreted metadata.yaml fields variables . update ( metadata ) # Update variables with user-provided variables here variables = read_variable_files ( args . template_variables_path , variables ) # Add header-includes metadata with <meta> information for the HTML output's <head> variables [ \"pandoc\" ][ \"header-includes\" ] = get_header_includes ( variables ) assert args . skip_citations # Extend Pandoc's metadata.bibliography field with manual references paths bibliographies = variables [ \"pandoc\" ] . get ( \"bibliography\" , []) if isinstance ( bibliographies , str ): bibliographies = [ bibliographies ] assert isinstance ( bibliographies , list ) bibliographies . extend ( args . manual_references_paths ) bibliographies = list ( map ( os . fspath , bibliographies )) variables [ \"pandoc\" ][ \"bibliography\" ] = bibliographies # enable pandoc-manubot-cite option to write bibliography to a file variables [ \"pandoc\" ][ \"manubot-output-bibliography\" ] = os . fspath ( args . references_path ) variables [ \"pandoc\" ][ \"manubot-output-citekeys\" ] = os . fspath ( args . citations_path ) variables [ \"pandoc\" ][ \"manubot-requests-cache-path\" ] = os . fspath ( args . requests_cache_path ) variables [ \"pandoc\" ][ \"manubot-clear-requests-cache\" ] = args . clear_requests_cache return variables prepare_manuscript def prepare_manuscript ( args ) Compile manuscript, creating manuscript.md and references.json as inputs for pandoc. View Source def prepare_manuscript ( args ): \"\"\" Compile manuscript, creating manuscript.md and references.json as inputs for pandoc. \"\"\" text = get_text ( args . content_directory ) assert args . skip_citations variables = load_variables ( args ) variables [ \"manubot\" ][ \"manuscript_stats\" ] = get_manuscript_stats ( text ) with args . variables_path . open ( \"w\" , encoding = \"utf-8\" ) as write_file : json . dump ( variables , write_file , ensure_ascii = False , indent = 2 ) write_file . write ( \" \\n \" ) text = template_with_jinja2 ( text , variables ) # Write manuscript for pandoc yaml = get_configured_yaml () with args . manuscript_path . open ( \"w\" , encoding = \"utf-8\" ) as write_file : yaml . dump ( variables [ \"pandoc\" ], write_file , default_flow_style = False , explicit_start = True , explicit_end = True , width = float ( \"inf\" ), allow_unicode = True , sort_keys = False , ) write_file . write ( \" \\n \" ) write_file . write ( text ) read_variable_files def read_variable_files ( paths : List [ str ], variables : Optional [ dict ] = None ) -> dict Read multiple serialized data files into a user_variables dictionary. Provide paths (a list of URLs or local file paths). Paths can optionally have a namespace prepended. For example: paths = [ 'https://git.io/vbkqm' , # update the dictionary's top-level 'namespace_1=https://git.io/vbkqm' , # store under 'namespace_1' key 'namespace_2=some_local_path.json' , # store under 'namespace_2' key ] If a namespace is not provided, the JSON must contain a dictionary as its top level. Namespaces should consist only of ASCII alphanumeric characters (includes underscores, first character cannot be numeric). Pass a dictionary to variables to update an existing dictionary rather than create a new dictionary. View Source def read_variable_files ( paths : List [ str ], variables : Optional [ dict ] = None ) -> dict : \"\"\" Read multiple serialized data files into a user_variables dictionary. Provide `paths` (a list of URLs or local file paths). Paths can optionally have a namespace prepended. For example: ```python paths = [ 'https://git.io/vbkqm', # update the dictionary's top-level 'namespace_1=https://git.io/vbkqm', # store under 'namespace_1' key 'namespace_2=some_local_path.json', # store under 'namespace_2' key ] ``` If a namespace is not provided, the JSON must contain a dictionary as its top level. Namespaces should consist only of ASCII alphanumeric characters (includes underscores, first character cannot be numeric). Pass a dictionary to `variables` to update an existing dictionary rather than create a new dictionary. \"\"\" if variables is None : variables = {} for path in paths : logging . info ( f \"Reading user-provided templating variables at {path!r}\" ) # Match only namespaces that are valid jinja2 variable names # http://jinja.pocoo.org/docs/2.10/api/#identifier-naming match = re . match ( r \"([a-zA-Z_][a-zA-Z0-9_]*)=(.+)\" , path ) if match : namespace , path = match . groups () logging . info ( f \"Using the {namespace!r} namespace for template variables from {path!r}\" ) try : if match : obj = { namespace : read_serialized_data ( path )} else : obj = read_serialized_dict ( path ) except Exception : logging . exception ( f \"Error reading template variables from {path!r}\" ) continue assert isinstance ( obj , dict ) conflicts = variables . keys () & obj . keys () if conflicts : logging . warning ( f \"Template variables in {path!r} overwrite existing \" \"values for the following keys: \\n \" + \" \\n \" . join ( conflicts ) ) variables . update ( obj ) logging . debug ( f \"Reading user-provided templating variables complete: \\n \" f \"{json.dumps(variables, indent=2, ensure_ascii=False)}\" ) return variables template_with_jinja2 def template_with_jinja2 ( text , variables ) Template using jinja2 with the variables dictionary unpacked as keyword arguments. View Source def template_with_jinja2 ( text , variables ): \"\"\" Template using jinja2 with the variables dictionary unpacked as keyword arguments. \"\"\" jinja_environment = jinja2 . Environment ( loader = jinja2 . BaseLoader (), undefined = jinja2 . make_logging_undefined ( logging . getLogger ()), autoescape = False , comment_start_string = \"{##\" , comment_end_string = \"##}\" , extensions = [ \"jinja2.ext.do\" , \"jinja2.ext.loopcontrols\" ], ) template = jinja_environment . from_string ( text ) return template . render ( ** variables )","title":"Util"},{"location":"reference/manubot/process/util/#module-manubotprocessutil","text":"None None View Source import json import logging import os import re import warnings from typing import List , Optional import jinja2 from manubot.process.ci import get_continuous_integration_parameters from manubot.process.manuscript import datetime_now , get_manuscript_stats , get_text from manubot.process.metadata import ( get_header_includes , get_manuscript_urls , get_software_versions , get_thumbnail_url , ) from manubot.util import get_configured_yaml , read_serialized_data , read_serialized_dict def read_variable_files ( paths : List [ str ], variables : Optional [ dict ] = None ) -> dict : \"\"\" Read multiple serialized data files into a user_variables dictionary. Provide `paths` (a list of URLs or local file paths). Paths can optionally have a namespace prepended. For example: ```python paths = [ 'https://git.io/vbkqm', # update the dictionary's top-level 'namespace_1=https://git.io/vbkqm', # store under 'namespace_1' key 'namespace_2=some_local_path.json', # store under 'namespace_2' key ] ``` If a namespace is not provided, the JSON must contain a dictionary as its top level. Namespaces should consist only of ASCII alphanumeric characters (includes underscores, first character cannot be numeric). Pass a dictionary to `variables` to update an existing dictionary rather than create a new dictionary. \"\"\" if variables is None : variables = {} for path in paths : logging . info ( f \"Reading user-provided templating variables at { path !r} \" ) # Match only namespaces that are valid jinja2 variable names # http://jinja.pocoo.org/docs/2.10/api/#identifier-naming match = re . match ( r \"([a-zA-Z_][a-zA-Z0-9_]*)=(.+)\" , path ) if match : namespace , path = match . groups () logging . info ( f \"Using the { namespace !r} namespace for template variables from { path !r} \" ) try : if match : obj = { namespace : read_serialized_data ( path )} else : obj = read_serialized_dict ( path ) except Exception : logging . exception ( f \"Error reading template variables from { path !r} \" ) continue assert isinstance ( obj , dict ) conflicts = variables . keys () & obj . keys () if conflicts : logging . warning ( f \"Template variables in { path !r} overwrite existing \" \"values for the following keys: \\n \" + \" \\n \" . join ( conflicts ) ) variables . update ( obj ) logging . debug ( f \"Reading user-provided templating variables complete: \\n \" f \" { json . dumps ( variables , indent = 2 , ensure_ascii = False ) } \" ) return variables def _convert_field_to_list ( dictionary , field , separator = False , deprecation_warning_key = None ): \"\"\" Convert `dictionary[field]` to a list. If value is a string and `separator` is specified, split by `separator`. If `deprecation_warning_key` is provided, warn when `dictionary[field]` is a string. \"\"\" if field not in dictionary : return dictionary value = dictionary [ field ] if isinstance ( value , list ): return dictionary if isinstance ( value , str ): if separator is False : dictionary [ field ] = [ value ] else : dictionary [ field ] = value . split ( separator ) if deprecation_warning_key : warnings . warn ( f \"Expected list for { dictionary . get ( deprecation_warning_key ) } 's { field } . \" + ( f \"Assuming multiple { field } are ` { separator } ` separated. \" if separator else \"\" ) + f \"Please switch { field } to a list.\" , category = DeprecationWarning , ) return dictionary raise ValueError ( \"Unsupported value type {value.__class__.__name__} \" ) def add_author_affiliations ( variables : dict ) -> dict : \"\"\" Edit variables to contain numbered author affiliations. Specifically, add a list of affiliation_numbers for each author and add a list of affiliations to the top-level of variables. If no authors have any affiliations, variables is left unmodified. \"\"\" affiliations = list () for author in variables [ \"authors\" ]: _convert_field_to_list ( dictionary = author , field = \"affiliations\" , separator = \"; \" , deprecation_warning_key = \"name\" , ) _convert_field_to_list ( dictionary = author , field = \"funders\" , ) affiliations . extend ( author . get ( \"affiliations\" , [])) if not affiliations : return variables affiliations = list ( dict . fromkeys ( affiliations )) # deduplicate affil_to_number = { affil : i for i , affil in enumerate ( affiliations , start = 1 )} for author in variables [ \"authors\" ]: numbers = [ affil_to_number [ affil ] for affil in author . get ( \"affiliations\" , [])] author [ \"affiliation_numbers\" ] = sorted ( numbers ) variables [ \"affiliations\" ] = [ dict ( affiliation = affil , affiliation_number = i ) for affil , i in affil_to_number . items () ] return variables def load_variables ( args ) -> dict : \"\"\" Read `metadata.yaml` and files specified by `--template-variables-path` to generate manuscript variables available for jinja2 templating. Returns a dictionary, refered to as `variables`, with the following keys: - `pandoc`: a dictionary for passing options to Pandoc via the `yaml_metadata_block`. Fields in `pandoc` are either generated by Manubot or hard-coded by the user if `metadata.yaml` includes a `pandoc` dictionary. - `manubot`: a dictionary for manubot-related information and metadata. Fields in `manubot` are either generated by Manubot or hard-coded by the user if `metadata.yaml` includes a `manubot` dictionary. - All fields from a manuscript's `metadata.yaml` that are not interpreted by Manubot are copied to `variables`. Interpreted fields include `pandoc`, `manubot`, `title`, `keywords`, `authors` (formerly `author_info`, now deprecated), `lang`, and `thumbnail`. - User-specified fields inserted according to the `--template-variables-path` option. User-specified variables take highest precedence and can overwrite values for existing keys like `pandoc` or `manubot` (dangerous). \"\"\" # Generated manuscript variables variables = { \"pandoc\" : {}, \"manubot\" : {}} # Read metadata which contains pandoc_yaml_metadata # as well as authors information. if args . meta_yaml_path . is_file (): metadata = read_serialized_dict ( args . meta_yaml_path ) else : metadata = {} logging . warning ( f \"missing { args . meta_yaml_path } file with yaml_metadata_block for pandoc\" ) # Interpreted keys that are intended for pandoc move_to_pandoc = \"title\" , \"keywords\" , \"lang\" for key in move_to_pandoc : if key in metadata : variables [ \"pandoc\" ][ key ] = metadata . pop ( key ) # Add date to metadata now = datetime_now () logging . info ( f \"Using { now : %Z } timezone. \\n \" f \"Dating manuscript with the current datetime: { now . isoformat () } \" ) variables [ \"pandoc\" ][ \"date-meta\" ] = now . date () . isoformat () variables [ \"manubot\" ][ \"date\" ] = f \" { now : %B } { now . day } , { now . year } \" # Process authors metadata if \"author_info\" in metadata : authors = metadata . pop ( \"author_info\" , []) warnings . warn ( \"metadata.yaml: 'author_info' is deprecated. Use 'authors' instead.\" , category = DeprecationWarning , ) else : authors = metadata . pop ( \"authors\" , []) if authors is None : authors = [] variables [ \"pandoc\" ][ \"author-meta\" ] = [ author [ \"name\" ] for author in authors ] variables [ \"manubot\" ][ \"authors\" ] = authors add_author_affiliations ( variables [ \"manubot\" ]) # Set repository version metadata for CI builds ci_params = get_continuous_integration_parameters () if ci_params : variables [ \"manubot\" ][ \"ci_source\" ] = ci_params # Add manuscript URLs variables [ \"manubot\" ] . update ( get_manuscript_urls ( metadata . pop ( \"html_url\" , None ))) # Add software versions variables [ \"manubot\" ] . update ( get_software_versions ( rootstock = not args . skip_remote )) # Add thumbnail URL if present thumbnail_url = get_thumbnail_url ( metadata . pop ( \"thumbnail\" , None )) if thumbnail_url : variables [ \"manubot\" ][ \"thumbnail_url\" ] = thumbnail_url # Update variables with metadata.yaml pandoc/manubot dicts for key in \"pandoc\" , \"manubot\" : dict_ = metadata . pop ( key , {}) if not isinstance ( dict_ , dict ): logging . warning ( f \"load_variables expected metadata.yaml field { key !r} to be a dict.\" f \"Received a { dict_ . __class__ . __name__ !r} instead.\" ) continue variables [ key ] . update ( dict_ ) # Update variables with uninterpreted metadata.yaml fields variables . update ( metadata ) # Update variables with user-provided variables here variables = read_variable_files ( args . template_variables_path , variables ) # Add header-includes metadata with <meta> information for the HTML output's <head> variables [ \"pandoc\" ][ \"header-includes\" ] = get_header_includes ( variables ) assert args . skip_citations # Extend Pandoc's metadata.bibliography field with manual references paths bibliographies = variables [ \"pandoc\" ] . get ( \"bibliography\" , []) if isinstance ( bibliographies , str ): bibliographies = [ bibliographies ] assert isinstance ( bibliographies , list ) bibliographies . extend ( args . manual_references_paths ) bibliographies = list ( map ( os . fspath , bibliographies )) variables [ \"pandoc\" ][ \"bibliography\" ] = bibliographies # enable pandoc-manubot-cite option to write bibliography to a file variables [ \"pandoc\" ][ \"manubot-output-bibliography\" ] = os . fspath ( args . references_path ) variables [ \"pandoc\" ][ \"manubot-output-citekeys\" ] = os . fspath ( args . citations_path ) variables [ \"pandoc\" ][ \"manubot-requests-cache-path\" ] = os . fspath ( args . requests_cache_path ) variables [ \"pandoc\" ][ \"manubot-clear-requests-cache\" ] = args . clear_requests_cache return variables def template_with_jinja2 ( text , variables ): \"\"\" Template using jinja2 with the variables dictionary unpacked as keyword arguments. \"\"\" jinja_environment = jinja2 . Environment ( loader = jinja2 . BaseLoader (), undefined = jinja2 . make_logging_undefined ( logging . getLogger ()), autoescape = False , comment_start_string = \"{##\" , comment_end_string = \"##}\" , extensions = [ \"jinja2.ext.do\" , \"jinja2.ext.loopcontrols\" ], ) template = jinja_environment . from_string ( text ) return template . render ( ** variables ) def prepare_manuscript ( args ): \"\"\" Compile manuscript, creating manuscript.md and references.json as inputs for pandoc. \"\"\" text = get_text ( args . content_directory ) assert args . skip_citations variables = load_variables ( args ) variables [ \"manubot\" ][ \"manuscript_stats\" ] = get_manuscript_stats ( text ) with args . variables_path . open ( \"w\" , encoding = \"utf-8\" ) as write_file : json . dump ( variables , write_file , ensure_ascii = False , indent = 2 ) write_file . write ( \" \\n \" ) text = template_with_jinja2 ( text , variables ) # Write manuscript for pandoc yaml = get_configured_yaml () with args . manuscript_path . open ( \"w\" , encoding = \"utf-8\" ) as write_file : yaml . dump ( variables [ \"pandoc\" ], write_file , default_flow_style = False , explicit_start = True , explicit_end = True , width = float ( \"inf\" ), allow_unicode = True , sort_keys = False , ) write_file . write ( \" \\n \" ) write_file . write ( text )","title":"Module manubot.process.util"},{"location":"reference/manubot/process/util/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/util/#add_author_affiliations","text":"def add_author_affiliations ( variables : dict ) -> dict Edit variables to contain numbered author affiliations. Specifically, add a list of affiliation_numbers for each author and add a list of affiliations to the top-level of variables. If no authors have any affiliations, variables is left unmodified. View Source def add_author_affiliations ( variables : dict ) -> dict : \"\"\" Edit variables to contain numbered author affiliations. Specifically, add a list of affiliation_numbers for each author and add a list of affiliations to the top-level of variables. If no authors have any affiliations, variables is left unmodified. \"\"\" affiliations = list () for author in variables [ \"authors\" ] : _convert_field_to_list ( dictionary = author , field = \"affiliations\" , separator = \"; \" , deprecation_warning_key = \"name\" , ) _convert_field_to_list ( dictionary = author , field = \"funders\" , ) affiliations . extend ( author . get ( \"affiliations\" , [] )) if not affiliations : return variables affiliations = list ( dict . fromkeys ( affiliations )) # deduplicate affil_to_number = { affil : i for i , affil in enumerate ( affiliations , start = 1 ) } for author in variables [ \"authors\" ] : numbers = [ affil_to_number[affil ] for affil in author . get ( \"affiliations\" , [] ) ] author [ \"affiliation_numbers\" ] = sorted ( numbers ) variables [ \"affiliations\" ] = [ dict(affiliation=affil, affiliation_number=i) for affil, i in affil_to_number.items() ] return variables","title":"add_author_affiliations"},{"location":"reference/manubot/process/util/#load_variables","text":"def load_variables ( args ) -> dict Read metadata.yaml and files specified by --template-variables-path to generate manuscript variables available for jinja2 templating. Returns a dictionary, refered to as variables , with the following keys: pandoc : a dictionary for passing options to Pandoc via the yaml_metadata_block . Fields in pandoc are either generated by Manubot or hard-coded by the user if metadata.yaml includes a pandoc dictionary. manubot : a dictionary for manubot-related information and metadata. Fields in manubot are either generated by Manubot or hard-coded by the user if metadata.yaml includes a manubot dictionary. All fields from a manuscript's metadata.yaml that are not interpreted by Manubot are copied to variables . Interpreted fields include pandoc , manubot , title , keywords , authors (formerly author_info , now deprecated), lang , and thumbnail . User-specified fields inserted according to the --template-variables-path option. User-specified variables take highest precedence and can overwrite values for existing keys like pandoc or manubot (dangerous). View Source def load_variables ( args ) -> dict : \"\"\" Read `metadata.yaml` and files specified by `--template-variables-path` to generate manuscript variables available for jinja2 templating. Returns a dictionary, refered to as `variables`, with the following keys: - `pandoc`: a dictionary for passing options to Pandoc via the `yaml_metadata_block`. Fields in `pandoc` are either generated by Manubot or hard-coded by the user if `metadata.yaml` includes a `pandoc` dictionary. - `manubot`: a dictionary for manubot-related information and metadata. Fields in `manubot` are either generated by Manubot or hard-coded by the user if `metadata.yaml` includes a `manubot` dictionary. - All fields from a manuscript's `metadata.yaml` that are not interpreted by Manubot are copied to `variables`. Interpreted fields include `pandoc`, `manubot`, `title`, `keywords`, `authors` (formerly `author_info`, now deprecated), `lang`, and `thumbnail`. - User-specified fields inserted according to the `--template-variables-path` option. User-specified variables take highest precedence and can overwrite values for existing keys like `pandoc` or `manubot` (dangerous). \"\"\" # Generated manuscript variables variables = { \"pandoc\" : {}, \"manubot\" : {}} # Read metadata which contains pandoc_yaml_metadata # as well as authors information. if args . meta_yaml_path . is_file (): metadata = read_serialized_dict ( args . meta_yaml_path ) else : metadata = {} logging . warning ( f \"missing {args.meta_yaml_path} file with yaml_metadata_block for pandoc\" ) # Interpreted keys that are intended for pandoc move_to_pandoc = \"title\" , \"keywords\" , \"lang\" for key in move_to_pandoc : if key in metadata : variables [ \"pandoc\" ][ key ] = metadata . pop ( key ) # Add date to metadata now = datetime_now () logging . info ( f \"Using {now:%Z} timezone. \\n \" f \"Dating manuscript with the current datetime: {now.isoformat()}\" ) variables [ \"pandoc\" ][ \"date-meta\" ] = now . date () . isoformat () variables [ \"manubot\" ][ \"date\" ] = f \"{now:%B} {now.day}, {now.year}\" # Process authors metadata if \"author_info\" in metadata : authors = metadata . pop ( \"author_info\" , []) warnings . warn ( \"metadata.yaml: 'author_info' is deprecated. Use 'authors' instead.\" , category = DeprecationWarning , ) else : authors = metadata . pop ( \"authors\" , []) if authors is None : authors = [] variables [ \"pandoc\" ][ \"author-meta\" ] = [ author [ \"name\" ] for author in authors ] variables [ \"manubot\" ][ \"authors\" ] = authors add_author_affiliations ( variables [ \"manubot\" ]) # Set repository version metadata for CI builds ci_params = get_continuous_integration_parameters () if ci_params : variables [ \"manubot\" ][ \"ci_source\" ] = ci_params # Add manuscript URLs variables [ \"manubot\" ] . update ( get_manuscript_urls ( metadata . pop ( \"html_url\" , None ))) # Add software versions variables [ \"manubot\" ] . update ( get_software_versions ( rootstock = not args . skip_remote )) # Add thumbnail URL if present thumbnail_url = get_thumbnail_url ( metadata . pop ( \"thumbnail\" , None )) if thumbnail_url : variables [ \"manubot\" ][ \"thumbnail_url\" ] = thumbnail_url # Update variables with metadata.yaml pandoc/manubot dicts for key in \"pandoc\" , \"manubot\" : dict_ = metadata . pop ( key , {}) if not isinstance ( dict_ , dict ): logging . warning ( f \"load_variables expected metadata.yaml field {key!r} to be a dict.\" f \"Received a {dict_.__class__.__name__!r} instead.\" ) continue variables [ key ] . update ( dict_ ) # Update variables with uninterpreted metadata.yaml fields variables . update ( metadata ) # Update variables with user-provided variables here variables = read_variable_files ( args . template_variables_path , variables ) # Add header-includes metadata with <meta> information for the HTML output's <head> variables [ \"pandoc\" ][ \"header-includes\" ] = get_header_includes ( variables ) assert args . skip_citations # Extend Pandoc's metadata.bibliography field with manual references paths bibliographies = variables [ \"pandoc\" ] . get ( \"bibliography\" , []) if isinstance ( bibliographies , str ): bibliographies = [ bibliographies ] assert isinstance ( bibliographies , list ) bibliographies . extend ( args . manual_references_paths ) bibliographies = list ( map ( os . fspath , bibliographies )) variables [ \"pandoc\" ][ \"bibliography\" ] = bibliographies # enable pandoc-manubot-cite option to write bibliography to a file variables [ \"pandoc\" ][ \"manubot-output-bibliography\" ] = os . fspath ( args . references_path ) variables [ \"pandoc\" ][ \"manubot-output-citekeys\" ] = os . fspath ( args . citations_path ) variables [ \"pandoc\" ][ \"manubot-requests-cache-path\" ] = os . fspath ( args . requests_cache_path ) variables [ \"pandoc\" ][ \"manubot-clear-requests-cache\" ] = args . clear_requests_cache return variables","title":"load_variables"},{"location":"reference/manubot/process/util/#prepare_manuscript","text":"def prepare_manuscript ( args ) Compile manuscript, creating manuscript.md and references.json as inputs for pandoc. View Source def prepare_manuscript ( args ): \"\"\" Compile manuscript, creating manuscript.md and references.json as inputs for pandoc. \"\"\" text = get_text ( args . content_directory ) assert args . skip_citations variables = load_variables ( args ) variables [ \"manubot\" ][ \"manuscript_stats\" ] = get_manuscript_stats ( text ) with args . variables_path . open ( \"w\" , encoding = \"utf-8\" ) as write_file : json . dump ( variables , write_file , ensure_ascii = False , indent = 2 ) write_file . write ( \" \\n \" ) text = template_with_jinja2 ( text , variables ) # Write manuscript for pandoc yaml = get_configured_yaml () with args . manuscript_path . open ( \"w\" , encoding = \"utf-8\" ) as write_file : yaml . dump ( variables [ \"pandoc\" ], write_file , default_flow_style = False , explicit_start = True , explicit_end = True , width = float ( \"inf\" ), allow_unicode = True , sort_keys = False , ) write_file . write ( \" \\n \" ) write_file . write ( text )","title":"prepare_manuscript"},{"location":"reference/manubot/process/util/#read_variable_files","text":"def read_variable_files ( paths : List [ str ], variables : Optional [ dict ] = None ) -> dict Read multiple serialized data files into a user_variables dictionary. Provide paths (a list of URLs or local file paths). Paths can optionally have a namespace prepended. For example: paths = [ 'https://git.io/vbkqm' , # update the dictionary's top-level 'namespace_1=https://git.io/vbkqm' , # store under 'namespace_1' key 'namespace_2=some_local_path.json' , # store under 'namespace_2' key ] If a namespace is not provided, the JSON must contain a dictionary as its top level. Namespaces should consist only of ASCII alphanumeric characters (includes underscores, first character cannot be numeric). Pass a dictionary to variables to update an existing dictionary rather than create a new dictionary. View Source def read_variable_files ( paths : List [ str ], variables : Optional [ dict ] = None ) -> dict : \"\"\" Read multiple serialized data files into a user_variables dictionary. Provide `paths` (a list of URLs or local file paths). Paths can optionally have a namespace prepended. For example: ```python paths = [ 'https://git.io/vbkqm', # update the dictionary's top-level 'namespace_1=https://git.io/vbkqm', # store under 'namespace_1' key 'namespace_2=some_local_path.json', # store under 'namespace_2' key ] ``` If a namespace is not provided, the JSON must contain a dictionary as its top level. Namespaces should consist only of ASCII alphanumeric characters (includes underscores, first character cannot be numeric). Pass a dictionary to `variables` to update an existing dictionary rather than create a new dictionary. \"\"\" if variables is None : variables = {} for path in paths : logging . info ( f \"Reading user-provided templating variables at {path!r}\" ) # Match only namespaces that are valid jinja2 variable names # http://jinja.pocoo.org/docs/2.10/api/#identifier-naming match = re . match ( r \"([a-zA-Z_][a-zA-Z0-9_]*)=(.+)\" , path ) if match : namespace , path = match . groups () logging . info ( f \"Using the {namespace!r} namespace for template variables from {path!r}\" ) try : if match : obj = { namespace : read_serialized_data ( path )} else : obj = read_serialized_dict ( path ) except Exception : logging . exception ( f \"Error reading template variables from {path!r}\" ) continue assert isinstance ( obj , dict ) conflicts = variables . keys () & obj . keys () if conflicts : logging . warning ( f \"Template variables in {path!r} overwrite existing \" \"values for the following keys: \\n \" + \" \\n \" . join ( conflicts ) ) variables . update ( obj ) logging . debug ( f \"Reading user-provided templating variables complete: \\n \" f \"{json.dumps(variables, indent=2, ensure_ascii=False)}\" ) return variables","title":"read_variable_files"},{"location":"reference/manubot/process/util/#template_with_jinja2","text":"def template_with_jinja2 ( text , variables ) Template using jinja2 with the variables dictionary unpacked as keyword arguments. View Source def template_with_jinja2 ( text , variables ): \"\"\" Template using jinja2 with the variables dictionary unpacked as keyword arguments. \"\"\" jinja_environment = jinja2 . Environment ( loader = jinja2 . BaseLoader (), undefined = jinja2 . make_logging_undefined ( logging . getLogger ()), autoescape = False , comment_start_string = \"{##\" , comment_end_string = \"##}\" , extensions = [ \"jinja2.ext.do\" , \"jinja2.ext.loopcontrols\" ], ) template = jinja_environment . from_string ( text ) return template . render ( ** variables )","title":"template_with_jinja2"},{"location":"reference/manubot/process/tests/","text":"Module manubot.process.tests None None Sub-modules manubot.process.tests.test_bibliography manubot.process.tests.test_ci manubot.process.tests.test_metadata manubot.process.tests.test_process_command manubot.process.tests.test_util","title":"Index"},{"location":"reference/manubot/process/tests/#module-manubotprocesstests","text":"None None","title":"Module manubot.process.tests"},{"location":"reference/manubot/process/tests/#sub-modules","text":"manubot.process.tests.test_bibliography manubot.process.tests.test_ci manubot.process.tests.test_metadata manubot.process.tests.test_process_command manubot.process.tests.test_util","title":"Sub-modules"},{"location":"reference/manubot/process/tests/test_bibliography/","text":"Module manubot.process.tests.test_bibliography None None View Source from manubot.pandoc.tests.test_bibliography import ( directory , skipif_no_pandoc , skipif_no_pandoc_citeproc , ) from manubot.process.bibliography import load_manual_references class Test_load_manual_references : \"\"\" Tests loading multiple bibliography paths \"\"\" def setup_method ( self ): bibliography_paths = sorted ( directory / x for x in directory . glob ( \"bibliographies/bibliography.*\" ) ) self . citation_to_csl_item = load_manual_references ( bibliography_paths ) print ( list ( self . citation_to_csl_item )) def test_csl_item_1 ( self ): assert \"doi:10.7554/elife.32822\" in self . citation_to_csl_item csl_item_1 = self . citation_to_csl_item [ \"doi:10.7554/elife.32822\" ] assert csl_item_1 [ \"title\" ] . startswith ( \"Sci-Hub\" ) assert ( \"Loaded from an external bibliography file by Manubot.\" in csl_item_1 [ \"note\" ] ) assert \"source_bibliography: bibliography.json\" in csl_item_1 [ \"note\" ] assert \"standard_id: doi:10.7554/elife.32822\" in csl_item_1 [ \"note\" ] @skipif_no_pandoc def test_csl_item_2 ( self ): # raw id corresponding to bibliography.bib assert \"noauthor_techblog:_nodate\" in self . citation_to_csl_item csl_item_2 = self . citation_to_csl_item [ \"noauthor_techblog:_nodate\" ] assert csl_item_2 [ \"title\" ] . startswith ( \"TechBlog\" ) assert \"source_bibliography: bibliography.bib\" in csl_item_2 [ \"note\" ] assert \"standard_id: noauthor_techblog:_nodate\" in csl_item_2 [ \"note\" ] @skipif_no_pandoc_citeproc def test_csl_item_3 ( self ): assert \"Beaulieu-Jones2017\" in self . citation_to_csl_item csl_item_3 = self . citation_to_csl_item [ \"Beaulieu-Jones2017\" ] assert csl_item_3 [ \"author\" ][ 0 ][ \"family\" ] == \"Beaulieu-Jones\" assert \"source_bibliography: bibliography.nbib\" in csl_item_3 [ \"note\" ] assert \"standard_id: Beaulieu-Jones2017\" in csl_item_3 [ \"note\" ] Classes Test_load_manual_references class Test_load_manual_references ( / , * args , ** kwargs ) View Source class Test_load_manual_references : \"\"\" Tests loading multiple bibliography paths \"\"\" def setup_method ( self ): bibliography_paths = sorted ( directory / x for x in directory . glob ( \"bibliographies/bibliography.*\" ) ) self . citation_to_csl_item = load_manual_references ( bibliography_paths ) print ( list ( self . citation_to_csl_item )) def test_csl_item_1 ( self ): assert \"doi:10.7554/elife.32822\" in self . citation_to_csl_item csl_item_1 = self . citation_to_csl_item [ \"doi:10.7554/elife.32822\" ] assert csl_item_1 [ \"title\" ] . startswith ( \"Sci-Hub\" ) assert ( \"Loaded from an external bibliography file by Manubot.\" in csl_item_1 [ \"note\" ] ) assert \"source_bibliography: bibliography.json\" in csl_item_1 [ \"note\" ] assert \"standard_id: doi:10.7554/elife.32822\" in csl_item_1 [ \"note\" ] @ skipif_no_pandoc def test_csl_item_2 ( self ): # raw id corresponding to bibliography.bib assert \"noauthor_techblog:_nodate\" in self . citation_to_csl_item csl_item_2 = self . citation_to_csl_item [ \"noauthor_techblog:_nodate\" ] assert csl_item_2 [ \"title\" ] . startswith ( \"TechBlog\" ) assert \"source_bibliography: bibliography.bib\" in csl_item_2 [ \"note\" ] assert \"standard_id: noauthor_techblog:_nodate\" in csl_item_2 [ \"note\" ] @ skipif_no_pandoc_citeproc def test_csl_item_3 ( self ): assert \"Beaulieu-Jones2017\" in self . citation_to_csl_item csl_item_3 = self . citation_to_csl_item [ \"Beaulieu-Jones2017\" ] assert csl_item_3 [ \"author\" ][ 0 ][ \"family\" ] == \"Beaulieu-Jones\" assert \"source_bibliography: bibliography.nbib\" in csl_item_3 [ \"note\" ] assert \"standard_id: Beaulieu-Jones2017\" in csl_item_3 [ \"note\" ] Methods setup_method def setup_method ( self ) View Source def setup_method ( self ): bibliography_paths = sorted ( directory / x for x in directory . glob ( \"bibliographies/bibliography.*\" ) ) self . citation_to_csl_item = load_manual_references ( bibliography_paths ) print ( list ( self . citation_to_csl_item )) test_csl_item_1 def test_csl_item_1 ( self ) View Source def test_csl_item_1(self): assert \"doi:10.7554/elife.32822\" in self.citation_to_csl_item csl_item_1 = self.citation_to_csl_item[\"doi:10.7554/elife.32822\"] assert csl_item_1[\"title\"].startswith(\"Sci-Hub\") assert ( \"Loaded from an external bibliography file by Manubot.\" in csl_item_1[\"note\"] ) assert \"source_bibliography: bibliography.json\" in csl_item_1[\"note\"] assert \"standard_id: doi:10.7554/elife.32822\" in csl_item_1[\"note\"] test_csl_item_2 def test_csl_item_2 ( self ) View Source @skipif_no_pandoc def test_csl_item_2 ( self ) : # raw id corresponding to bibliography . bib assert \"noauthor_techblog:_nodate\" in self . citation_to_csl_item csl_item_2 = self . citation_to_csl_item [ \"noauthor_techblog:_nodate\" ] assert csl_item_2 [ \"title\" ] . startswith ( \"TechBlog\" ) assert \"source_bibliography: bibliography.bib\" in csl_item_2 [ \"note\" ] assert \"standard_id: noauthor_techblog:_nodate\" in csl_item_2 [ \"note\" ] test_csl_item_3 def test_csl_item_3 ( self ) View Source @skipif_no_pandoc_citeproc def test_csl_item_3 ( self ) : assert \"Beaulieu-Jones2017\" in self . citation_to_csl_item csl_item_3 = self . citation_to_csl_item [ \"Beaulieu-Jones2017\" ] assert csl_item_3 [ \"author\" ][ 0 ][ \"family\" ] == \"Beaulieu-Jones\" assert \"source_bibliography: bibliography.nbib\" in csl_item_3 [ \"note\" ] assert \"standard_id: Beaulieu-Jones2017\" in csl_item_3 [ \"note\" ]","title":"Test Bibliography"},{"location":"reference/manubot/process/tests/test_bibliography/#module-manubotprocessteststest_bibliography","text":"None None View Source from manubot.pandoc.tests.test_bibliography import ( directory , skipif_no_pandoc , skipif_no_pandoc_citeproc , ) from manubot.process.bibliography import load_manual_references class Test_load_manual_references : \"\"\" Tests loading multiple bibliography paths \"\"\" def setup_method ( self ): bibliography_paths = sorted ( directory / x for x in directory . glob ( \"bibliographies/bibliography.*\" ) ) self . citation_to_csl_item = load_manual_references ( bibliography_paths ) print ( list ( self . citation_to_csl_item )) def test_csl_item_1 ( self ): assert \"doi:10.7554/elife.32822\" in self . citation_to_csl_item csl_item_1 = self . citation_to_csl_item [ \"doi:10.7554/elife.32822\" ] assert csl_item_1 [ \"title\" ] . startswith ( \"Sci-Hub\" ) assert ( \"Loaded from an external bibliography file by Manubot.\" in csl_item_1 [ \"note\" ] ) assert \"source_bibliography: bibliography.json\" in csl_item_1 [ \"note\" ] assert \"standard_id: doi:10.7554/elife.32822\" in csl_item_1 [ \"note\" ] @skipif_no_pandoc def test_csl_item_2 ( self ): # raw id corresponding to bibliography.bib assert \"noauthor_techblog:_nodate\" in self . citation_to_csl_item csl_item_2 = self . citation_to_csl_item [ \"noauthor_techblog:_nodate\" ] assert csl_item_2 [ \"title\" ] . startswith ( \"TechBlog\" ) assert \"source_bibliography: bibliography.bib\" in csl_item_2 [ \"note\" ] assert \"standard_id: noauthor_techblog:_nodate\" in csl_item_2 [ \"note\" ] @skipif_no_pandoc_citeproc def test_csl_item_3 ( self ): assert \"Beaulieu-Jones2017\" in self . citation_to_csl_item csl_item_3 = self . citation_to_csl_item [ \"Beaulieu-Jones2017\" ] assert csl_item_3 [ \"author\" ][ 0 ][ \"family\" ] == \"Beaulieu-Jones\" assert \"source_bibliography: bibliography.nbib\" in csl_item_3 [ \"note\" ] assert \"standard_id: Beaulieu-Jones2017\" in csl_item_3 [ \"note\" ]","title":"Module manubot.process.tests.test_bibliography"},{"location":"reference/manubot/process/tests/test_bibliography/#classes","text":"","title":"Classes"},{"location":"reference/manubot/process/tests/test_bibliography/#test_load_manual_references","text":"class Test_load_manual_references ( / , * args , ** kwargs ) View Source class Test_load_manual_references : \"\"\" Tests loading multiple bibliography paths \"\"\" def setup_method ( self ): bibliography_paths = sorted ( directory / x for x in directory . glob ( \"bibliographies/bibliography.*\" ) ) self . citation_to_csl_item = load_manual_references ( bibliography_paths ) print ( list ( self . citation_to_csl_item )) def test_csl_item_1 ( self ): assert \"doi:10.7554/elife.32822\" in self . citation_to_csl_item csl_item_1 = self . citation_to_csl_item [ \"doi:10.7554/elife.32822\" ] assert csl_item_1 [ \"title\" ] . startswith ( \"Sci-Hub\" ) assert ( \"Loaded from an external bibliography file by Manubot.\" in csl_item_1 [ \"note\" ] ) assert \"source_bibliography: bibliography.json\" in csl_item_1 [ \"note\" ] assert \"standard_id: doi:10.7554/elife.32822\" in csl_item_1 [ \"note\" ] @ skipif_no_pandoc def test_csl_item_2 ( self ): # raw id corresponding to bibliography.bib assert \"noauthor_techblog:_nodate\" in self . citation_to_csl_item csl_item_2 = self . citation_to_csl_item [ \"noauthor_techblog:_nodate\" ] assert csl_item_2 [ \"title\" ] . startswith ( \"TechBlog\" ) assert \"source_bibliography: bibliography.bib\" in csl_item_2 [ \"note\" ] assert \"standard_id: noauthor_techblog:_nodate\" in csl_item_2 [ \"note\" ] @ skipif_no_pandoc_citeproc def test_csl_item_3 ( self ): assert \"Beaulieu-Jones2017\" in self . citation_to_csl_item csl_item_3 = self . citation_to_csl_item [ \"Beaulieu-Jones2017\" ] assert csl_item_3 [ \"author\" ][ 0 ][ \"family\" ] == \"Beaulieu-Jones\" assert \"source_bibliography: bibliography.nbib\" in csl_item_3 [ \"note\" ] assert \"standard_id: Beaulieu-Jones2017\" in csl_item_3 [ \"note\" ]","title":"Test_load_manual_references"},{"location":"reference/manubot/process/tests/test_bibliography/#methods","text":"","title":"Methods"},{"location":"reference/manubot/process/tests/test_bibliography/#setup_method","text":"def setup_method ( self ) View Source def setup_method ( self ): bibliography_paths = sorted ( directory / x for x in directory . glob ( \"bibliographies/bibliography.*\" ) ) self . citation_to_csl_item = load_manual_references ( bibliography_paths ) print ( list ( self . citation_to_csl_item ))","title":"setup_method"},{"location":"reference/manubot/process/tests/test_bibliography/#test_csl_item_1","text":"def test_csl_item_1 ( self ) View Source def test_csl_item_1(self): assert \"doi:10.7554/elife.32822\" in self.citation_to_csl_item csl_item_1 = self.citation_to_csl_item[\"doi:10.7554/elife.32822\"] assert csl_item_1[\"title\"].startswith(\"Sci-Hub\") assert ( \"Loaded from an external bibliography file by Manubot.\" in csl_item_1[\"note\"] ) assert \"source_bibliography: bibliography.json\" in csl_item_1[\"note\"] assert \"standard_id: doi:10.7554/elife.32822\" in csl_item_1[\"note\"]","title":"test_csl_item_1"},{"location":"reference/manubot/process/tests/test_bibliography/#test_csl_item_2","text":"def test_csl_item_2 ( self ) View Source @skipif_no_pandoc def test_csl_item_2 ( self ) : # raw id corresponding to bibliography . bib assert \"noauthor_techblog:_nodate\" in self . citation_to_csl_item csl_item_2 = self . citation_to_csl_item [ \"noauthor_techblog:_nodate\" ] assert csl_item_2 [ \"title\" ] . startswith ( \"TechBlog\" ) assert \"source_bibliography: bibliography.bib\" in csl_item_2 [ \"note\" ] assert \"standard_id: noauthor_techblog:_nodate\" in csl_item_2 [ \"note\" ]","title":"test_csl_item_2"},{"location":"reference/manubot/process/tests/test_bibliography/#test_csl_item_3","text":"def test_csl_item_3 ( self ) View Source @skipif_no_pandoc_citeproc def test_csl_item_3 ( self ) : assert \"Beaulieu-Jones2017\" in self . citation_to_csl_item csl_item_3 = self . citation_to_csl_item [ \"Beaulieu-Jones2017\" ] assert csl_item_3 [ \"author\" ][ 0 ][ \"family\" ] == \"Beaulieu-Jones\" assert \"source_bibliography: bibliography.nbib\" in csl_item_3 [ \"note\" ] assert \"standard_id: Beaulieu-Jones2017\" in csl_item_3 [ \"note\" ]","title":"test_csl_item_3"},{"location":"reference/manubot/process/tests/test_ci/","text":"Module manubot.process.tests.test_ci None None View Source import os import re import pytest from ..ci import get_continuous_integration_parameters @pytest . mark . skipif ( \"GITHUB_ACTION\" not in os . environ , reason = \"tests environment variables set by GitHub Actions only\" , ) @pytest . mark . skipif ( os . getenv ( \"GITHUB_REPOSITORY\" ) != \"manubot/manubot\" , reason = \"test fails on forks\" ) def test_get_continuous_integration_parameters_github (): info = get_continuous_integration_parameters () assert info is not None assert info [ \"provider\" ] == \"github\" assert info [ \"repo_slug\" ] == \"manubot/manubot\" assert info [ \"repo_owner\" ] == \"manubot\" assert info [ \"repo_name\" ] == \"manubot\" assert info [ \"commit\" ] assert info [ \"triggering_commit\" ] assert info [ \"build_url\" ] . startswith ( \"https://github.com/manubot/manubot/commit/\" ) assert info [ \"job_url\" ] . startswith ( \"https://github.com/manubot/manubot/actions/runs/\" ) @pytest . mark . skipif ( \"TRAVIS\" not in os . environ , reason = \"tests environment variables set by Travis builds only\" , ) @pytest . mark . skipif ( os . getenv ( \"TRAVIS_REPO_SLUG\" ) != \"manubot/manubot\" , reason = \"test fails on forks\" ) @pytest . mark . travis def test_get_continuous_integration_parameters_travis (): info = get_continuous_integration_parameters () assert info is not None assert info [ \"provider\" ] == \"travis\" assert info [ \"repo_slug\" ] == \"manubot/manubot\" assert info [ \"repo_owner\" ] == \"manubot\" assert info [ \"repo_name\" ] == \"manubot\" assert info [ \"commit\" ] assert info [ \"triggering_commit\" ] assert info [ \"build_url\" ] . startswith ( \"https://travis-ci.com/manubot/manubot/builds/\" ) assert info [ \"job_url\" ] . startswith ( \"https://travis-ci.com/manubot/manubot/jobs/\" ) @pytest . mark . skipif ( \"APPVEYOR\" not in os . environ , reason = \"tests environment variables set by AppVeyor builds only\" , ) @pytest . mark . skipif ( os . getenv ( \"APPVEYOR_REPO_NAME\" ) != \"manubot/manubot\" , reason = \"test fails on forks\" ) @pytest . mark . appveyor def test_get_continuous_integration_parameters_appveyor (): info = get_continuous_integration_parameters () assert info is not None assert info [ \"provider\" ] == \"appveyor\" assert info [ \"provider_account\" ] == \"manubot\" assert info [ \"repo_slug\" ] == \"manubot/manubot\" assert info [ \"repo_owner\" ] == \"manubot\" assert info [ \"repo_name\" ] == \"manubot\" assert info [ \"commit\" ] assert info [ \"triggering_commit\" ] assert info [ \"build_url\" ] . startswith ( \"https://ci.appveyor.com/project/manubot/manubot/builds/\" ) assert info [ \"job_url\" ] . startswith ( \"https://ci.appveyor.com/project/manubot/manubot/build/job/\" ) assert re . fullmatch ( pattern = r \"https://ci\\.appveyor\\.com/project/manubot/manubot/builds/[0-9]+/artifacts\" , string = info [ \"artifact_url\" ], ) Functions test_get_continuous_integration_parameters_appveyor def test_get_continuous_integration_parameters_appveyor ( ) View Source @ pytest . mark . skipif ( \"APPVEYOR\" not in os . environ , reason = \"tests environment variables set by AppVeyor builds only\" , ) @ pytest . mark . skipif ( os . getenv ( \"APPVEYOR_REPO_NAME\" ) != \"manubot/manubot\" , reason = \"test fails on forks\" ) @ pytest . mark . appveyor def test_get_continuous_integration_parameters_appveyor (): info = get_continuous_integration_parameters () assert info is not None assert info [ \"provider\" ] == \"appveyor\" assert info [ \"provider_account\" ] == \"manubot\" assert info [ \"repo_slug\" ] == \"manubot/manubot\" assert info [ \"repo_owner\" ] == \"manubot\" assert info [ \"repo_name\" ] == \"manubot\" assert info [ \"commit\" ] assert info [ \"triggering_commit\" ] assert info [ \"build_url\" ] . startswith ( \"https://ci.appveyor.com/project/manubot/manubot/builds/\" ) assert info [ \"job_url\" ] . startswith ( \"https://ci.appveyor.com/project/manubot/manubot/build/job/\" ) assert re . fullmatch ( pattern = r \"https://ci\\.appveyor\\.com/project/manubot/manubot/builds/[0-9]+/artifacts\" , string = info [ \"artifact_url\" ], ) test_get_continuous_integration_parameters_github def test_get_continuous_integration_parameters_github ( ) View Source @ pytest . mark . skipif ( \"GITHUB_ACTION\" not in os . environ , reason = \"tests environment variables set by GitHub Actions only\" , ) @ pytest . mark . skipif ( os . getenv ( \"GITHUB_REPOSITORY\" ) != \"manubot/manubot\" , reason = \"test fails on forks\" ) def test_get_continuous_integration_parameters_github (): info = get_continuous_integration_parameters () assert info is not None assert info [ \"provider\" ] == \"github\" assert info [ \"repo_slug\" ] == \"manubot/manubot\" assert info [ \"repo_owner\" ] == \"manubot\" assert info [ \"repo_name\" ] == \"manubot\" assert info [ \"commit\" ] assert info [ \"triggering_commit\" ] assert info [ \"build_url\" ] . startswith ( \"https://github.com/manubot/manubot/commit/\" ) assert info [ \"job_url\" ] . startswith ( \"https://github.com/manubot/manubot/actions/runs/\" ) test_get_continuous_integration_parameters_travis def test_get_continuous_integration_parameters_travis ( ) View Source @ pytest . mark . skipif ( \"TRAVIS\" not in os . environ , reason = \"tests environment variables set by Travis builds only\" , ) @ pytest . mark . skipif ( os . getenv ( \"TRAVIS_REPO_SLUG\" ) != \"manubot/manubot\" , reason = \"test fails on forks\" ) @ pytest . mark . travis def test_get_continuous_integration_parameters_travis (): info = get_continuous_integration_parameters () assert info is not None assert info [ \"provider\" ] == \"travis\" assert info [ \"repo_slug\" ] == \"manubot/manubot\" assert info [ \"repo_owner\" ] == \"manubot\" assert info [ \"repo_name\" ] == \"manubot\" assert info [ \"commit\" ] assert info [ \"triggering_commit\" ] assert info [ \"build_url\" ] . startswith ( \"https://travis-ci.com/manubot/manubot/builds/\" ) assert info [ \"job_url\" ] . startswith ( \"https://travis-ci.com/manubot/manubot/jobs/\" )","title":"Test Ci"},{"location":"reference/manubot/process/tests/test_ci/#module-manubotprocessteststest_ci","text":"None None View Source import os import re import pytest from ..ci import get_continuous_integration_parameters @pytest . mark . skipif ( \"GITHUB_ACTION\" not in os . environ , reason = \"tests environment variables set by GitHub Actions only\" , ) @pytest . mark . skipif ( os . getenv ( \"GITHUB_REPOSITORY\" ) != \"manubot/manubot\" , reason = \"test fails on forks\" ) def test_get_continuous_integration_parameters_github (): info = get_continuous_integration_parameters () assert info is not None assert info [ \"provider\" ] == \"github\" assert info [ \"repo_slug\" ] == \"manubot/manubot\" assert info [ \"repo_owner\" ] == \"manubot\" assert info [ \"repo_name\" ] == \"manubot\" assert info [ \"commit\" ] assert info [ \"triggering_commit\" ] assert info [ \"build_url\" ] . startswith ( \"https://github.com/manubot/manubot/commit/\" ) assert info [ \"job_url\" ] . startswith ( \"https://github.com/manubot/manubot/actions/runs/\" ) @pytest . mark . skipif ( \"TRAVIS\" not in os . environ , reason = \"tests environment variables set by Travis builds only\" , ) @pytest . mark . skipif ( os . getenv ( \"TRAVIS_REPO_SLUG\" ) != \"manubot/manubot\" , reason = \"test fails on forks\" ) @pytest . mark . travis def test_get_continuous_integration_parameters_travis (): info = get_continuous_integration_parameters () assert info is not None assert info [ \"provider\" ] == \"travis\" assert info [ \"repo_slug\" ] == \"manubot/manubot\" assert info [ \"repo_owner\" ] == \"manubot\" assert info [ \"repo_name\" ] == \"manubot\" assert info [ \"commit\" ] assert info [ \"triggering_commit\" ] assert info [ \"build_url\" ] . startswith ( \"https://travis-ci.com/manubot/manubot/builds/\" ) assert info [ \"job_url\" ] . startswith ( \"https://travis-ci.com/manubot/manubot/jobs/\" ) @pytest . mark . skipif ( \"APPVEYOR\" not in os . environ , reason = \"tests environment variables set by AppVeyor builds only\" , ) @pytest . mark . skipif ( os . getenv ( \"APPVEYOR_REPO_NAME\" ) != \"manubot/manubot\" , reason = \"test fails on forks\" ) @pytest . mark . appveyor def test_get_continuous_integration_parameters_appveyor (): info = get_continuous_integration_parameters () assert info is not None assert info [ \"provider\" ] == \"appveyor\" assert info [ \"provider_account\" ] == \"manubot\" assert info [ \"repo_slug\" ] == \"manubot/manubot\" assert info [ \"repo_owner\" ] == \"manubot\" assert info [ \"repo_name\" ] == \"manubot\" assert info [ \"commit\" ] assert info [ \"triggering_commit\" ] assert info [ \"build_url\" ] . startswith ( \"https://ci.appveyor.com/project/manubot/manubot/builds/\" ) assert info [ \"job_url\" ] . startswith ( \"https://ci.appveyor.com/project/manubot/manubot/build/job/\" ) assert re . fullmatch ( pattern = r \"https://ci\\.appveyor\\.com/project/manubot/manubot/builds/[0-9]+/artifacts\" , string = info [ \"artifact_url\" ], )","title":"Module manubot.process.tests.test_ci"},{"location":"reference/manubot/process/tests/test_ci/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/tests/test_ci/#test_get_continuous_integration_parameters_appveyor","text":"def test_get_continuous_integration_parameters_appveyor ( ) View Source @ pytest . mark . skipif ( \"APPVEYOR\" not in os . environ , reason = \"tests environment variables set by AppVeyor builds only\" , ) @ pytest . mark . skipif ( os . getenv ( \"APPVEYOR_REPO_NAME\" ) != \"manubot/manubot\" , reason = \"test fails on forks\" ) @ pytest . mark . appveyor def test_get_continuous_integration_parameters_appveyor (): info = get_continuous_integration_parameters () assert info is not None assert info [ \"provider\" ] == \"appveyor\" assert info [ \"provider_account\" ] == \"manubot\" assert info [ \"repo_slug\" ] == \"manubot/manubot\" assert info [ \"repo_owner\" ] == \"manubot\" assert info [ \"repo_name\" ] == \"manubot\" assert info [ \"commit\" ] assert info [ \"triggering_commit\" ] assert info [ \"build_url\" ] . startswith ( \"https://ci.appveyor.com/project/manubot/manubot/builds/\" ) assert info [ \"job_url\" ] . startswith ( \"https://ci.appveyor.com/project/manubot/manubot/build/job/\" ) assert re . fullmatch ( pattern = r \"https://ci\\.appveyor\\.com/project/manubot/manubot/builds/[0-9]+/artifacts\" , string = info [ \"artifact_url\" ], )","title":"test_get_continuous_integration_parameters_appveyor"},{"location":"reference/manubot/process/tests/test_ci/#test_get_continuous_integration_parameters_github","text":"def test_get_continuous_integration_parameters_github ( ) View Source @ pytest . mark . skipif ( \"GITHUB_ACTION\" not in os . environ , reason = \"tests environment variables set by GitHub Actions only\" , ) @ pytest . mark . skipif ( os . getenv ( \"GITHUB_REPOSITORY\" ) != \"manubot/manubot\" , reason = \"test fails on forks\" ) def test_get_continuous_integration_parameters_github (): info = get_continuous_integration_parameters () assert info is not None assert info [ \"provider\" ] == \"github\" assert info [ \"repo_slug\" ] == \"manubot/manubot\" assert info [ \"repo_owner\" ] == \"manubot\" assert info [ \"repo_name\" ] == \"manubot\" assert info [ \"commit\" ] assert info [ \"triggering_commit\" ] assert info [ \"build_url\" ] . startswith ( \"https://github.com/manubot/manubot/commit/\" ) assert info [ \"job_url\" ] . startswith ( \"https://github.com/manubot/manubot/actions/runs/\" )","title":"test_get_continuous_integration_parameters_github"},{"location":"reference/manubot/process/tests/test_ci/#test_get_continuous_integration_parameters_travis","text":"def test_get_continuous_integration_parameters_travis ( ) View Source @ pytest . mark . skipif ( \"TRAVIS\" not in os . environ , reason = \"tests environment variables set by Travis builds only\" , ) @ pytest . mark . skipif ( os . getenv ( \"TRAVIS_REPO_SLUG\" ) != \"manubot/manubot\" , reason = \"test fails on forks\" ) @ pytest . mark . travis def test_get_continuous_integration_parameters_travis (): info = get_continuous_integration_parameters () assert info is not None assert info [ \"provider\" ] == \"travis\" assert info [ \"repo_slug\" ] == \"manubot/manubot\" assert info [ \"repo_owner\" ] == \"manubot\" assert info [ \"repo_name\" ] == \"manubot\" assert info [ \"commit\" ] assert info [ \"triggering_commit\" ] assert info [ \"build_url\" ] . startswith ( \"https://travis-ci.com/manubot/manubot/builds/\" ) assert info [ \"job_url\" ] . startswith ( \"https://travis-ci.com/manubot/manubot/jobs/\" )","title":"test_get_continuous_integration_parameters_travis"},{"location":"reference/manubot/process/tests/test_metadata/","text":"Module manubot.process.tests.test_metadata None None View Source import copy import pytest from ..ci import get_continuous_integration_parameters from ..metadata import get_header_includes , get_manuscript_urls , get_thumbnail_url def test_get_header_includes_description_abstract (): # test that abstract and description set different fields if both supplied variables = { \"pandoc\" : { \"abstract\" : \"value for abstract\" }, \"manubot\" : { \"description\" : \"value for description\" }, } header_includes = get_header_includes ( copy . deepcopy ( variables )) assert ( '<meta name=\"citation_abstract\" content=\"value for abstract\" />' in header_includes ) assert ( '<meta name=\"description\" content=\"value for description\" />' in header_includes ) # test that abstract is used for description if description is not defined del variables [ \"manubot\" ][ \"description\" ] header_includes = get_header_includes ( copy . deepcopy ( variables )) assert ( '<meta name=\"citation_abstract\" content=\"value for abstract\" />' in header_includes ) assert '<meta name=\"description\" content=\"value for abstract\" />' in header_includes # test that if neither abstract nor description is defined, neither are inserted del variables [ \"pandoc\" ][ \"abstract\" ] header_includes = get_header_includes ( copy . deepcopy ( variables )) assert 'meta name=\"citation_abstract\"' not in header_includes assert 'meta name=\"description\"' not in header_includes ci_params = get_continuous_integration_parameters () or {} local_only = pytest . mark . skipif ( ci_params , reason = \"skipping on CI build since test assumes local behavior\" ) ci_only = pytest . mark . skipif ( not ci_params , reason = \"skipping on local build since test assumes supported CI behavior\" , ) repo_raw_url_template = ( f \"https://github.com/ { ci_params . get ( 'repo_slug' , '' ) } \" f \"/raw/ { ci_params . get ( 'triggering_commit' , '' ) } /\" ) example_thumbnail_url = ( repo_raw_url_template + \"manubot/process/tests/manuscripts/example/thumbnail.png\" ) @pytest . mark . parametrize ( ( \"thumbnail\" , \"expected\" ), [ pytest . param ( None , None , id = \"None-on-local\" , marks = local_only ), pytest . param ( \"\" , None , id = \"empty-on-local\" , marks = local_only ), pytest . param ( None , example_thumbnail_url , id = \"None-on-ci\" , marks = ci_only ), pytest . param ( \"\" , example_thumbnail_url , id = \"empty-on-ci\" , marks = ci_only ), pytest . param ( \"path/to/thumbnail.png\" , None , id = \"local-path-on-local\" , marks = local_only ), pytest . param ( \"path/to/thumbnail.png\" , repo_raw_url_template + \"path/to/thumbnail.png\" , id = \"local-path-on-ci\" , marks = ci_only , ), pytest . param ( \"http://example.com/thumbnail.png\" , \"http://example.com/thumbnail.png\" , id = \"url-http\" , ), pytest . param ( \"https://example.com/thumbnail.png\" , \"https://example.com/thumbnail.png\" , id = \"url-https\" , ), ], ) def test_get_thumbnail_url ( thumbnail , expected ): thumbnail_url = get_thumbnail_url ( thumbnail ) assert expected == thumbnail_url @pytest . mark . parametrize ( ( \"html_url\" , \"expected\" ), [ pytest . param ( None , {}, id = \"html_url-none-local\" , marks = local_only ), pytest . param ( \"https://example.com/manuscript/\" , { \"html_url\" : \"https://example.com/manuscript/\" , \"pdf_url\" : \"https://example.com/manuscript/manuscript.pdf\" , }, id = \"html_url-set-local\" , marks = local_only , ), pytest . param ( None , { \"html_url\" : \"https://manubot.github.io/manubot/\" , \"pdf_url\" : \"https://manubot.github.io/manubot/manuscript.pdf\" , \"html_url_versioned\" : f \"https://manubot.github.io/manubot/v/ { ci_params . get ( 'commit' ) } /\" , \"pdf_url_versioned\" : f \"https://manubot.github.io/manubot/v/ { ci_params . get ( 'commit' ) } /manuscript.pdf\" , }, id = \"html_url-none-ci\" , marks = ci_only , ), pytest . param ( \"https://example.com/manuscript/\" , { \"html_url\" : \"https://example.com/manuscript/\" , \"pdf_url\" : \"https://example.com/manuscript/manuscript.pdf\" , \"html_url_versioned\" : f \"https://example.com/manuscript/v/ { ci_params . get ( 'commit' ) } /\" , \"pdf_url_versioned\" : f \"https://example.com/manuscript/v/ { ci_params . get ( 'commit' ) } /manuscript.pdf\" , }, id = \"html_url-set-ci\" , marks = ci_only , ), ], ) def test_get_manuscript_urls ( html_url , expected ): urls = get_manuscript_urls ( html_url ) assert urls == expected Variables ci_only ci_params example_thumbnail_url local_only repo_raw_url_template Functions test_get_header_includes_description_abstract def test_get_header_includes_description_abstract ( ) View Source def test_get_header_includes_description_abstract (): # test that abstract and description set different fields if both supplied variables = { \"pandoc\" : { \"abstract\" : \"value for abstract\" }, \"manubot\" : { \"description\" : \"value for description\" }, } header_includes = get_header_includes ( copy . deepcopy ( variables )) assert ( '<meta name=\"citation_abstract\" content=\"value for abstract\" />' in header_includes ) assert ( '<meta name=\"description\" content=\"value for description\" />' in header_includes ) # test that abstract is used for description if description is not defined del variables [ \"manubot\" ][ \"description\" ] header_includes = get_header_includes ( copy . deepcopy ( variables )) assert ( '<meta name=\"citation_abstract\" content=\"value for abstract\" />' in header_includes ) assert '<meta name=\"description\" content=\"value for abstract\" />' in header_includes # test that if neither abstract nor description is defined, neither are inserted del variables [ \"pandoc\" ][ \"abstract\" ] header_includes = get_header_includes ( copy . deepcopy ( variables )) assert 'meta name=\"citation_abstract\"' not in header_includes assert 'meta name=\"description\"' not in header_includes test_get_manuscript_urls def test_get_manuscript_urls ( html_url , expected ) View Source @pytest . mark . parametrize ( ( \"html_url\" , \"expected\" ), [ pytest.param(None, {}, id=\"html_url-none-local\", marks=local_only), pytest.param( \"https://example.com/manuscript/\", { \"html_url\": \"https://example.com/manuscript/\", \"pdf_url\": \"https://example.com/manuscript/manuscript.pdf\", }, id=\"html_url-set-local\", marks=local_only, ), pytest.param( None, { \"html_url\": \"https://manubot.github.io/manubot/\", \"pdf_url\": \"https://manubot.github.io/manubot/manuscript.pdf\", \"html_url_versioned\": f\"https://manubot.github.io/manubot/v/{ci_params.get('commit')}/\", \"pdf_url_versioned\": f\"https://manubot.github.io/manubot/v/{ci_params.get('commit')}/manuscript.pdf\", }, id=\"html_url-none-ci\", marks=ci_only, ), pytest.param( \"https://example.com/manuscript/\", { \"html_url\": \"https://example.com/manuscript/\", \"pdf_url\": \"https://example.com/manuscript/manuscript.pdf\", \"html_url_versioned\": f\"https://example.com/manuscript/v/{ci_params.get('commit')}/\", \"pdf_url_versioned\": f\"https://example.com/manuscript/v/{ci_params.get('commit')}/manuscript.pdf\", }, id=\"html_url-set-ci\", marks=ci_only, ), ] , ) def test_get_manuscript_urls ( html_url , expected ) : urls = get_manuscript_urls ( html_url ) assert urls == expected test_get_thumbnail_url def test_get_thumbnail_url ( thumbnail , expected ) View Source @pytest . mark . parametrize ( ( \"thumbnail\" , \"expected\" ), [ pytest.param(None, None, id=\"None-on-local\", marks=local_only), pytest.param(\"\", None, id=\"empty-on-local\", marks=local_only), pytest.param(None, example_thumbnail_url, id=\"None-on-ci\", marks=ci_only), pytest.param(\"\", example_thumbnail_url, id=\"empty-on-ci\", marks=ci_only), pytest.param( \"path/to/thumbnail.png\", None, id=\"local-path-on-local\", marks=local_only ), pytest.param( \"path/to/thumbnail.png\", repo_raw_url_template + \"path/to/thumbnail.png\", id=\"local-path-on-ci\", marks=ci_only, ), pytest.param( \"http://example.com/thumbnail.png\", \"http://example.com/thumbnail.png\", id=\"url-http\", ), pytest.param( \"https://example.com/thumbnail.png\", \"https://example.com/thumbnail.png\", id=\"url-https\", ), ] , ) def test_get_thumbnail_url ( thumbnail , expected ) : thumbnail_url = get_thumbnail_url ( thumbnail ) assert expected == thumbnail_url","title":"Test Metadata"},{"location":"reference/manubot/process/tests/test_metadata/#module-manubotprocessteststest_metadata","text":"None None View Source import copy import pytest from ..ci import get_continuous_integration_parameters from ..metadata import get_header_includes , get_manuscript_urls , get_thumbnail_url def test_get_header_includes_description_abstract (): # test that abstract and description set different fields if both supplied variables = { \"pandoc\" : { \"abstract\" : \"value for abstract\" }, \"manubot\" : { \"description\" : \"value for description\" }, } header_includes = get_header_includes ( copy . deepcopy ( variables )) assert ( '<meta name=\"citation_abstract\" content=\"value for abstract\" />' in header_includes ) assert ( '<meta name=\"description\" content=\"value for description\" />' in header_includes ) # test that abstract is used for description if description is not defined del variables [ \"manubot\" ][ \"description\" ] header_includes = get_header_includes ( copy . deepcopy ( variables )) assert ( '<meta name=\"citation_abstract\" content=\"value for abstract\" />' in header_includes ) assert '<meta name=\"description\" content=\"value for abstract\" />' in header_includes # test that if neither abstract nor description is defined, neither are inserted del variables [ \"pandoc\" ][ \"abstract\" ] header_includes = get_header_includes ( copy . deepcopy ( variables )) assert 'meta name=\"citation_abstract\"' not in header_includes assert 'meta name=\"description\"' not in header_includes ci_params = get_continuous_integration_parameters () or {} local_only = pytest . mark . skipif ( ci_params , reason = \"skipping on CI build since test assumes local behavior\" ) ci_only = pytest . mark . skipif ( not ci_params , reason = \"skipping on local build since test assumes supported CI behavior\" , ) repo_raw_url_template = ( f \"https://github.com/ { ci_params . get ( 'repo_slug' , '' ) } \" f \"/raw/ { ci_params . get ( 'triggering_commit' , '' ) } /\" ) example_thumbnail_url = ( repo_raw_url_template + \"manubot/process/tests/manuscripts/example/thumbnail.png\" ) @pytest . mark . parametrize ( ( \"thumbnail\" , \"expected\" ), [ pytest . param ( None , None , id = \"None-on-local\" , marks = local_only ), pytest . param ( \"\" , None , id = \"empty-on-local\" , marks = local_only ), pytest . param ( None , example_thumbnail_url , id = \"None-on-ci\" , marks = ci_only ), pytest . param ( \"\" , example_thumbnail_url , id = \"empty-on-ci\" , marks = ci_only ), pytest . param ( \"path/to/thumbnail.png\" , None , id = \"local-path-on-local\" , marks = local_only ), pytest . param ( \"path/to/thumbnail.png\" , repo_raw_url_template + \"path/to/thumbnail.png\" , id = \"local-path-on-ci\" , marks = ci_only , ), pytest . param ( \"http://example.com/thumbnail.png\" , \"http://example.com/thumbnail.png\" , id = \"url-http\" , ), pytest . param ( \"https://example.com/thumbnail.png\" , \"https://example.com/thumbnail.png\" , id = \"url-https\" , ), ], ) def test_get_thumbnail_url ( thumbnail , expected ): thumbnail_url = get_thumbnail_url ( thumbnail ) assert expected == thumbnail_url @pytest . mark . parametrize ( ( \"html_url\" , \"expected\" ), [ pytest . param ( None , {}, id = \"html_url-none-local\" , marks = local_only ), pytest . param ( \"https://example.com/manuscript/\" , { \"html_url\" : \"https://example.com/manuscript/\" , \"pdf_url\" : \"https://example.com/manuscript/manuscript.pdf\" , }, id = \"html_url-set-local\" , marks = local_only , ), pytest . param ( None , { \"html_url\" : \"https://manubot.github.io/manubot/\" , \"pdf_url\" : \"https://manubot.github.io/manubot/manuscript.pdf\" , \"html_url_versioned\" : f \"https://manubot.github.io/manubot/v/ { ci_params . get ( 'commit' ) } /\" , \"pdf_url_versioned\" : f \"https://manubot.github.io/manubot/v/ { ci_params . get ( 'commit' ) } /manuscript.pdf\" , }, id = \"html_url-none-ci\" , marks = ci_only , ), pytest . param ( \"https://example.com/manuscript/\" , { \"html_url\" : \"https://example.com/manuscript/\" , \"pdf_url\" : \"https://example.com/manuscript/manuscript.pdf\" , \"html_url_versioned\" : f \"https://example.com/manuscript/v/ { ci_params . get ( 'commit' ) } /\" , \"pdf_url_versioned\" : f \"https://example.com/manuscript/v/ { ci_params . get ( 'commit' ) } /manuscript.pdf\" , }, id = \"html_url-set-ci\" , marks = ci_only , ), ], ) def test_get_manuscript_urls ( html_url , expected ): urls = get_manuscript_urls ( html_url ) assert urls == expected","title":"Module manubot.process.tests.test_metadata"},{"location":"reference/manubot/process/tests/test_metadata/#variables","text":"ci_only ci_params example_thumbnail_url local_only repo_raw_url_template","title":"Variables"},{"location":"reference/manubot/process/tests/test_metadata/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/tests/test_metadata/#test_get_header_includes_description_abstract","text":"def test_get_header_includes_description_abstract ( ) View Source def test_get_header_includes_description_abstract (): # test that abstract and description set different fields if both supplied variables = { \"pandoc\" : { \"abstract\" : \"value for abstract\" }, \"manubot\" : { \"description\" : \"value for description\" }, } header_includes = get_header_includes ( copy . deepcopy ( variables )) assert ( '<meta name=\"citation_abstract\" content=\"value for abstract\" />' in header_includes ) assert ( '<meta name=\"description\" content=\"value for description\" />' in header_includes ) # test that abstract is used for description if description is not defined del variables [ \"manubot\" ][ \"description\" ] header_includes = get_header_includes ( copy . deepcopy ( variables )) assert ( '<meta name=\"citation_abstract\" content=\"value for abstract\" />' in header_includes ) assert '<meta name=\"description\" content=\"value for abstract\" />' in header_includes # test that if neither abstract nor description is defined, neither are inserted del variables [ \"pandoc\" ][ \"abstract\" ] header_includes = get_header_includes ( copy . deepcopy ( variables )) assert 'meta name=\"citation_abstract\"' not in header_includes assert 'meta name=\"description\"' not in header_includes","title":"test_get_header_includes_description_abstract"},{"location":"reference/manubot/process/tests/test_metadata/#test_get_manuscript_urls","text":"def test_get_manuscript_urls ( html_url , expected ) View Source @pytest . mark . parametrize ( ( \"html_url\" , \"expected\" ), [ pytest.param(None, {}, id=\"html_url-none-local\", marks=local_only), pytest.param( \"https://example.com/manuscript/\", { \"html_url\": \"https://example.com/manuscript/\", \"pdf_url\": \"https://example.com/manuscript/manuscript.pdf\", }, id=\"html_url-set-local\", marks=local_only, ), pytest.param( None, { \"html_url\": \"https://manubot.github.io/manubot/\", \"pdf_url\": \"https://manubot.github.io/manubot/manuscript.pdf\", \"html_url_versioned\": f\"https://manubot.github.io/manubot/v/{ci_params.get('commit')}/\", \"pdf_url_versioned\": f\"https://manubot.github.io/manubot/v/{ci_params.get('commit')}/manuscript.pdf\", }, id=\"html_url-none-ci\", marks=ci_only, ), pytest.param( \"https://example.com/manuscript/\", { \"html_url\": \"https://example.com/manuscript/\", \"pdf_url\": \"https://example.com/manuscript/manuscript.pdf\", \"html_url_versioned\": f\"https://example.com/manuscript/v/{ci_params.get('commit')}/\", \"pdf_url_versioned\": f\"https://example.com/manuscript/v/{ci_params.get('commit')}/manuscript.pdf\", }, id=\"html_url-set-ci\", marks=ci_only, ), ] , ) def test_get_manuscript_urls ( html_url , expected ) : urls = get_manuscript_urls ( html_url ) assert urls == expected","title":"test_get_manuscript_urls"},{"location":"reference/manubot/process/tests/test_metadata/#test_get_thumbnail_url","text":"def test_get_thumbnail_url ( thumbnail , expected ) View Source @pytest . mark . parametrize ( ( \"thumbnail\" , \"expected\" ), [ pytest.param(None, None, id=\"None-on-local\", marks=local_only), pytest.param(\"\", None, id=\"empty-on-local\", marks=local_only), pytest.param(None, example_thumbnail_url, id=\"None-on-ci\", marks=ci_only), pytest.param(\"\", example_thumbnail_url, id=\"empty-on-ci\", marks=ci_only), pytest.param( \"path/to/thumbnail.png\", None, id=\"local-path-on-local\", marks=local_only ), pytest.param( \"path/to/thumbnail.png\", repo_raw_url_template + \"path/to/thumbnail.png\", id=\"local-path-on-ci\", marks=ci_only, ), pytest.param( \"http://example.com/thumbnail.png\", \"http://example.com/thumbnail.png\", id=\"url-http\", ), pytest.param( \"https://example.com/thumbnail.png\", \"https://example.com/thumbnail.png\", id=\"url-https\", ), ] , ) def test_get_thumbnail_url ( thumbnail , expected ) : thumbnail_url = get_thumbnail_url ( thumbnail ) assert expected == thumbnail_url","title":"test_get_thumbnail_url"},{"location":"reference/manubot/process/tests/test_process_command/","text":"Module manubot.process.tests.test_process_command None None View Source import pathlib import subprocess import pytest from manubot.util import shlex_join directory = pathlib . Path ( __file__ ) . parent . resolve () # List of manuscripts for testing. All subdirectories of ./manuscripts manuscripts = [ path . name for path in directory . joinpath ( \"manuscripts\" ) . iterdir () if path . is_dir () ] @pytest . mark . integration @pytest . mark . parametrize ( \"manuscript\" , manuscripts ) def test_example_manuscript ( manuscript ): \"\"\" Test command line execution of manubot to build an example manuscript. \"\"\" manuscript_dir = directory . joinpath ( \"manuscripts\" , manuscript ) args = [ \"manubot\" , \"process\" , \"--log-level\" , \"INFO\" , \"--skip-citations\" , \"--content-directory\" , str ( manuscript_dir . joinpath ( \"content\" )), \"--output-directory\" , str ( manuscript_dir . joinpath ( \"output\" )), ] if manuscript == \"variables\" : args . extend ( [ \"--template-variables-path\" , str ( manuscript_dir . joinpath ( \"content/template-variables.json\" )), ] ) process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" ) print ( shlex_join ( process . args )) print ( process . stderr ) assert process . returncode == 0 Variables directory manuscripts Functions test_example_manuscript def test_example_manuscript ( manuscript ) Test command line execution of manubot to build an example manuscript. View Source @ pytest . mark . integration @ pytest . mark . parametrize ( \"manuscript\" , manuscripts ) def test_example_manuscript ( manuscript ): \"\"\" Test command line execution of manubot to build an example manuscript. \"\"\" manuscript_dir = directory . joinpath ( \"manuscripts\" , manuscript ) args = [ \"manubot\" , \"process\" , \"--log-level\" , \"INFO\" , \"--skip-citations\" , \"--content-directory\" , str ( manuscript_dir . joinpath ( \"content\" )), \"--output-directory\" , str ( manuscript_dir . joinpath ( \"output\" )), ] if manuscript == \"variables\" : args . extend ( [ \"--template-variables-path\" , str ( manuscript_dir . joinpath ( \"content/template-variables.json\" )), ] ) process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" ) print ( shlex_join ( process . args )) print ( process . stderr ) assert process . returncode == 0","title":"Test Process Command"},{"location":"reference/manubot/process/tests/test_process_command/#module-manubotprocessteststest_process_command","text":"None None View Source import pathlib import subprocess import pytest from manubot.util import shlex_join directory = pathlib . Path ( __file__ ) . parent . resolve () # List of manuscripts for testing. All subdirectories of ./manuscripts manuscripts = [ path . name for path in directory . joinpath ( \"manuscripts\" ) . iterdir () if path . is_dir () ] @pytest . mark . integration @pytest . mark . parametrize ( \"manuscript\" , manuscripts ) def test_example_manuscript ( manuscript ): \"\"\" Test command line execution of manubot to build an example manuscript. \"\"\" manuscript_dir = directory . joinpath ( \"manuscripts\" , manuscript ) args = [ \"manubot\" , \"process\" , \"--log-level\" , \"INFO\" , \"--skip-citations\" , \"--content-directory\" , str ( manuscript_dir . joinpath ( \"content\" )), \"--output-directory\" , str ( manuscript_dir . joinpath ( \"output\" )), ] if manuscript == \"variables\" : args . extend ( [ \"--template-variables-path\" , str ( manuscript_dir . joinpath ( \"content/template-variables.json\" )), ] ) process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" ) print ( shlex_join ( process . args )) print ( process . stderr ) assert process . returncode == 0","title":"Module manubot.process.tests.test_process_command"},{"location":"reference/manubot/process/tests/test_process_command/#variables","text":"directory manuscripts","title":"Variables"},{"location":"reference/manubot/process/tests/test_process_command/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/tests/test_process_command/#test_example_manuscript","text":"def test_example_manuscript ( manuscript ) Test command line execution of manubot to build an example manuscript. View Source @ pytest . mark . integration @ pytest . mark . parametrize ( \"manuscript\" , manuscripts ) def test_example_manuscript ( manuscript ): \"\"\" Test command line execution of manubot to build an example manuscript. \"\"\" manuscript_dir = directory . joinpath ( \"manuscripts\" , manuscript ) args = [ \"manubot\" , \"process\" , \"--log-level\" , \"INFO\" , \"--skip-citations\" , \"--content-directory\" , str ( manuscript_dir . joinpath ( \"content\" )), \"--output-directory\" , str ( manuscript_dir . joinpath ( \"output\" )), ] if manuscript == \"variables\" : args . extend ( [ \"--template-variables-path\" , str ( manuscript_dir . joinpath ( \"content/template-variables.json\" )), ] ) process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" ) print ( shlex_join ( process . args )) print ( process . stderr ) assert process . returncode == 0","title":"test_example_manuscript"},{"location":"reference/manubot/process/tests/test_util/","text":"Module manubot.process.tests.test_util None None View Source import pathlib import pytest from ..util import add_author_affiliations , read_variable_files directory = pathlib . Path ( __file__ ) . parent . resolve () def test_read_variable_files_empty (): paths = [] user_variables = read_variable_files ( paths ) assert isinstance ( user_variables , dict ) and not user_variables def test_read_variable_files (): \"\"\" Test reading multiple JSON files, from both local paths and URLs. \"\"\" local_path = \"manuscripts/variables/content/template-variables.json\" local_path = directory . joinpath ( local_path ) paths = [ \"https://git.io/vbkqm\" , \"https://git.io/vbkqm\" , \"namespace_1=https://git.io/vbkqm\" , \"namespace_2=https://git.io/vbkqm\" , f \"namespace_2= { local_path } \" , f \"namespace_3= { local_path } \" , ] user_variables = read_variable_files ( paths ) assert \"namespace_1\" in user_variables assert \"namespace_2\" in user_variables assert \"namespace_3\" in user_variables assert user_variables [ \"generated_by\" ] == \"Manubot\" assert \"violet\" in user_variables [ \"namespace_1\" ][ \"rainbow\" ] assert \"yellow\" in user_variables [ \"namespace_2\" ][ \"rainbow\" ] assert \"orange\" in user_variables [ \"namespace_3\" ][ \"rainbow\" ] def test_add_author_affiliations_empty (): variables = {} variables [ \"authors\" ] = [{ \"name\" : \"Jane Roe\" }, { \"name\" : \"John Doe\" }] returned_variables = add_author_affiliations ( variables ) assert variables is returned_variables assert \"affiliations\" not in variables for author in variables [ \"authors\" ]: assert \"affiliation_numbers\" not in author def test_add_author_affiliations (): variables = {} variables [ \"authors\" ] = [ # Deprecated affiliations format (as a string that's `; ` separated) { \"name\" : \"Jane Roe\" , \"affiliations\" : \"Department of Doe, University of Roe; Peppertea University\" , }, # Prefered affiliations format as a list { \"name\" : \"John Doe\" , \"affiliations\" : [ \"Unique University\" , \"Peppertea University\" ], }, ] with pytest . warns ( DeprecationWarning ): returned_variables = add_author_affiliations ( variables ) assert variables is returned_variables assert variables [ \"affiliations\" ] == [ { \"affiliation\" : \"Department of Doe, University of Roe\" , \"affiliation_number\" : 1 , }, { \"affiliation\" : \"Peppertea University\" , \"affiliation_number\" : 2 }, { \"affiliation\" : \"Unique University\" , \"affiliation_number\" : 3 }, ] authors = variables [ \"authors\" ] assert authors [ 0 ][ \"affiliations\" ] == [ \"Department of Doe, University of Roe\" , \"Peppertea University\" , ] assert authors [ 0 ][ \"affiliation_numbers\" ] == [ 1 , 2 ] assert authors [ 1 ][ \"affiliation_numbers\" ] == [ 2 , 3 ] Variables directory Functions test_add_author_affiliations def test_add_author_affiliations ( ) View Source def test_add_author_affiliations (): variables = {} variables [ \"authors\" ] = [ # Deprecated affiliations format (as a string that's `; ` separated) { \"name\" : \"Jane Roe\" , \"affiliations\" : \"Department of Doe, University of Roe; Peppertea University\" , }, # Prefered affiliations format as a list { \"name\" : \"John Doe\" , \"affiliations\" : [ \"Unique University\" , \"Peppertea University\" ], }, ] with pytest . warns ( DeprecationWarning ): returned_variables = add_author_affiliations ( variables ) assert variables is returned_variables assert variables [ \"affiliations\" ] == [ { \"affiliation\" : \"Department of Doe, University of Roe\" , \"affiliation_number\" : 1 , }, { \"affiliation\" : \"Peppertea University\" , \"affiliation_number\" : 2 }, { \"affiliation\" : \"Unique University\" , \"affiliation_number\" : 3 }, ] authors = variables [ \"authors\" ] assert authors [ 0 ][ \"affiliations\" ] == [ \"Department of Doe, University of Roe\" , \"Peppertea University\" , ] assert authors [ 0 ][ \"affiliation_numbers\" ] == [ 1 , 2 ] assert authors [ 1 ][ \"affiliation_numbers\" ] == [ 2 , 3 ] test_add_author_affiliations_empty def test_add_author_affiliations_empty ( ) View Source def test_add_author_affiliations_empty (): variables = {} variables [ \"authors\" ] = [{ \"name\" : \"Jane Roe\" }, { \"name\" : \"John Doe\" }] returned_variables = add_author_affiliations ( variables ) assert variables is returned_variables assert \"affiliations\" not in variables for author in variables [ \"authors\" ]: assert \"affiliation_numbers\" not in author test_read_variable_files def test_read_variable_files ( ) Test reading multiple JSON files, from both local paths and URLs. View Source def test_read_variable_files (): \"\"\" Test reading multiple JSON files, from both local paths and URLs. \"\"\" local_path = \"manuscripts/variables/content/template-variables.json\" local_path = directory . joinpath ( local_path ) paths = [ \"https://git.io/vbkqm\" , \"https://git.io/vbkqm\" , \"namespace_1=https://git.io/vbkqm\" , \"namespace_2=https://git.io/vbkqm\" , f \"namespace_2={local_path}\" , f \"namespace_3={local_path}\" , ] user_variables = read_variable_files ( paths ) assert \"namespace_1\" in user_variables assert \"namespace_2\" in user_variables assert \"namespace_3\" in user_variables assert user_variables [ \"generated_by\" ] == \"Manubot\" assert \"violet\" in user_variables [ \"namespace_1\" ][ \"rainbow\" ] assert \"yellow\" in user_variables [ \"namespace_2\" ][ \"rainbow\" ] assert \"orange\" in user_variables [ \"namespace_3\" ][ \"rainbow\" ] test_read_variable_files_empty def test_read_variable_files_empty ( ) View Source def test_read_variable_files_empty (): paths = [] user_variables = read_variable_files ( paths ) assert isinstance ( user_variables , dict ) and not user_variables","title":"Test Util"},{"location":"reference/manubot/process/tests/test_util/#module-manubotprocessteststest_util","text":"None None View Source import pathlib import pytest from ..util import add_author_affiliations , read_variable_files directory = pathlib . Path ( __file__ ) . parent . resolve () def test_read_variable_files_empty (): paths = [] user_variables = read_variable_files ( paths ) assert isinstance ( user_variables , dict ) and not user_variables def test_read_variable_files (): \"\"\" Test reading multiple JSON files, from both local paths and URLs. \"\"\" local_path = \"manuscripts/variables/content/template-variables.json\" local_path = directory . joinpath ( local_path ) paths = [ \"https://git.io/vbkqm\" , \"https://git.io/vbkqm\" , \"namespace_1=https://git.io/vbkqm\" , \"namespace_2=https://git.io/vbkqm\" , f \"namespace_2= { local_path } \" , f \"namespace_3= { local_path } \" , ] user_variables = read_variable_files ( paths ) assert \"namespace_1\" in user_variables assert \"namespace_2\" in user_variables assert \"namespace_3\" in user_variables assert user_variables [ \"generated_by\" ] == \"Manubot\" assert \"violet\" in user_variables [ \"namespace_1\" ][ \"rainbow\" ] assert \"yellow\" in user_variables [ \"namespace_2\" ][ \"rainbow\" ] assert \"orange\" in user_variables [ \"namespace_3\" ][ \"rainbow\" ] def test_add_author_affiliations_empty (): variables = {} variables [ \"authors\" ] = [{ \"name\" : \"Jane Roe\" }, { \"name\" : \"John Doe\" }] returned_variables = add_author_affiliations ( variables ) assert variables is returned_variables assert \"affiliations\" not in variables for author in variables [ \"authors\" ]: assert \"affiliation_numbers\" not in author def test_add_author_affiliations (): variables = {} variables [ \"authors\" ] = [ # Deprecated affiliations format (as a string that's `; ` separated) { \"name\" : \"Jane Roe\" , \"affiliations\" : \"Department of Doe, University of Roe; Peppertea University\" , }, # Prefered affiliations format as a list { \"name\" : \"John Doe\" , \"affiliations\" : [ \"Unique University\" , \"Peppertea University\" ], }, ] with pytest . warns ( DeprecationWarning ): returned_variables = add_author_affiliations ( variables ) assert variables is returned_variables assert variables [ \"affiliations\" ] == [ { \"affiliation\" : \"Department of Doe, University of Roe\" , \"affiliation_number\" : 1 , }, { \"affiliation\" : \"Peppertea University\" , \"affiliation_number\" : 2 }, { \"affiliation\" : \"Unique University\" , \"affiliation_number\" : 3 }, ] authors = variables [ \"authors\" ] assert authors [ 0 ][ \"affiliations\" ] == [ \"Department of Doe, University of Roe\" , \"Peppertea University\" , ] assert authors [ 0 ][ \"affiliation_numbers\" ] == [ 1 , 2 ] assert authors [ 1 ][ \"affiliation_numbers\" ] == [ 2 , 3 ]","title":"Module manubot.process.tests.test_util"},{"location":"reference/manubot/process/tests/test_util/#variables","text":"directory","title":"Variables"},{"location":"reference/manubot/process/tests/test_util/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/tests/test_util/#test_add_author_affiliations","text":"def test_add_author_affiliations ( ) View Source def test_add_author_affiliations (): variables = {} variables [ \"authors\" ] = [ # Deprecated affiliations format (as a string that's `; ` separated) { \"name\" : \"Jane Roe\" , \"affiliations\" : \"Department of Doe, University of Roe; Peppertea University\" , }, # Prefered affiliations format as a list { \"name\" : \"John Doe\" , \"affiliations\" : [ \"Unique University\" , \"Peppertea University\" ], }, ] with pytest . warns ( DeprecationWarning ): returned_variables = add_author_affiliations ( variables ) assert variables is returned_variables assert variables [ \"affiliations\" ] == [ { \"affiliation\" : \"Department of Doe, University of Roe\" , \"affiliation_number\" : 1 , }, { \"affiliation\" : \"Peppertea University\" , \"affiliation_number\" : 2 }, { \"affiliation\" : \"Unique University\" , \"affiliation_number\" : 3 }, ] authors = variables [ \"authors\" ] assert authors [ 0 ][ \"affiliations\" ] == [ \"Department of Doe, University of Roe\" , \"Peppertea University\" , ] assert authors [ 0 ][ \"affiliation_numbers\" ] == [ 1 , 2 ] assert authors [ 1 ][ \"affiliation_numbers\" ] == [ 2 , 3 ]","title":"test_add_author_affiliations"},{"location":"reference/manubot/process/tests/test_util/#test_add_author_affiliations_empty","text":"def test_add_author_affiliations_empty ( ) View Source def test_add_author_affiliations_empty (): variables = {} variables [ \"authors\" ] = [{ \"name\" : \"Jane Roe\" }, { \"name\" : \"John Doe\" }] returned_variables = add_author_affiliations ( variables ) assert variables is returned_variables assert \"affiliations\" not in variables for author in variables [ \"authors\" ]: assert \"affiliation_numbers\" not in author","title":"test_add_author_affiliations_empty"},{"location":"reference/manubot/process/tests/test_util/#test_read_variable_files","text":"def test_read_variable_files ( ) Test reading multiple JSON files, from both local paths and URLs. View Source def test_read_variable_files (): \"\"\" Test reading multiple JSON files, from both local paths and URLs. \"\"\" local_path = \"manuscripts/variables/content/template-variables.json\" local_path = directory . joinpath ( local_path ) paths = [ \"https://git.io/vbkqm\" , \"https://git.io/vbkqm\" , \"namespace_1=https://git.io/vbkqm\" , \"namespace_2=https://git.io/vbkqm\" , f \"namespace_2={local_path}\" , f \"namespace_3={local_path}\" , ] user_variables = read_variable_files ( paths ) assert \"namespace_1\" in user_variables assert \"namespace_2\" in user_variables assert \"namespace_3\" in user_variables assert user_variables [ \"generated_by\" ] == \"Manubot\" assert \"violet\" in user_variables [ \"namespace_1\" ][ \"rainbow\" ] assert \"yellow\" in user_variables [ \"namespace_2\" ][ \"rainbow\" ] assert \"orange\" in user_variables [ \"namespace_3\" ][ \"rainbow\" ]","title":"test_read_variable_files"},{"location":"reference/manubot/process/tests/test_util/#test_read_variable_files_empty","text":"def test_read_variable_files_empty ( ) View Source def test_read_variable_files_empty (): paths = [] user_variables = read_variable_files ( paths ) assert isinstance ( user_variables , dict ) and not user_variables","title":"test_read_variable_files_empty"},{"location":"reference/manubot/tests/","text":"Module manubot.tests None None Sub-modules manubot.tests.test_command manubot.tests.test_imports manubot.tests.test_readme manubot.tests.test_util","title":"Index"},{"location":"reference/manubot/tests/#module-manubottests","text":"None None","title":"Module manubot.tests"},{"location":"reference/manubot/tests/#sub-modules","text":"manubot.tests.test_command manubot.tests.test_imports manubot.tests.test_readme manubot.tests.test_util","title":"Sub-modules"},{"location":"reference/manubot/tests/test_command/","text":"Module manubot.tests.test_command None None View Source import subprocess import manubot def test_version (): stdout = subprocess . check_output ([ \"manubot\" , \"--version\" ], universal_newlines = True ) version_str = f \"v { manubot . __version__ } \" assert version_str == stdout . rstrip () def test_missing_subcommand (): process = subprocess . run ( [ \"manubot\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ) print ( process . stderr ) assert process . returncode == 2 assert \"error: the following arguments are required: subcommand\" in process . stderr Functions test_missing_subcommand def test_missing_subcommand ( ) View Source def test_missing_subcommand(): process = subprocess.run( [\"manubot\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding=\"utf-8\", ) print(process.stderr) assert process.returncode == 2 assert \"error: the following arguments are required: subcommand\" in process.stderr test_version def test_version ( ) View Source def test_version(): stdout = subprocess.check_output([\"manubot\", \"--version\"], universal_newlines=True) version_str = f\"v{manubot.__version__}\" assert version_str == stdout.rstrip()","title":"Test Command"},{"location":"reference/manubot/tests/test_command/#module-manubotteststest_command","text":"None None View Source import subprocess import manubot def test_version (): stdout = subprocess . check_output ([ \"manubot\" , \"--version\" ], universal_newlines = True ) version_str = f \"v { manubot . __version__ } \" assert version_str == stdout . rstrip () def test_missing_subcommand (): process = subprocess . run ( [ \"manubot\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ) print ( process . stderr ) assert process . returncode == 2 assert \"error: the following arguments are required: subcommand\" in process . stderr","title":"Module manubot.tests.test_command"},{"location":"reference/manubot/tests/test_command/#functions","text":"","title":"Functions"},{"location":"reference/manubot/tests/test_command/#test_missing_subcommand","text":"def test_missing_subcommand ( ) View Source def test_missing_subcommand(): process = subprocess.run( [\"manubot\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding=\"utf-8\", ) print(process.stderr) assert process.returncode == 2 assert \"error: the following arguments are required: subcommand\" in process.stderr","title":"test_missing_subcommand"},{"location":"reference/manubot/tests/test_command/#test_version","text":"def test_version ( ) View Source def test_version(): stdout = subprocess.check_output([\"manubot\", \"--version\"], universal_newlines=True) version_str = f\"v{manubot.__version__}\" assert version_str == stdout.rstrip()","title":"test_version"},{"location":"reference/manubot/tests/test_imports/","text":"Module manubot.tests.test_imports None None View Source def test_imports (): import manubot.cite import manubot.cite.arxiv import manubot.cite.doi import manubot.cite.pubmed import manubot.cite.url import manubot.process.manuscript manubot Functions test_imports def test_imports ( ) View Source def test_imports (): import manubot.cite import manubot.cite.arxiv import manubot.cite.doi import manubot.cite.pubmed import manubot.cite.url import manubot.process.manuscript manubot","title":"Test Imports"},{"location":"reference/manubot/tests/test_imports/#module-manubotteststest_imports","text":"None None View Source def test_imports (): import manubot.cite import manubot.cite.arxiv import manubot.cite.doi import manubot.cite.pubmed import manubot.cite.url import manubot.process.manuscript manubot","title":"Module manubot.tests.test_imports"},{"location":"reference/manubot/tests/test_imports/#functions","text":"","title":"Functions"},{"location":"reference/manubot/tests/test_imports/#test_imports","text":"def test_imports ( ) View Source def test_imports (): import manubot.cite import manubot.cite.arxiv import manubot.cite.doi import manubot.cite.pubmed import manubot.cite.url import manubot.process.manuscript manubot","title":"test_imports"},{"location":"reference/manubot/tests/test_readme/","text":"Module manubot.tests.test_readme None None View Source import pathlib import re import shlex import subprocess import pytest readme_path = pathlib . Path ( __file__ ) . parent . parent . parent / \"README.md\" readme = readme_path . read_text ( encoding = \"utf-8-sig\" ) template = r \"\"\" <!-- test codeblock contains output of ` {command} ` --> ``` {output} ``` \"\"\" pattern = template . format ( command = r \"(?P<command>.+?)\" , output = r \"(?P<output>.+?)\" ) pattern = re . compile ( pattern , re . DOTALL ) matches = list ( pattern . finditer ( readme )) @pytest . mark . parametrize ( argnames = [ \"command\" , \"expected\" ], argvalues = [ match . groups () for match in matches ], ids = [ match . group ( \"command\" ) for match in matches ], ) def test_readme_codeblock_contains_output_from ( command , expected ): \"\"\" If this test fails, ensure that codeblocks in README.md have the correct output. To enable this check for output in a codeblock, use the following construct: <!-- test codeblock contains output of `{command}` --> ``` {expected} ``` \"\"\" output = _get_output_from ( command ) assert output == expected def _get_output_from ( command ): return subprocess . check_output ( shlex . split ( command ), encoding = \"utf-8\" ) def _match_to_repl ( match ): template_dict = match . groupdict () template_dict [ \"output\" ] = _get_output_from ( template_dict [ \"command\" ]) return template . format ( ** template_dict ) if __name__ == \"__main__\" : \"\"\" Run `python manubot/tests/test_readme.py` to populate README codeblocks with output from the specified commands. \"\"\" repl_readme = pattern . sub ( repl = _match_to_repl , string = readme ) readme_path . write_text ( repl_readme , encoding = \"utf-8\" ) Variables matches pattern readme readme_path template Functions test_readme_codeblock_contains_output_from def test_readme_codeblock_contains_output_from ( command , expected ) If this test fails, ensure that codeblocks in README.md have the correct output. To enable this check for output in a codeblock, use the following construct: {expected} View Source @ pytest . mark . parametrize ( argnames = [ \"command\" , \"expected\" ], argvalues = [ match . groups () for match in matches ], ids = [ match . group ( \"command\" ) for match in matches ], ) def test_readme_codeblock_contains_output_from ( command , expected ): \"\"\" If this test fails, ensure that codeblocks in README.md have the correct output. To enable this check for output in a codeblock, use the following construct: <!-- test codeblock contains output of `{command}` --> ``` {expected} ``` \"\"\" output = _get_output_from ( command ) assert output == expected","title":"Test Readme"},{"location":"reference/manubot/tests/test_readme/#module-manubotteststest_readme","text":"None None View Source import pathlib import re import shlex import subprocess import pytest readme_path = pathlib . Path ( __file__ ) . parent . parent . parent / \"README.md\" readme = readme_path . read_text ( encoding = \"utf-8-sig\" ) template = r \"\"\" <!-- test codeblock contains output of ` {command} ` --> ``` {output} ``` \"\"\" pattern = template . format ( command = r \"(?P<command>.+?)\" , output = r \"(?P<output>.+?)\" ) pattern = re . compile ( pattern , re . DOTALL ) matches = list ( pattern . finditer ( readme )) @pytest . mark . parametrize ( argnames = [ \"command\" , \"expected\" ], argvalues = [ match . groups () for match in matches ], ids = [ match . group ( \"command\" ) for match in matches ], ) def test_readme_codeblock_contains_output_from ( command , expected ): \"\"\" If this test fails, ensure that codeblocks in README.md have the correct output. To enable this check for output in a codeblock, use the following construct: <!-- test codeblock contains output of `{command}` --> ``` {expected} ``` \"\"\" output = _get_output_from ( command ) assert output == expected def _get_output_from ( command ): return subprocess . check_output ( shlex . split ( command ), encoding = \"utf-8\" ) def _match_to_repl ( match ): template_dict = match . groupdict () template_dict [ \"output\" ] = _get_output_from ( template_dict [ \"command\" ]) return template . format ( ** template_dict ) if __name__ == \"__main__\" : \"\"\" Run `python manubot/tests/test_readme.py` to populate README codeblocks with output from the specified commands. \"\"\" repl_readme = pattern . sub ( repl = _match_to_repl , string = readme ) readme_path . write_text ( repl_readme , encoding = \"utf-8\" )","title":"Module manubot.tests.test_readme"},{"location":"reference/manubot/tests/test_readme/#variables","text":"matches pattern readme readme_path template","title":"Variables"},{"location":"reference/manubot/tests/test_readme/#functions","text":"","title":"Functions"},{"location":"reference/manubot/tests/test_readme/#test_readme_codeblock_contains_output_from","text":"def test_readme_codeblock_contains_output_from ( command , expected ) If this test fails, ensure that codeblocks in README.md have the correct output. To enable this check for output in a codeblock, use the following construct: {expected} View Source @ pytest . mark . parametrize ( argnames = [ \"command\" , \"expected\" ], argvalues = [ match . groups () for match in matches ], ids = [ match . group ( \"command\" ) for match in matches ], ) def test_readme_codeblock_contains_output_from ( command , expected ): \"\"\" If this test fails, ensure that codeblocks in README.md have the correct output. To enable this check for output in a codeblock, use the following construct: <!-- test codeblock contains output of `{command}` --> ``` {expected} ``` \"\"\" output = _get_output_from ( command ) assert output == expected","title":"test_readme_codeblock_contains_output_from"},{"location":"reference/manubot/tests/test_util/","text":"Module manubot.tests.test_util None None View Source import pytest import manubot.util def test_shlex_join (): import pathlib args = [ \"command\" , \"positional arg\" , \"path_arg\" , pathlib . Path ( \"path\" )] output = manubot . util . shlex_join ( args ) assert output == \"command 'positional arg' path_arg path\" raw_repo_url = ( \"https://github.com/manubot/manubot/raw/ebac7abd754015a5ec24a6fff39c35a72d4dffb0/\" ) raw_manuscript_url = f \" { raw_repo_url } manubot/process/tests/manuscripts/example/\" def test_read_serialized_data_url_yaml (): url = raw_manuscript_url + \"content/metadata.yaml\" obj = manubot . util . read_serialized_data ( url ) assert obj [ \"title\" ] == \"Example manuscript for testing\" obj = manubot . util . read_serialized_dict ( url ) assert obj [ \"title\" ] == \"Example manuscript for testing\" def test_read_serialized_data_url_json (): url = raw_manuscript_url + \"content/manual-references.json\" obj = manubot . util . read_serialized_data ( url ) assert obj [ 0 ][ \"container-title\" ] == \"Engineuring\" with pytest . raises ( TypeError , match = \"Received 'list' instead\" ): manubot . util . read_serialized_dict ( url ) def test_read_serialized_dict_url_toml (): url = raw_repo_url + \"pyproject.toml\" obj = manubot . util . read_serialized_dict ( url ) assert \"black\" in obj [ \"tool\" ] Variables raw_manuscript_url raw_repo_url Functions test_read_serialized_data_url_json def test_read_serialized_data_url_json ( ) View Source def test_read_serialized_data_url_json(): url = raw_manuscript_url + \"content/manual-references.json\" obj = manubot.util.read_serialized_data(url) assert obj[0][\"container-title\"] == \"Engineuring\" with pytest.raises(TypeError, match=\"Received 'list' instead\"): manubot.util.read_serialized_dict(url) test_read_serialized_data_url_yaml def test_read_serialized_data_url_yaml ( ) View Source def test_read_serialized_data_url_yaml () : url = raw_manuscript_url + \" content/metadata.yaml \" obj = manubot . util . read_serialized_data ( url ) assert obj [ \" title \" ] == \" Example manuscript for testing \" obj = manubot . util . read_serialized_dict ( url ) assert obj [ \" title \" ] == \" Example manuscript for testing \" test_read_serialized_dict_url_toml def test_read_serialized_dict_url_toml ( ) View Source def test_read_serialized_dict_url_toml (): url = raw_repo_url + \"pyproject.toml\" obj = manubot . util . read_serialized_dict ( url ) assert \"black\" in obj [ \"tool\" ] test_shlex_join def test_shlex_join ( ) View Source def test_shlex_join (): import pathlib args = [ \"command\" , \"positional arg\" , \"path_arg\" , pathlib . Path ( \"path\" )] output = manubot . util . shlex_join ( args ) assert output == \"command 'positional arg' path_arg path\"","title":"Test Util"},{"location":"reference/manubot/tests/test_util/#module-manubotteststest_util","text":"None None View Source import pytest import manubot.util def test_shlex_join (): import pathlib args = [ \"command\" , \"positional arg\" , \"path_arg\" , pathlib . Path ( \"path\" )] output = manubot . util . shlex_join ( args ) assert output == \"command 'positional arg' path_arg path\" raw_repo_url = ( \"https://github.com/manubot/manubot/raw/ebac7abd754015a5ec24a6fff39c35a72d4dffb0/\" ) raw_manuscript_url = f \" { raw_repo_url } manubot/process/tests/manuscripts/example/\" def test_read_serialized_data_url_yaml (): url = raw_manuscript_url + \"content/metadata.yaml\" obj = manubot . util . read_serialized_data ( url ) assert obj [ \"title\" ] == \"Example manuscript for testing\" obj = manubot . util . read_serialized_dict ( url ) assert obj [ \"title\" ] == \"Example manuscript for testing\" def test_read_serialized_data_url_json (): url = raw_manuscript_url + \"content/manual-references.json\" obj = manubot . util . read_serialized_data ( url ) assert obj [ 0 ][ \"container-title\" ] == \"Engineuring\" with pytest . raises ( TypeError , match = \"Received 'list' instead\" ): manubot . util . read_serialized_dict ( url ) def test_read_serialized_dict_url_toml (): url = raw_repo_url + \"pyproject.toml\" obj = manubot . util . read_serialized_dict ( url ) assert \"black\" in obj [ \"tool\" ]","title":"Module manubot.tests.test_util"},{"location":"reference/manubot/tests/test_util/#variables","text":"raw_manuscript_url raw_repo_url","title":"Variables"},{"location":"reference/manubot/tests/test_util/#functions","text":"","title":"Functions"},{"location":"reference/manubot/tests/test_util/#test_read_serialized_data_url_json","text":"def test_read_serialized_data_url_json ( ) View Source def test_read_serialized_data_url_json(): url = raw_manuscript_url + \"content/manual-references.json\" obj = manubot.util.read_serialized_data(url) assert obj[0][\"container-title\"] == \"Engineuring\" with pytest.raises(TypeError, match=\"Received 'list' instead\"): manubot.util.read_serialized_dict(url)","title":"test_read_serialized_data_url_json"},{"location":"reference/manubot/tests/test_util/#test_read_serialized_data_url_yaml","text":"def test_read_serialized_data_url_yaml ( ) View Source def test_read_serialized_data_url_yaml () : url = raw_manuscript_url + \" content/metadata.yaml \" obj = manubot . util . read_serialized_data ( url ) assert obj [ \" title \" ] == \" Example manuscript for testing \" obj = manubot . util . read_serialized_dict ( url ) assert obj [ \" title \" ] == \" Example manuscript for testing \"","title":"test_read_serialized_data_url_yaml"},{"location":"reference/manubot/tests/test_util/#test_read_serialized_dict_url_toml","text":"def test_read_serialized_dict_url_toml ( ) View Source def test_read_serialized_dict_url_toml (): url = raw_repo_url + \"pyproject.toml\" obj = manubot . util . read_serialized_dict ( url ) assert \"black\" in obj [ \"tool\" ]","title":"test_read_serialized_dict_url_toml"},{"location":"reference/manubot/tests/test_util/#test_shlex_join","text":"def test_shlex_join ( ) View Source def test_shlex_join (): import pathlib args = [ \"command\" , \"positional arg\" , \"path_arg\" , pathlib . Path ( \"path\" )] output = manubot . util . shlex_join ( args ) assert output == \"command 'positional arg' path_arg path\"","title":"test_shlex_join"},{"location":"reference/manubot/webpage/","text":"Module manubot.webpage None None Sub-modules manubot.webpage.webpage_command","title":"Index"},{"location":"reference/manubot/webpage/#module-manubotwebpage","text":"None None","title":"Module manubot.webpage"},{"location":"reference/manubot/webpage/#sub-modules","text":"manubot.webpage.webpage_command","title":"Sub-modules"},{"location":"reference/manubot/webpage/webpage_command/","text":"Module manubot.webpage.webpage_command None None View Source import logging import pathlib import shutil import subprocess from manubot.util import shlex_join def cli_webpage ( args ): \"\"\" Execute manubot webpage commands. args should be an argparse.Namespace object created by parser.parse_args. \"\"\" configure_args ( args ) logging . debug ( f \"Running `manubot webpage` with the following args: \\n { args } \" ) if args . timestamp : ots_upgrade ( args ) create_version ( args ) def configure_args ( args ): \"\"\" Perform additional processing of arguments that is not handled by argparse. Derive additional variables and add them to args. For example, add directories to args and create them if neccessary. Note that versions_directory is the parent of version_directory. \"\"\" args_dict = vars ( args ) # If --timestamp specified, check that opentimestamps-client is installed if args . timestamp : ots_executable_path = shutil . which ( \"ots\" ) if not ots_executable_path : logging . error ( \"manubot webpage --timestamp was specified but opentimestamps-client not found on system. \" \"Setting --timestamp=False. \" \"Fix this by installing https://pypi.org/project/opentimestamps-client/\" ) args_dict [ \"timestamp\" ] = False # Directory where Manubot outputs reside args_dict [ \"output_directory\" ] = pathlib . Path ( \"output\" ) # Set webpage directory args_dict [ \"webpage_directory\" ] = pathlib . Path ( \"webpage\" ) args . webpage_directory . mkdir ( exist_ok = True ) # Create webpage/v directory (if it doesn't already exist) args_dict [ \"versions_directory\" ] = args . webpage_directory . joinpath ( \"v\" ) args . versions_directory . mkdir ( exist_ok = True ) # Checkout existing version directories checkout_existing_versions ( args ) # Apply --version argument defaults if args . version is None : from manubot.process.ci import get_continuous_integration_parameters ci_params = get_continuous_integration_parameters () if ci_params : args_dict [ \"version\" ] = ci_params . get ( \"commit\" , \"local\" ) else : args_dict [ \"version\" ] = \"local\" # Create empty webpage/v/version directory version_directory = args . versions_directory . joinpath ( args . version ) if version_directory . is_dir (): logging . warning ( f \" { version_directory } exists: replacing it with an empty directory\" ) shutil . rmtree ( version_directory ) version_directory . mkdir () args_dict [ \"version_directory\" ] = version_directory # Symlink webpage/v/latest to point to webpage/v/commit latest_directory = args . versions_directory . joinpath ( \"latest\" ) if latest_directory . is_symlink () or latest_directory . is_file (): latest_directory . unlink () elif latest_directory . is_dir (): shutil . rmtree ( latest_directory ) latest_directory . symlink_to ( args . version , target_is_directory = True ) args_dict [ \"latest_directory\" ] = latest_directory # Create freeze directory freeze_directory = args . versions_directory . joinpath ( \"freeze\" ) freeze_directory . mkdir ( exist_ok = True ) args_dict [ \"freeze_directory\" ] = freeze_directory return args def checkout_existing_versions ( args ): \"\"\" Must populate webpage/v from the gh-pages branch to get history References: http://clubmate.fi/git-checkout-file-or-directories-from-another-branch/ https://stackoverflow.com/a/2668947/4651668 https://stackoverflow.com/a/16493707/4651668 Command modeled after: git --work-tree=webpage checkout upstream/gh-pages -- v \"\"\" if not args . checkout : return command = [ \"git\" , f \"--work-tree= { args . webpage_directory } \" , \"checkout\" , args . checkout , \"--\" , \"v\" , ] logging . info ( f \"Attempting checkout with the following command: \\n { shlex_join ( command ) } \" ) process = subprocess . run ( command , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) if process . returncode == 0 : # Addresses an odd behavior where git checkout stages v/* files that don't actually exist subprocess . run ([ \"git\" , \"add\" , \"v\" ], stdout = subprocess . PIPE ) else : output = process . stdout . decode () message = ( f \"Checkout returned a nonzero exit status. See output: \\n { output . rstrip () } \" ) if \"pathspec\" in output : message += ( \" \\n Manubot note: if there are no preexisting webpage versions (like for a newly created manuscript), \" \"the pathspec error above is expected and can be safely ignored.\" ) # see https://github.com/manubot/rootstock/issues/183 logging . warning ( message ) def create_version ( args ): \"\"\" Populate the version directory for a new version. \"\"\" # Copy content/images to webpage/v/commit/images images_src = pathlib . Path ( \"content/images\" ) if images_src . exists (): shutil . copytree ( src = images_src , dst = args . version_directory . joinpath ( \"images\" )) # Copy output files to to webpage/v/version/ renamer = { \"manuscript.html\" : \"index.html\" , \"manuscript.pdf\" : \"manuscript.pdf\" } for src , dst in renamer . items (): src_path = args . output_directory . joinpath ( src ) if not src_path . exists (): continue dst_path = args . version_directory . joinpath ( dst ) shutil . copy2 ( src = src_path , dst = dst_path ) if args . timestamp : ots_stamp ( dst_path ) # Create v/freeze to redirect to v/commit path = pathlib . Path ( __file__ ) . with_name ( \"redirect-template.html\" ) redirect_html = path . read_text () redirect_html = redirect_html . format ( url = f \"../ { args . version } /\" ) args . freeze_directory . joinpath ( \"index.html\" ) . write_text ( redirect_html ) def get_versions ( args ): \"\"\" Extract versions from the webpage/v directory, which should each contain a manuscript. \"\"\" versions = { x . name for x in args . versions_directory . iterdir () if x . is_dir ()} versions -= { \"freeze\" , \"latest\" } versions = sorted ( versions ) return versions def ots_upgrade ( args ): \"\"\" Upgrade OpenTimestamps .ots files in versioned commit directory trees. Upgrades each .ots file with a separate ots upgrade subprocess call due to https://github.com/opentimestamps/opentimestamps-client/issues/71 \"\"\" ots_paths = list () for version in get_versions ( args ): ots_paths . extend ( args . versions_directory . joinpath ( version ) . glob ( \"**/*.ots\" )) ots_paths . sort () for ots_path in ots_paths : process_args = [ \"ots\" ] if args . no_ots_cache : process_args . append ( \"--no-cache\" ) else : process_args . extend ([ \"--cache\" , str ( args . ots_cache )]) process_args . extend ([ \"upgrade\" , str ( ots_path )]) process = subprocess . run ( process_args , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , universal_newlines = True , ) message = f \">>> { shlex_join ( process . args ) } \\n { process . stdout } \" if process . returncode != 0 : logging . warning ( f \"OpenTimestamp upgrade failed with exit code { process . returncode } . \\n { message } \" ) elif not process . stdout . strip () == \"Success! Timestamp complete\" : logging . info ( message ) backup_path = ots_path . with_suffix ( \".ots.bak\" ) if backup_path . exists (): if process . returncode == 0 : backup_path . unlink () else : # Restore original timestamp if failure backup_path . rename ( ots_path ) def ots_stamp ( path ): \"\"\" Timestamp a file using OpenTimestamps. This function calls `ots stamp path`. If `path` does not exist, this function does nothing. \"\"\" process_args = [ \"ots\" , \"stamp\" , str ( path )] process = subprocess . run ( process_args , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , universal_newlines = True , ) if process . returncode != 0 : logging . warning ( f \"OpenTimestamp command returned nonzero code ( { process . returncode } ). \\n \" f \">>> { shlex_join ( process . args ) } \\n \" f \" { process . stdout } \" ) Functions checkout_existing_versions def checkout_existing_versions ( args ) Must populate webpage/v from the gh-pages branch to get history References: http://clubmate.fi/git-checkout-file-or-directories-from-another-branch/ https://stackoverflow.com/a/2668947/4651668 https://stackoverflow.com/a/16493707/4651668 Command modeled after: git --work-tree=webpage checkout upstream/gh-pages -- v View Source def checkout_existing_versions ( args ) : \"\"\" Must populate webpage / v from the gh - pages branch to get history References : http : // clubmate . fi / git - checkout - file - or - directories - from - another - branch / https : // stackoverflow . com / a / 2668947 / 4651668 https : // stackoverflow . com / a / 16493707 / 4651668 Command modeled after : git -- work - tree = webpage checkout upstream / gh - pages -- v \"\"\" if not args . checkout : return command = [ \" git \" , f \" --work-tree={args.webpage_directory} \" , \" checkout \" , args . checkout , \" -- \" , \" v \" , ] logging . info ( f \" Attempting checkout with the following command: \\n {shlex_join(command)} \" ) process = subprocess . run ( command , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) if process . returncode == 0 : # Addresses an odd behavior where git checkout stages v /* files that don't actually exist subprocess.run([\"git\", \"add\", \"v\"], stdout=subprocess.PIPE) else: output = process.stdout.decode() message = ( f\"Checkout returned a nonzero exit status. See output:\\n{output.rstrip()}\" ) if \"pathspec\" in output: message += ( \"\\nManubot note: if there are no preexisting webpage versions (like for a newly created manuscript), \" \"the pathspec error above is expected and can be safely ignored.\" ) # see https://github.com/manubot/rootstock/issues/183 logging.warning(message) cli_webpage def cli_webpage ( args ) Execute manubot webpage commands. args should be an argparse.Namespace object created by parser.parse_args. View Source def cli_webpage ( args ) : \"\"\" Execute manubot webpage commands . args should be an argparse . Namespace object created by parser . parse_args . \"\"\" configure_args ( args ) logging . debug ( f \" Running `manubot webpage` with the following args: \\n {args} \" ) if args . timestamp : ots_upgrade ( args ) create_version ( args ) configure_args def configure_args ( args ) Perform additional processing of arguments that is not handled by argparse. Derive additional variables and add them to args. For example, add directories to args and create them if neccessary. Note that versions_directory is the parent of version_directory. View Source def configure_args ( args ): \"\"\" Perform additional processing of arguments that is not handled by argparse. Derive additional variables and add them to args. For example, add directories to args and create them if neccessary. Note that versions_directory is the parent of version_directory. \"\"\" args_dict = vars ( args ) # If --timestamp specified, check that opentimestamps-client is installed if args . timestamp : ots_executable_path = shutil . which ( \"ots\" ) if not ots_executable_path : logging . error ( \"manubot webpage --timestamp was specified but opentimestamps-client not found on system. \" \"Setting --timestamp=False. \" \"Fix this by installing https://pypi.org/project/opentimestamps-client/\" ) args_dict [ \"timestamp\" ] = False # Directory where Manubot outputs reside args_dict [ \"output_directory\" ] = pathlib . Path ( \"output\" ) # Set webpage directory args_dict [ \"webpage_directory\" ] = pathlib . Path ( \"webpage\" ) args . webpage_directory . mkdir ( exist_ok = True ) # Create webpage/v directory (if it doesn't already exist) args_dict [ \"versions_directory\" ] = args . webpage_directory . joinpath ( \"v\" ) args . versions_directory . mkdir ( exist_ok = True ) # Checkout existing version directories checkout_existing_versions ( args ) # Apply --version argument defaults if args . version is None : from manubot . process . ci import get_continuous_integration_parameters ci_params = get_continuous_integration_parameters () if ci_params : args_dict [ \"version\" ] = ci_params . get ( \"commit\" , \"local\" ) else : args_dict [ \"version\" ] = \"local\" # Create empty webpage/v/version directory version_directory = args . versions_directory . joinpath ( args . version ) if version_directory . is_dir (): logging . warning ( f \"{version_directory} exists: replacing it with an empty directory\" ) shutil . rmtree ( version_directory ) version_directory . mkdir () args_dict [ \"version_directory\" ] = version_directory # Symlink webpage/v/latest to point to webpage/v/commit latest_directory = args . versions_directory . joinpath ( \"latest\" ) if latest_directory . is_symlink () or latest_directory . is_file (): latest_directory . unlink () elif latest_directory . is_dir (): shutil . rmtree ( latest_directory ) latest_directory . symlink_to ( args . version , target_is_directory = True ) args_dict [ \"latest_directory\" ] = latest_directory # Create freeze directory freeze_directory = args . versions_directory . joinpath ( \"freeze\" ) freeze_directory . mkdir ( exist_ok = True ) args_dict [ \"freeze_directory\" ] = freeze_directory return args create_version def create_version ( args ) Populate the version directory for a new version. View Source def create_version ( args ) : \"\"\" Populate the version directory for a new version . \"\"\" # Copy content / images to webpage / v / commit / images images_src = pathlib . Path ( \" content/images \" ) if images_src . exists () : shutil . copytree ( src = images_src , dst = args . version_directory . joinpath ( \" images \" )) # Copy output files to to webpage / v / version / renamer = { \" manuscript.html \" : \" index.html \" , \" manuscript.pdf \" : \" manuscript.pdf \" } for src , dst in renamer . items () : src_path = args . output_directory . joinpath ( src ) if not src_path . exists () : continue dst_path = args . version_directory . joinpath ( dst ) shutil . copy2 ( src = src_path , dst = dst_path ) if args . timestamp : ots_stamp ( dst_path ) # Create v / freeze to redirect to v / commit path = pathlib . Path ( __file__ ) . with_name ( \" redirect-template.html \" ) redirect_html = path . read_text () redirect_html = redirect_html . format ( url = f \" ../{args.version}/ \" ) args . freeze_directory . joinpath ( \" index.html \" ) . write_text ( redirect_html ) get_versions def get_versions ( args ) Extract versions from the webpage/v directory, which should each contain a manuscript. View Source def get_versions ( args ) : \"\"\" Extract versions from the webpage / v directory , which should each contain a manuscript . \"\"\" versions = { x . name for x in args . versions_directory . iterdir () if x . is_dir () } versions -= { \" freeze \" , \" latest \" } versions = sorted ( versions ) return versions ots_stamp def ots_stamp ( path ) Timestamp a file using OpenTimestamps. This function calls ots stamp path . If path does not exist, this function does nothing. View Source def ots_stamp ( path ) : \" \"\" Timestamp a file using OpenTimestamps. This function calls `ots stamp path`. If `path` does not exist, this function does nothing. \"\" \" process_args = [ \"ots\" , \"stamp\" , str ( path ) ] process = subprocess . run ( process_args , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , universal_newlines = True , ) if process . returncode != 0 : logging . warning ( f \"OpenTimestamp command returned nonzero code ({process.returncode}). \\n \" f \">>> {shlex_join(process.args)} \\n \" f \"{process.stdout}\" ) ots_upgrade def ots_upgrade ( args ) Upgrade OpenTimestamps .ots files in versioned commit directory trees. Upgrades each .ots file with a separate ots upgrade subprocess call due to https://github.com/opentimestamps/opentimestamps-client/issues/71 View Source def ots_upgrade ( args ) : \"\"\" Upgrade OpenTimestamps . ots files in versioned commit directory trees . Upgrades each . ots file with a separate ots upgrade subprocess call due to https : // github . com / opentimestamps / opentimestamps - client / issues / 71 \"\"\" ots_paths = list () for version in get_versions ( args ) : ots_paths . extend ( args . versions_directory . joinpath ( version ) . glob ( \" **/*.ots \" )) ots_paths . sort () for ots_path in ots_paths : process_args = [ \" ots \" ] if args . no_ots_cache : process_args . append ( \" --no-cache \" ) else : process_args . extend ( [ \" --cache \" , str ( args . ots_cache ) ] ) process_args . extend ( [ \" upgrade \" , str ( ots_path ) ] ) process = subprocess . run ( process_args , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , universal_newlines = True , ) message = f \" >>> {shlex_join(process.args)} \\n {process.stdout} \" if process . returncode != 0 : logging . warning ( f \" OpenTimestamp upgrade failed with exit code {process.returncode}. \\n {message} \" ) elif not process . stdout . strip () == \" Success! Timestamp complete \" : logging . info ( message ) backup_path = ots_path . with_suffix ( \" .ots.bak \" ) if backup_path . exists () : if process . returncode == 0 : backup_path . unlink () else : # Restore original timestamp if failure backup_path . rename ( ots_path )","title":"Webpage Command"},{"location":"reference/manubot/webpage/webpage_command/#module-manubotwebpagewebpage_command","text":"None None View Source import logging import pathlib import shutil import subprocess from manubot.util import shlex_join def cli_webpage ( args ): \"\"\" Execute manubot webpage commands. args should be an argparse.Namespace object created by parser.parse_args. \"\"\" configure_args ( args ) logging . debug ( f \"Running `manubot webpage` with the following args: \\n { args } \" ) if args . timestamp : ots_upgrade ( args ) create_version ( args ) def configure_args ( args ): \"\"\" Perform additional processing of arguments that is not handled by argparse. Derive additional variables and add them to args. For example, add directories to args and create them if neccessary. Note that versions_directory is the parent of version_directory. \"\"\" args_dict = vars ( args ) # If --timestamp specified, check that opentimestamps-client is installed if args . timestamp : ots_executable_path = shutil . which ( \"ots\" ) if not ots_executable_path : logging . error ( \"manubot webpage --timestamp was specified but opentimestamps-client not found on system. \" \"Setting --timestamp=False. \" \"Fix this by installing https://pypi.org/project/opentimestamps-client/\" ) args_dict [ \"timestamp\" ] = False # Directory where Manubot outputs reside args_dict [ \"output_directory\" ] = pathlib . Path ( \"output\" ) # Set webpage directory args_dict [ \"webpage_directory\" ] = pathlib . Path ( \"webpage\" ) args . webpage_directory . mkdir ( exist_ok = True ) # Create webpage/v directory (if it doesn't already exist) args_dict [ \"versions_directory\" ] = args . webpage_directory . joinpath ( \"v\" ) args . versions_directory . mkdir ( exist_ok = True ) # Checkout existing version directories checkout_existing_versions ( args ) # Apply --version argument defaults if args . version is None : from manubot.process.ci import get_continuous_integration_parameters ci_params = get_continuous_integration_parameters () if ci_params : args_dict [ \"version\" ] = ci_params . get ( \"commit\" , \"local\" ) else : args_dict [ \"version\" ] = \"local\" # Create empty webpage/v/version directory version_directory = args . versions_directory . joinpath ( args . version ) if version_directory . is_dir (): logging . warning ( f \" { version_directory } exists: replacing it with an empty directory\" ) shutil . rmtree ( version_directory ) version_directory . mkdir () args_dict [ \"version_directory\" ] = version_directory # Symlink webpage/v/latest to point to webpage/v/commit latest_directory = args . versions_directory . joinpath ( \"latest\" ) if latest_directory . is_symlink () or latest_directory . is_file (): latest_directory . unlink () elif latest_directory . is_dir (): shutil . rmtree ( latest_directory ) latest_directory . symlink_to ( args . version , target_is_directory = True ) args_dict [ \"latest_directory\" ] = latest_directory # Create freeze directory freeze_directory = args . versions_directory . joinpath ( \"freeze\" ) freeze_directory . mkdir ( exist_ok = True ) args_dict [ \"freeze_directory\" ] = freeze_directory return args def checkout_existing_versions ( args ): \"\"\" Must populate webpage/v from the gh-pages branch to get history References: http://clubmate.fi/git-checkout-file-or-directories-from-another-branch/ https://stackoverflow.com/a/2668947/4651668 https://stackoverflow.com/a/16493707/4651668 Command modeled after: git --work-tree=webpage checkout upstream/gh-pages -- v \"\"\" if not args . checkout : return command = [ \"git\" , f \"--work-tree= { args . webpage_directory } \" , \"checkout\" , args . checkout , \"--\" , \"v\" , ] logging . info ( f \"Attempting checkout with the following command: \\n { shlex_join ( command ) } \" ) process = subprocess . run ( command , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) if process . returncode == 0 : # Addresses an odd behavior where git checkout stages v/* files that don't actually exist subprocess . run ([ \"git\" , \"add\" , \"v\" ], stdout = subprocess . PIPE ) else : output = process . stdout . decode () message = ( f \"Checkout returned a nonzero exit status. See output: \\n { output . rstrip () } \" ) if \"pathspec\" in output : message += ( \" \\n Manubot note: if there are no preexisting webpage versions (like for a newly created manuscript), \" \"the pathspec error above is expected and can be safely ignored.\" ) # see https://github.com/manubot/rootstock/issues/183 logging . warning ( message ) def create_version ( args ): \"\"\" Populate the version directory for a new version. \"\"\" # Copy content/images to webpage/v/commit/images images_src = pathlib . Path ( \"content/images\" ) if images_src . exists (): shutil . copytree ( src = images_src , dst = args . version_directory . joinpath ( \"images\" )) # Copy output files to to webpage/v/version/ renamer = { \"manuscript.html\" : \"index.html\" , \"manuscript.pdf\" : \"manuscript.pdf\" } for src , dst in renamer . items (): src_path = args . output_directory . joinpath ( src ) if not src_path . exists (): continue dst_path = args . version_directory . joinpath ( dst ) shutil . copy2 ( src = src_path , dst = dst_path ) if args . timestamp : ots_stamp ( dst_path ) # Create v/freeze to redirect to v/commit path = pathlib . Path ( __file__ ) . with_name ( \"redirect-template.html\" ) redirect_html = path . read_text () redirect_html = redirect_html . format ( url = f \"../ { args . version } /\" ) args . freeze_directory . joinpath ( \"index.html\" ) . write_text ( redirect_html ) def get_versions ( args ): \"\"\" Extract versions from the webpage/v directory, which should each contain a manuscript. \"\"\" versions = { x . name for x in args . versions_directory . iterdir () if x . is_dir ()} versions -= { \"freeze\" , \"latest\" } versions = sorted ( versions ) return versions def ots_upgrade ( args ): \"\"\" Upgrade OpenTimestamps .ots files in versioned commit directory trees. Upgrades each .ots file with a separate ots upgrade subprocess call due to https://github.com/opentimestamps/opentimestamps-client/issues/71 \"\"\" ots_paths = list () for version in get_versions ( args ): ots_paths . extend ( args . versions_directory . joinpath ( version ) . glob ( \"**/*.ots\" )) ots_paths . sort () for ots_path in ots_paths : process_args = [ \"ots\" ] if args . no_ots_cache : process_args . append ( \"--no-cache\" ) else : process_args . extend ([ \"--cache\" , str ( args . ots_cache )]) process_args . extend ([ \"upgrade\" , str ( ots_path )]) process = subprocess . run ( process_args , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , universal_newlines = True , ) message = f \">>> { shlex_join ( process . args ) } \\n { process . stdout } \" if process . returncode != 0 : logging . warning ( f \"OpenTimestamp upgrade failed with exit code { process . returncode } . \\n { message } \" ) elif not process . stdout . strip () == \"Success! Timestamp complete\" : logging . info ( message ) backup_path = ots_path . with_suffix ( \".ots.bak\" ) if backup_path . exists (): if process . returncode == 0 : backup_path . unlink () else : # Restore original timestamp if failure backup_path . rename ( ots_path ) def ots_stamp ( path ): \"\"\" Timestamp a file using OpenTimestamps. This function calls `ots stamp path`. If `path` does not exist, this function does nothing. \"\"\" process_args = [ \"ots\" , \"stamp\" , str ( path )] process = subprocess . run ( process_args , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , universal_newlines = True , ) if process . returncode != 0 : logging . warning ( f \"OpenTimestamp command returned nonzero code ( { process . returncode } ). \\n \" f \">>> { shlex_join ( process . args ) } \\n \" f \" { process . stdout } \" )","title":"Module manubot.webpage.webpage_command"},{"location":"reference/manubot/webpage/webpage_command/#functions","text":"","title":"Functions"},{"location":"reference/manubot/webpage/webpage_command/#checkout_existing_versions","text":"def checkout_existing_versions ( args ) Must populate webpage/v from the gh-pages branch to get history References: http://clubmate.fi/git-checkout-file-or-directories-from-another-branch/ https://stackoverflow.com/a/2668947/4651668 https://stackoverflow.com/a/16493707/4651668 Command modeled after: git --work-tree=webpage checkout upstream/gh-pages -- v View Source def checkout_existing_versions ( args ) : \"\"\" Must populate webpage / v from the gh - pages branch to get history References : http : // clubmate . fi / git - checkout - file - or - directories - from - another - branch / https : // stackoverflow . com / a / 2668947 / 4651668 https : // stackoverflow . com / a / 16493707 / 4651668 Command modeled after : git -- work - tree = webpage checkout upstream / gh - pages -- v \"\"\" if not args . checkout : return command = [ \" git \" , f \" --work-tree={args.webpage_directory} \" , \" checkout \" , args . checkout , \" -- \" , \" v \" , ] logging . info ( f \" Attempting checkout with the following command: \\n {shlex_join(command)} \" ) process = subprocess . run ( command , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) if process . returncode == 0 : # Addresses an odd behavior where git checkout stages v /* files that don't actually exist subprocess.run([\"git\", \"add\", \"v\"], stdout=subprocess.PIPE) else: output = process.stdout.decode() message = ( f\"Checkout returned a nonzero exit status. See output:\\n{output.rstrip()}\" ) if \"pathspec\" in output: message += ( \"\\nManubot note: if there are no preexisting webpage versions (like for a newly created manuscript), \" \"the pathspec error above is expected and can be safely ignored.\" ) # see https://github.com/manubot/rootstock/issues/183 logging.warning(message)","title":"checkout_existing_versions"},{"location":"reference/manubot/webpage/webpage_command/#cli_webpage","text":"def cli_webpage ( args ) Execute manubot webpage commands. args should be an argparse.Namespace object created by parser.parse_args. View Source def cli_webpage ( args ) : \"\"\" Execute manubot webpage commands . args should be an argparse . Namespace object created by parser . parse_args . \"\"\" configure_args ( args ) logging . debug ( f \" Running `manubot webpage` with the following args: \\n {args} \" ) if args . timestamp : ots_upgrade ( args ) create_version ( args )","title":"cli_webpage"},{"location":"reference/manubot/webpage/webpage_command/#configure_args","text":"def configure_args ( args ) Perform additional processing of arguments that is not handled by argparse. Derive additional variables and add them to args. For example, add directories to args and create them if neccessary. Note that versions_directory is the parent of version_directory. View Source def configure_args ( args ): \"\"\" Perform additional processing of arguments that is not handled by argparse. Derive additional variables and add them to args. For example, add directories to args and create them if neccessary. Note that versions_directory is the parent of version_directory. \"\"\" args_dict = vars ( args ) # If --timestamp specified, check that opentimestamps-client is installed if args . timestamp : ots_executable_path = shutil . which ( \"ots\" ) if not ots_executable_path : logging . error ( \"manubot webpage --timestamp was specified but opentimestamps-client not found on system. \" \"Setting --timestamp=False. \" \"Fix this by installing https://pypi.org/project/opentimestamps-client/\" ) args_dict [ \"timestamp\" ] = False # Directory where Manubot outputs reside args_dict [ \"output_directory\" ] = pathlib . Path ( \"output\" ) # Set webpage directory args_dict [ \"webpage_directory\" ] = pathlib . Path ( \"webpage\" ) args . webpage_directory . mkdir ( exist_ok = True ) # Create webpage/v directory (if it doesn't already exist) args_dict [ \"versions_directory\" ] = args . webpage_directory . joinpath ( \"v\" ) args . versions_directory . mkdir ( exist_ok = True ) # Checkout existing version directories checkout_existing_versions ( args ) # Apply --version argument defaults if args . version is None : from manubot . process . ci import get_continuous_integration_parameters ci_params = get_continuous_integration_parameters () if ci_params : args_dict [ \"version\" ] = ci_params . get ( \"commit\" , \"local\" ) else : args_dict [ \"version\" ] = \"local\" # Create empty webpage/v/version directory version_directory = args . versions_directory . joinpath ( args . version ) if version_directory . is_dir (): logging . warning ( f \"{version_directory} exists: replacing it with an empty directory\" ) shutil . rmtree ( version_directory ) version_directory . mkdir () args_dict [ \"version_directory\" ] = version_directory # Symlink webpage/v/latest to point to webpage/v/commit latest_directory = args . versions_directory . joinpath ( \"latest\" ) if latest_directory . is_symlink () or latest_directory . is_file (): latest_directory . unlink () elif latest_directory . is_dir (): shutil . rmtree ( latest_directory ) latest_directory . symlink_to ( args . version , target_is_directory = True ) args_dict [ \"latest_directory\" ] = latest_directory # Create freeze directory freeze_directory = args . versions_directory . joinpath ( \"freeze\" ) freeze_directory . mkdir ( exist_ok = True ) args_dict [ \"freeze_directory\" ] = freeze_directory return args","title":"configure_args"},{"location":"reference/manubot/webpage/webpage_command/#create_version","text":"def create_version ( args ) Populate the version directory for a new version. View Source def create_version ( args ) : \"\"\" Populate the version directory for a new version . \"\"\" # Copy content / images to webpage / v / commit / images images_src = pathlib . Path ( \" content/images \" ) if images_src . exists () : shutil . copytree ( src = images_src , dst = args . version_directory . joinpath ( \" images \" )) # Copy output files to to webpage / v / version / renamer = { \" manuscript.html \" : \" index.html \" , \" manuscript.pdf \" : \" manuscript.pdf \" } for src , dst in renamer . items () : src_path = args . output_directory . joinpath ( src ) if not src_path . exists () : continue dst_path = args . version_directory . joinpath ( dst ) shutil . copy2 ( src = src_path , dst = dst_path ) if args . timestamp : ots_stamp ( dst_path ) # Create v / freeze to redirect to v / commit path = pathlib . Path ( __file__ ) . with_name ( \" redirect-template.html \" ) redirect_html = path . read_text () redirect_html = redirect_html . format ( url = f \" ../{args.version}/ \" ) args . freeze_directory . joinpath ( \" index.html \" ) . write_text ( redirect_html )","title":"create_version"},{"location":"reference/manubot/webpage/webpage_command/#get_versions","text":"def get_versions ( args ) Extract versions from the webpage/v directory, which should each contain a manuscript. View Source def get_versions ( args ) : \"\"\" Extract versions from the webpage / v directory , which should each contain a manuscript . \"\"\" versions = { x . name for x in args . versions_directory . iterdir () if x . is_dir () } versions -= { \" freeze \" , \" latest \" } versions = sorted ( versions ) return versions","title":"get_versions"},{"location":"reference/manubot/webpage/webpage_command/#ots_stamp","text":"def ots_stamp ( path ) Timestamp a file using OpenTimestamps. This function calls ots stamp path . If path does not exist, this function does nothing. View Source def ots_stamp ( path ) : \" \"\" Timestamp a file using OpenTimestamps. This function calls `ots stamp path`. If `path` does not exist, this function does nothing. \"\" \" process_args = [ \"ots\" , \"stamp\" , str ( path ) ] process = subprocess . run ( process_args , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , universal_newlines = True , ) if process . returncode != 0 : logging . warning ( f \"OpenTimestamp command returned nonzero code ({process.returncode}). \\n \" f \">>> {shlex_join(process.args)} \\n \" f \"{process.stdout}\" )","title":"ots_stamp"},{"location":"reference/manubot/webpage/webpage_command/#ots_upgrade","text":"def ots_upgrade ( args ) Upgrade OpenTimestamps .ots files in versioned commit directory trees. Upgrades each .ots file with a separate ots upgrade subprocess call due to https://github.com/opentimestamps/opentimestamps-client/issues/71 View Source def ots_upgrade ( args ) : \"\"\" Upgrade OpenTimestamps . ots files in versioned commit directory trees . Upgrades each . ots file with a separate ots upgrade subprocess call due to https : // github . com / opentimestamps / opentimestamps - client / issues / 71 \"\"\" ots_paths = list () for version in get_versions ( args ) : ots_paths . extend ( args . versions_directory . joinpath ( version ) . glob ( \" **/*.ots \" )) ots_paths . sort () for ots_path in ots_paths : process_args = [ \" ots \" ] if args . no_ots_cache : process_args . append ( \" --no-cache \" ) else : process_args . extend ( [ \" --cache \" , str ( args . ots_cache ) ] ) process_args . extend ( [ \" upgrade \" , str ( ots_path ) ] ) process = subprocess . run ( process_args , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , universal_newlines = True , ) message = f \" >>> {shlex_join(process.args)} \\n {process.stdout} \" if process . returncode != 0 : logging . warning ( f \" OpenTimestamp upgrade failed with exit code {process.returncode}. \\n {message} \" ) elif not process . stdout . strip () == \" Success! Timestamp complete \" : logging . info ( message ) backup_path = ots_path . with_suffix ( \" .ots.bak \" ) if backup_path . exists () : if process . returncode == 0 : backup_path . unlink () else : # Restore original timestamp if failure backup_path . rename ( ots_path )","title":"ots_upgrade"}]}
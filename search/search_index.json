{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Python utilities for Manubot: Manuscripts, open and automated Manubot is a workflow and set of tools for the next generation of scholarly publishing. This repository contains a Python package with several Manubot-related utilities, as described in the usage section below. Package documentation is available at https://manubot.github.io/manubot (auto-generated from the Python source code). The manubot cite command-line interface retrieves and formats bibliographic metadata for user-supplied persistent identifiers like DOIs or PubMed IDs. The manubot process command-line interface prepares scholarly manuscripts for Pandoc consumption. The manubot process command is used by Manubot manuscripts, which are based off the Rootstock template , to automate several aspects of manuscript generation. See Rootstock's manuscript usage guide for more information. Note: If you want to experience Manubot by editing an existing manuscript, see https://github.com/manubot/try-manubot . If you want to create a new manuscript, see https://github.com/manubot/rootstock . To cite the Manubot project or for more information on its design and history, see: Open collaborative writing with Manubot Daniel S. Himmelstein, Vincent Rubinetti, David R. Slochower, Dongbo Hu, Venkat S. Malladi, Casey S. Greene, Anthony Gitter PLOS Computational Biology (2019-06-24) https://doi.org/c7np DOI: 10.1371/journal.pcbi.1007128 \u00b7 PMID: 31233491 \u00b7 PMCID: PMC6611653 The Manubot version of this manuscript is available at https://greenelab.github.io/meta-review/ . Installation If you are using the manubot Python package as part of a manuscript repository, installation of this package is handled though the Rootstock's environment specification . For other use cases, this package can be installed via pip . Install the latest release version from PyPI : pip install --upgrade manubot Or install from the source code on GitHub , using the version specified by a commit hash: COMMIT = d2160151e52750895571079a6e257beb6e0b1278 pip install --upgrade git+https://github.com/manubot/manubot@ $COMMIT The --upgrade argument ensures pip updates an existing manubot installation if present. Usage Installing the python package creates the manubot command line program. Here is the usage information as per manubot --help : usage : manubot [- h ] [-- version ] { process , cite , webpage } ... Manubot : the manuscript bot for scholarly writing optional arguments : - h , -- help show this help message and exit -- version show program ' s version number and exit subcommands : All operations are done through subcommands : { process , cite , webpage } process process manuscript content cite citation to CSL command line utility webpage deploy Manubot outputs to a webpage directory tree Note that all operations are done through the following sub-commands. Process The manubot process program is the primary interface to using Manubot. There are two required arguments: --content-directory and --output-directory , which specify the respective paths to the content and output directories. The content directory stores the manuscript source files. Files generated by Manubot are saved to the output directory. One common setup is to create a directory for a manuscript that contains both the content and output directory. Under this setup, you can run the Manubot using: manubot process \\ --skip-citations \\ --content-directory = content \\ --output-directory = output See manubot process --help for documentation of all command line arguments: usage : manubot process [- h ] -- content - directory CONTENT_DIRECTORY -- output - directory OUTPUT_DIRECTORY [-- template - variables - path TEMPLATE_VARIABLES_PATH ] -- skip - citations [-- cache - directory CACHE_DIRECTORY ] [-- clear - requests - cache ] [-- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL } ] Process manuscript content to create outputs for Pandoc consumption . Performs bibliographic processing and templating . optional arguments : - h , -- help show this help message and exit -- content - directory CONTENT_DIRECTORY Directory where manuscript content files are located . -- output - directory OUTPUT_DIRECTORY Directory to output files generated by this script . -- template - variables - path TEMPLATE_VARIABLES_PATH Path or URL of a file containing template variables for jinja2 . Serialization format is inferred from the file extension , with support for JSON , YAML , and TOML . If the format cannot be detected , the parser assumes JSON . Specify this argument multiple times to read multiple files . Variables can be applied to a namespace ( i . e . stored under a dictionary key ) like ` -- template - variables - path = namespace = path_or_url ` . Namespaces must match the regex ` [ a - zA - Z_ ][ a - zA - Z0 - 9 _ ]* ` . -- skip - citations Skip citation and reference processing . Support for citation and reference processing has been moved from ` manubot process ` to the pandoc - manubot - cite filter . Therefore this argument is now required . If citation - tags . tsv is found in content , these tags will be inserted in the markdown output using the reference - link syntax for citekey aliases . Appends content / manual - references * . * paths to Pandoc 's metadata . bibliography field . -- cache - directory CACHE_DIRECTORY Custom cache directory . If not specified , caches to output - directory . -- clear - requests - cache -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL } Set the logging level for stderr logging Manual references Manubot has the ability to rely on user-provided reference metadata rather than generating it. manubot process searches the content directory for files containing manually-provided reference metadata that match the glob manual-references*.* . These files are stored in the Pandoc metadata bibliography field, such that they can be loaded by pandoc-manubot-cite . Cite manubot cite is a command line utility to create CSL JSON items for one or more citation keys. Citation keys should be in the format source:identifier . For example, the following example generates CSL JSON for four references: manubot cite doi:10.1098/rsif.2017.0387 pubmed:29424689 pmc:PMC5640425 arxiv:1806.05726 The following terminal recording demonstrates the main features of manubot cite : Additional usage information is available from manubot cite --help : usage : manubot cite [- h ] [-- render ] [-- csl CSL ] [-- bibliography BIBLIOGRAPHY ] [-- format { plain , markdown , docx , html , jats }] [-- output OUTPUT ] [-- allow - invalid - csl - data ] [-- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL }] citekeys [ citekeys ...] Retrieve bibliographic metadata for one or more citation keys . positional arguments : citekeys One or more ( space separated ) citation keys to produce CSL for . optional arguments : - h , -- help show this help message and exit -- render Whether to render CSL Data into a formatted reference list using Pandoc . Pandoc version 2.0 or higher is required for complete support of available output formats . -- csl CSL When -- render , specify an XML CSL definition to style references ( i . e . Pandoc 's --csl option). Defaults to Manubot' s style . -- bibliography BIBLIOGRAPHY File to read manual reference metadata . Specify multiple times to load multiple files . Similar to pandoc -- bibliography . -- format { plain , markdown , docx , html , jats } When -- render , format to use for output file . If not specified , attempt to infer this from filename extension . Otherwise , default to plain . -- output OUTPUT Specify a file to write output , otherwise default to stdout . -- allow - invalid - csl - data Allow CSL Items that do not conform to the JSON Schema . Skips CSL pruning . -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL } Set the logging level for stderr logging Pandoc filter This package creates the pandoc-manubot-cite Pandoc filter, providing access to Manubot's cite-by-ID functionality from within a Pandoc workflow. Currently, this filter is experimental and subject to breaking changes at any point. usage : pandoc - manubot - cite [ -h ] [ --input [INPUT ] ] [ --output [OUTPUT ] ] target_format Pandoc filter for citation by persistent identifier . Filters are command - line programs that read and write a JSON - encoded abstract syntax tree for Pandoc . Unless you are debugging , run this filter as part of a pandoc command by specifying --filter=pandoc-manubot-cite. positional arguments : target_format output format of the pandoc command , as per Pandoc ' s --to option optional arguments : - h , --help show this help message and exit --input [INPUT] path read JSON input (defaults to stdin) --output [OUTPUT] path to write JSON output (defaults to stdout) Other Pandoc filters exist that do something similar: pandoc-url2cite , pandoc-url2cite-hs , & pwcite . Currently, pandoc-manubot-cite supports the most types of persistent identifiers. We're interested in creating as much compatibility as possible between these filters and their syntaxes. Manual references Manual references are loaded from the references and bibliography Pandoc metadata fields. If a manual reference filename ends with .json or .yaml , it's assumed to contain CSL Data (i.e. Citation Style Language JSON). Otherwise, the format is inferred from the extension and converted to CSL JSON using the pandoc-citeproc --bib2json utility . The standard citation key for manual references is inferred from the CSL JSON id or note field. When no prefix is provided, such as doi: , url: , or raw: , a raw: prefix is automatically added. If multiple manual reference files load metadata for the same standard citation id , precedence is assigned according to descending filename order. Webpage The manubot webpage command populates a webpage directory with Manubot output files. usage : manubot webpage [- h ] [-- checkout [ CHECKOUT ]] [-- version VERSION ] [-- timestamp ] [-- no - ots - cache | -- ots - cache OTS_CACHE ] [-- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL }] Update the webpage directory tree with Manubot output files . This command should be run from the root directory of a Manubot manuscript that follows the Rootstock layout , containing ` output ` and ` webpage ` directories . HTML and PDF outputs are copied to the webpage directory , which is structured as static source files for website hosting . optional arguments : - h , -- help show this help message and exit -- checkout [ CHECKOUT ] branch to checkout / v directory contents from . For example , -- checkout = upstream / gh - pages . -- checkout is equivalent to -- checkout = gh - pages . If -- checkout is ommitted , no checkout is performed . -- version VERSION Used to create webpage /v/ { version } directory . Generally a commit hash , tag , or 'local' . When omitted , version defaults to the commit hash on CI builds and 'local' elsewhere . -- timestamp timestamp versioned manuscripts in webpage / v using OpenTimestamps . Specify this flag to create timestamps for the current HTML and PDF outputs and upgrade any timestamps from past manuscript versions . -- no - ots - cache disable the timestamp cache . -- ots - cache OTS_CACHE location for the timestamp cache ( default : ci /cache/ ots ). -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL } Set the logging level for stderr logging Development Environment Create a development environment using: conda create --name manubot-dev --channel conda-forge \\ python = 3 .8 pandoc = 2 .8 conda activate manubot-dev # assumes conda >= 4.4 pip install --editable \".[all]\" Commands Below are some common commands used for development. They assume the working directory is set to the repository's root, and the conda environment is activated. # run the test suite pytest # reformat Python files according to the black style rules (required to pass CI) black . # detect any flake8 linting violations flake8 # regenerate the README codeblocks for --help messages python manubot/tests/test_readme.py # generate the docs portray as_html --overwrite --output_dir = docs # process the example testing manuscript manubot process \\ --content-directory = manubot/process/tests/manuscripts/example/content \\ --output-directory = manubot/process/tests/manuscripts/example/output \\ --skip-citations \\ --log-level = INFO Release instructions This section is only relevant for project maintainers. Travis CI deployments are used to upload releases to PyPI . To create a new release, create release notes and bump the __version__ in manubot/__init__.py . The following commands can help draft release notes: # commit list since v0.2.4 tag git log --oneline v0.2.4..HEAD # commit authors since v0.2.4 tag git log v0.2.4..HEAD --format = '%aN <%aE>' After the release notes are complete, run the following commands: TAG = v ` python setup.py --version ` # Commit updated __version__ info git add manubot/__init__.py release-notes/ $TAG .md git commit --message = \"Prepare $TAG release\" git push After the previous commit is part of master , for example after a PR is merged, create a tag to trigger the version deployment: # Create & push tag (assuming upstream is the manubot organization remote) git tag --annotate $TAG --file = release-notes/ $TAG .md git push upstream $TAG Goals & Acknowledgments Our goal is to create scholarly infrastructure that encourages open science and assists reproducibility. Accordingly, we hope for the Manubot software and philosophy to be adopted widely, by both academic and commercial entities. As such, Manubot is free/libre and open source software (see LICENSE.md ). We would like to thank the contributors and funders whose support makes this project possible. Specifically, Manubot development has been financially supported by: the Alfred P. Sloan Foundation in Grant G-2018-11163 to @dhimmel . the Gordon & Betty Moore Foundation ( @DDD-Moore ) in Grant GBMF4552 to @cgreene .","title":"Home"},{"location":"#python-utilities-for-manubot-manuscripts-open-and-automated","text":"Manubot is a workflow and set of tools for the next generation of scholarly publishing. This repository contains a Python package with several Manubot-related utilities, as described in the usage section below. Package documentation is available at https://manubot.github.io/manubot (auto-generated from the Python source code). The manubot cite command-line interface retrieves and formats bibliographic metadata for user-supplied persistent identifiers like DOIs or PubMed IDs. The manubot process command-line interface prepares scholarly manuscripts for Pandoc consumption. The manubot process command is used by Manubot manuscripts, which are based off the Rootstock template , to automate several aspects of manuscript generation. See Rootstock's manuscript usage guide for more information. Note: If you want to experience Manubot by editing an existing manuscript, see https://github.com/manubot/try-manubot . If you want to create a new manuscript, see https://github.com/manubot/rootstock . To cite the Manubot project or for more information on its design and history, see: Open collaborative writing with Manubot Daniel S. Himmelstein, Vincent Rubinetti, David R. Slochower, Dongbo Hu, Venkat S. Malladi, Casey S. Greene, Anthony Gitter PLOS Computational Biology (2019-06-24) https://doi.org/c7np DOI: 10.1371/journal.pcbi.1007128 \u00b7 PMID: 31233491 \u00b7 PMCID: PMC6611653 The Manubot version of this manuscript is available at https://greenelab.github.io/meta-review/ .","title":"Python utilities for Manubot: Manuscripts, open and automated"},{"location":"#installation","text":"If you are using the manubot Python package as part of a manuscript repository, installation of this package is handled though the Rootstock's environment specification . For other use cases, this package can be installed via pip . Install the latest release version from PyPI : pip install --upgrade manubot Or install from the source code on GitHub , using the version specified by a commit hash: COMMIT = d2160151e52750895571079a6e257beb6e0b1278 pip install --upgrade git+https://github.com/manubot/manubot@ $COMMIT The --upgrade argument ensures pip updates an existing manubot installation if present.","title":"Installation"},{"location":"#usage","text":"Installing the python package creates the manubot command line program. Here is the usage information as per manubot --help : usage : manubot [- h ] [-- version ] { process , cite , webpage } ... Manubot : the manuscript bot for scholarly writing optional arguments : - h , -- help show this help message and exit -- version show program ' s version number and exit subcommands : All operations are done through subcommands : { process , cite , webpage } process process manuscript content cite citation to CSL command line utility webpage deploy Manubot outputs to a webpage directory tree Note that all operations are done through the following sub-commands.","title":"Usage"},{"location":"#process","text":"The manubot process program is the primary interface to using Manubot. There are two required arguments: --content-directory and --output-directory , which specify the respective paths to the content and output directories. The content directory stores the manuscript source files. Files generated by Manubot are saved to the output directory. One common setup is to create a directory for a manuscript that contains both the content and output directory. Under this setup, you can run the Manubot using: manubot process \\ --skip-citations \\ --content-directory = content \\ --output-directory = output See manubot process --help for documentation of all command line arguments: usage : manubot process [- h ] -- content - directory CONTENT_DIRECTORY -- output - directory OUTPUT_DIRECTORY [-- template - variables - path TEMPLATE_VARIABLES_PATH ] -- skip - citations [-- cache - directory CACHE_DIRECTORY ] [-- clear - requests - cache ] [-- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL } ] Process manuscript content to create outputs for Pandoc consumption . Performs bibliographic processing and templating . optional arguments : - h , -- help show this help message and exit -- content - directory CONTENT_DIRECTORY Directory where manuscript content files are located . -- output - directory OUTPUT_DIRECTORY Directory to output files generated by this script . -- template - variables - path TEMPLATE_VARIABLES_PATH Path or URL of a file containing template variables for jinja2 . Serialization format is inferred from the file extension , with support for JSON , YAML , and TOML . If the format cannot be detected , the parser assumes JSON . Specify this argument multiple times to read multiple files . Variables can be applied to a namespace ( i . e . stored under a dictionary key ) like ` -- template - variables - path = namespace = path_or_url ` . Namespaces must match the regex ` [ a - zA - Z_ ][ a - zA - Z0 - 9 _ ]* ` . -- skip - citations Skip citation and reference processing . Support for citation and reference processing has been moved from ` manubot process ` to the pandoc - manubot - cite filter . Therefore this argument is now required . If citation - tags . tsv is found in content , these tags will be inserted in the markdown output using the reference - link syntax for citekey aliases . Appends content / manual - references * . * paths to Pandoc 's metadata . bibliography field . -- cache - directory CACHE_DIRECTORY Custom cache directory . If not specified , caches to output - directory . -- clear - requests - cache -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL } Set the logging level for stderr logging","title":"Process"},{"location":"#manual-references","text":"Manubot has the ability to rely on user-provided reference metadata rather than generating it. manubot process searches the content directory for files containing manually-provided reference metadata that match the glob manual-references*.* . These files are stored in the Pandoc metadata bibliography field, such that they can be loaded by pandoc-manubot-cite .","title":"Manual references"},{"location":"#cite","text":"manubot cite is a command line utility to create CSL JSON items for one or more citation keys. Citation keys should be in the format source:identifier . For example, the following example generates CSL JSON for four references: manubot cite doi:10.1098/rsif.2017.0387 pubmed:29424689 pmc:PMC5640425 arxiv:1806.05726 The following terminal recording demonstrates the main features of manubot cite : Additional usage information is available from manubot cite --help : usage : manubot cite [- h ] [-- render ] [-- csl CSL ] [-- bibliography BIBLIOGRAPHY ] [-- format { plain , markdown , docx , html , jats }] [-- output OUTPUT ] [-- allow - invalid - csl - data ] [-- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL }] citekeys [ citekeys ...] Retrieve bibliographic metadata for one or more citation keys . positional arguments : citekeys One or more ( space separated ) citation keys to produce CSL for . optional arguments : - h , -- help show this help message and exit -- render Whether to render CSL Data into a formatted reference list using Pandoc . Pandoc version 2.0 or higher is required for complete support of available output formats . -- csl CSL When -- render , specify an XML CSL definition to style references ( i . e . Pandoc 's --csl option). Defaults to Manubot' s style . -- bibliography BIBLIOGRAPHY File to read manual reference metadata . Specify multiple times to load multiple files . Similar to pandoc -- bibliography . -- format { plain , markdown , docx , html , jats } When -- render , format to use for output file . If not specified , attempt to infer this from filename extension . Otherwise , default to plain . -- output OUTPUT Specify a file to write output , otherwise default to stdout . -- allow - invalid - csl - data Allow CSL Items that do not conform to the JSON Schema . Skips CSL pruning . -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL } Set the logging level for stderr logging","title":"Cite"},{"location":"#pandoc-filter","text":"This package creates the pandoc-manubot-cite Pandoc filter, providing access to Manubot's cite-by-ID functionality from within a Pandoc workflow. Currently, this filter is experimental and subject to breaking changes at any point. usage : pandoc - manubot - cite [ -h ] [ --input [INPUT ] ] [ --output [OUTPUT ] ] target_format Pandoc filter for citation by persistent identifier . Filters are command - line programs that read and write a JSON - encoded abstract syntax tree for Pandoc . Unless you are debugging , run this filter as part of a pandoc command by specifying --filter=pandoc-manubot-cite. positional arguments : target_format output format of the pandoc command , as per Pandoc ' s --to option optional arguments : - h , --help show this help message and exit --input [INPUT] path read JSON input (defaults to stdin) --output [OUTPUT] path to write JSON output (defaults to stdout) Other Pandoc filters exist that do something similar: pandoc-url2cite , pandoc-url2cite-hs , & pwcite . Currently, pandoc-manubot-cite supports the most types of persistent identifiers. We're interested in creating as much compatibility as possible between these filters and their syntaxes.","title":"Pandoc filter"},{"location":"#manual-references_1","text":"Manual references are loaded from the references and bibliography Pandoc metadata fields. If a manual reference filename ends with .json or .yaml , it's assumed to contain CSL Data (i.e. Citation Style Language JSON). Otherwise, the format is inferred from the extension and converted to CSL JSON using the pandoc-citeproc --bib2json utility . The standard citation key for manual references is inferred from the CSL JSON id or note field. When no prefix is provided, such as doi: , url: , or raw: , a raw: prefix is automatically added. If multiple manual reference files load metadata for the same standard citation id , precedence is assigned according to descending filename order.","title":"Manual references"},{"location":"#webpage","text":"The manubot webpage command populates a webpage directory with Manubot output files. usage : manubot webpage [- h ] [-- checkout [ CHECKOUT ]] [-- version VERSION ] [-- timestamp ] [-- no - ots - cache | -- ots - cache OTS_CACHE ] [-- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL }] Update the webpage directory tree with Manubot output files . This command should be run from the root directory of a Manubot manuscript that follows the Rootstock layout , containing ` output ` and ` webpage ` directories . HTML and PDF outputs are copied to the webpage directory , which is structured as static source files for website hosting . optional arguments : - h , -- help show this help message and exit -- checkout [ CHECKOUT ] branch to checkout / v directory contents from . For example , -- checkout = upstream / gh - pages . -- checkout is equivalent to -- checkout = gh - pages . If -- checkout is ommitted , no checkout is performed . -- version VERSION Used to create webpage /v/ { version } directory . Generally a commit hash , tag , or 'local' . When omitted , version defaults to the commit hash on CI builds and 'local' elsewhere . -- timestamp timestamp versioned manuscripts in webpage / v using OpenTimestamps . Specify this flag to create timestamps for the current HTML and PDF outputs and upgrade any timestamps from past manuscript versions . -- no - ots - cache disable the timestamp cache . -- ots - cache OTS_CACHE location for the timestamp cache ( default : ci /cache/ ots ). -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL } Set the logging level for stderr logging","title":"Webpage"},{"location":"#development","text":"","title":"Development"},{"location":"#environment","text":"Create a development environment using: conda create --name manubot-dev --channel conda-forge \\ python = 3 .8 pandoc = 2 .8 conda activate manubot-dev # assumes conda >= 4.4 pip install --editable \".[all]\"","title":"Environment"},{"location":"#commands","text":"Below are some common commands used for development. They assume the working directory is set to the repository's root, and the conda environment is activated. # run the test suite pytest # reformat Python files according to the black style rules (required to pass CI) black . # detect any flake8 linting violations flake8 # regenerate the README codeblocks for --help messages python manubot/tests/test_readme.py # generate the docs portray as_html --overwrite --output_dir = docs # process the example testing manuscript manubot process \\ --content-directory = manubot/process/tests/manuscripts/example/content \\ --output-directory = manubot/process/tests/manuscripts/example/output \\ --skip-citations \\ --log-level = INFO","title":"Commands"},{"location":"#release-instructions","text":"This section is only relevant for project maintainers. Travis CI deployments are used to upload releases to PyPI . To create a new release, create release notes and bump the __version__ in manubot/__init__.py . The following commands can help draft release notes: # commit list since v0.2.4 tag git log --oneline v0.2.4..HEAD # commit authors since v0.2.4 tag git log v0.2.4..HEAD --format = '%aN <%aE>' After the release notes are complete, run the following commands: TAG = v ` python setup.py --version ` # Commit updated __version__ info git add manubot/__init__.py release-notes/ $TAG .md git commit --message = \"Prepare $TAG release\" git push After the previous commit is part of master , for example after a PR is merged, create a tag to trigger the version deployment: # Create & push tag (assuming upstream is the manubot organization remote) git tag --annotate $TAG --file = release-notes/ $TAG .md git push upstream $TAG","title":"Release instructions"},{"location":"#goals-acknowledgments","text":"Our goal is to create scholarly infrastructure that encourages open science and assists reproducibility. Accordingly, we hope for the Manubot software and philosophy to be adopted widely, by both academic and commercial entities. As such, Manubot is free/libre and open source software (see LICENSE.md ). We would like to thank the contributors and funders whose support makes this project possible. Specifically, Manubot development has been financially supported by: the Alfred P. Sloan Foundation in Grant G-2018-11163 to @dhimmel . the Gordon & Betty Moore Foundation ( @DDD-Moore ) in Grant GBMF4552 to @cgreene .","title":"Goals &amp; Acknowledgments"},{"location":"LICENSE/","text":"BSD-2-Clause Plus Patent License Copyright \u00a9 2017\u20132020, Contributors & the Greene Lab at the University of Pennsylvania Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Subject to the terms and conditions of this license, each copyright holder and contributor hereby grants to those receiving rights under this license a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except for failure to satisfy the conditions of this license) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer this software, where such license applies only to those patent claims, already acquired or hereafter acquired, licensable by such copyright holder or contributor that are necessarily infringed by: (a) their Contribution(s) (the licensed copyrights of copyright holders and non-copyrightable additions of contributors, in source or binary form) alone; or (b) combination of their Contribution(s) with the work of authorship to which such Contribution(s) was added by such copyright holder or contributor, if, at the time the Contribution is added, such addition causes such combination to be necessarily infringed. The patent license shall not apply to any other combinations which include the Contribution. Except as expressly stated above, no rights or licenses from any copyright holder or contributor is granted under this license, whether expressly, by implication, estoppel or otherwise. DISCLAIMER THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"License"},{"location":"LICENSE/#bsd-2-clause-plus-patent-license","text":"Copyright \u00a9 2017\u20132020, Contributors & the Greene Lab at the University of Pennsylvania Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Subject to the terms and conditions of this license, each copyright holder and contributor hereby grants to those receiving rights under this license a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except for failure to satisfy the conditions of this license) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer this software, where such license applies only to those patent claims, already acquired or hereafter acquired, licensable by such copyright holder or contributor that are necessarily infringed by: (a) their Contribution(s) (the licensed copyrights of copyright holders and non-copyrightable additions of contributors, in source or binary form) alone; or (b) combination of their Contribution(s) with the work of authorship to which such Contribution(s) was added by such copyright holder or contributor, if, at the time the Contribution is added, such addition causes such combination to be necessarily infringed. The patent license shall not apply to any other combinations which include the Contribution. Except as expressly stated above, no rights or licenses from any copyright holder or contributor is granted under this license, whether expressly, by implication, estoppel or otherwise. DISCLAIMER THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"BSD-2-Clause Plus Patent License"},{"location":"media/terminal-recordings/","text":"Terminal recordings manubot cite Terminal recordings were created using asciinema . The manubot cite recording is online at https://asciinema.org/a/205085?speed=2. GIF and SVG outputs were created using asciicast2gif and svg-term-cli with the following commands: asciicast2gif -s 2.0 https://asciinema.org/a/205085.cast manubot-cite-cast.gif svg-term --window --cast=205085 --out=manubot-cite-cast.svg","title":"Terminal recordings"},{"location":"media/terminal-recordings/#terminal-recordings","text":"","title":"Terminal recordings"},{"location":"media/terminal-recordings/#manubot-cite","text":"Terminal recordings were created using asciinema . The manubot cite recording is online at https://asciinema.org/a/205085?speed=2. GIF and SVG outputs were created using asciicast2gif and svg-term-cli with the following commands: asciicast2gif -s 2.0 https://asciinema.org/a/205085.cast manubot-cite-cast.gif svg-term --window --cast=205085 --out=manubot-cite-cast.svg","title":"manubot cite"},{"location":"reference/manubot/","text":"Module manubot View Source __version__ = \"0.3.1\" Sub-modules manubot.cite manubot.command manubot.pandoc manubot.process manubot.tests manubot.util manubot.webpage","title":"Index"},{"location":"reference/manubot/#module-manubot","text":"View Source __version__ = \"0.3.1\"","title":"Module manubot"},{"location":"reference/manubot/#sub-modules","text":"manubot.cite manubot.command manubot.pandoc manubot.process manubot.tests manubot.util manubot.webpage","title":"Sub-modules"},{"location":"reference/manubot/command/","text":"Module manubot.command Manubot's command line interface View Source \"\"\" Manubot's command line interface \"\"\" import argparse import logging import pathlib import sys import warnings import manubot from manubot.util import import_function def parse_arguments (): \"\"\" Read and process command line arguments. \"\"\" parser = argparse . ArgumentParser ( description = \"Manubot: the manuscript bot for scholarly writing\" ) parser . add_argument ( \"--version\" , action = \"version\" , version = f \"v{manubot.__version__}\" ) subparsers = parser . add_subparsers ( title = \"subcommands\" , description = \"All operations are done through subcommands:\" ) # Require specifying a sub-command subparsers . required = True # https://bugs.python.org/issue26510 subparsers . dest = \"subcommand\" # https://bugs.python.org/msg186387 add_subparser_process ( subparsers ) add_subparser_cite ( subparsers ) add_subparser_webpage ( subparsers ) for subparser in subparsers . choices . values (): subparser . add_argument ( \"--log-level\" , default = \"WARNING\" , choices = [ \"DEBUG\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" ], help = \"Set the logging level for stderr logging\" , ) args = parser . parse_args () return args def add_subparser_process ( subparsers ): parser = subparsers . add_parser ( name = \"process\" , help = \"process manuscript content\" , description = \"Process manuscript content to create outputs for Pandoc consumption. \" \"Performs bibliographic processing and templating.\" , ) parser . add_argument ( \"--content-directory\" , type = pathlib . Path , required = True , help = \"Directory where manuscript content files are located.\" , ) parser . add_argument ( \"--output-directory\" , type = pathlib . Path , required = True , help = \"Directory to output files generated by this script.\" , ) parser . add_argument ( \"--template-variables-path\" , action = \"append\" , default = [], help = \"Path or URL of a file containing template variables for jinja2. \" \"Serialization format is inferred from the file extension, with support for JSON, YAML, and TOML. \" \"If the format cannot be detected, the parser assumes JSON. \" \"Specify this argument multiple times to read multiple files. \" \"Variables can be applied to a namespace (i.e. stored under a dictionary key) \" \"like `--template-variables-path=namespace=path_or_url`. \" \"Namespaces must match the regex `[a-zA-Z_][a-zA-Z0-9_]*`.\" , ) parser . add_argument ( \"--skip-citations\" , action = \"store_true\" , required = True , help = \"Skip citation and reference processing. \" \"Support for citation and reference processing has been moved from `manubot process` to the pandoc-manubot-cite filter. \" \"Therefore this argument is now required. \" \"If citation-tags.tsv is found in content, \" \"these tags will be inserted in the markdown output using the reference-link syntax for citekey aliases. \" \"Appends content/manual-references*.* paths to Pandoc's metadata.bibliography field.\" , ) parser . add_argument ( \"--cache-directory\" , type = pathlib . Path , help = \"Custom cache directory. If not specified, caches to output-directory.\" , ) parser . add_argument ( \"--clear-requests-cache\" , action = \"store_true\" ) parser . set_defaults ( function = \"manubot.process.process_command.cli_process\" ) def add_subparser_cite ( subparsers ): parser = subparsers . add_parser ( name = \"cite\" , help = \"citation to CSL command line utility\" , description = \"Retrieve bibliographic metadata for one or more citation keys.\" , ) parser . add_argument ( \"--render\" , action = \"store_true\" , help = \"Whether to render CSL Data into a formatted reference list using Pandoc. \" \"Pandoc version 2.0 or higher is required for complete support of available output formats.\" , ) parser . add_argument ( \"--csl\" , default = \"https://github.com/greenelab/manubot-rootstock/raw/master/build/assets/style.csl\" , help = \"When --render, specify an XML CSL definition to style references (i.e. Pandoc's --csl option). \" \"Defaults to Manubot's style.\" , ) parser . add_argument ( \"--bibliography\" , default = [], action = \"append\" , help = \"File to read manual reference metadata. \" \"Specify multiple times to load multiple files. \" \"Similar to pandoc --bibliography.\" , ) parser . add_argument ( \"--format\" , choices = [ \"plain\" , \"markdown\" , \"docx\" , \"html\" , \"jats\" ], help = \"When --render, format to use for output file. \" \"If not specified, attempt to infer this from filename extension. \" \"Otherwise, default to plain.\" , ) parser . add_argument ( \"--output\" , type = pathlib . Path , help = \"Specify a file to write output, otherwise default to stdout.\" , ) parser . add_argument ( \"--allow-invalid-csl-data\" , dest = \"prune_csl\" , action = \"store_false\" , help = \"Allow CSL Items that do not conform to the JSON Schema. Skips CSL pruning.\" , ) parser . add_argument ( \"citekeys\" , nargs = \"+\" , help = \"One or more (space separated) citation keys to produce CSL for.\" , ) parser . set_defaults ( function = \"manubot.cite.cite_command.cli_cite\" ) def add_subparser_webpage ( subparsers ): parser = subparsers . add_parser ( name = \"webpage\" , help = \"deploy Manubot outputs to a webpage directory tree\" , description = \"Update the webpage directory tree with Manubot output files. \" \"This command should be run from the root directory of a Manubot manuscript that follows the Rootstock layout, containing `output` and `webpage` directories. \" \"HTML and PDF outputs are copied to the webpage directory, which is structured as static source files for website hosting.\" , ) parser . add_argument ( \"--checkout\" , nargs = \"?\" , const = \"gh-pages\" , default = None , help = \"branch to checkout /v directory contents from. \" \"For example, --checkout=upstream/gh-pages. \" \"--checkout is equivalent to --checkout=gh-pages. \" \"If --checkout is ommitted, no checkout is performed.\" , ) parser . add_argument ( \"--version\" , help = \"Used to create webpage/v/{version} directory. \" \"Generally a commit hash, tag, or 'local'. \" \"When omitted, version defaults to the commit hash on CI builds and 'local' elsewhere.\" , ) parser . add_argument ( \"--timestamp\" , action = \"store_true\" , help = \"timestamp versioned manuscripts in webpage/v using OpenTimestamps. \" \"Specify this flag to create timestamps for the current HTML and PDF outputs and upgrade any timestamps from past manuscript versions.\" , ) cache_group = parser . add_mutually_exclusive_group () cache_group . add_argument ( \"--no-ots-cache\" , action = \"store_true\" , help = \"disable the timestamp cache.\" ) cache_group . add_argument ( \"--ots-cache\" , default = pathlib . Path ( \"ci/cache/ots\" ), type = pathlib . Path , help = \"location for the timestamp cache (default: ci/cache/ots).\" , ) parser . set_defaults ( function = \"manubot.webpage.webpage_command.cli_webpage\" ) def setup_logging_and_errors () -> dict : \"\"\" Configure warnings and logging. Set up an ErrorHandler to detect whether messages have been logged at or above the ERROR level. \"\"\" import errorhandler # Track if message gets logged with severity of error or greater # See https://stackoverflow.com/a/45446664/4651668 error_handler = errorhandler . ErrorHandler () # Log DeprecationWarnings warnings . simplefilter ( \"always\" , DeprecationWarning ) logging . captureWarnings ( True ) # Log to stderr logger = logging . getLogger () stream_handler = logging . StreamHandler ( stream = sys . stderr ) stream_handler . setFormatter ( logging . Formatter ( \"## {levelname} \\n {message}\" , style = \"{\" ) ) logger . addHandler ( stream_handler ) return { \"logger\" : logger , \"error_handler\" : error_handler , } def exit_if_error_handler_fired ( error_handler ): \"\"\" If a message has been logged with severity of ERROR or greater, exit Python with a nonzero code. \"\"\" if error_handler . fired : logging . critical ( \"Failure: exiting with code 1 due to logged errors\" ) raise SystemExit ( 1 ) def main (): \"\"\" Called as a console_scripts entry point in setup.py. This function defines the manubot command line script. \"\"\" diagnostics = setup_logging_and_errors () args = parse_arguments () diagnostics [ \"logger\" ] . setLevel ( getattr ( logging , args . log_level )) function = import_function ( args . function ) function ( args ) exit_if_error_handler_fired ( diagnostics [ \"error_handler\" ]) Functions add_subparser_cite def add_subparser_cite ( subparsers ) View Source def add_subparser_cite ( subparsers ): parser = subparsers . add_parser ( name = \"cite\" , help = \"citation to CSL command line utility\" , description = \"Retrieve bibliographic metadata for one or more citation keys.\" , ) parser . add_argument ( \"--render\" , action = \"store_true\" , help = \"Whether to render CSL Data into a formatted reference list using Pandoc. \" \"Pandoc version 2.0 or higher is required for complete support of available output formats.\" , ) parser . add_argument ( \"--csl\" , default = \"https://github.com/greenelab/manubot-rootstock/raw/master/build/assets/style.csl\" , help = \"When --render, specify an XML CSL definition to style references (i.e. Pandoc's --csl option). \" \"Defaults to Manubot's style.\" , ) parser . add_argument ( \"--bibliography\" , default = [], action = \"append\" , help = \"File to read manual reference metadata. \" \"Specify multiple times to load multiple files. \" \"Similar to pandoc --bibliography.\" , ) parser . add_argument ( \"--format\" , choices = [ \"plain\" , \"markdown\" , \"docx\" , \"html\" , \"jats\" ], help = \"When --render, format to use for output file. \" \"If not specified, attempt to infer this from filename extension. \" \"Otherwise, default to plain.\" , ) parser . add_argument ( \"--output\" , type = pathlib . Path , help = \"Specify a file to write output, otherwise default to stdout.\" , ) parser . add_argument ( \"--allow-invalid-csl-data\" , dest = \"prune_csl\" , action = \"store_false\" , help = \"Allow CSL Items that do not conform to the JSON Schema. Skips CSL pruning.\" , ) parser . add_argument ( \"citekeys\" , nargs = \"+\" , help = \"One or more (space separated) citation keys to produce CSL for.\" , ) parser . set_defaults ( function = \"manubot.cite.cite_command.cli_cite\" ) add_subparser_process def add_subparser_process ( subparsers ) View Source def add_subparser_process ( subparsers ): parser = subparsers . add_parser ( name = \"process\" , help = \"process manuscript content\" , description = \"Process manuscript content to create outputs for Pandoc consumption. \" \"Performs bibliographic processing and templating.\" , ) parser . add_argument ( \"--content-directory\" , type = pathlib . Path , required = True , help = \"Directory where manuscript content files are located.\" , ) parser . add_argument ( \"--output-directory\" , type = pathlib . Path , required = True , help = \"Directory to output files generated by this script.\" , ) parser . add_argument ( \"--template-variables-path\" , action = \"append\" , default = [], help = \"Path or URL of a file containing template variables for jinja2. \" \"Serialization format is inferred from the file extension, with support for JSON, YAML, and TOML. \" \"If the format cannot be detected, the parser assumes JSON. \" \"Specify this argument multiple times to read multiple files. \" \"Variables can be applied to a namespace (i.e. stored under a dictionary key) \" \"like `--template-variables-path=namespace=path_or_url`. \" \"Namespaces must match the regex `[a-zA-Z_][a-zA-Z0-9_]*`.\" , ) parser . add_argument ( \"--skip-citations\" , action = \"store_true\" , required = True , help = \"Skip citation and reference processing. \" \"Support for citation and reference processing has been moved from `manubot process` to the pandoc-manubot-cite filter. \" \"Therefore this argument is now required. \" \"If citation-tags.tsv is found in content, \" \"these tags will be inserted in the markdown output using the reference-link syntax for citekey aliases. \" \"Appends content/manual-references*.* paths to Pandoc's metadata.bibliography field.\" , ) parser . add_argument ( \"--cache-directory\" , type = pathlib . Path , help = \"Custom cache directory. If not specified, caches to output-directory.\" , ) parser . add_argument ( \"--clear-requests-cache\" , action = \"store_true\" ) parser . set_defaults ( function = \"manubot.process.process_command.cli_process\" ) add_subparser_webpage def add_subparser_webpage ( subparsers ) View Source def add_subparser_webpage ( subparsers ): parser = subparsers . add_parser ( name = \"webpage\" , help = \"deploy Manubot outputs to a webpage directory tree\" , description = \"Update the webpage directory tree with Manubot output files. \" \"This command should be run from the root directory of a Manubot manuscript that follows the Rootstock layout, containing `output` and `webpage` directories. \" \"HTML and PDF outputs are copied to the webpage directory, which is structured as static source files for website hosting.\" , ) parser . add_argument ( \"--checkout\" , nargs = \"?\" , const = \"gh-pages\" , default = None , help = \"branch to checkout /v directory contents from. \" \"For example, --checkout=upstream/gh-pages. \" \"--checkout is equivalent to --checkout=gh-pages. \" \"If --checkout is ommitted, no checkout is performed.\" , ) parser . add_argument ( \"--version\" , help = \"Used to create webpage/v/{version} directory. \" \"Generally a commit hash, tag, or 'local'. \" \"When omitted, version defaults to the commit hash on CI builds and 'local' elsewhere.\" , ) parser . add_argument ( \"--timestamp\" , action = \"store_true\" , help = \"timestamp versioned manuscripts in webpage/v using OpenTimestamps. \" \"Specify this flag to create timestamps for the current HTML and PDF outputs and upgrade any timestamps from past manuscript versions.\" , ) cache_group = parser . add_mutually_exclusive_group () cache_group . add_argument ( \"--no-ots-cache\" , action = \"store_true\" , help = \"disable the timestamp cache.\" ) cache_group . add_argument ( \"--ots-cache\" , default = pathlib . Path ( \"ci/cache/ots\" ), type = pathlib . Path , help = \"location for the timestamp cache (default: ci/cache/ots).\" , ) parser . set_defaults ( function = \"manubot.webpage.webpage_command.cli_webpage\" ) exit_if_error_handler_fired def exit_if_error_handler_fired ( error_handler ) If a message has been logged with severity of ERROR or greater, exit Python with a nonzero code. View Source def exit_if_error_handler_fired ( error_handler ): \"\"\" If a message has been logged with severity of ERROR or greater, exit Python with a nonzero code. \"\"\" if error_handler . fired : logging . critical ( \"Failure: exiting with code 1 due to logged errors\" ) raise SystemExit ( 1 ) main def main ( ) Called as a console_scripts entry point in setup.py. This function defines the manubot command line script. View Source def main (): \"\"\" Called as a console_scripts entry point in setup.py. This function defines the manubot command line script. \"\"\" diagnostics = setup_logging_and_errors () args = parse_arguments () diagnostics [ \"logger\" ]. setLevel ( getattr ( logging , args . log_level )) function = import_function ( args . function ) function ( args ) exit_if_error_handler_fired ( diagnostics [ \"error_handler\" ]) parse_arguments def parse_arguments ( ) Read and process command line arguments. View Source def parse_arguments (): \"\"\" Read and process command line arguments. \"\"\" parser = argparse . ArgumentParser ( description = \"Manubot: the manuscript bot for scholarly writing\" ) parser . add_argument ( \"--version\" , action = \"version\" , version = f \"v{manubot.__version__}\" ) subparsers = parser . add_subparsers ( title = \"subcommands\" , description = \"All operations are done through subcommands:\" ) # Require specifying a sub - command subparsers . required = True # https : // bugs . python . org / issue26510 subparsers . dest = \"subcommand\" # https : // bugs . python . org / msg186387 add_subparser_process ( subparsers ) add_subparser_cite ( subparsers ) add_subparser_webpage ( subparsers ) for subparser in subparsers . choices . values (): subparser . add_argument ( \"--log-level\" , default = \"WARNING\" , choices = [ \"DEBUG\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" ], help = \"Set the logging level for stderr logging\" , ) args = parser . parse_args () return args setup_logging_and_errors def setup_logging_and_errors ( ) -> dict Configure warnings and logging. Set up an ErrorHandler to detect whether messages have been logged at or above the ERROR level. View Source def setup_logging_and_errors () -> dict : \"\"\" Configure warnings and logging. Set up an ErrorHandler to detect whether messages have been logged at or above the ERROR level. \"\"\" import errorhandler # Track if message gets logged with severity of error or greater # See https://stackoverflow.com/a/45446664/4651668 error_handler = errorhandler . ErrorHandler () # Log DeprecationWarnings warnings . simplefilter ( \"always\" , DeprecationWarning ) logging . captureWarnings ( True ) # Log to stderr logger = logging . getLogger () stream_handler = logging . StreamHandler ( stream = sys . stderr ) stream_handler . setFormatter ( logging . Formatter ( \"## {levelname} \\n {message}\" , style = \"{\" ) ) logger . addHandler ( stream_handler ) return { \"logger\" : logger , \"error_handler\" : error_handler , }","title":"Command"},{"location":"reference/manubot/command/#module-manubotcommand","text":"Manubot's command line interface View Source \"\"\" Manubot's command line interface \"\"\" import argparse import logging import pathlib import sys import warnings import manubot from manubot.util import import_function def parse_arguments (): \"\"\" Read and process command line arguments. \"\"\" parser = argparse . ArgumentParser ( description = \"Manubot: the manuscript bot for scholarly writing\" ) parser . add_argument ( \"--version\" , action = \"version\" , version = f \"v{manubot.__version__}\" ) subparsers = parser . add_subparsers ( title = \"subcommands\" , description = \"All operations are done through subcommands:\" ) # Require specifying a sub-command subparsers . required = True # https://bugs.python.org/issue26510 subparsers . dest = \"subcommand\" # https://bugs.python.org/msg186387 add_subparser_process ( subparsers ) add_subparser_cite ( subparsers ) add_subparser_webpage ( subparsers ) for subparser in subparsers . choices . values (): subparser . add_argument ( \"--log-level\" , default = \"WARNING\" , choices = [ \"DEBUG\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" ], help = \"Set the logging level for stderr logging\" , ) args = parser . parse_args () return args def add_subparser_process ( subparsers ): parser = subparsers . add_parser ( name = \"process\" , help = \"process manuscript content\" , description = \"Process manuscript content to create outputs for Pandoc consumption. \" \"Performs bibliographic processing and templating.\" , ) parser . add_argument ( \"--content-directory\" , type = pathlib . Path , required = True , help = \"Directory where manuscript content files are located.\" , ) parser . add_argument ( \"--output-directory\" , type = pathlib . Path , required = True , help = \"Directory to output files generated by this script.\" , ) parser . add_argument ( \"--template-variables-path\" , action = \"append\" , default = [], help = \"Path or URL of a file containing template variables for jinja2. \" \"Serialization format is inferred from the file extension, with support for JSON, YAML, and TOML. \" \"If the format cannot be detected, the parser assumes JSON. \" \"Specify this argument multiple times to read multiple files. \" \"Variables can be applied to a namespace (i.e. stored under a dictionary key) \" \"like `--template-variables-path=namespace=path_or_url`. \" \"Namespaces must match the regex `[a-zA-Z_][a-zA-Z0-9_]*`.\" , ) parser . add_argument ( \"--skip-citations\" , action = \"store_true\" , required = True , help = \"Skip citation and reference processing. \" \"Support for citation and reference processing has been moved from `manubot process` to the pandoc-manubot-cite filter. \" \"Therefore this argument is now required. \" \"If citation-tags.tsv is found in content, \" \"these tags will be inserted in the markdown output using the reference-link syntax for citekey aliases. \" \"Appends content/manual-references*.* paths to Pandoc's metadata.bibliography field.\" , ) parser . add_argument ( \"--cache-directory\" , type = pathlib . Path , help = \"Custom cache directory. If not specified, caches to output-directory.\" , ) parser . add_argument ( \"--clear-requests-cache\" , action = \"store_true\" ) parser . set_defaults ( function = \"manubot.process.process_command.cli_process\" ) def add_subparser_cite ( subparsers ): parser = subparsers . add_parser ( name = \"cite\" , help = \"citation to CSL command line utility\" , description = \"Retrieve bibliographic metadata for one or more citation keys.\" , ) parser . add_argument ( \"--render\" , action = \"store_true\" , help = \"Whether to render CSL Data into a formatted reference list using Pandoc. \" \"Pandoc version 2.0 or higher is required for complete support of available output formats.\" , ) parser . add_argument ( \"--csl\" , default = \"https://github.com/greenelab/manubot-rootstock/raw/master/build/assets/style.csl\" , help = \"When --render, specify an XML CSL definition to style references (i.e. Pandoc's --csl option). \" \"Defaults to Manubot's style.\" , ) parser . add_argument ( \"--bibliography\" , default = [], action = \"append\" , help = \"File to read manual reference metadata. \" \"Specify multiple times to load multiple files. \" \"Similar to pandoc --bibliography.\" , ) parser . add_argument ( \"--format\" , choices = [ \"plain\" , \"markdown\" , \"docx\" , \"html\" , \"jats\" ], help = \"When --render, format to use for output file. \" \"If not specified, attempt to infer this from filename extension. \" \"Otherwise, default to plain.\" , ) parser . add_argument ( \"--output\" , type = pathlib . Path , help = \"Specify a file to write output, otherwise default to stdout.\" , ) parser . add_argument ( \"--allow-invalid-csl-data\" , dest = \"prune_csl\" , action = \"store_false\" , help = \"Allow CSL Items that do not conform to the JSON Schema. Skips CSL pruning.\" , ) parser . add_argument ( \"citekeys\" , nargs = \"+\" , help = \"One or more (space separated) citation keys to produce CSL for.\" , ) parser . set_defaults ( function = \"manubot.cite.cite_command.cli_cite\" ) def add_subparser_webpage ( subparsers ): parser = subparsers . add_parser ( name = \"webpage\" , help = \"deploy Manubot outputs to a webpage directory tree\" , description = \"Update the webpage directory tree with Manubot output files. \" \"This command should be run from the root directory of a Manubot manuscript that follows the Rootstock layout, containing `output` and `webpage` directories. \" \"HTML and PDF outputs are copied to the webpage directory, which is structured as static source files for website hosting.\" , ) parser . add_argument ( \"--checkout\" , nargs = \"?\" , const = \"gh-pages\" , default = None , help = \"branch to checkout /v directory contents from. \" \"For example, --checkout=upstream/gh-pages. \" \"--checkout is equivalent to --checkout=gh-pages. \" \"If --checkout is ommitted, no checkout is performed.\" , ) parser . add_argument ( \"--version\" , help = \"Used to create webpage/v/{version} directory. \" \"Generally a commit hash, tag, or 'local'. \" \"When omitted, version defaults to the commit hash on CI builds and 'local' elsewhere.\" , ) parser . add_argument ( \"--timestamp\" , action = \"store_true\" , help = \"timestamp versioned manuscripts in webpage/v using OpenTimestamps. \" \"Specify this flag to create timestamps for the current HTML and PDF outputs and upgrade any timestamps from past manuscript versions.\" , ) cache_group = parser . add_mutually_exclusive_group () cache_group . add_argument ( \"--no-ots-cache\" , action = \"store_true\" , help = \"disable the timestamp cache.\" ) cache_group . add_argument ( \"--ots-cache\" , default = pathlib . Path ( \"ci/cache/ots\" ), type = pathlib . Path , help = \"location for the timestamp cache (default: ci/cache/ots).\" , ) parser . set_defaults ( function = \"manubot.webpage.webpage_command.cli_webpage\" ) def setup_logging_and_errors () -> dict : \"\"\" Configure warnings and logging. Set up an ErrorHandler to detect whether messages have been logged at or above the ERROR level. \"\"\" import errorhandler # Track if message gets logged with severity of error or greater # See https://stackoverflow.com/a/45446664/4651668 error_handler = errorhandler . ErrorHandler () # Log DeprecationWarnings warnings . simplefilter ( \"always\" , DeprecationWarning ) logging . captureWarnings ( True ) # Log to stderr logger = logging . getLogger () stream_handler = logging . StreamHandler ( stream = sys . stderr ) stream_handler . setFormatter ( logging . Formatter ( \"## {levelname} \\n {message}\" , style = \"{\" ) ) logger . addHandler ( stream_handler ) return { \"logger\" : logger , \"error_handler\" : error_handler , } def exit_if_error_handler_fired ( error_handler ): \"\"\" If a message has been logged with severity of ERROR or greater, exit Python with a nonzero code. \"\"\" if error_handler . fired : logging . critical ( \"Failure: exiting with code 1 due to logged errors\" ) raise SystemExit ( 1 ) def main (): \"\"\" Called as a console_scripts entry point in setup.py. This function defines the manubot command line script. \"\"\" diagnostics = setup_logging_and_errors () args = parse_arguments () diagnostics [ \"logger\" ] . setLevel ( getattr ( logging , args . log_level )) function = import_function ( args . function ) function ( args ) exit_if_error_handler_fired ( diagnostics [ \"error_handler\" ])","title":"Module manubot.command"},{"location":"reference/manubot/command/#functions","text":"","title":"Functions"},{"location":"reference/manubot/command/#add_subparser_cite","text":"def add_subparser_cite ( subparsers ) View Source def add_subparser_cite ( subparsers ): parser = subparsers . add_parser ( name = \"cite\" , help = \"citation to CSL command line utility\" , description = \"Retrieve bibliographic metadata for one or more citation keys.\" , ) parser . add_argument ( \"--render\" , action = \"store_true\" , help = \"Whether to render CSL Data into a formatted reference list using Pandoc. \" \"Pandoc version 2.0 or higher is required for complete support of available output formats.\" , ) parser . add_argument ( \"--csl\" , default = \"https://github.com/greenelab/manubot-rootstock/raw/master/build/assets/style.csl\" , help = \"When --render, specify an XML CSL definition to style references (i.e. Pandoc's --csl option). \" \"Defaults to Manubot's style.\" , ) parser . add_argument ( \"--bibliography\" , default = [], action = \"append\" , help = \"File to read manual reference metadata. \" \"Specify multiple times to load multiple files. \" \"Similar to pandoc --bibliography.\" , ) parser . add_argument ( \"--format\" , choices = [ \"plain\" , \"markdown\" , \"docx\" , \"html\" , \"jats\" ], help = \"When --render, format to use for output file. \" \"If not specified, attempt to infer this from filename extension. \" \"Otherwise, default to plain.\" , ) parser . add_argument ( \"--output\" , type = pathlib . Path , help = \"Specify a file to write output, otherwise default to stdout.\" , ) parser . add_argument ( \"--allow-invalid-csl-data\" , dest = \"prune_csl\" , action = \"store_false\" , help = \"Allow CSL Items that do not conform to the JSON Schema. Skips CSL pruning.\" , ) parser . add_argument ( \"citekeys\" , nargs = \"+\" , help = \"One or more (space separated) citation keys to produce CSL for.\" , ) parser . set_defaults ( function = \"manubot.cite.cite_command.cli_cite\" )","title":"add_subparser_cite"},{"location":"reference/manubot/command/#add_subparser_process","text":"def add_subparser_process ( subparsers ) View Source def add_subparser_process ( subparsers ): parser = subparsers . add_parser ( name = \"process\" , help = \"process manuscript content\" , description = \"Process manuscript content to create outputs for Pandoc consumption. \" \"Performs bibliographic processing and templating.\" , ) parser . add_argument ( \"--content-directory\" , type = pathlib . Path , required = True , help = \"Directory where manuscript content files are located.\" , ) parser . add_argument ( \"--output-directory\" , type = pathlib . Path , required = True , help = \"Directory to output files generated by this script.\" , ) parser . add_argument ( \"--template-variables-path\" , action = \"append\" , default = [], help = \"Path or URL of a file containing template variables for jinja2. \" \"Serialization format is inferred from the file extension, with support for JSON, YAML, and TOML. \" \"If the format cannot be detected, the parser assumes JSON. \" \"Specify this argument multiple times to read multiple files. \" \"Variables can be applied to a namespace (i.e. stored under a dictionary key) \" \"like `--template-variables-path=namespace=path_or_url`. \" \"Namespaces must match the regex `[a-zA-Z_][a-zA-Z0-9_]*`.\" , ) parser . add_argument ( \"--skip-citations\" , action = \"store_true\" , required = True , help = \"Skip citation and reference processing. \" \"Support for citation and reference processing has been moved from `manubot process` to the pandoc-manubot-cite filter. \" \"Therefore this argument is now required. \" \"If citation-tags.tsv is found in content, \" \"these tags will be inserted in the markdown output using the reference-link syntax for citekey aliases. \" \"Appends content/manual-references*.* paths to Pandoc's metadata.bibliography field.\" , ) parser . add_argument ( \"--cache-directory\" , type = pathlib . Path , help = \"Custom cache directory. If not specified, caches to output-directory.\" , ) parser . add_argument ( \"--clear-requests-cache\" , action = \"store_true\" ) parser . set_defaults ( function = \"manubot.process.process_command.cli_process\" )","title":"add_subparser_process"},{"location":"reference/manubot/command/#add_subparser_webpage","text":"def add_subparser_webpage ( subparsers ) View Source def add_subparser_webpage ( subparsers ): parser = subparsers . add_parser ( name = \"webpage\" , help = \"deploy Manubot outputs to a webpage directory tree\" , description = \"Update the webpage directory tree with Manubot output files. \" \"This command should be run from the root directory of a Manubot manuscript that follows the Rootstock layout, containing `output` and `webpage` directories. \" \"HTML and PDF outputs are copied to the webpage directory, which is structured as static source files for website hosting.\" , ) parser . add_argument ( \"--checkout\" , nargs = \"?\" , const = \"gh-pages\" , default = None , help = \"branch to checkout /v directory contents from. \" \"For example, --checkout=upstream/gh-pages. \" \"--checkout is equivalent to --checkout=gh-pages. \" \"If --checkout is ommitted, no checkout is performed.\" , ) parser . add_argument ( \"--version\" , help = \"Used to create webpage/v/{version} directory. \" \"Generally a commit hash, tag, or 'local'. \" \"When omitted, version defaults to the commit hash on CI builds and 'local' elsewhere.\" , ) parser . add_argument ( \"--timestamp\" , action = \"store_true\" , help = \"timestamp versioned manuscripts in webpage/v using OpenTimestamps. \" \"Specify this flag to create timestamps for the current HTML and PDF outputs and upgrade any timestamps from past manuscript versions.\" , ) cache_group = parser . add_mutually_exclusive_group () cache_group . add_argument ( \"--no-ots-cache\" , action = \"store_true\" , help = \"disable the timestamp cache.\" ) cache_group . add_argument ( \"--ots-cache\" , default = pathlib . Path ( \"ci/cache/ots\" ), type = pathlib . Path , help = \"location for the timestamp cache (default: ci/cache/ots).\" , ) parser . set_defaults ( function = \"manubot.webpage.webpage_command.cli_webpage\" )","title":"add_subparser_webpage"},{"location":"reference/manubot/command/#exit_if_error_handler_fired","text":"def exit_if_error_handler_fired ( error_handler ) If a message has been logged with severity of ERROR or greater, exit Python with a nonzero code. View Source def exit_if_error_handler_fired ( error_handler ): \"\"\" If a message has been logged with severity of ERROR or greater, exit Python with a nonzero code. \"\"\" if error_handler . fired : logging . critical ( \"Failure: exiting with code 1 due to logged errors\" ) raise SystemExit ( 1 )","title":"exit_if_error_handler_fired"},{"location":"reference/manubot/command/#main","text":"def main ( ) Called as a console_scripts entry point in setup.py. This function defines the manubot command line script. View Source def main (): \"\"\" Called as a console_scripts entry point in setup.py. This function defines the manubot command line script. \"\"\" diagnostics = setup_logging_and_errors () args = parse_arguments () diagnostics [ \"logger\" ]. setLevel ( getattr ( logging , args . log_level )) function = import_function ( args . function ) function ( args ) exit_if_error_handler_fired ( diagnostics [ \"error_handler\" ])","title":"main"},{"location":"reference/manubot/command/#parse_arguments","text":"def parse_arguments ( ) Read and process command line arguments. View Source def parse_arguments (): \"\"\" Read and process command line arguments. \"\"\" parser = argparse . ArgumentParser ( description = \"Manubot: the manuscript bot for scholarly writing\" ) parser . add_argument ( \"--version\" , action = \"version\" , version = f \"v{manubot.__version__}\" ) subparsers = parser . add_subparsers ( title = \"subcommands\" , description = \"All operations are done through subcommands:\" ) # Require specifying a sub - command subparsers . required = True # https : // bugs . python . org / issue26510 subparsers . dest = \"subcommand\" # https : // bugs . python . org / msg186387 add_subparser_process ( subparsers ) add_subparser_cite ( subparsers ) add_subparser_webpage ( subparsers ) for subparser in subparsers . choices . values (): subparser . add_argument ( \"--log-level\" , default = \"WARNING\" , choices = [ \"DEBUG\" , \"INFO\" , \"WARNING\" , \"ERROR\" , \"CRITICAL\" ], help = \"Set the logging level for stderr logging\" , ) args = parser . parse_args () return args","title":"parse_arguments"},{"location":"reference/manubot/command/#setup_logging_and_errors","text":"def setup_logging_and_errors ( ) -> dict Configure warnings and logging. Set up an ErrorHandler to detect whether messages have been logged at or above the ERROR level. View Source def setup_logging_and_errors () -> dict : \"\"\" Configure warnings and logging. Set up an ErrorHandler to detect whether messages have been logged at or above the ERROR level. \"\"\" import errorhandler # Track if message gets logged with severity of error or greater # See https://stackoverflow.com/a/45446664/4651668 error_handler = errorhandler . ErrorHandler () # Log DeprecationWarnings warnings . simplefilter ( \"always\" , DeprecationWarning ) logging . captureWarnings ( True ) # Log to stderr logger = logging . getLogger () stream_handler = logging . StreamHandler ( stream = sys . stderr ) stream_handler . setFormatter ( logging . Formatter ( \"## {levelname} \\n {message}\" , style = \"{\" ) ) logger . addHandler ( stream_handler ) return { \"logger\" : logger , \"error_handler\" : error_handler , }","title":"setup_logging_and_errors"},{"location":"reference/manubot/util/","text":"Module manubot.util View Source import importlib import json import logging import os import pathlib import platform import shlex import shutil import subprocess import sys # Email address that forwards to Manubot maintainers contact_email = \"contact@manubot.org\" def import_function ( name ): \"\"\" Import a function in a module specified by name. For example, if name were 'manubot.cite.cite_command.cli_cite', the cli_cite function would be returned as an object. See https://stackoverflow.com/a/8790232/4651668. \"\"\" module_name , function_name = name . rsplit ( \".\" , 1 ) module = importlib . import_module ( module_name ) return getattr ( module , function_name ) def get_manubot_user_agent (): \"\"\" Return a User-Agent string for web request headers to help services identify requests as coming from Manubot. \"\"\" try : from manubot import __version__ as manubot_version except ImportError : manubot_version = \"\" return ( f \"manubot/{manubot_version} \" f \"({platform.system()}; Python/{sys.version_info.major}.{sys.version_info.minor}) \" f \"<{contact_email}>\" ) def shlex_join ( split_command ): \"\"\" Backport shlex.join for Python < 3.8. Also cast all args to str to increase versatility. https://github.com/python/cpython/pull/7605 https://bugs.python.org/issue22454 \"\"\" return \" \" . join ( shlex . quote ( str ( arg )) for arg in split_command ) \"\"\"Valid schemes for HTTP URL detection\"\"\" _http_url_schemes = { \"http\" , \"https\" } def is_http_url ( string : str ) -> bool : \"\"\" Return whether `string` is an HTTP(s) Uniform Resource Locator (URL). \"\"\" from urllib.parse import urlparse parsed_url = urlparse ( string ) return parsed_url . scheme in _http_url_schemes def read_serialized_data ( path : str ): \"\"\" Read seralized data from a local file path or web-address. If file format extension is not detected in path, assumes JSON. If a URL does not contain the appropriate suffix, one workaround is to hack the fragment like https://example.org#/variables.toml \"\"\" import requests path_str = os . fspath ( path ) path_lib = pathlib . Path ( path ) supported_suffixes = { \".json\" , \".yaml\" , \".yml\" , \".toml\" } suffixes = set ( path_lib . suffixes ) if is_http_url ( path_str ): response = requests . get ( path_str ) if not suffixes & supported_suffixes : # if URL has no supported suffixes, evaluate suffixes of final redirect suffixes = set ( pathlib . Path ( response . url ) . suffixes ) text = response . text else : text = path_lib . read_text ( encoding = \"utf-8-sig\" ) if { \".yaml\" , \".yml\" } & suffixes : import yaml try : return yaml . safe_load ( text ) except yaml . parser . ParserError as error : _lint_yaml ( path ) raise error if \".toml\" in suffixes : import toml return toml . loads ( text ) if \".json\" not in suffixes : logging . info ( f \"read_serialized_data cannot infer serialization format from the extension of {path_str!r}. \" f \"Supported extensions are {', '.join(supported_suffixes)}. \" \"Assuming JSON.\" ) return json . loads ( text ) \"\"\" yamllint configuration as per https://yamllint.readthedocs.io/en/stable/configuration.html \"\"\" _yamllint_config = { \"extends\" : \"relaxed\" , \"rules\" : { \"line-length\" : \"disable\" , \"trailing-spaces\" : { \"level\" : \"warning\" }}, } def _lint_yaml ( path ): if not shutil . which ( \"yamllint\" ): logging . info ( f \"yamllint executable not found, skipping linting for {path}\" ) return args = [ \"yamllint\" , \"--config-data\" , json . dumps ( _yamllint_config , indent = None ), os . fspath ( path ), ] sys . stderr . write ( f \"yamllint {path}: \\n \" ) subprocess . run ( args , stdout = sys . stderr ) def read_serialized_dict ( path : str ) -> dict : \"\"\" Read serialized data, confirming that the top-level object is a dictionary. Delegates to `read_serialized_data`. \"\"\" data = read_serialized_data ( path ) if isinstance ( data , dict ): return data raise TypeError ( f \"Expected data encoded by {path!r} to be a dictionary at the top-level. \" f \"Received {data.__class__.__name__!r} instead.\" ) Variables contact_email Functions get_manubot_user_agent def get_manubot_user_agent ( ) Return a User-Agent string for web request headers to help services identify requests as coming from Manubot. View Source def get_manubot_user_agent (): \"\"\" Return a User-Agent string for web request headers to help services identify requests as coming from Manubot. \"\"\" try : from manubot import __version__ as manubot_version except ImportError : manubot_version = \"\" return ( f \"manubot/{manubot_version} \" f \"({platform.system()}; Python/{sys.version_info.major}.{sys.version_info.minor}) \" f \"<{contact_email}>\" ) import_function def import_function ( name ) Import a function in a module specified by name. For example, if name were 'manubot.cite.cite_command.cli_cite', the cli_cite function would be returned as an object. See https://stackoverflow.com/a/8790232/4651668. View Source def import_function ( name ): \"\"\" Import a function in a module specified by name. For example, if name were 'manubot.cite.cite_command.cli_cite', the cli_cite function would be returned as an object. See https://stackoverflow.com/a/8790232/4651668. \"\"\" module_name , function_name = name . rsplit ( \".\" , 1 ) module = importlib . import_module ( module_name ) return getattr ( module , function_name ) is_http_url def is_http_url ( string : str ) -> bool Return whether string is an HTTP(s) Uniform Resource Locator (URL). View Source def is_http_url ( string : str ) -> bool : \"\"\" Return whether `string` is an HTTP(s) Uniform Resource Locator (URL). \"\"\" from urllib.parse import urlparse parsed_url = urlparse ( string ) return parsed_url . scheme in _http_url_schemes read_serialized_data def read_serialized_data ( path : str ) Read seralized data from a local file path or web-address. If file format extension is not detected in path, assumes JSON. If a URL does not contain the appropriate suffix, one workaround is to hack the fragment like https://example.org#/variables.toml View Source def read_serialized_data ( path : str ): \"\"\" Read seralized data from a local file path or web-address. If file format extension is not detected in path, assumes JSON. If a URL does not contain the appropriate suffix, one workaround is to hack the fragment like https://example.org#/variables.toml \"\"\" import requests path_str = os . fspath ( path ) path_lib = pathlib . Path ( path ) supported_suffixes = { \".json\" , \".yaml\" , \".yml\" , \".toml\" } suffixes = set ( path_lib . suffixes ) if is_http_url ( path_str ): response = requests . get ( path_str ) if not suffixes & supported_suffixes : # if URL has no supported suffixes, evaluate suffixes of final redirect suffixes = set ( pathlib . Path ( response . url ) . suffixes ) text = response . text else : text = path_lib . read_text ( encoding = \"utf-8-sig\" ) if { \".yaml\" , \".yml\" } & suffixes : import yaml try : return yaml . safe_load ( text ) except yaml . parser . ParserError as error : _lint_yaml ( path ) raise error if \".toml\" in suffixes : import toml return toml . loads ( text ) if \".json\" not in suffixes : logging . info ( f \"read_serialized_data cannot infer serialization format from the extension of {path_str!r}. \" f \"Supported extensions are {', '.join(supported_suffixes)}. \" \"Assuming JSON.\" ) return json . loads ( text ) read_serialized_dict def read_serialized_dict ( path : str ) -> dict Read serialized data, confirming that the top-level object is a dictionary. Delegates to read_serialized_data . View Source def read_serialized_dict ( path : str ) -> dict : \"\"\" Read serialized data, confirming that the top-level object is a dictionary. Delegates to `read_serialized_data`. \"\"\" data = read_serialized_data ( path ) if isinstance ( data , dict ): return data raise TypeError ( f \"Expected data encoded by {path!r} to be a dictionary at the top-level. \" f \"Received {data.__class__.__name__!r} instead.\" ) shlex_join def shlex_join ( split_command ) Backport shlex.join for Python < 3.8. Also cast all args to str to increase versatility. https://github.com/python/cpython/pull/7605 https://bugs.python.org/issue22454 View Source def shlex_join ( split_command ): \"\"\" Backport shlex.join for Python < 3.8. Also cast all args to str to increase versatility. https://github.com/python/cpython/pull/7605 https://bugs.python.org/issue22454 \"\"\" return \" \" . join ( shlex . quote ( str ( arg )) for arg in split_command )","title":"Util"},{"location":"reference/manubot/util/#module-manubotutil","text":"View Source import importlib import json import logging import os import pathlib import platform import shlex import shutil import subprocess import sys # Email address that forwards to Manubot maintainers contact_email = \"contact@manubot.org\" def import_function ( name ): \"\"\" Import a function in a module specified by name. For example, if name were 'manubot.cite.cite_command.cli_cite', the cli_cite function would be returned as an object. See https://stackoverflow.com/a/8790232/4651668. \"\"\" module_name , function_name = name . rsplit ( \".\" , 1 ) module = importlib . import_module ( module_name ) return getattr ( module , function_name ) def get_manubot_user_agent (): \"\"\" Return a User-Agent string for web request headers to help services identify requests as coming from Manubot. \"\"\" try : from manubot import __version__ as manubot_version except ImportError : manubot_version = \"\" return ( f \"manubot/{manubot_version} \" f \"({platform.system()}; Python/{sys.version_info.major}.{sys.version_info.minor}) \" f \"<{contact_email}>\" ) def shlex_join ( split_command ): \"\"\" Backport shlex.join for Python < 3.8. Also cast all args to str to increase versatility. https://github.com/python/cpython/pull/7605 https://bugs.python.org/issue22454 \"\"\" return \" \" . join ( shlex . quote ( str ( arg )) for arg in split_command ) \"\"\"Valid schemes for HTTP URL detection\"\"\" _http_url_schemes = { \"http\" , \"https\" } def is_http_url ( string : str ) -> bool : \"\"\" Return whether `string` is an HTTP(s) Uniform Resource Locator (URL). \"\"\" from urllib.parse import urlparse parsed_url = urlparse ( string ) return parsed_url . scheme in _http_url_schemes def read_serialized_data ( path : str ): \"\"\" Read seralized data from a local file path or web-address. If file format extension is not detected in path, assumes JSON. If a URL does not contain the appropriate suffix, one workaround is to hack the fragment like https://example.org#/variables.toml \"\"\" import requests path_str = os . fspath ( path ) path_lib = pathlib . Path ( path ) supported_suffixes = { \".json\" , \".yaml\" , \".yml\" , \".toml\" } suffixes = set ( path_lib . suffixes ) if is_http_url ( path_str ): response = requests . get ( path_str ) if not suffixes & supported_suffixes : # if URL has no supported suffixes, evaluate suffixes of final redirect suffixes = set ( pathlib . Path ( response . url ) . suffixes ) text = response . text else : text = path_lib . read_text ( encoding = \"utf-8-sig\" ) if { \".yaml\" , \".yml\" } & suffixes : import yaml try : return yaml . safe_load ( text ) except yaml . parser . ParserError as error : _lint_yaml ( path ) raise error if \".toml\" in suffixes : import toml return toml . loads ( text ) if \".json\" not in suffixes : logging . info ( f \"read_serialized_data cannot infer serialization format from the extension of {path_str!r}. \" f \"Supported extensions are {', '.join(supported_suffixes)}. \" \"Assuming JSON.\" ) return json . loads ( text ) \"\"\" yamllint configuration as per https://yamllint.readthedocs.io/en/stable/configuration.html \"\"\" _yamllint_config = { \"extends\" : \"relaxed\" , \"rules\" : { \"line-length\" : \"disable\" , \"trailing-spaces\" : { \"level\" : \"warning\" }}, } def _lint_yaml ( path ): if not shutil . which ( \"yamllint\" ): logging . info ( f \"yamllint executable not found, skipping linting for {path}\" ) return args = [ \"yamllint\" , \"--config-data\" , json . dumps ( _yamllint_config , indent = None ), os . fspath ( path ), ] sys . stderr . write ( f \"yamllint {path}: \\n \" ) subprocess . run ( args , stdout = sys . stderr ) def read_serialized_dict ( path : str ) -> dict : \"\"\" Read serialized data, confirming that the top-level object is a dictionary. Delegates to `read_serialized_data`. \"\"\" data = read_serialized_data ( path ) if isinstance ( data , dict ): return data raise TypeError ( f \"Expected data encoded by {path!r} to be a dictionary at the top-level. \" f \"Received {data.__class__.__name__!r} instead.\" )","title":"Module manubot.util"},{"location":"reference/manubot/util/#variables","text":"contact_email","title":"Variables"},{"location":"reference/manubot/util/#functions","text":"","title":"Functions"},{"location":"reference/manubot/util/#get_manubot_user_agent","text":"def get_manubot_user_agent ( ) Return a User-Agent string for web request headers to help services identify requests as coming from Manubot. View Source def get_manubot_user_agent (): \"\"\" Return a User-Agent string for web request headers to help services identify requests as coming from Manubot. \"\"\" try : from manubot import __version__ as manubot_version except ImportError : manubot_version = \"\" return ( f \"manubot/{manubot_version} \" f \"({platform.system()}; Python/{sys.version_info.major}.{sys.version_info.minor}) \" f \"<{contact_email}>\" )","title":"get_manubot_user_agent"},{"location":"reference/manubot/util/#import_function","text":"def import_function ( name ) Import a function in a module specified by name. For example, if name were 'manubot.cite.cite_command.cli_cite', the cli_cite function would be returned as an object. See https://stackoverflow.com/a/8790232/4651668. View Source def import_function ( name ): \"\"\" Import a function in a module specified by name. For example, if name were 'manubot.cite.cite_command.cli_cite', the cli_cite function would be returned as an object. See https://stackoverflow.com/a/8790232/4651668. \"\"\" module_name , function_name = name . rsplit ( \".\" , 1 ) module = importlib . import_module ( module_name ) return getattr ( module , function_name )","title":"import_function"},{"location":"reference/manubot/util/#is_http_url","text":"def is_http_url ( string : str ) -> bool Return whether string is an HTTP(s) Uniform Resource Locator (URL). View Source def is_http_url ( string : str ) -> bool : \"\"\" Return whether `string` is an HTTP(s) Uniform Resource Locator (URL). \"\"\" from urllib.parse import urlparse parsed_url = urlparse ( string ) return parsed_url . scheme in _http_url_schemes","title":"is_http_url"},{"location":"reference/manubot/util/#read_serialized_data","text":"def read_serialized_data ( path : str ) Read seralized data from a local file path or web-address. If file format extension is not detected in path, assumes JSON. If a URL does not contain the appropriate suffix, one workaround is to hack the fragment like https://example.org#/variables.toml View Source def read_serialized_data ( path : str ): \"\"\" Read seralized data from a local file path or web-address. If file format extension is not detected in path, assumes JSON. If a URL does not contain the appropriate suffix, one workaround is to hack the fragment like https://example.org#/variables.toml \"\"\" import requests path_str = os . fspath ( path ) path_lib = pathlib . Path ( path ) supported_suffixes = { \".json\" , \".yaml\" , \".yml\" , \".toml\" } suffixes = set ( path_lib . suffixes ) if is_http_url ( path_str ): response = requests . get ( path_str ) if not suffixes & supported_suffixes : # if URL has no supported suffixes, evaluate suffixes of final redirect suffixes = set ( pathlib . Path ( response . url ) . suffixes ) text = response . text else : text = path_lib . read_text ( encoding = \"utf-8-sig\" ) if { \".yaml\" , \".yml\" } & suffixes : import yaml try : return yaml . safe_load ( text ) except yaml . parser . ParserError as error : _lint_yaml ( path ) raise error if \".toml\" in suffixes : import toml return toml . loads ( text ) if \".json\" not in suffixes : logging . info ( f \"read_serialized_data cannot infer serialization format from the extension of {path_str!r}. \" f \"Supported extensions are {', '.join(supported_suffixes)}. \" \"Assuming JSON.\" ) return json . loads ( text )","title":"read_serialized_data"},{"location":"reference/manubot/util/#read_serialized_dict","text":"def read_serialized_dict ( path : str ) -> dict Read serialized data, confirming that the top-level object is a dictionary. Delegates to read_serialized_data . View Source def read_serialized_dict ( path : str ) -> dict : \"\"\" Read serialized data, confirming that the top-level object is a dictionary. Delegates to `read_serialized_data`. \"\"\" data = read_serialized_data ( path ) if isinstance ( data , dict ): return data raise TypeError ( f \"Expected data encoded by {path!r} to be a dictionary at the top-level. \" f \"Received {data.__class__.__name__!r} instead.\" )","title":"read_serialized_dict"},{"location":"reference/manubot/util/#shlex_join","text":"def shlex_join ( split_command ) Backport shlex.join for Python < 3.8. Also cast all args to str to increase versatility. https://github.com/python/cpython/pull/7605 https://bugs.python.org/issue22454 View Source def shlex_join ( split_command ): \"\"\" Backport shlex.join for Python < 3.8. Also cast all args to str to increase versatility. https://github.com/python/cpython/pull/7605 https://bugs.python.org/issue22454 \"\"\" return \" \" . join ( shlex . quote ( str ( arg )) for arg in split_command )","title":"shlex_join"},{"location":"reference/manubot/cite/","text":"Module manubot.cite View Source __all__ = [ \"citekey_to_csl_item\" , ] from manubot.cite.citekey import citekey_to_csl_item Sub-modules manubot.cite.arxiv manubot.cite.citations manubot.cite.cite_command manubot.cite.citekey manubot.cite.citeproc manubot.cite.csl_item manubot.cite.curie manubot.cite.doi manubot.cite.handlers manubot.cite.isbn manubot.cite.pubmed manubot.cite.tests manubot.cite.unpaywall manubot.cite.url manubot.cite.wikidata manubot.cite.zotero Functions citekey_to_csl_item def citekey_to_csl_item ( citekey , prune = True , manual_refs = {}, log_level : Union [ str , int ] = 'WARNING' ) Generate a CSL_Item for the input citekey. View Source def citekey_to_csl_item ( citekey , prune = True , manual_refs = {}, log_level : tp . Union [ str , int ] = \"WARNING\" ): \"\"\" Generate a CSL_Item for the input citekey. \"\"\" from manubot import __version__ as manubot_version # https://stackoverflow.com/a/35704430/4651668 log_level = logging . _checkLevel ( log_level ) if not isinstance ( citekey , CiteKey ): citekey = CiteKey ( citekey ) if citekey . standard_id in manual_refs : return manual_refs [ citekey . standard_id ] try : csl_item = citekey . csl_item except Exception as error : logging . log ( log_level , f \"Generating csl_item for {citekey.standard_id!r} failed \" f \"due to a {error.__class__.__name__}: \\n {error}\" , ) logging . info ( error , exc_info = True ) return None # update csl_item with manubot generated metadata note_text = f \"This CSL JSON Item was automatically generated by Manubot v{manubot_version} using citation-by-identifier.\" note_dict = { \"standard_id\" : citekey . standard_id } csl_item . note_append_text ( note_text ) csl_item . note_append_dict ( note_dict ) csl_item . set_id ( citekey . short_id ) csl_item . clean ( prune = prune ) return csl_item","title":"Index"},{"location":"reference/manubot/cite/#module-manubotcite","text":"View Source __all__ = [ \"citekey_to_csl_item\" , ] from manubot.cite.citekey import citekey_to_csl_item","title":"Module manubot.cite"},{"location":"reference/manubot/cite/#sub-modules","text":"manubot.cite.arxiv manubot.cite.citations manubot.cite.cite_command manubot.cite.citekey manubot.cite.citeproc manubot.cite.csl_item manubot.cite.curie manubot.cite.doi manubot.cite.handlers manubot.cite.isbn manubot.cite.pubmed manubot.cite.tests manubot.cite.unpaywall manubot.cite.url manubot.cite.wikidata manubot.cite.zotero","title":"Sub-modules"},{"location":"reference/manubot/cite/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/#citekey_to_csl_item","text":"def citekey_to_csl_item ( citekey , prune = True , manual_refs = {}, log_level : Union [ str , int ] = 'WARNING' ) Generate a CSL_Item for the input citekey. View Source def citekey_to_csl_item ( citekey , prune = True , manual_refs = {}, log_level : tp . Union [ str , int ] = \"WARNING\" ): \"\"\" Generate a CSL_Item for the input citekey. \"\"\" from manubot import __version__ as manubot_version # https://stackoverflow.com/a/35704430/4651668 log_level = logging . _checkLevel ( log_level ) if not isinstance ( citekey , CiteKey ): citekey = CiteKey ( citekey ) if citekey . standard_id in manual_refs : return manual_refs [ citekey . standard_id ] try : csl_item = citekey . csl_item except Exception as error : logging . log ( log_level , f \"Generating csl_item for {citekey.standard_id!r} failed \" f \"due to a {error.__class__.__name__}: \\n {error}\" , ) logging . info ( error , exc_info = True ) return None # update csl_item with manubot generated metadata note_text = f \"This CSL JSON Item was automatically generated by Manubot v{manubot_version} using citation-by-identifier.\" note_dict = { \"standard_id\" : citekey . standard_id } csl_item . note_append_text ( note_text ) csl_item . note_append_dict ( note_dict ) csl_item . set_id ( citekey . short_id ) csl_item . clean ( prune = prune ) return csl_item","title":"citekey_to_csl_item"},{"location":"reference/manubot/cite/arxiv/","text":"Module manubot.cite.arxiv View Source import logging import re import xml.etree.ElementTree import requests from .csl_item import CSL_Item from .handlers import Handler from manubot.util import get_manubot_user_agent class Handler_arXiv ( Handler ): standard_prefix = \"arxiv\" prefixes = [ \"arxiv\" , ] accession_pattern = re . compile ( r \"(?P<versionless_id>[0-9]{4}\\.[0-9]{4,5}|[a-z\\-]+(\\.[A-Z]{2})?/[0-9]{7})(?P<version>v[0-9]+)?\" ) def inspect ( self , citekey ): # https://arxiv.org/help/arxiv_identifier if not self . _get_pattern () . fullmatch ( citekey . accession ): return \"arXiv identifiers must conform to syntax described at https://arxiv.org/help/arxiv_identifier.\" def get_csl_item ( self , citekey ): return get_arxiv_csl_item ( citekey . standard_accession ) class CSL_Item_arXiv ( CSL_Item ): def _set_invariant_fields ( self ): # Set journal/publisher to arXiv self [ \"container-title\" ] = \"arXiv\" self [ \"publisher\" ] = \"arXiv\" # Set CSL type to report for preprint self [ \"type\" ] = \"report\" return self def log_journal_doi ( self , arxiv_id , journal_ref = None ): if \"DOI\" not in self : return msg = f \"arXiv article {arxiv_id} published at https://doi.org/{self['DOI']}\" if journal_ref : msg += f \" \u2014 {journal_ref}\" logging . info ( msg ) def set_identifier_fields ( self , arxiv_id ): self . set_id ( f \"arxiv:{arxiv_id}\" ) self [ \"URL\" ] = f \"https://arxiv.org/abs/{arxiv_id}\" self [ \"number\" ] = arxiv_id _ , version = split_arxiv_id_version ( arxiv_id ) if version : self [ \"version\" ] = version def split_arxiv_id_version ( arxiv_id : str ): \"\"\" Return (versionless_id, version) tuple. Version refers to the verion suffix like 'v2' or None. \"\"\" match = re . match ( Handler_arXiv . accession_pattern , arxiv_id ) return match . group ( \"versionless_id\" ), match . group ( \"version\" ) def get_arxiv_csl_item ( arxiv_id : str ): \"\"\" Return csl_item item for an arXiv identifier. Chooses which arXiv API to use based on whether arxiv_id is versioned, since only one endpoint supports versioning. \"\"\" _ , version = split_arxiv_id_version ( arxiv_id ) if version : return get_arxiv_csl_item_export_api ( arxiv_id ) return get_arxiv_csl_item_oai ( arxiv_id ) def query_arxiv_api ( url , params ): headers = { \"User-Agent\" : get_manubot_user_agent ()} response = requests . get ( url , params , headers = headers ) xml_tree = xml . etree . ElementTree . fromstring ( response . text ) return xml_tree def get_arxiv_csl_item_export_api ( arxiv_id ): \"\"\" Return csl_item item for an arXiv record. arxiv_id can be versioned, like `1512.00567v2`, or versionless, like `1512.00567`. If versionless, the arXiv API will return metadata for the latest version. Legacy IDs, such as `cond-mat/0703470v2`, are also supported. If arXiv has an associated DOI for the record, a warning is logged to alert the user that an alternative version of record exists. References: - https://arxiv.org/help/api/index - http://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html - https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" xml_tree = query_arxiv_api ( url = \"https://export.arxiv.org/api/query\" , params = { \"id_list\" : arxiv_id , \"max_results\" : 1 }, ) # XML namespace prefixes prefix = \"{http://www.w3.org/2005/Atom}\" alt_prefix = \"{http://arxiv.org/schemas/atom}\" # Parse XML ( entry ,) = xml_tree . findall ( prefix + \"entry\" ) # Create dictionary for CSL Item csl_item = CSL_Item_arXiv () # Extract versioned arXiv ID url = entry . findtext ( prefix + \"id\" ) pattern = re . compile ( r \"arxiv.org/abs/(.+)\" ) match = pattern . search ( url ) versioned_id = match . group ( 1 ) csl_item . set_identifier_fields ( versioned_id ) # Extrat CSL title field csl_item [ \"title\" ] = entry . findtext ( prefix + \"title\" ) # Extract CSL date field published = entry . findtext ( prefix + \"published\" ) csl_item . set_date ( published , variable = \"issued\" ) # Extract authors authors = list () for elem in entry . findall ( prefix + \"author\" ): name = elem . findtext ( prefix + \"name\" ) author = { \"literal\" : name } authors . append ( author ) csl_item [ \"author\" ] = authors csl_item . _set_invariant_fields () # Extract abstract abstract = entry . findtext ( prefix + \"summary\" ) . strip () if abstract : # remove newlines that were added to wrap abstract abstract = remove_newlines ( abstract ) csl_item [ \"abstract\" ] = abstract # Check if the article has been published with a DOI doi = entry . findtext ( f \"{alt_prefix}doi\" ) if doi : csl_item [ \"DOI\" ] = doi journal_ref = entry . findtext ( alt_prefix + \"journal_ref\" ) csl_item . log_journal_doi ( arxiv_id , journal_ref ) return csl_item def get_arxiv_csl_item_oai ( arxiv_id ): \"\"\" Generate a CSL Item for an unversioned arXiv identifier using arXiv's OAI_PMH v2.0 API <https://arxiv.org/help/oa>. This endpoint does not support versioned `arxiv_id`. \"\"\" # XML namespace prefixes ns_oai = \"{http://www.openarchives.org/OAI/2.0/}\" ns_arxiv = \"{http://arxiv.org/OAI/arXiv/}\" xml_tree = query_arxiv_api ( url = \"https://export.arxiv.org/oai2\" , params = { \"verb\" : \"GetRecord\" , \"metadataPrefix\" : \"arXiv\" , \"identifier\" : f \"oai:arXiv.org:{arxiv_id}\" , }, ) # Create dictionary for CSL Item csl_item = CSL_Item_arXiv () # Extract parent XML elements ( header_elem ,) = xml_tree . findall ( f \"{ns_oai}GetRecord/{ns_oai}record/{ns_oai}header\" ) ( metadata_elem ,) = xml_tree . findall ( f \"{ns_oai}GetRecord/{ns_oai}record/{ns_oai}metadata\" ) ( arxiv_elem ,) = metadata_elem . findall ( f \"{ns_arxiv}arXiv\" ) # Set identifier fields response_arxiv_id = arxiv_elem . findtext ( f \"{ns_arxiv}id\" ) if arxiv_id != response_arxiv_id : logging . warning ( f \"arXiv oai2 query returned a different arxiv_id:\" \" {arxiv_id} became {response_arxiv_id}\" ) csl_item . set_identifier_fields ( response_arxiv_id ) # Set title and date title = arxiv_elem . findtext ( f \"{ns_arxiv}title\" ) if title : csl_item [ \"title\" ] = \" \" . join ( title . split ()) datestamp = header_elem . findtext ( f \"{ns_oai}datestamp\" ) csl_item . set_date ( datestamp , \"issued\" ) # Extract authors author_elems = arxiv_elem . findall ( f \"{ns_arxiv}authors/{ns_arxiv}author\" ) authors = list () for author_elem in author_elems : author = {} given = author_elem . findtext ( f \"{ns_arxiv}forenames\" ) family = author_elem . findtext ( f \"{ns_arxiv}keyname\" ) if given : author [ \"given\" ] = given if family : author [ \"family\" ] = family authors . append ( author ) csl_item [ \"author\" ] = authors csl_item . _set_invariant_fields () abstract = arxiv_elem . findtext ( f \"{ns_arxiv}abstract\" ) if abstract : csl_item [ \"abstract\" ] = remove_newlines ( abstract ) license = arxiv_elem . findtext ( f \"{ns_arxiv}license\" ) if license : csl_item . note_append_dict ({ \"license\" : license }) doi = arxiv_elem . findtext ( f \"{ns_arxiv}doi\" ) if doi : csl_item [ \"DOI\" ] = doi journal_ref = arxiv_elem . findtext ( f \"{ns_arxiv}journal-ref\" ) csl_item . log_journal_doi ( arxiv_id , journal_ref ) return csl_item def remove_newlines ( text ): return re . sub ( pattern = r \"\\n(?!\\s)\" , repl = \" \" , string = text ) def get_arxiv_csl_item_zotero ( arxiv_id ): \"\"\" Generate CSL JSON Data for an arXiv ID using Zotero's translation-server. \"\"\" from manubot.cite.zotero import get_csl_item return get_csl_item ( f \"arxiv:{arxiv_id}\" ) Functions get_arxiv_csl_item def get_arxiv_csl_item ( arxiv_id : str ) Return csl_item item for an arXiv identifier. Chooses which arXiv API to use based on whether arxiv_id is versioned, since only one endpoint supports versioning. View Source def get_arxiv_csl_item ( arxiv_id : str ): \"\"\" Return csl_item item for an arXiv identifier. Chooses which arXiv API to use based on whether arxiv_id is versioned, since only one endpoint supports versioning. \"\"\" _ , version = split_arxiv_id_version ( arxiv_id ) if version : return get_arxiv_csl_item_export_api ( arxiv_id ) return get_arxiv_csl_item_oai ( arxiv_id ) get_arxiv_csl_item_export_api def get_arxiv_csl_item_export_api ( arxiv_id ) Return csl_item item for an arXiv record. arxiv_id can be versioned, like 1512.00567v2 , or versionless, like 1512.00567 . If versionless, the arXiv API will return metadata for the latest version. Legacy IDs, such as cond-mat/0703470v2 , are also supported. If arXiv has an associated DOI for the record, a warning is logged to alert the user that an alternative version of record exists. References: - https://arxiv.org/help/api/index - http://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html - https://github.com/citation-style-language/schema/blob/master/csl-data.json View Source def get_arxiv_csl_item_export_api ( arxiv_id ): \"\"\" Return csl_item item for an arXiv record. arxiv_id can be versioned, like `1512.00567v2`, or versionless, like `1512.00567`. If versionless, the arXiv API will return metadata for the latest version. Legacy IDs, such as `cond-mat/0703470v2`, are also supported. If arXiv has an associated DOI for the record, a warning is logged to alert the user that an alternative version of record exists. References: - https://arxiv.org/help/api/index - http://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html - https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" xml_tree = query_arxiv_api ( url = \"https://export.arxiv.org/api/query\" , params = { \"id_list\" : arxiv_id , \"max_results\" : 1 } , ) # XML namespace prefixes prefix = \"{http://www.w3.org/2005/Atom}\" alt_prefix = \"{http://arxiv.org/schemas/atom}\" # Parse XML ( entry ,) = xml_tree . findall ( prefix + \"entry\" ) # Create dictionary for CSL Item csl_item = CSL_Item_arXiv () # Extract versioned arXiv ID url = entry . findtext ( prefix + \"id\" ) pattern = re . compile ( r \"arxiv.org/abs/(.+)\" ) match = pattern . search ( url ) versioned_id = match . group ( 1 ) csl_item . set_identifier_fields ( versioned_id ) # Extrat CSL title field csl_item [ \"title\" ] = entry . findtext ( prefix + \"title\" ) # Extract CSL date field published = entry . findtext ( prefix + \"published\" ) csl_item . set_date ( published , variable = \"issued\" ) # Extract authors authors = list () for elem in entry . findall ( prefix + \"author\" ): name = elem . findtext ( prefix + \"name\" ) author = { \"literal\" : name } authors . append ( author ) csl_item [ \"author\" ] = authors csl_item . _set_invariant_fields () # Extract abstract abstract = entry . findtext ( prefix + \"summary\" ). strip () if abstract : # remove newlines that were added to wrap abstract abstract = remove_newlines ( abstract ) csl_item [ \"abstract\" ] = abstract # Check if the article has been published with a DOI doi = entry . findtext ( f \"{alt_prefix}doi\" ) if doi : csl_item [ \"DOI\" ] = doi journal_ref = entry . findtext ( alt_prefix + \"journal_ref\" ) csl_item . log_journal_doi ( arxiv_id , journal_ref ) return csl_item get_arxiv_csl_item_oai def get_arxiv_csl_item_oai ( arxiv_id ) Generate a CSL Item for an unversioned arXiv identifier using arXiv's OAI_PMH v2.0 API https://arxiv.org/help/oa . This endpoint does not support versioned arxiv_id . View Source def get_arxiv_csl_item_oai ( arxiv_id ): \"\"\" Generate a CSL Item for an unversioned arXiv identifier using arXiv's OAI_PMH v2.0 API <https://arxiv.org/help/oa>. This endpoint does not support versioned `arxiv_id`. \"\"\" # XML namespace prefixes ns_oai = \"{http://www.openarchives.org/OAI/2.0/}\" ns_arxiv = \"{http://arxiv.org/OAI/arXiv/}\" xml_tree = query_arxiv_api ( url = \"https://export.arxiv.org/oai2\" , params = { \"verb\" : \"GetRecord\" , \"metadataPrefix\" : \"arXiv\" , \"identifier\" : f \"oai:arXiv.org:{arxiv_id}\" , } , ) # Create dictionary for CSL Item csl_item = CSL_Item_arXiv () # Extract parent XML elements ( header_elem ,) = xml_tree . findall ( f \"{ns_oai}GetRecord/{ns_oai}record/{ns_oai}header\" ) ( metadata_elem ,) = xml_tree . findall ( f \"{ns_oai}GetRecord/{ns_oai}record/{ns_oai}metadata\" ) ( arxiv_elem ,) = metadata_elem . findall ( f \"{ns_arxiv}arXiv\" ) # Set identifier fields response_arxiv_id = arxiv_elem . findtext ( f \"{ns_arxiv}id\" ) if arxiv_id != response_arxiv_id : logging . warning ( f \"arXiv oai2 query returned a different arxiv_id:\" \" {arxiv_id} became {response_arxiv_id}\" ) csl_item . set_identifier_fields ( response_arxiv_id ) # Set title and date title = arxiv_elem . findtext ( f \"{ns_arxiv}title\" ) if title : csl_item [ \"title\" ] = \" \" . join ( title . split ()) datestamp = header_elem . findtext ( f \"{ns_oai}datestamp\" ) csl_item . set_date ( datestamp , \"issued\" ) # Extract authors author_elems = arxiv_elem . findall ( f \"{ns_arxiv}authors/{ns_arxiv}author\" ) authors = list () for author_elem in author_elems : author = {} given = author_elem . findtext ( f \"{ns_arxiv}forenames\" ) family = author_elem . findtext ( f \"{ns_arxiv}keyname\" ) if given : author [ \"given\" ] = given if family : author [ \"family\" ] = family authors . append ( author ) csl_item [ \"author\" ] = authors csl_item . _set_invariant_fields () abstract = arxiv_elem . findtext ( f \"{ns_arxiv}abstract\" ) if abstract : csl_item [ \"abstract\" ] = remove_newlines ( abstract ) license = arxiv_elem . findtext ( f \"{ns_arxiv}license\" ) if license : csl_item . note_append_dict ( { \"license\" : license } ) doi = arxiv_elem . findtext ( f \"{ns_arxiv}doi\" ) if doi : csl_item [ \"DOI\" ] = doi journal_ref = arxiv_elem . findtext ( f \"{ns_arxiv}journal-ref\" ) csl_item . log_journal_doi ( arxiv_id , journal_ref ) return csl_item get_arxiv_csl_item_zotero def get_arxiv_csl_item_zotero ( arxiv_id ) Generate CSL JSON Data for an arXiv ID using Zotero's translation-server. View Source def get_arxiv_csl_item_zotero ( arxiv_id ): \"\"\" Generate CSL JSON Data for an arXiv ID using Zotero's translation-server. \"\"\" from manubot.cite.zotero import get_csl_item return get_csl_item ( f \"arxiv:{arxiv_id}\" ) query_arxiv_api def query_arxiv_api ( url , params ) View Source def query_arxiv_api ( url , params ): headers = { \"User-Agent\" : get_manubot_user_agent () } response = requests . get ( url , params , headers = headers ) xml_tree = xml . etree . ElementTree . fromstring ( response . text ) return xml_tree remove_newlines def remove_newlines ( text ) View Source def remove_newlines ( text ): return re . sub ( pattern = r \"\\n(?!\\s)\" , repl = \" \" , string = text ) split_arxiv_id_version def split_arxiv_id_version ( arxiv_id : str ) Return (versionless_id, version) tuple. Version refers to the verion suffix like 'v2' or None. View Source def split_arxiv_id_version ( arxiv_id : str ): \"\"\" Return (versionless_id, version) tuple. Version refers to the verion suffix like 'v2' or None. \"\"\" match = re . match ( Handler_arXiv . accession_pattern , arxiv_id ) return match . group ( \"versionless_id\" ), match . group ( \"version\" ) Classes CSL_Item_arXiv class CSL_Item_arXiv ( dictionary = None , ** kwargs ) CSL_Item represents bibliographic information for a single citeable work. On a technical side CSL_Item is a Python dictionary with extra methods that help cleaning and manipulating it. These methods relate to: - adding an id key and value for CSL item - correcting bibliographic information and its structure - adding and reading a custom note to CSL item More information on CSL JSON (a list of CSL_Items) is available at: - https://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html - http://docs.citationstyles.org/en/1.0.1/specification.html#standard-variables - https://github.com/citation-style-language/schema/blob/master/csl-data.json View Source class CSL_Item_arXiv ( CSL_Item ): def _set_invariant_fields ( self ): # Set journal/publisher to arXiv self [ \"container-title\" ] = \"arXiv\" self [ \"publisher\" ] = \"arXiv\" # Set CSL type to report for preprint self [ \"type\" ] = \"report\" return self def log_journal_doi ( self , arxiv_id , journal_ref = None ): if \"DOI\" not in self: return msg = f \"arXiv article {arxiv_id} published at https://doi.org/{self['DOI']}\" if journal_ref: msg += f \" \u2014 {journal_ref}\" logging . info ( msg ) def set_identifier_fields ( self , arxiv_id ): self . set_id ( f \"arxiv:{arxiv_id}\" ) self [ \"URL\" ] = f \"https://arxiv.org/abs/{arxiv_id}\" self [ \"number\" ] = arxiv_id _ , version = split_arxiv_id_version ( arxiv_id ) if version: self [ \"version\" ] = version Ancestors (in MRO) manubot.cite.csl_item.CSL_Item builtins.dict Class variables type_mapping Instance variables note Return the value of the \"note\" field as a string. If \"note\" key is not set, return empty string. note_dict Return a dictionary with key-value pairs encoded by this CSL Item's note. Extracts both forms (line-entry and braced-entry) of key-value pairs from the CSL JSON \"cheater syntax\" https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields Assigning to this dict will not update self[\"note\"] . Methods clean def clean ( self , prune : bool = True ) Sanitize and touch-up a potentially dirty CSL_Item. The following steps are performed: - update incorrect values for \"type\" field when a correct variant is known - remove fields that violate the JSON Schema (if prune=True) - set default value for \"type\" if missing, since CSL JSON requires type - validate against the CSL JSON schema (if prune=True) to ensure output CSL_Item is clean View Source def clean ( self , prune : bool = True ): \"\"\" Sanitize and touch-up a potentially dirty CSL_Item. The following steps are performed: - update incorrect values for \" type \" field when a correct variant is known - remove fields that violate the JSON Schema (if prune=True) - set default value for \" type \" if missing, since CSL JSON requires type - validate against the CSL JSON schema (if prune=True) to ensure output CSL_Item is clean \"\"\" logging . debug ( f \"Starting CSL_Item.clean with{'' if prune else 'out'}\" f \"CSL pruning for id: {self.get('id', 'id not specified')}\" ) self . correct_invalid_type () if prune : self . prune_against_schema () self . set_default_type () if prune : self . validate_against_schema () return self clear def clear ( ... ) D.clear() -> None. Remove all items from D. copy def copy ( ... ) D.copy() -> a shallow copy of D correct_invalid_type def correct_invalid_type ( self ) Correct invalid CSL item type. Does nothing if type not present. For detail see https://github.com/CrossRef/rest-api-doc/issues/187 View Source def correct_invalid_type ( self ): \"\"\" Correct invalid CSL item type. Does nothing if `type` not present. For detail see https://github.com/CrossRef/rest-api-doc/issues/187 \"\"\" if \"type\" in self : # Replace a type from in CSL_Item.type_mapping.keys(), # leave type intact in other cases. t = self [ \"type\" ] self [ \"type\" ] = self . type_mapping . get ( t , t ) return self fromkeys def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value. get def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default. get_date def get_date ( self , variable = 'issued' , fill = False ) Return a CSL date-variable as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). variable: which CSL JSON date variable to retrieve fill: if True, set missing months to January and missing days to the first day of the month. View Source def get_date ( self , variable = \"issued\" , fill = False ) : \"\"\" Return a CSL date-variable as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). variable: which CSL JSON date variable to retrieve fill: if True, set missing months to January and missing days to the first day of the month. \"\"\" try : date_parts = self [ variable ][ \"date-parts\" ][ 0 ] except ( IndexError , KeyError ) : return None return date_parts_to_string ( date_parts , fill = fill ) infer_id def infer_id ( self ) Detect and set a non-null/empty value for \"id\" or else raise a ValueError. View Source def infer_id ( self ): \"\"\" Detect and set a non-null/empty value for \" id \" or else raise a ValueError. \"\"\" if self . get ( \"standard_citation\" ): # \"standard_citation\" field is set with a non - null / empty value return self . set_id ( self . pop ( \"standard_citation\" )) if self . note_dict . get ( \"standard_id\" ): # \"standard_id\" note field is set with a non - null / empty value return self . set_id ( self . note_dict [ \"standard_id\" ]) if self . get ( \"id\" ): # \"id\" field exists and is set with a non - null / empty value return self . set_id ( self [ \"id\" ]) raise ValueError ( \"infer_id could not detect a field with a citation / standard_citation. \" 'Consider setting the CSL Item \"id\" field.' ) items def items ( ... ) D.items() -> a set-like object providing a view on D's items keys def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys log_journal_doi def log_journal_doi ( self , arxiv_id , journal_ref = None ) View Source def log_journal_doi ( self , arxiv_id , journal_ref = None ): if \"DOI\" not in self : return msg = f \"arXiv article {arxiv_id} published at https://doi.org/{self['DOI']}\" if journal_ref : msg += f \" \u2014 {journal_ref}\" logging . info ( msg ) note_append_dict def note_append_dict ( self , dictionary : dict ) Append key-value pairs specified by dictionary to the note field of a CSL Item. Uses the the CSL JSON \"cheater syntax\" to encode additional values not defined by the CSL JSON schema. View Source def note_append_dict ( self , dictionary : dict ): \"\"\" Append key-value pairs specified by `dictionary` to the note field of a CSL Item. Uses the the [CSL JSON \" cheater syntax \"](https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields) to encode additional values not defined by the CSL JSON schema. \"\"\" for key , value in dictionary . items (): if not re . fullmatch ( r \"[A-Z]+|[-_a-z]+\" , key ): logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"it does not conform to the variable_name syntax as per https://git.io/fjTzW.\" ) continue if \"\\n\" in value : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"the value contains a newline: {value!r}\" ) continue self . note_append_text ( f \"{key}: {value}\" ) note_append_text def note_append_text ( self , text : str ) Append text to the note field of a CSL Item. View Source def note_append_text ( self , text : str ): \"\"\" Append text to the note field of a CSL Item. \"\"\" if not text : return note = self . note if note and not note . endswith ( \"\\n\" ): note += \"\\n\" note += text self . note = note pop def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised popitem def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. prune_against_schema def prune_against_schema ( self ) Remove fields that violate the CSL Item JSON Schema. View Source def prune_against_schema ( self ): \"\"\" Remove fields that violate the CSL Item JSON Schema. \"\"\" from .citeproc import remove_jsonschema_errors ( csl_item ,) = remove_jsonschema_errors ([ self ], in_place = True ) assert csl_item is self return self set_date def set_date ( self , date , variable = 'issued' ) date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). variable: which variable to assign the date to. View Source def set_date ( self , date , variable = \"issued\" ) : \"\"\" date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). variable: which variable to assign the date to. \"\"\" date_parts = date_to_date_parts ( date ) if date_parts : self [ variable ] = { \"date-parts\" : [ date_parts ] } return self set_default_type def set_default_type ( self ) Set type to 'entry', if type not specified. View Source def set_default_type ( self ): \"\"\" Set type to 'entry', if type not specified. \"\"\" self [ \"type\" ] = self . get ( \"type\" , \"entry\" ) return self set_id def set_id ( self , id_ ) View Source def set_id ( self , id_ ): self [ \"id\" ] = id_ return self set_identifier_fields def set_identifier_fields ( self , arxiv_id ) View Source def set_identifier_fields ( self , arxiv_id ): self . set_id ( f \"arxiv:{arxiv_id}\" ) self [ \"URL\" ] = f \"https://arxiv.org/abs/{arxiv_id}\" self [ \"number\" ] = arxiv_id _ , version = split_arxiv_id_version ( arxiv_id ) if version : self [ \"version\" ] = version setdefault def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. standardize_id def standardize_id ( self ) Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \"id\" field. The standard_id is extracted from a \"standard_citation\" field, the \"note\" field, or the \"id\" field. The extracted citation is checked for validity and standardized, after which it is the final \"standard_id\". Regarding csl_item modification, the csl_item \"id\" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id and original_id. Note that the Manubot software generally refers to the \"id\" of a CSL Item as a citekey. However, in this context, we use \"id\" rather than \"citekey\" for consistency with CSL's \"id\" field. View Source def standardize_id ( self ): \"\"\" Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \" id \" field. The standard_id is extracted from a \" standard_citation \" field, the \" note \" field, or the \" id \" field. The extracted citation is checked for validity and standardized, after which it is the final \" standard_id \". Regarding csl_item modification, the csl_item \" id \" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id and original_id. Note that the Manubot software generally refers to the \" id \" of a CSL Item as a citekey. However, in this context, we use \" id \" rather than \" citekey \" for consistency with CSL's \" id \" field. \"\"\" original_id = self . get ( \"id\" ) self . infer_id () original_standard_id = self [ \"id\" ] citekey = CiteKey ( original_standard_id ) standard_id = citekey . standard_id add_to_note = {} note_dict = self . note_dict if original_id and original_id != standard_id : if original_id != note_dict . get ( \"original_id\" ): add_to_note [ \"original_id\" ] = original_id if original_standard_id and original_standard_id != standard_id : if original_standard_id != note_dict . get ( \"original_standard_id\" ): add_to_note [ \"original_standard_id\" ] = original_standard_id if standard_id != note_dict . get ( \"standard_id\" ): add_to_note [ \"standard_id\" ] = standard_id self . note_append_dict ( dictionary = add_to_note ) self . set_id ( standard_id ) return self update def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k] validate_against_schema def validate_against_schema ( self ) Confirm that the CSL_Item validates. If not, raises a jsonschema.exceptions.ValidationError. View Source def validate_against_schema ( self ): \"\"\" Confirm that the CSL_Item validates. If not, raises a jsonschema.exceptions.ValidationError. \"\"\" from .citeproc import get_jsonschema_csl_validator validator = get_jsonschema_csl_validator () validator . validate ([ self ]) return self values def values ( ... ) D.values() -> an object providing a view on D's values Handler_arXiv class Handler_arXiv ( prefix_lower : str ) A Handler is a class that provides support for a certain type of citekey. For example, a Handler subclass could provide support for DOI citekeys. Subclasses enable custom logic for different citekey prefixes, including how to standardize the citekey and how to retrieve CSL Item metadata. View Source class Handler_arXiv ( Handler ): standard_prefix = \"arxiv\" prefixes = [ \"arxiv\" , ] accession_pattern = re . compile ( r \"(?P<versionless_id>[0-9]{4}\\.[0-9]{4,5}|[a-z\\-]+(\\.[A-Z]{2})?/[0-9]{7})(?P<version>v[0-9]+)?\" ) def inspect ( self , citekey ): # https://arxiv.org/help/arxiv_identifier if not self . _get_pattern (). fullmatch ( citekey . accession ): return \"arXiv identifiers must conform to syntax described at https://arxiv.org/help/arxiv_identifier.\" def get_csl_item ( self , citekey ): return get_arxiv_csl_item ( citekey . standard_accession ) Ancestors (in MRO) manubot.cite.handlers.Handler Class variables accession_pattern prefixes standard_prefix Methods get_csl_item def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ): return get_arxiv_csl_item ( citekey . standard_accession ) inspect def inspect ( self , citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey ): # https : // arxiv . org / help / arxiv_identifier if not self . _get_pattern (). fullmatch ( citekey . accession ): return \"arXiv identifiers must conform to syntax described at https://arxiv.org/help/arxiv_identifier.\" standardize_prefix_accession def standardize_prefix_accession ( self , accession ) Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession ): \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession","title":"Arxiv"},{"location":"reference/manubot/cite/arxiv/#module-manubotcitearxiv","text":"View Source import logging import re import xml.etree.ElementTree import requests from .csl_item import CSL_Item from .handlers import Handler from manubot.util import get_manubot_user_agent class Handler_arXiv ( Handler ): standard_prefix = \"arxiv\" prefixes = [ \"arxiv\" , ] accession_pattern = re . compile ( r \"(?P<versionless_id>[0-9]{4}\\.[0-9]{4,5}|[a-z\\-]+(\\.[A-Z]{2})?/[0-9]{7})(?P<version>v[0-9]+)?\" ) def inspect ( self , citekey ): # https://arxiv.org/help/arxiv_identifier if not self . _get_pattern () . fullmatch ( citekey . accession ): return \"arXiv identifiers must conform to syntax described at https://arxiv.org/help/arxiv_identifier.\" def get_csl_item ( self , citekey ): return get_arxiv_csl_item ( citekey . standard_accession ) class CSL_Item_arXiv ( CSL_Item ): def _set_invariant_fields ( self ): # Set journal/publisher to arXiv self [ \"container-title\" ] = \"arXiv\" self [ \"publisher\" ] = \"arXiv\" # Set CSL type to report for preprint self [ \"type\" ] = \"report\" return self def log_journal_doi ( self , arxiv_id , journal_ref = None ): if \"DOI\" not in self : return msg = f \"arXiv article {arxiv_id} published at https://doi.org/{self['DOI']}\" if journal_ref : msg += f \" \u2014 {journal_ref}\" logging . info ( msg ) def set_identifier_fields ( self , arxiv_id ): self . set_id ( f \"arxiv:{arxiv_id}\" ) self [ \"URL\" ] = f \"https://arxiv.org/abs/{arxiv_id}\" self [ \"number\" ] = arxiv_id _ , version = split_arxiv_id_version ( arxiv_id ) if version : self [ \"version\" ] = version def split_arxiv_id_version ( arxiv_id : str ): \"\"\" Return (versionless_id, version) tuple. Version refers to the verion suffix like 'v2' or None. \"\"\" match = re . match ( Handler_arXiv . accession_pattern , arxiv_id ) return match . group ( \"versionless_id\" ), match . group ( \"version\" ) def get_arxiv_csl_item ( arxiv_id : str ): \"\"\" Return csl_item item for an arXiv identifier. Chooses which arXiv API to use based on whether arxiv_id is versioned, since only one endpoint supports versioning. \"\"\" _ , version = split_arxiv_id_version ( arxiv_id ) if version : return get_arxiv_csl_item_export_api ( arxiv_id ) return get_arxiv_csl_item_oai ( arxiv_id ) def query_arxiv_api ( url , params ): headers = { \"User-Agent\" : get_manubot_user_agent ()} response = requests . get ( url , params , headers = headers ) xml_tree = xml . etree . ElementTree . fromstring ( response . text ) return xml_tree def get_arxiv_csl_item_export_api ( arxiv_id ): \"\"\" Return csl_item item for an arXiv record. arxiv_id can be versioned, like `1512.00567v2`, or versionless, like `1512.00567`. If versionless, the arXiv API will return metadata for the latest version. Legacy IDs, such as `cond-mat/0703470v2`, are also supported. If arXiv has an associated DOI for the record, a warning is logged to alert the user that an alternative version of record exists. References: - https://arxiv.org/help/api/index - http://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html - https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" xml_tree = query_arxiv_api ( url = \"https://export.arxiv.org/api/query\" , params = { \"id_list\" : arxiv_id , \"max_results\" : 1 }, ) # XML namespace prefixes prefix = \"{http://www.w3.org/2005/Atom}\" alt_prefix = \"{http://arxiv.org/schemas/atom}\" # Parse XML ( entry ,) = xml_tree . findall ( prefix + \"entry\" ) # Create dictionary for CSL Item csl_item = CSL_Item_arXiv () # Extract versioned arXiv ID url = entry . findtext ( prefix + \"id\" ) pattern = re . compile ( r \"arxiv.org/abs/(.+)\" ) match = pattern . search ( url ) versioned_id = match . group ( 1 ) csl_item . set_identifier_fields ( versioned_id ) # Extrat CSL title field csl_item [ \"title\" ] = entry . findtext ( prefix + \"title\" ) # Extract CSL date field published = entry . findtext ( prefix + \"published\" ) csl_item . set_date ( published , variable = \"issued\" ) # Extract authors authors = list () for elem in entry . findall ( prefix + \"author\" ): name = elem . findtext ( prefix + \"name\" ) author = { \"literal\" : name } authors . append ( author ) csl_item [ \"author\" ] = authors csl_item . _set_invariant_fields () # Extract abstract abstract = entry . findtext ( prefix + \"summary\" ) . strip () if abstract : # remove newlines that were added to wrap abstract abstract = remove_newlines ( abstract ) csl_item [ \"abstract\" ] = abstract # Check if the article has been published with a DOI doi = entry . findtext ( f \"{alt_prefix}doi\" ) if doi : csl_item [ \"DOI\" ] = doi journal_ref = entry . findtext ( alt_prefix + \"journal_ref\" ) csl_item . log_journal_doi ( arxiv_id , journal_ref ) return csl_item def get_arxiv_csl_item_oai ( arxiv_id ): \"\"\" Generate a CSL Item for an unversioned arXiv identifier using arXiv's OAI_PMH v2.0 API <https://arxiv.org/help/oa>. This endpoint does not support versioned `arxiv_id`. \"\"\" # XML namespace prefixes ns_oai = \"{http://www.openarchives.org/OAI/2.0/}\" ns_arxiv = \"{http://arxiv.org/OAI/arXiv/}\" xml_tree = query_arxiv_api ( url = \"https://export.arxiv.org/oai2\" , params = { \"verb\" : \"GetRecord\" , \"metadataPrefix\" : \"arXiv\" , \"identifier\" : f \"oai:arXiv.org:{arxiv_id}\" , }, ) # Create dictionary for CSL Item csl_item = CSL_Item_arXiv () # Extract parent XML elements ( header_elem ,) = xml_tree . findall ( f \"{ns_oai}GetRecord/{ns_oai}record/{ns_oai}header\" ) ( metadata_elem ,) = xml_tree . findall ( f \"{ns_oai}GetRecord/{ns_oai}record/{ns_oai}metadata\" ) ( arxiv_elem ,) = metadata_elem . findall ( f \"{ns_arxiv}arXiv\" ) # Set identifier fields response_arxiv_id = arxiv_elem . findtext ( f \"{ns_arxiv}id\" ) if arxiv_id != response_arxiv_id : logging . warning ( f \"arXiv oai2 query returned a different arxiv_id:\" \" {arxiv_id} became {response_arxiv_id}\" ) csl_item . set_identifier_fields ( response_arxiv_id ) # Set title and date title = arxiv_elem . findtext ( f \"{ns_arxiv}title\" ) if title : csl_item [ \"title\" ] = \" \" . join ( title . split ()) datestamp = header_elem . findtext ( f \"{ns_oai}datestamp\" ) csl_item . set_date ( datestamp , \"issued\" ) # Extract authors author_elems = arxiv_elem . findall ( f \"{ns_arxiv}authors/{ns_arxiv}author\" ) authors = list () for author_elem in author_elems : author = {} given = author_elem . findtext ( f \"{ns_arxiv}forenames\" ) family = author_elem . findtext ( f \"{ns_arxiv}keyname\" ) if given : author [ \"given\" ] = given if family : author [ \"family\" ] = family authors . append ( author ) csl_item [ \"author\" ] = authors csl_item . _set_invariant_fields () abstract = arxiv_elem . findtext ( f \"{ns_arxiv}abstract\" ) if abstract : csl_item [ \"abstract\" ] = remove_newlines ( abstract ) license = arxiv_elem . findtext ( f \"{ns_arxiv}license\" ) if license : csl_item . note_append_dict ({ \"license\" : license }) doi = arxiv_elem . findtext ( f \"{ns_arxiv}doi\" ) if doi : csl_item [ \"DOI\" ] = doi journal_ref = arxiv_elem . findtext ( f \"{ns_arxiv}journal-ref\" ) csl_item . log_journal_doi ( arxiv_id , journal_ref ) return csl_item def remove_newlines ( text ): return re . sub ( pattern = r \"\\n(?!\\s)\" , repl = \" \" , string = text ) def get_arxiv_csl_item_zotero ( arxiv_id ): \"\"\" Generate CSL JSON Data for an arXiv ID using Zotero's translation-server. \"\"\" from manubot.cite.zotero import get_csl_item return get_csl_item ( f \"arxiv:{arxiv_id}\" )","title":"Module manubot.cite.arxiv"},{"location":"reference/manubot/cite/arxiv/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/arxiv/#get_arxiv_csl_item","text":"def get_arxiv_csl_item ( arxiv_id : str ) Return csl_item item for an arXiv identifier. Chooses which arXiv API to use based on whether arxiv_id is versioned, since only one endpoint supports versioning. View Source def get_arxiv_csl_item ( arxiv_id : str ): \"\"\" Return csl_item item for an arXiv identifier. Chooses which arXiv API to use based on whether arxiv_id is versioned, since only one endpoint supports versioning. \"\"\" _ , version = split_arxiv_id_version ( arxiv_id ) if version : return get_arxiv_csl_item_export_api ( arxiv_id ) return get_arxiv_csl_item_oai ( arxiv_id )","title":"get_arxiv_csl_item"},{"location":"reference/manubot/cite/arxiv/#get_arxiv_csl_item_export_api","text":"def get_arxiv_csl_item_export_api ( arxiv_id ) Return csl_item item for an arXiv record. arxiv_id can be versioned, like 1512.00567v2 , or versionless, like 1512.00567 . If versionless, the arXiv API will return metadata for the latest version. Legacy IDs, such as cond-mat/0703470v2 , are also supported. If arXiv has an associated DOI for the record, a warning is logged to alert the user that an alternative version of record exists. References: - https://arxiv.org/help/api/index - http://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html - https://github.com/citation-style-language/schema/blob/master/csl-data.json View Source def get_arxiv_csl_item_export_api ( arxiv_id ): \"\"\" Return csl_item item for an arXiv record. arxiv_id can be versioned, like `1512.00567v2`, or versionless, like `1512.00567`. If versionless, the arXiv API will return metadata for the latest version. Legacy IDs, such as `cond-mat/0703470v2`, are also supported. If arXiv has an associated DOI for the record, a warning is logged to alert the user that an alternative version of record exists. References: - https://arxiv.org/help/api/index - http://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html - https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" xml_tree = query_arxiv_api ( url = \"https://export.arxiv.org/api/query\" , params = { \"id_list\" : arxiv_id , \"max_results\" : 1 } , ) # XML namespace prefixes prefix = \"{http://www.w3.org/2005/Atom}\" alt_prefix = \"{http://arxiv.org/schemas/atom}\" # Parse XML ( entry ,) = xml_tree . findall ( prefix + \"entry\" ) # Create dictionary for CSL Item csl_item = CSL_Item_arXiv () # Extract versioned arXiv ID url = entry . findtext ( prefix + \"id\" ) pattern = re . compile ( r \"arxiv.org/abs/(.+)\" ) match = pattern . search ( url ) versioned_id = match . group ( 1 ) csl_item . set_identifier_fields ( versioned_id ) # Extrat CSL title field csl_item [ \"title\" ] = entry . findtext ( prefix + \"title\" ) # Extract CSL date field published = entry . findtext ( prefix + \"published\" ) csl_item . set_date ( published , variable = \"issued\" ) # Extract authors authors = list () for elem in entry . findall ( prefix + \"author\" ): name = elem . findtext ( prefix + \"name\" ) author = { \"literal\" : name } authors . append ( author ) csl_item [ \"author\" ] = authors csl_item . _set_invariant_fields () # Extract abstract abstract = entry . findtext ( prefix + \"summary\" ). strip () if abstract : # remove newlines that were added to wrap abstract abstract = remove_newlines ( abstract ) csl_item [ \"abstract\" ] = abstract # Check if the article has been published with a DOI doi = entry . findtext ( f \"{alt_prefix}doi\" ) if doi : csl_item [ \"DOI\" ] = doi journal_ref = entry . findtext ( alt_prefix + \"journal_ref\" ) csl_item . log_journal_doi ( arxiv_id , journal_ref ) return csl_item","title":"get_arxiv_csl_item_export_api"},{"location":"reference/manubot/cite/arxiv/#get_arxiv_csl_item_oai","text":"def get_arxiv_csl_item_oai ( arxiv_id ) Generate a CSL Item for an unversioned arXiv identifier using arXiv's OAI_PMH v2.0 API https://arxiv.org/help/oa . This endpoint does not support versioned arxiv_id . View Source def get_arxiv_csl_item_oai ( arxiv_id ): \"\"\" Generate a CSL Item for an unversioned arXiv identifier using arXiv's OAI_PMH v2.0 API <https://arxiv.org/help/oa>. This endpoint does not support versioned `arxiv_id`. \"\"\" # XML namespace prefixes ns_oai = \"{http://www.openarchives.org/OAI/2.0/}\" ns_arxiv = \"{http://arxiv.org/OAI/arXiv/}\" xml_tree = query_arxiv_api ( url = \"https://export.arxiv.org/oai2\" , params = { \"verb\" : \"GetRecord\" , \"metadataPrefix\" : \"arXiv\" , \"identifier\" : f \"oai:arXiv.org:{arxiv_id}\" , } , ) # Create dictionary for CSL Item csl_item = CSL_Item_arXiv () # Extract parent XML elements ( header_elem ,) = xml_tree . findall ( f \"{ns_oai}GetRecord/{ns_oai}record/{ns_oai}header\" ) ( metadata_elem ,) = xml_tree . findall ( f \"{ns_oai}GetRecord/{ns_oai}record/{ns_oai}metadata\" ) ( arxiv_elem ,) = metadata_elem . findall ( f \"{ns_arxiv}arXiv\" ) # Set identifier fields response_arxiv_id = arxiv_elem . findtext ( f \"{ns_arxiv}id\" ) if arxiv_id != response_arxiv_id : logging . warning ( f \"arXiv oai2 query returned a different arxiv_id:\" \" {arxiv_id} became {response_arxiv_id}\" ) csl_item . set_identifier_fields ( response_arxiv_id ) # Set title and date title = arxiv_elem . findtext ( f \"{ns_arxiv}title\" ) if title : csl_item [ \"title\" ] = \" \" . join ( title . split ()) datestamp = header_elem . findtext ( f \"{ns_oai}datestamp\" ) csl_item . set_date ( datestamp , \"issued\" ) # Extract authors author_elems = arxiv_elem . findall ( f \"{ns_arxiv}authors/{ns_arxiv}author\" ) authors = list () for author_elem in author_elems : author = {} given = author_elem . findtext ( f \"{ns_arxiv}forenames\" ) family = author_elem . findtext ( f \"{ns_arxiv}keyname\" ) if given : author [ \"given\" ] = given if family : author [ \"family\" ] = family authors . append ( author ) csl_item [ \"author\" ] = authors csl_item . _set_invariant_fields () abstract = arxiv_elem . findtext ( f \"{ns_arxiv}abstract\" ) if abstract : csl_item [ \"abstract\" ] = remove_newlines ( abstract ) license = arxiv_elem . findtext ( f \"{ns_arxiv}license\" ) if license : csl_item . note_append_dict ( { \"license\" : license } ) doi = arxiv_elem . findtext ( f \"{ns_arxiv}doi\" ) if doi : csl_item [ \"DOI\" ] = doi journal_ref = arxiv_elem . findtext ( f \"{ns_arxiv}journal-ref\" ) csl_item . log_journal_doi ( arxiv_id , journal_ref ) return csl_item","title":"get_arxiv_csl_item_oai"},{"location":"reference/manubot/cite/arxiv/#get_arxiv_csl_item_zotero","text":"def get_arxiv_csl_item_zotero ( arxiv_id ) Generate CSL JSON Data for an arXiv ID using Zotero's translation-server. View Source def get_arxiv_csl_item_zotero ( arxiv_id ): \"\"\" Generate CSL JSON Data for an arXiv ID using Zotero's translation-server. \"\"\" from manubot.cite.zotero import get_csl_item return get_csl_item ( f \"arxiv:{arxiv_id}\" )","title":"get_arxiv_csl_item_zotero"},{"location":"reference/manubot/cite/arxiv/#query_arxiv_api","text":"def query_arxiv_api ( url , params ) View Source def query_arxiv_api ( url , params ): headers = { \"User-Agent\" : get_manubot_user_agent () } response = requests . get ( url , params , headers = headers ) xml_tree = xml . etree . ElementTree . fromstring ( response . text ) return xml_tree","title":"query_arxiv_api"},{"location":"reference/manubot/cite/arxiv/#remove_newlines","text":"def remove_newlines ( text ) View Source def remove_newlines ( text ): return re . sub ( pattern = r \"\\n(?!\\s)\" , repl = \" \" , string = text )","title":"remove_newlines"},{"location":"reference/manubot/cite/arxiv/#split_arxiv_id_version","text":"def split_arxiv_id_version ( arxiv_id : str ) Return (versionless_id, version) tuple. Version refers to the verion suffix like 'v2' or None. View Source def split_arxiv_id_version ( arxiv_id : str ): \"\"\" Return (versionless_id, version) tuple. Version refers to the verion suffix like 'v2' or None. \"\"\" match = re . match ( Handler_arXiv . accession_pattern , arxiv_id ) return match . group ( \"versionless_id\" ), match . group ( \"version\" )","title":"split_arxiv_id_version"},{"location":"reference/manubot/cite/arxiv/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/arxiv/#csl_item_arxiv","text":"class CSL_Item_arXiv ( dictionary = None , ** kwargs ) CSL_Item represents bibliographic information for a single citeable work. On a technical side CSL_Item is a Python dictionary with extra methods that help cleaning and manipulating it. These methods relate to: - adding an id key and value for CSL item - correcting bibliographic information and its structure - adding and reading a custom note to CSL item More information on CSL JSON (a list of CSL_Items) is available at: - https://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html - http://docs.citationstyles.org/en/1.0.1/specification.html#standard-variables - https://github.com/citation-style-language/schema/blob/master/csl-data.json View Source class CSL_Item_arXiv ( CSL_Item ): def _set_invariant_fields ( self ): # Set journal/publisher to arXiv self [ \"container-title\" ] = \"arXiv\" self [ \"publisher\" ] = \"arXiv\" # Set CSL type to report for preprint self [ \"type\" ] = \"report\" return self def log_journal_doi ( self , arxiv_id , journal_ref = None ): if \"DOI\" not in self: return msg = f \"arXiv article {arxiv_id} published at https://doi.org/{self['DOI']}\" if journal_ref: msg += f \" \u2014 {journal_ref}\" logging . info ( msg ) def set_identifier_fields ( self , arxiv_id ): self . set_id ( f \"arxiv:{arxiv_id}\" ) self [ \"URL\" ] = f \"https://arxiv.org/abs/{arxiv_id}\" self [ \"number\" ] = arxiv_id _ , version = split_arxiv_id_version ( arxiv_id ) if version: self [ \"version\" ] = version","title":"CSL_Item_arXiv"},{"location":"reference/manubot/cite/arxiv/#ancestors-in-mro","text":"manubot.cite.csl_item.CSL_Item builtins.dict","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/arxiv/#class-variables","text":"type_mapping","title":"Class variables"},{"location":"reference/manubot/cite/arxiv/#instance-variables","text":"note Return the value of the \"note\" field as a string. If \"note\" key is not set, return empty string. note_dict Return a dictionary with key-value pairs encoded by this CSL Item's note. Extracts both forms (line-entry and braced-entry) of key-value pairs from the CSL JSON \"cheater syntax\" https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields Assigning to this dict will not update self[\"note\"] .","title":"Instance variables"},{"location":"reference/manubot/cite/arxiv/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/arxiv/#clean","text":"def clean ( self , prune : bool = True ) Sanitize and touch-up a potentially dirty CSL_Item. The following steps are performed: - update incorrect values for \"type\" field when a correct variant is known - remove fields that violate the JSON Schema (if prune=True) - set default value for \"type\" if missing, since CSL JSON requires type - validate against the CSL JSON schema (if prune=True) to ensure output CSL_Item is clean View Source def clean ( self , prune : bool = True ): \"\"\" Sanitize and touch-up a potentially dirty CSL_Item. The following steps are performed: - update incorrect values for \" type \" field when a correct variant is known - remove fields that violate the JSON Schema (if prune=True) - set default value for \" type \" if missing, since CSL JSON requires type - validate against the CSL JSON schema (if prune=True) to ensure output CSL_Item is clean \"\"\" logging . debug ( f \"Starting CSL_Item.clean with{'' if prune else 'out'}\" f \"CSL pruning for id: {self.get('id', 'id not specified')}\" ) self . correct_invalid_type () if prune : self . prune_against_schema () self . set_default_type () if prune : self . validate_against_schema () return self","title":"clean"},{"location":"reference/manubot/cite/arxiv/#clear","text":"def clear ( ... ) D.clear() -> None. Remove all items from D.","title":"clear"},{"location":"reference/manubot/cite/arxiv/#copy","text":"def copy ( ... ) D.copy() -> a shallow copy of D","title":"copy"},{"location":"reference/manubot/cite/arxiv/#correct_invalid_type","text":"def correct_invalid_type ( self ) Correct invalid CSL item type. Does nothing if type not present. For detail see https://github.com/CrossRef/rest-api-doc/issues/187 View Source def correct_invalid_type ( self ): \"\"\" Correct invalid CSL item type. Does nothing if `type` not present. For detail see https://github.com/CrossRef/rest-api-doc/issues/187 \"\"\" if \"type\" in self : # Replace a type from in CSL_Item.type_mapping.keys(), # leave type intact in other cases. t = self [ \"type\" ] self [ \"type\" ] = self . type_mapping . get ( t , t ) return self","title":"correct_invalid_type"},{"location":"reference/manubot/cite/arxiv/#fromkeys","text":"def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value.","title":"fromkeys"},{"location":"reference/manubot/cite/arxiv/#get","text":"def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default.","title":"get"},{"location":"reference/manubot/cite/arxiv/#get_date","text":"def get_date ( self , variable = 'issued' , fill = False ) Return a CSL date-variable as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). variable: which CSL JSON date variable to retrieve fill: if True, set missing months to January and missing days to the first day of the month. View Source def get_date ( self , variable = \"issued\" , fill = False ) : \"\"\" Return a CSL date-variable as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). variable: which CSL JSON date variable to retrieve fill: if True, set missing months to January and missing days to the first day of the month. \"\"\" try : date_parts = self [ variable ][ \"date-parts\" ][ 0 ] except ( IndexError , KeyError ) : return None return date_parts_to_string ( date_parts , fill = fill )","title":"get_date"},{"location":"reference/manubot/cite/arxiv/#infer_id","text":"def infer_id ( self ) Detect and set a non-null/empty value for \"id\" or else raise a ValueError. View Source def infer_id ( self ): \"\"\" Detect and set a non-null/empty value for \" id \" or else raise a ValueError. \"\"\" if self . get ( \"standard_citation\" ): # \"standard_citation\" field is set with a non - null / empty value return self . set_id ( self . pop ( \"standard_citation\" )) if self . note_dict . get ( \"standard_id\" ): # \"standard_id\" note field is set with a non - null / empty value return self . set_id ( self . note_dict [ \"standard_id\" ]) if self . get ( \"id\" ): # \"id\" field exists and is set with a non - null / empty value return self . set_id ( self [ \"id\" ]) raise ValueError ( \"infer_id could not detect a field with a citation / standard_citation. \" 'Consider setting the CSL Item \"id\" field.' )","title":"infer_id"},{"location":"reference/manubot/cite/arxiv/#items","text":"def items ( ... ) D.items() -> a set-like object providing a view on D's items","title":"items"},{"location":"reference/manubot/cite/arxiv/#keys","text":"def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys","title":"keys"},{"location":"reference/manubot/cite/arxiv/#log_journal_doi","text":"def log_journal_doi ( self , arxiv_id , journal_ref = None ) View Source def log_journal_doi ( self , arxiv_id , journal_ref = None ): if \"DOI\" not in self : return msg = f \"arXiv article {arxiv_id} published at https://doi.org/{self['DOI']}\" if journal_ref : msg += f \" \u2014 {journal_ref}\" logging . info ( msg )","title":"log_journal_doi"},{"location":"reference/manubot/cite/arxiv/#note_append_dict","text":"def note_append_dict ( self , dictionary : dict ) Append key-value pairs specified by dictionary to the note field of a CSL Item. Uses the the CSL JSON \"cheater syntax\" to encode additional values not defined by the CSL JSON schema. View Source def note_append_dict ( self , dictionary : dict ): \"\"\" Append key-value pairs specified by `dictionary` to the note field of a CSL Item. Uses the the [CSL JSON \" cheater syntax \"](https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields) to encode additional values not defined by the CSL JSON schema. \"\"\" for key , value in dictionary . items (): if not re . fullmatch ( r \"[A-Z]+|[-_a-z]+\" , key ): logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"it does not conform to the variable_name syntax as per https://git.io/fjTzW.\" ) continue if \"\\n\" in value : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"the value contains a newline: {value!r}\" ) continue self . note_append_text ( f \"{key}: {value}\" )","title":"note_append_dict"},{"location":"reference/manubot/cite/arxiv/#note_append_text","text":"def note_append_text ( self , text : str ) Append text to the note field of a CSL Item. View Source def note_append_text ( self , text : str ): \"\"\" Append text to the note field of a CSL Item. \"\"\" if not text : return note = self . note if note and not note . endswith ( \"\\n\" ): note += \"\\n\" note += text self . note = note","title":"note_append_text"},{"location":"reference/manubot/cite/arxiv/#pop","text":"def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised","title":"pop"},{"location":"reference/manubot/cite/arxiv/#popitem","text":"def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty.","title":"popitem"},{"location":"reference/manubot/cite/arxiv/#prune_against_schema","text":"def prune_against_schema ( self ) Remove fields that violate the CSL Item JSON Schema. View Source def prune_against_schema ( self ): \"\"\" Remove fields that violate the CSL Item JSON Schema. \"\"\" from .citeproc import remove_jsonschema_errors ( csl_item ,) = remove_jsonschema_errors ([ self ], in_place = True ) assert csl_item is self return self","title":"prune_against_schema"},{"location":"reference/manubot/cite/arxiv/#set_date","text":"def set_date ( self , date , variable = 'issued' ) date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). variable: which variable to assign the date to. View Source def set_date ( self , date , variable = \"issued\" ) : \"\"\" date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). variable: which variable to assign the date to. \"\"\" date_parts = date_to_date_parts ( date ) if date_parts : self [ variable ] = { \"date-parts\" : [ date_parts ] } return self","title":"set_date"},{"location":"reference/manubot/cite/arxiv/#set_default_type","text":"def set_default_type ( self ) Set type to 'entry', if type not specified. View Source def set_default_type ( self ): \"\"\" Set type to 'entry', if type not specified. \"\"\" self [ \"type\" ] = self . get ( \"type\" , \"entry\" ) return self","title":"set_default_type"},{"location":"reference/manubot/cite/arxiv/#set_id","text":"def set_id ( self , id_ ) View Source def set_id ( self , id_ ): self [ \"id\" ] = id_ return self","title":"set_id"},{"location":"reference/manubot/cite/arxiv/#set_identifier_fields","text":"def set_identifier_fields ( self , arxiv_id ) View Source def set_identifier_fields ( self , arxiv_id ): self . set_id ( f \"arxiv:{arxiv_id}\" ) self [ \"URL\" ] = f \"https://arxiv.org/abs/{arxiv_id}\" self [ \"number\" ] = arxiv_id _ , version = split_arxiv_id_version ( arxiv_id ) if version : self [ \"version\" ] = version","title":"set_identifier_fields"},{"location":"reference/manubot/cite/arxiv/#setdefault","text":"def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default.","title":"setdefault"},{"location":"reference/manubot/cite/arxiv/#standardize_id","text":"def standardize_id ( self ) Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \"id\" field. The standard_id is extracted from a \"standard_citation\" field, the \"note\" field, or the \"id\" field. The extracted citation is checked for validity and standardized, after which it is the final \"standard_id\". Regarding csl_item modification, the csl_item \"id\" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id and original_id. Note that the Manubot software generally refers to the \"id\" of a CSL Item as a citekey. However, in this context, we use \"id\" rather than \"citekey\" for consistency with CSL's \"id\" field. View Source def standardize_id ( self ): \"\"\" Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \" id \" field. The standard_id is extracted from a \" standard_citation \" field, the \" note \" field, or the \" id \" field. The extracted citation is checked for validity and standardized, after which it is the final \" standard_id \". Regarding csl_item modification, the csl_item \" id \" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id and original_id. Note that the Manubot software generally refers to the \" id \" of a CSL Item as a citekey. However, in this context, we use \" id \" rather than \" citekey \" for consistency with CSL's \" id \" field. \"\"\" original_id = self . get ( \"id\" ) self . infer_id () original_standard_id = self [ \"id\" ] citekey = CiteKey ( original_standard_id ) standard_id = citekey . standard_id add_to_note = {} note_dict = self . note_dict if original_id and original_id != standard_id : if original_id != note_dict . get ( \"original_id\" ): add_to_note [ \"original_id\" ] = original_id if original_standard_id and original_standard_id != standard_id : if original_standard_id != note_dict . get ( \"original_standard_id\" ): add_to_note [ \"original_standard_id\" ] = original_standard_id if standard_id != note_dict . get ( \"standard_id\" ): add_to_note [ \"standard_id\" ] = standard_id self . note_append_dict ( dictionary = add_to_note ) self . set_id ( standard_id ) return self","title":"standardize_id"},{"location":"reference/manubot/cite/arxiv/#update","text":"def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k]","title":"update"},{"location":"reference/manubot/cite/arxiv/#validate_against_schema","text":"def validate_against_schema ( self ) Confirm that the CSL_Item validates. If not, raises a jsonschema.exceptions.ValidationError. View Source def validate_against_schema ( self ): \"\"\" Confirm that the CSL_Item validates. If not, raises a jsonschema.exceptions.ValidationError. \"\"\" from .citeproc import get_jsonschema_csl_validator validator = get_jsonschema_csl_validator () validator . validate ([ self ]) return self","title":"validate_against_schema"},{"location":"reference/manubot/cite/arxiv/#values","text":"def values ( ... ) D.values() -> an object providing a view on D's values","title":"values"},{"location":"reference/manubot/cite/arxiv/#handler_arxiv","text":"class Handler_arXiv ( prefix_lower : str ) A Handler is a class that provides support for a certain type of citekey. For example, a Handler subclass could provide support for DOI citekeys. Subclasses enable custom logic for different citekey prefixes, including how to standardize the citekey and how to retrieve CSL Item metadata. View Source class Handler_arXiv ( Handler ): standard_prefix = \"arxiv\" prefixes = [ \"arxiv\" , ] accession_pattern = re . compile ( r \"(?P<versionless_id>[0-9]{4}\\.[0-9]{4,5}|[a-z\\-]+(\\.[A-Z]{2})?/[0-9]{7})(?P<version>v[0-9]+)?\" ) def inspect ( self , citekey ): # https://arxiv.org/help/arxiv_identifier if not self . _get_pattern (). fullmatch ( citekey . accession ): return \"arXiv identifiers must conform to syntax described at https://arxiv.org/help/arxiv_identifier.\" def get_csl_item ( self , citekey ): return get_arxiv_csl_item ( citekey . standard_accession )","title":"Handler_arXiv"},{"location":"reference/manubot/cite/arxiv/#ancestors-in-mro_1","text":"manubot.cite.handlers.Handler","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/arxiv/#class-variables_1","text":"accession_pattern prefixes standard_prefix","title":"Class variables"},{"location":"reference/manubot/cite/arxiv/#methods_1","text":"","title":"Methods"},{"location":"reference/manubot/cite/arxiv/#get_csl_item","text":"def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ): return get_arxiv_csl_item ( citekey . standard_accession )","title":"get_csl_item"},{"location":"reference/manubot/cite/arxiv/#inspect","text":"def inspect ( self , citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey ): # https : // arxiv . org / help / arxiv_identifier if not self . _get_pattern (). fullmatch ( citekey . accession ): return \"arXiv identifiers must conform to syntax described at https://arxiv.org/help/arxiv_identifier.\"","title":"inspect"},{"location":"reference/manubot/cite/arxiv/#standardize_prefix_accession","text":"def standardize_prefix_accession ( self , accession ) Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession ): \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession","title":"standardize_prefix_accession"},{"location":"reference/manubot/cite/citations/","text":"Module manubot.cite.citations View Source import dataclasses import itertools import json import logging import pathlib import typing as tp from manubot.cite.citekey import CiteKey , citekey_to_csl_item @dataclasses.dataclass class Citations : \"\"\" Class for operating on a set of citations provided by their citekey input_ids. \"\"\" # Input citekey IDs as strings input_ids : list # Citation key aliases aliases : dict = dataclasses . field ( default_factory = dict ) # manual references dictionary of standard_id to CSL_Item. manual_refs : dict = dataclasses . field ( default_factory = dict ) # level to log failures related to CSL Item generation csl_item_failure_log_level : tp . Union [ str , int ] = \"WARNING\" # whether to prune csl items according to the JSON Schema prune_csl_items : bool = True def __post_init__ ( self ): input_ids = list ( dict . fromkeys ( self . input_ids )) # deduplicate self . citekeys = [ CiteKey ( x , self . aliases ) for x in input_ids ] def filter_pandoc_xnos ( self ) -> list : \"\"\" Filter self.citekeys to remove pandoc-xnos style citekeys. Return removed citekeys. \"\"\" keep , remove = [], [] for citekey in self . citekeys : remove_ = citekey . is_pandoc_xnos_prefix ( log_case_warning = True ) ( keep , remove )[ remove_ ] . append ( citekey ) self . citekeys = keep return remove def filter_unhandled ( self ) -> list : \"\"\" Filter self.citekeys to remove unhandled citekeys. Return removed citekeys. \"\"\" keep , remove = [], [] for citekey in self . citekeys : ( remove , keep )[ citekey . is_handled_prefix ] . append ( citekey ) self . citekeys = keep return remove def group_citekeys_by ( self , attribute : str = \"standard_id\" ) -> tp . List [ tp . Tuple [ str , list ]]: \"\"\" Group `self.citekeys` by `attribute`. \"\"\" def get_key ( x ): return getattr ( x , attribute ) citekeys = sorted ( self . citekeys , key = get_key ) groups = itertools . groupby ( citekeys , get_key ) return [( key , list ( group )) for key , group in groups ] def unique_citekeys_by ( self , attribute : str = \"standard_id\" ) -> list : return [ citekeys [ 0 ] for key , citekeys in self . group_citekeys_by ( attribute )] def check_collisions ( self ): \"\"\" Check for short_id hash collisions \"\"\" for short_id , citekeys in self . group_citekeys_by ( \"short_id\" ): standard_ids = sorted ( set ( x . standard_id for x in citekeys )) if len ( standard_ids ) == 1 : continue logging . error ( \"Congratulations! Hash collision. Please report to https://git.io/JfuhH. \\n \" f \"Multiple standard_ids hashed to {short_id}: {standard_ids}\" ) def check_multiple_input_ids ( self ): \"\"\" Identify different input_ids referring to the same reference. \"\"\" for standard_id , citekeys in self . group_citekeys_by ( \"standard_id\" ): input_ids = [ x . input_id for x in citekeys ] if len ( input_ids ) < 2 : continue logging . warning ( f \"Multiple citekey input_ids refer to the same standard_id {standard_id}: \\n {input_ids}\" ) def inspect ( self , log_level = None ): \"\"\" If log_level is not None, log combined inspection report at this level. \"\"\" citekeys = self . unique_citekeys_by ( \"dealiased_id\" ) reports = [] for citekey in citekeys : report = citekey . inspect () if not report : continue reports . append ( f \"{citekey.dealiased_id} -- {report}\" ) report = \" \\n \" . join ( reports ) if reports and log_level is not None : log_level = logging . _checkLevel ( log_level ) msg = f \"Inspection of dealiased citekeys revealed potential problems: \\n {report}\" logging . log ( log_level , msg ) return report def load_manual_references ( self , * args , ** kwargs ): \"\"\" Load manual references \"\"\" from manubot.process.bibliography import load_manual_references manual_refs = load_manual_references ( * args , ** kwargs ) self . manual_refs . update ( manual_refs ) def get_csl_items ( self ) -> tp . List : \"\"\" Produce a list of CSL_Items. I.e. a references list / bibliography for `self.citekeys`. \"\"\" # dictionary of input_id to CSL_Item ID (i.e. short_id), # excludes standard_ids for which CSL Items could not be generated. self . input_to_csl_id = {} self . csl_items = [] groups = self . group_citekeys_by ( \"standard_id\" ) for _standard_id , citekeys in groups : csl_item = citekey_to_csl_item ( citekey = citekeys [ 0 ], prune = self . prune_csl_items , log_level = self . csl_item_failure_log_level , manual_refs = self . manual_refs , ) if csl_item : for ck in citekeys : self . input_to_csl_id [ ck . input_id ] = csl_item [ \"id\" ] self . csl_items . append ( csl_item ) return self . csl_items @property def citekeys_tsv ( self ) -> str : import io import csv fields = [ \"input_id\" , \"dealiased_id\" , \"standard_id\" , \"short_id\" ] output = io . StringIO () writer = csv . DictWriter ( output , fieldnames = fields , delimiter = \" \\t \" ) writer . writeheader () for citekey in self . citekeys : row = { x : getattr ( citekey , x ) for x in fields } writer . writerow ( row ) return output . getvalue () @property def csl_json ( self ) -> str : assert hasattr ( self , \"csl_items\" ) json_str = json . dumps ( self . csl_items , indent = 2 , ensure_ascii = False ) json_str += \" \\n \" return json_str def write_csl_json ( self , path ): \"\"\" Write CSL Items to a JSON file at `path`. If `path` evaluates as False, do nothing. \"\"\" if not path : return path = pathlib . Path ( path ) path . write_text ( self . csl_json , encoding = \"utf-8\" ) def write_citekeys_tsv ( self , path ): \"\"\" Write `self.citekeys_tsv` to a file. If `path` evaluates as False, do nothing. \"\"\" if not path : return path = pathlib . Path ( path ) path . write_text ( self . citekeys_tsv , encoding = \"utf-8\" ) Classes Citations class Citations ( input_ids : list , aliases : dict = < factory > , manual_refs : dict = < factory > , csl_item_failure_log_level : Union [ str , int ] = 'WARNING' , prune_csl_items : bool = True ) Class for operating on a set of citations provided by their citekey input_ids. View Source class Citations : \"\"\" Class for operating on a set of citations provided by their citekey input_ids. \"\"\" # Input citekey IDs as strings input_ids : list # Citation key aliases aliases : dict = dataclasses . field ( default_factory = dict ) # manual references dictionary of standard_id to CSL_Item. manual_refs : dict = dataclasses . field ( default_factory = dict ) # level to log failures related to CSL Item generation csl_item_failure_log_level : tp . Union [ str , int ] = \"WARNING\" # whether to prune csl items according to the JSON Schema prune_csl_items : bool = True def __post_init__ ( self ): input_ids = list ( dict . fromkeys ( self . input_ids )) # deduplicate self . citekeys = [ CiteKey ( x , self . aliases ) for x in input_ids ] def filter_pandoc_xnos ( self ) -> list : \"\"\" Filter self.citekeys to remove pandoc-xnos style citekeys. Return removed citekeys. \"\"\" keep , remove = [], [] for citekey in self . citekeys : remove_ = citekey . is_pandoc_xnos_prefix ( log_case_warning = True ) ( keep , remove )[ remove_ ]. append ( citekey ) self . citekeys = keep return remove def filter_unhandled ( self ) -> list : \"\"\" Filter self.citekeys to remove unhandled citekeys. Return removed citekeys. \"\"\" keep , remove = [], [] for citekey in self . citekeys : ( remove , keep )[ citekey . is_handled_prefix ]. append ( citekey ) self . citekeys = keep return remove def group_citekeys_by ( self , attribute : str = \"standard_id\" ) -> tp . List [ tp . Tuple [ str , list ]]: \"\"\" Group `self.citekeys` by `attribute`. \"\"\" def get_key ( x ): return getattr ( x , attribute ) citekeys = sorted ( self . citekeys , key = get_key ) groups = itertools . groupby ( citekeys , get_key ) return [( key , list ( group )) for key , group in groups ] def unique_citekeys_by ( self , attribute : str = \"standard_id\" ) -> list : return [ citekeys [ 0 ] for key , citekeys in self . group_citekeys_by ( attribute )] def check_collisions ( self ): \"\"\" Check for short_id hash collisions \"\"\" for short_id , citekeys in self . group_citekeys_by ( \"short_id\" ): standard_ids = sorted ( set ( x . standard_id for x in citekeys )) if len ( standard_ids ) == 1 : continue logging . error ( \"Congratulations! Hash collision. Please report to https://git.io/JfuhH.\\n\" f \"Multiple standard_ids hashed to {short_id}: {standard_ids}\" ) def check_multiple_input_ids ( self ): \"\"\" Identify different input_ids referring to the same reference. \"\"\" for standard_id , citekeys in self . group_citekeys_by ( \"standard_id\" ): input_ids = [ x . input_id for x in citekeys ] if len ( input_ids ) < 2 : continue logging . warning ( f \"Multiple citekey input_ids refer to the same standard_id {standard_id}:\\n{input_ids}\" ) def inspect ( self , log_level = None ): \"\"\" If log_level is not None, log combined inspection report at this level. \"\"\" citekeys = self . unique_citekeys_by ( \"dealiased_id\" ) reports = [] for citekey in citekeys : report = citekey . inspect () if not report : continue reports . append ( f \"{citekey.dealiased_id} -- {report}\" ) report = \"\\n\" . join ( reports ) if reports and log_level is not None : log_level = logging . _checkLevel ( log_level ) msg = f \"Inspection of dealiased citekeys revealed potential problems:\\n{report}\" logging . log ( log_level , msg ) return report def load_manual_references ( self , * args , ** kwargs ): \"\"\" Load manual references \"\"\" from manubot . process . bibliography import load_manual_references manual_refs = load_manual_references ( * args , ** kwargs ) self . manual_refs . update ( manual_refs ) def get_csl_items ( self ) -> tp . List : \"\"\" Produce a list of CSL_Items. I.e. a references list / bibliography for `self.citekeys`. \"\"\" # dictionary of input_id to CSL_Item ID (i.e. short_id), # excludes standard_ids for which CSL Items could not be generated. self . input_to_csl_id = {} self . csl_items = [] groups = self . group_citekeys_by ( \"standard_id\" ) for _standard_id , citekeys in groups : csl_item = citekey_to_csl_item ( citekey = citekeys [ 0 ], prune = self . prune_csl_items , log_level = self . csl_item_failure_log_level , manual_refs = self . manual_refs , ) if csl_item : for ck in citekeys : self . input_to_csl_id [ ck . input_id ] = csl_item [ \"id\" ] self . csl_items . append ( csl_item ) return self . csl_items @ property def citekeys_tsv ( self ) -> str : import io import csv fields = [ \"input_id\" , \"dealiased_id\" , \"standard_id\" , \"short_id\" ] output = io . StringIO () writer = csv . DictWriter ( output , fieldnames = fields , delimiter = \"\\t\" ) writer . writeheader () for citekey in self . citekeys : row = { x : getattr ( citekey , x ) for x in fields } writer . writerow ( row ) return output . getvalue () @ property def csl_json ( self ) -> str : assert hasattr ( self , \"csl_items\" ) json_str = json . dumps ( self . csl_items , indent = 2 , ensure_ascii = False ) json_str += \"\\n\" return json_str def write_csl_json ( self , path ): \"\"\" Write CSL Items to a JSON file at `path`. If `path` evaluates as False, do nothing. \"\"\" if not path : return path = pathlib . Path ( path ) path . write_text ( self . csl_json , encoding = \"utf-8\" ) def write_citekeys_tsv ( self , path ): \"\"\" Write `self.citekeys_tsv` to a file. If `path` evaluates as False, do nothing. \"\"\" if not path : return path = pathlib . Path ( path ) path . write_text ( self . citekeys_tsv , encoding = \"utf-8\" ) Class variables csl_item_failure_log_level prune_csl_items Instance variables citekeys_tsv csl_json Methods check_collisions def check_collisions ( self ) Check for short_id hash collisions View Source def check_collisions ( self ): \"\"\" Check for short_id hash collisions \"\"\" for short_id , citekeys in self . group_citekeys_by ( \"short_id\" ): standard_ids = sorted ( set ( x . standard_id for x in citekeys )) if len ( standard_ids ) == 1 : continue logging . error ( \"Congratulations! Hash collision. Please report to https://git.io/JfuhH.\\n\" f \"Multiple standard_ids hashed to {short_id}: {standard_ids}\" ) check_multiple_input_ids def check_multiple_input_ids ( self ) Identify different input_ids referring to the same reference. View Source def check_multiple_input_ids ( self ): \"\"\" Identify different input_ids referring to the same reference. \"\"\" for standard_id , citekeys in self . group_citekeys_by ( \"standard_id\" ): input_ids = [ x . input_id for x in citekeys ] if len ( input_ids ) < 2 : continue logging . warning ( f \"Multiple citekey input_ids refer to the same standard_id {standard_id}:\\n{input_ids}\" ) filter_pandoc_xnos def filter_pandoc_xnos ( self ) -> list Filter self.citekeys to remove pandoc-xnos style citekeys. Return removed citekeys. View Source def filter_pandoc_xnos ( self ) -> list : \"\"\" Filter self.citekeys to remove pandoc-xnos style citekeys. Return removed citekeys. \"\"\" keep , remove = [] , [] for citekey in self . citekeys : remove_ = citekey . is_pandoc_xnos_prefix ( log_case_warning = True ) ( keep , remove ) [ remove_ ] . append ( citekey ) self . citekeys = keep return remove filter_unhandled def filter_unhandled ( self ) -> list Filter self.citekeys to remove unhandled citekeys. Return removed citekeys. View Source def filter_unhandled ( self ) -> list : \"\"\" Filter self.citekeys to remove unhandled citekeys. Return removed citekeys. \"\"\" keep , remove = [], [] for citekey in self . citekeys : ( remove , keep )[ citekey . is_handled_prefix ]. append ( citekey ) self . citekeys = keep return remove get_csl_items def get_csl_items ( self ) -> List Produce a list of CSL_Items. I.e. a references list / bibliography for self.citekeys . View Source def get_csl_items ( self ) -> tp . List : \"\"\" Produce a list of CSL_Items. I.e. a references list / bibliography for `self.citekeys`. \"\"\" # dictionary of input_id to CSL_Item ID ( i . e . short_id ), # excludes standard_ids for which CSL Items could not be generated . self . input_to_csl_id = {} self . csl_items = [] groups = self . group_citekeys_by ( \"standard_id\" ) for _standard_id , citekeys in groups : csl_item = citekey_to_csl_item ( citekey = citekeys [ 0 ], prune = self . prune_csl_items , log_level = self . csl_item_failure_log_level , manual_refs = self . manual_refs , ) if csl_item : for ck in citekeys : self . input_to_csl_id [ ck . input_id ] = csl_item [ \"id\" ] self . csl_items . append ( csl_item ) return self . csl_items group_citekeys_by def group_citekeys_by ( self , attribute : str = 'standard_id' ) -> List [ Tuple [ str , list ]] Group self.citekeys by attribute . View Source def group_citekeys_by ( self , attribute : str = \"standard_id\" ) -> tp . List [ tp . Tuple [ str , list ]]: \"\"\" Group `self.citekeys` by `attribute`. \"\"\" def get_key ( x ): return getattr ( x , attribute ) citekeys = sorted ( self . citekeys , key = get_key ) groups = itertools . groupby ( citekeys , get_key ) return [( key , list ( group )) for key , group in groups ] inspect def inspect ( self , log_level = None ) If log_level is not None, log combined inspection report at this level. View Source def inspect ( self , log_level = None ): \"\"\" If log_level is not None, log combined inspection report at this level. \"\"\" citekeys = self . unique_citekeys_by ( \"dealiased_id\" ) reports = [] for citekey in citekeys : report = citekey . inspect () if not report : continue reports . append ( f \"{citekey.dealiased_id} -- {report}\" ) report = \"\\n\" . join ( reports ) if reports and log_level is not None : log_level = logging . _checkLevel ( log_level ) msg = f \"Inspection of dealiased citekeys revealed potential problems:\\n{report}\" logging . log ( log_level , msg ) return report load_manual_references def load_manual_references ( self , * args , ** kwargs ) Load manual references View Source def load_manual_references ( self , * args , ** kwargs ): \"\"\" Load manual references \"\"\" from manubot.process.bibliography import load_manual_references manual_refs = load_manual_references ( * args , ** kwargs ) self . manual_refs . update ( manual_refs ) unique_citekeys_by def unique_citekeys_by ( self , attribute : str = 'standard_id' ) -> list View Source def unique_citekeys_by ( self , attribute : str = \"standard_id\" ) -> list : return [ citekeys [ 0 ] for key , citekeys in self . group_citekeys_by ( attribute )] write_citekeys_tsv def write_citekeys_tsv ( self , path ) Write self.citekeys_tsv to a file. If path evaluates as False, do nothing. View Source def write_citekeys_tsv ( self , path ): \"\"\" Write `self.citekeys_tsv` to a file. If `path` evaluates as False, do nothing. \"\"\" if not path : return path = pathlib . Path ( path ) path . write_text ( self . citekeys_tsv , encoding = \"utf-8\" ) write_csl_json def write_csl_json ( self , path ) Write CSL Items to a JSON file at path . If path evaluates as False, do nothing. View Source def write_csl_json ( self , path ): \"\"\" Write CSL Items to a JSON file at `path`. If `path` evaluates as False, do nothing. \"\"\" if not path : return path = pathlib . Path ( path ) path . write_text ( self . csl_json , encoding = \"utf-8\" )","title":"Citations"},{"location":"reference/manubot/cite/citations/#module-manubotcitecitations","text":"View Source import dataclasses import itertools import json import logging import pathlib import typing as tp from manubot.cite.citekey import CiteKey , citekey_to_csl_item @dataclasses.dataclass class Citations : \"\"\" Class for operating on a set of citations provided by their citekey input_ids. \"\"\" # Input citekey IDs as strings input_ids : list # Citation key aliases aliases : dict = dataclasses . field ( default_factory = dict ) # manual references dictionary of standard_id to CSL_Item. manual_refs : dict = dataclasses . field ( default_factory = dict ) # level to log failures related to CSL Item generation csl_item_failure_log_level : tp . Union [ str , int ] = \"WARNING\" # whether to prune csl items according to the JSON Schema prune_csl_items : bool = True def __post_init__ ( self ): input_ids = list ( dict . fromkeys ( self . input_ids )) # deduplicate self . citekeys = [ CiteKey ( x , self . aliases ) for x in input_ids ] def filter_pandoc_xnos ( self ) -> list : \"\"\" Filter self.citekeys to remove pandoc-xnos style citekeys. Return removed citekeys. \"\"\" keep , remove = [], [] for citekey in self . citekeys : remove_ = citekey . is_pandoc_xnos_prefix ( log_case_warning = True ) ( keep , remove )[ remove_ ] . append ( citekey ) self . citekeys = keep return remove def filter_unhandled ( self ) -> list : \"\"\" Filter self.citekeys to remove unhandled citekeys. Return removed citekeys. \"\"\" keep , remove = [], [] for citekey in self . citekeys : ( remove , keep )[ citekey . is_handled_prefix ] . append ( citekey ) self . citekeys = keep return remove def group_citekeys_by ( self , attribute : str = \"standard_id\" ) -> tp . List [ tp . Tuple [ str , list ]]: \"\"\" Group `self.citekeys` by `attribute`. \"\"\" def get_key ( x ): return getattr ( x , attribute ) citekeys = sorted ( self . citekeys , key = get_key ) groups = itertools . groupby ( citekeys , get_key ) return [( key , list ( group )) for key , group in groups ] def unique_citekeys_by ( self , attribute : str = \"standard_id\" ) -> list : return [ citekeys [ 0 ] for key , citekeys in self . group_citekeys_by ( attribute )] def check_collisions ( self ): \"\"\" Check for short_id hash collisions \"\"\" for short_id , citekeys in self . group_citekeys_by ( \"short_id\" ): standard_ids = sorted ( set ( x . standard_id for x in citekeys )) if len ( standard_ids ) == 1 : continue logging . error ( \"Congratulations! Hash collision. Please report to https://git.io/JfuhH. \\n \" f \"Multiple standard_ids hashed to {short_id}: {standard_ids}\" ) def check_multiple_input_ids ( self ): \"\"\" Identify different input_ids referring to the same reference. \"\"\" for standard_id , citekeys in self . group_citekeys_by ( \"standard_id\" ): input_ids = [ x . input_id for x in citekeys ] if len ( input_ids ) < 2 : continue logging . warning ( f \"Multiple citekey input_ids refer to the same standard_id {standard_id}: \\n {input_ids}\" ) def inspect ( self , log_level = None ): \"\"\" If log_level is not None, log combined inspection report at this level. \"\"\" citekeys = self . unique_citekeys_by ( \"dealiased_id\" ) reports = [] for citekey in citekeys : report = citekey . inspect () if not report : continue reports . append ( f \"{citekey.dealiased_id} -- {report}\" ) report = \" \\n \" . join ( reports ) if reports and log_level is not None : log_level = logging . _checkLevel ( log_level ) msg = f \"Inspection of dealiased citekeys revealed potential problems: \\n {report}\" logging . log ( log_level , msg ) return report def load_manual_references ( self , * args , ** kwargs ): \"\"\" Load manual references \"\"\" from manubot.process.bibliography import load_manual_references manual_refs = load_manual_references ( * args , ** kwargs ) self . manual_refs . update ( manual_refs ) def get_csl_items ( self ) -> tp . List : \"\"\" Produce a list of CSL_Items. I.e. a references list / bibliography for `self.citekeys`. \"\"\" # dictionary of input_id to CSL_Item ID (i.e. short_id), # excludes standard_ids for which CSL Items could not be generated. self . input_to_csl_id = {} self . csl_items = [] groups = self . group_citekeys_by ( \"standard_id\" ) for _standard_id , citekeys in groups : csl_item = citekey_to_csl_item ( citekey = citekeys [ 0 ], prune = self . prune_csl_items , log_level = self . csl_item_failure_log_level , manual_refs = self . manual_refs , ) if csl_item : for ck in citekeys : self . input_to_csl_id [ ck . input_id ] = csl_item [ \"id\" ] self . csl_items . append ( csl_item ) return self . csl_items @property def citekeys_tsv ( self ) -> str : import io import csv fields = [ \"input_id\" , \"dealiased_id\" , \"standard_id\" , \"short_id\" ] output = io . StringIO () writer = csv . DictWriter ( output , fieldnames = fields , delimiter = \" \\t \" ) writer . writeheader () for citekey in self . citekeys : row = { x : getattr ( citekey , x ) for x in fields } writer . writerow ( row ) return output . getvalue () @property def csl_json ( self ) -> str : assert hasattr ( self , \"csl_items\" ) json_str = json . dumps ( self . csl_items , indent = 2 , ensure_ascii = False ) json_str += \" \\n \" return json_str def write_csl_json ( self , path ): \"\"\" Write CSL Items to a JSON file at `path`. If `path` evaluates as False, do nothing. \"\"\" if not path : return path = pathlib . Path ( path ) path . write_text ( self . csl_json , encoding = \"utf-8\" ) def write_citekeys_tsv ( self , path ): \"\"\" Write `self.citekeys_tsv` to a file. If `path` evaluates as False, do nothing. \"\"\" if not path : return path = pathlib . Path ( path ) path . write_text ( self . citekeys_tsv , encoding = \"utf-8\" )","title":"Module manubot.cite.citations"},{"location":"reference/manubot/cite/citations/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/citations/#citations","text":"class Citations ( input_ids : list , aliases : dict = < factory > , manual_refs : dict = < factory > , csl_item_failure_log_level : Union [ str , int ] = 'WARNING' , prune_csl_items : bool = True ) Class for operating on a set of citations provided by their citekey input_ids. View Source class Citations : \"\"\" Class for operating on a set of citations provided by their citekey input_ids. \"\"\" # Input citekey IDs as strings input_ids : list # Citation key aliases aliases : dict = dataclasses . field ( default_factory = dict ) # manual references dictionary of standard_id to CSL_Item. manual_refs : dict = dataclasses . field ( default_factory = dict ) # level to log failures related to CSL Item generation csl_item_failure_log_level : tp . Union [ str , int ] = \"WARNING\" # whether to prune csl items according to the JSON Schema prune_csl_items : bool = True def __post_init__ ( self ): input_ids = list ( dict . fromkeys ( self . input_ids )) # deduplicate self . citekeys = [ CiteKey ( x , self . aliases ) for x in input_ids ] def filter_pandoc_xnos ( self ) -> list : \"\"\" Filter self.citekeys to remove pandoc-xnos style citekeys. Return removed citekeys. \"\"\" keep , remove = [], [] for citekey in self . citekeys : remove_ = citekey . is_pandoc_xnos_prefix ( log_case_warning = True ) ( keep , remove )[ remove_ ]. append ( citekey ) self . citekeys = keep return remove def filter_unhandled ( self ) -> list : \"\"\" Filter self.citekeys to remove unhandled citekeys. Return removed citekeys. \"\"\" keep , remove = [], [] for citekey in self . citekeys : ( remove , keep )[ citekey . is_handled_prefix ]. append ( citekey ) self . citekeys = keep return remove def group_citekeys_by ( self , attribute : str = \"standard_id\" ) -> tp . List [ tp . Tuple [ str , list ]]: \"\"\" Group `self.citekeys` by `attribute`. \"\"\" def get_key ( x ): return getattr ( x , attribute ) citekeys = sorted ( self . citekeys , key = get_key ) groups = itertools . groupby ( citekeys , get_key ) return [( key , list ( group )) for key , group in groups ] def unique_citekeys_by ( self , attribute : str = \"standard_id\" ) -> list : return [ citekeys [ 0 ] for key , citekeys in self . group_citekeys_by ( attribute )] def check_collisions ( self ): \"\"\" Check for short_id hash collisions \"\"\" for short_id , citekeys in self . group_citekeys_by ( \"short_id\" ): standard_ids = sorted ( set ( x . standard_id for x in citekeys )) if len ( standard_ids ) == 1 : continue logging . error ( \"Congratulations! Hash collision. Please report to https://git.io/JfuhH.\\n\" f \"Multiple standard_ids hashed to {short_id}: {standard_ids}\" ) def check_multiple_input_ids ( self ): \"\"\" Identify different input_ids referring to the same reference. \"\"\" for standard_id , citekeys in self . group_citekeys_by ( \"standard_id\" ): input_ids = [ x . input_id for x in citekeys ] if len ( input_ids ) < 2 : continue logging . warning ( f \"Multiple citekey input_ids refer to the same standard_id {standard_id}:\\n{input_ids}\" ) def inspect ( self , log_level = None ): \"\"\" If log_level is not None, log combined inspection report at this level. \"\"\" citekeys = self . unique_citekeys_by ( \"dealiased_id\" ) reports = [] for citekey in citekeys : report = citekey . inspect () if not report : continue reports . append ( f \"{citekey.dealiased_id} -- {report}\" ) report = \"\\n\" . join ( reports ) if reports and log_level is not None : log_level = logging . _checkLevel ( log_level ) msg = f \"Inspection of dealiased citekeys revealed potential problems:\\n{report}\" logging . log ( log_level , msg ) return report def load_manual_references ( self , * args , ** kwargs ): \"\"\" Load manual references \"\"\" from manubot . process . bibliography import load_manual_references manual_refs = load_manual_references ( * args , ** kwargs ) self . manual_refs . update ( manual_refs ) def get_csl_items ( self ) -> tp . List : \"\"\" Produce a list of CSL_Items. I.e. a references list / bibliography for `self.citekeys`. \"\"\" # dictionary of input_id to CSL_Item ID (i.e. short_id), # excludes standard_ids for which CSL Items could not be generated. self . input_to_csl_id = {} self . csl_items = [] groups = self . group_citekeys_by ( \"standard_id\" ) for _standard_id , citekeys in groups : csl_item = citekey_to_csl_item ( citekey = citekeys [ 0 ], prune = self . prune_csl_items , log_level = self . csl_item_failure_log_level , manual_refs = self . manual_refs , ) if csl_item : for ck in citekeys : self . input_to_csl_id [ ck . input_id ] = csl_item [ \"id\" ] self . csl_items . append ( csl_item ) return self . csl_items @ property def citekeys_tsv ( self ) -> str : import io import csv fields = [ \"input_id\" , \"dealiased_id\" , \"standard_id\" , \"short_id\" ] output = io . StringIO () writer = csv . DictWriter ( output , fieldnames = fields , delimiter = \"\\t\" ) writer . writeheader () for citekey in self . citekeys : row = { x : getattr ( citekey , x ) for x in fields } writer . writerow ( row ) return output . getvalue () @ property def csl_json ( self ) -> str : assert hasattr ( self , \"csl_items\" ) json_str = json . dumps ( self . csl_items , indent = 2 , ensure_ascii = False ) json_str += \"\\n\" return json_str def write_csl_json ( self , path ): \"\"\" Write CSL Items to a JSON file at `path`. If `path` evaluates as False, do nothing. \"\"\" if not path : return path = pathlib . Path ( path ) path . write_text ( self . csl_json , encoding = \"utf-8\" ) def write_citekeys_tsv ( self , path ): \"\"\" Write `self.citekeys_tsv` to a file. If `path` evaluates as False, do nothing. \"\"\" if not path : return path = pathlib . Path ( path ) path . write_text ( self . citekeys_tsv , encoding = \"utf-8\" )","title":"Citations"},{"location":"reference/manubot/cite/citations/#class-variables","text":"csl_item_failure_log_level prune_csl_items","title":"Class variables"},{"location":"reference/manubot/cite/citations/#instance-variables","text":"citekeys_tsv csl_json","title":"Instance variables"},{"location":"reference/manubot/cite/citations/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/citations/#check_collisions","text":"def check_collisions ( self ) Check for short_id hash collisions View Source def check_collisions ( self ): \"\"\" Check for short_id hash collisions \"\"\" for short_id , citekeys in self . group_citekeys_by ( \"short_id\" ): standard_ids = sorted ( set ( x . standard_id for x in citekeys )) if len ( standard_ids ) == 1 : continue logging . error ( \"Congratulations! Hash collision. Please report to https://git.io/JfuhH.\\n\" f \"Multiple standard_ids hashed to {short_id}: {standard_ids}\" )","title":"check_collisions"},{"location":"reference/manubot/cite/citations/#check_multiple_input_ids","text":"def check_multiple_input_ids ( self ) Identify different input_ids referring to the same reference. View Source def check_multiple_input_ids ( self ): \"\"\" Identify different input_ids referring to the same reference. \"\"\" for standard_id , citekeys in self . group_citekeys_by ( \"standard_id\" ): input_ids = [ x . input_id for x in citekeys ] if len ( input_ids ) < 2 : continue logging . warning ( f \"Multiple citekey input_ids refer to the same standard_id {standard_id}:\\n{input_ids}\" )","title":"check_multiple_input_ids"},{"location":"reference/manubot/cite/citations/#filter_pandoc_xnos","text":"def filter_pandoc_xnos ( self ) -> list Filter self.citekeys to remove pandoc-xnos style citekeys. Return removed citekeys. View Source def filter_pandoc_xnos ( self ) -> list : \"\"\" Filter self.citekeys to remove pandoc-xnos style citekeys. Return removed citekeys. \"\"\" keep , remove = [] , [] for citekey in self . citekeys : remove_ = citekey . is_pandoc_xnos_prefix ( log_case_warning = True ) ( keep , remove ) [ remove_ ] . append ( citekey ) self . citekeys = keep return remove","title":"filter_pandoc_xnos"},{"location":"reference/manubot/cite/citations/#filter_unhandled","text":"def filter_unhandled ( self ) -> list Filter self.citekeys to remove unhandled citekeys. Return removed citekeys. View Source def filter_unhandled ( self ) -> list : \"\"\" Filter self.citekeys to remove unhandled citekeys. Return removed citekeys. \"\"\" keep , remove = [], [] for citekey in self . citekeys : ( remove , keep )[ citekey . is_handled_prefix ]. append ( citekey ) self . citekeys = keep return remove","title":"filter_unhandled"},{"location":"reference/manubot/cite/citations/#get_csl_items","text":"def get_csl_items ( self ) -> List Produce a list of CSL_Items. I.e. a references list / bibliography for self.citekeys . View Source def get_csl_items ( self ) -> tp . List : \"\"\" Produce a list of CSL_Items. I.e. a references list / bibliography for `self.citekeys`. \"\"\" # dictionary of input_id to CSL_Item ID ( i . e . short_id ), # excludes standard_ids for which CSL Items could not be generated . self . input_to_csl_id = {} self . csl_items = [] groups = self . group_citekeys_by ( \"standard_id\" ) for _standard_id , citekeys in groups : csl_item = citekey_to_csl_item ( citekey = citekeys [ 0 ], prune = self . prune_csl_items , log_level = self . csl_item_failure_log_level , manual_refs = self . manual_refs , ) if csl_item : for ck in citekeys : self . input_to_csl_id [ ck . input_id ] = csl_item [ \"id\" ] self . csl_items . append ( csl_item ) return self . csl_items","title":"get_csl_items"},{"location":"reference/manubot/cite/citations/#group_citekeys_by","text":"def group_citekeys_by ( self , attribute : str = 'standard_id' ) -> List [ Tuple [ str , list ]] Group self.citekeys by attribute . View Source def group_citekeys_by ( self , attribute : str = \"standard_id\" ) -> tp . List [ tp . Tuple [ str , list ]]: \"\"\" Group `self.citekeys` by `attribute`. \"\"\" def get_key ( x ): return getattr ( x , attribute ) citekeys = sorted ( self . citekeys , key = get_key ) groups = itertools . groupby ( citekeys , get_key ) return [( key , list ( group )) for key , group in groups ]","title":"group_citekeys_by"},{"location":"reference/manubot/cite/citations/#inspect","text":"def inspect ( self , log_level = None ) If log_level is not None, log combined inspection report at this level. View Source def inspect ( self , log_level = None ): \"\"\" If log_level is not None, log combined inspection report at this level. \"\"\" citekeys = self . unique_citekeys_by ( \"dealiased_id\" ) reports = [] for citekey in citekeys : report = citekey . inspect () if not report : continue reports . append ( f \"{citekey.dealiased_id} -- {report}\" ) report = \"\\n\" . join ( reports ) if reports and log_level is not None : log_level = logging . _checkLevel ( log_level ) msg = f \"Inspection of dealiased citekeys revealed potential problems:\\n{report}\" logging . log ( log_level , msg ) return report","title":"inspect"},{"location":"reference/manubot/cite/citations/#load_manual_references","text":"def load_manual_references ( self , * args , ** kwargs ) Load manual references View Source def load_manual_references ( self , * args , ** kwargs ): \"\"\" Load manual references \"\"\" from manubot.process.bibliography import load_manual_references manual_refs = load_manual_references ( * args , ** kwargs ) self . manual_refs . update ( manual_refs )","title":"load_manual_references"},{"location":"reference/manubot/cite/citations/#unique_citekeys_by","text":"def unique_citekeys_by ( self , attribute : str = 'standard_id' ) -> list View Source def unique_citekeys_by ( self , attribute : str = \"standard_id\" ) -> list : return [ citekeys [ 0 ] for key , citekeys in self . group_citekeys_by ( attribute )]","title":"unique_citekeys_by"},{"location":"reference/manubot/cite/citations/#write_citekeys_tsv","text":"def write_citekeys_tsv ( self , path ) Write self.citekeys_tsv to a file. If path evaluates as False, do nothing. View Source def write_citekeys_tsv ( self , path ): \"\"\" Write `self.citekeys_tsv` to a file. If `path` evaluates as False, do nothing. \"\"\" if not path : return path = pathlib . Path ( path ) path . write_text ( self . citekeys_tsv , encoding = \"utf-8\" )","title":"write_citekeys_tsv"},{"location":"reference/manubot/cite/citations/#write_csl_json","text":"def write_csl_json ( self , path ) Write CSL Items to a JSON file at path . If path evaluates as False, do nothing. View Source def write_csl_json ( self , path ): \"\"\" Write CSL Items to a JSON file at `path`. If `path` evaluates as False, do nothing. \"\"\" if not path : return path = pathlib . Path ( path ) path . write_text ( self . csl_json , encoding = \"utf-8\" )","title":"write_csl_json"},{"location":"reference/manubot/cite/cite_command/","text":"Module manubot.cite.cite_command View Source import json import logging import pathlib import subprocess import sys from manubot.pandoc.util import get_pandoc_info from manubot.util import shlex_join from manubot.cite.citations import Citations # For manubot cite, infer --format from --output filename extensions extension_to_format = { \".txt\" : \"plain\" , \".md\" : \"markdown\" , \".docx\" : \"docx\" , \".html\" : \"html\" , \".xml\" : \"jats\" , } def call_pandoc ( metadata , path , format = \"plain\" ): \"\"\" path is the path to write to. \"\"\" _exit_without_pandoc () info = get_pandoc_info () _check_pandoc_version ( info , metadata , format ) metadata_block = \"--- \\n {yaml} \\n ... \\n \" . format ( yaml = json . dumps ( metadata , ensure_ascii = False , indent = 2 ) ) args = [ \"pandoc\" , \"--filter\" , \"pandoc-citeproc\" , \"--output\" , str ( path ) if path else \"-\" , ] if format == \"markdown\" : args . extend ([ \"--to\" , \"markdown_strict\" , \"--wrap\" , \"none\" ]) elif format == \"jats\" : args . extend ([ \"--to\" , \"jats\" , \"--standalone\" ]) elif format == \"docx\" : args . extend ([ \"--to\" , \"docx\" ]) elif format == \"html\" : args . extend ([ \"--to\" , \"html\" ]) elif format == \"plain\" : args . extend ([ \"--to\" , \"plain\" , \"--wrap\" , \"none\" ]) if info [ \"pandoc version\" ] >= ( 2 ,): # Do not use ALL_CAPS for bold & underscores for italics # https://github.com/jgm/pandoc/issues/4834#issuecomment-412972008 filter_path = ( pathlib . Path ( __file__ ) . joinpath ( \"..\" , \"plain-pandoc-filter.lua\" ) . resolve () ) assert filter_path . exists () args . extend ([ \"--lua-filter\" , str ( filter_path )]) logging . info ( \"call_pandoc subprocess args: \\n \" + shlex_join ( args )) process = subprocess . run ( args = args , input = metadata_block . encode (),) process . check_returncode () def cli_cite ( args ): \"\"\" Main function for the manubot cite command-line interface. Does not allow user to directly specify Pandoc's --to argument, due to inconsistent citation rendering by output format. See https://github.com/jgm/pandoc/issues/4834 \"\"\" citations = Citations ( args . citekeys ) citations . load_manual_references ( paths = args . bibliography ) citations . inspect ( log_level = \"WARNING\" ) csl_items = citations . get_csl_items () # output CSL JSON data, if --render is False if not args . render : write_file = args . output . open ( \"wb\" ) if args . output else sys . stdout . buffer with write_file : write_file . write ( citations . csl_json . encode ()) return # use Pandoc to render references if not args . format and args . output : vars ( args )[ \"format\" ] = extension_to_format . get ( args . output . suffix ) if not args . format : vars ( args )[ \"format\" ] = \"plain\" pandoc_metadata = { \"nocite\" : \"@*\" , \"csl\" : args . csl , \"references\" : csl_items } call_pandoc ( metadata = pandoc_metadata , path = args . output , format = args . format ) def _exit_without_pandoc (): \"\"\" Given info from get_pandoc_info, exit Python if Pandoc is not available. \"\"\" info = get_pandoc_info () for command in \"pandoc\" , \"pandoc-citeproc\" : if not info [ command ]: logging . critical ( f '\"{command}\" not found on system. ' f \"Check that Pandoc is installed.\" ) raise SystemExit ( 1 ) def _check_pandoc_version ( info , metadata , format ): \"\"\" Given info from get_pandoc_info, check that Pandoc's version is sufficient to perform the citation rendering command specified by metadata and format. Please add additional minimum version information to this function, as its discovered. \"\"\" issues = list () if format == \"jats\" and info [ \"pandoc version\" ] < ( 2 ,): issues . append ( \"--jats requires pandoc >= v2.0.\" ) # --csl=URL did not work in https://travis-ci.org/greenelab/manubot/builds/417314743#L796, # but exact version where this fails unknown # if metadata.get('csl', '').startswith('http') and pandoc_version < (2,): # issues.append('--csl=URL requires pandoc >= v2.0.') issues = \" \\n \" . join ( issues ) if issues : logging . critical ( f \"issues with pandoc version detected: \\n {issues}\" ) Variables extension_to_format Functions call_pandoc def call_pandoc ( metadata , path , format = 'plain' ) path is the path to write to. View Source def call_pandoc ( metadata , path , format = \"plain\" ): \"\"\" path is the path to write to. \"\"\" _exit_without_pandoc () info = get_pandoc_info () _check_pandoc_version ( info , metadata , format ) metadata_block = \"---\\n{yaml}\\n...\\n\" . format ( yaml = json . dumps ( metadata , ensure_ascii = False , indent = 2 ) ) args = [ \"pandoc\" , \"--filter\" , \"pandoc-citeproc\" , \"--output\" , str ( path ) if path else \"-\" , ] if format == \"markdown\" : args . extend ([ \"--to\" , \"markdown_strict\" , \"--wrap\" , \"none\" ]) elif format == \"jats\" : args . extend ([ \"--to\" , \"jats\" , \"--standalone\" ]) elif format == \"docx\" : args . extend ([ \"--to\" , \"docx\" ]) elif format == \"html\" : args . extend ([ \"--to\" , \"html\" ]) elif format == \"plain\" : args . extend ([ \"--to\" , \"plain\" , \"--wrap\" , \"none\" ]) if info [ \"pandoc version\" ] >= ( 2 ,): # Do not use ALL_CAPS for bold & underscores for italics # https : // github . com / jgm / pandoc / issues / 4834 # issuecomment - 412972008 filter_path = ( pathlib . Path ( __file__ ) . joinpath ( \"..\" , \"plain-pandoc-filter.lua\" ) . resolve () ) assert filter_path . exists () args . extend ([ \"--lua-filter\" , str ( filter_path )]) logging . info ( \"call_pandoc subprocess args:\\n\" + shlex_join ( args )) process = subprocess . run ( args = args , input = metadata_block . encode (),) process . check_returncode () cli_cite def cli_cite ( args ) Main function for the manubot cite command-line interface. Does not allow user to directly specify Pandoc's --to argument, due to inconsistent citation rendering by output format. See https://github.com/jgm/pandoc/issues/4834 View Source def cli_cite ( args ): \"\"\" Main function for the manubot cite command-line interface. Does not allow user to directly specify Pandoc's --to argument, due to inconsistent citation rendering by output format. See https://github.com/jgm/pandoc/issues/4834 \"\"\" citations = Citations ( args . citekeys ) citations . load_manual_references ( paths = args . bibliography ) citations . inspect ( log_level = \"WARNING\" ) csl_items = citations . get_csl_items () # output CSL JSON data , if --render is False if not args . render : write_file = args . output . open ( \"wb\" ) if args . output else sys . stdout . buffer with write_file : write_file . write ( citations . csl_json . encode ()) return # use Pandoc to render references if not args . format and args . output : vars ( args )[ \"format\" ] = extension_to_format . get ( args . output . suffix ) if not args . format : vars ( args )[ \"format\" ] = \"plain\" pandoc_metadata = { \"nocite\" : \"@*\" , \"csl\" : args . csl , \"references\" : csl_items } call_pandoc ( metadata = pandoc_metadata , path = args . output , format = args . format )","title":"Cite Command"},{"location":"reference/manubot/cite/cite_command/#module-manubotcitecite_command","text":"View Source import json import logging import pathlib import subprocess import sys from manubot.pandoc.util import get_pandoc_info from manubot.util import shlex_join from manubot.cite.citations import Citations # For manubot cite, infer --format from --output filename extensions extension_to_format = { \".txt\" : \"plain\" , \".md\" : \"markdown\" , \".docx\" : \"docx\" , \".html\" : \"html\" , \".xml\" : \"jats\" , } def call_pandoc ( metadata , path , format = \"plain\" ): \"\"\" path is the path to write to. \"\"\" _exit_without_pandoc () info = get_pandoc_info () _check_pandoc_version ( info , metadata , format ) metadata_block = \"--- \\n {yaml} \\n ... \\n \" . format ( yaml = json . dumps ( metadata , ensure_ascii = False , indent = 2 ) ) args = [ \"pandoc\" , \"--filter\" , \"pandoc-citeproc\" , \"--output\" , str ( path ) if path else \"-\" , ] if format == \"markdown\" : args . extend ([ \"--to\" , \"markdown_strict\" , \"--wrap\" , \"none\" ]) elif format == \"jats\" : args . extend ([ \"--to\" , \"jats\" , \"--standalone\" ]) elif format == \"docx\" : args . extend ([ \"--to\" , \"docx\" ]) elif format == \"html\" : args . extend ([ \"--to\" , \"html\" ]) elif format == \"plain\" : args . extend ([ \"--to\" , \"plain\" , \"--wrap\" , \"none\" ]) if info [ \"pandoc version\" ] >= ( 2 ,): # Do not use ALL_CAPS for bold & underscores for italics # https://github.com/jgm/pandoc/issues/4834#issuecomment-412972008 filter_path = ( pathlib . Path ( __file__ ) . joinpath ( \"..\" , \"plain-pandoc-filter.lua\" ) . resolve () ) assert filter_path . exists () args . extend ([ \"--lua-filter\" , str ( filter_path )]) logging . info ( \"call_pandoc subprocess args: \\n \" + shlex_join ( args )) process = subprocess . run ( args = args , input = metadata_block . encode (),) process . check_returncode () def cli_cite ( args ): \"\"\" Main function for the manubot cite command-line interface. Does not allow user to directly specify Pandoc's --to argument, due to inconsistent citation rendering by output format. See https://github.com/jgm/pandoc/issues/4834 \"\"\" citations = Citations ( args . citekeys ) citations . load_manual_references ( paths = args . bibliography ) citations . inspect ( log_level = \"WARNING\" ) csl_items = citations . get_csl_items () # output CSL JSON data, if --render is False if not args . render : write_file = args . output . open ( \"wb\" ) if args . output else sys . stdout . buffer with write_file : write_file . write ( citations . csl_json . encode ()) return # use Pandoc to render references if not args . format and args . output : vars ( args )[ \"format\" ] = extension_to_format . get ( args . output . suffix ) if not args . format : vars ( args )[ \"format\" ] = \"plain\" pandoc_metadata = { \"nocite\" : \"@*\" , \"csl\" : args . csl , \"references\" : csl_items } call_pandoc ( metadata = pandoc_metadata , path = args . output , format = args . format ) def _exit_without_pandoc (): \"\"\" Given info from get_pandoc_info, exit Python if Pandoc is not available. \"\"\" info = get_pandoc_info () for command in \"pandoc\" , \"pandoc-citeproc\" : if not info [ command ]: logging . critical ( f '\"{command}\" not found on system. ' f \"Check that Pandoc is installed.\" ) raise SystemExit ( 1 ) def _check_pandoc_version ( info , metadata , format ): \"\"\" Given info from get_pandoc_info, check that Pandoc's version is sufficient to perform the citation rendering command specified by metadata and format. Please add additional minimum version information to this function, as its discovered. \"\"\" issues = list () if format == \"jats\" and info [ \"pandoc version\" ] < ( 2 ,): issues . append ( \"--jats requires pandoc >= v2.0.\" ) # --csl=URL did not work in https://travis-ci.org/greenelab/manubot/builds/417314743#L796, # but exact version where this fails unknown # if metadata.get('csl', '').startswith('http') and pandoc_version < (2,): # issues.append('--csl=URL requires pandoc >= v2.0.') issues = \" \\n \" . join ( issues ) if issues : logging . critical ( f \"issues with pandoc version detected: \\n {issues}\" )","title":"Module manubot.cite.cite_command"},{"location":"reference/manubot/cite/cite_command/#variables","text":"extension_to_format","title":"Variables"},{"location":"reference/manubot/cite/cite_command/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/cite_command/#call_pandoc","text":"def call_pandoc ( metadata , path , format = 'plain' ) path is the path to write to. View Source def call_pandoc ( metadata , path , format = \"plain\" ): \"\"\" path is the path to write to. \"\"\" _exit_without_pandoc () info = get_pandoc_info () _check_pandoc_version ( info , metadata , format ) metadata_block = \"---\\n{yaml}\\n...\\n\" . format ( yaml = json . dumps ( metadata , ensure_ascii = False , indent = 2 ) ) args = [ \"pandoc\" , \"--filter\" , \"pandoc-citeproc\" , \"--output\" , str ( path ) if path else \"-\" , ] if format == \"markdown\" : args . extend ([ \"--to\" , \"markdown_strict\" , \"--wrap\" , \"none\" ]) elif format == \"jats\" : args . extend ([ \"--to\" , \"jats\" , \"--standalone\" ]) elif format == \"docx\" : args . extend ([ \"--to\" , \"docx\" ]) elif format == \"html\" : args . extend ([ \"--to\" , \"html\" ]) elif format == \"plain\" : args . extend ([ \"--to\" , \"plain\" , \"--wrap\" , \"none\" ]) if info [ \"pandoc version\" ] >= ( 2 ,): # Do not use ALL_CAPS for bold & underscores for italics # https : // github . com / jgm / pandoc / issues / 4834 # issuecomment - 412972008 filter_path = ( pathlib . Path ( __file__ ) . joinpath ( \"..\" , \"plain-pandoc-filter.lua\" ) . resolve () ) assert filter_path . exists () args . extend ([ \"--lua-filter\" , str ( filter_path )]) logging . info ( \"call_pandoc subprocess args:\\n\" + shlex_join ( args )) process = subprocess . run ( args = args , input = metadata_block . encode (),) process . check_returncode ()","title":"call_pandoc"},{"location":"reference/manubot/cite/cite_command/#cli_cite","text":"def cli_cite ( args ) Main function for the manubot cite command-line interface. Does not allow user to directly specify Pandoc's --to argument, due to inconsistent citation rendering by output format. See https://github.com/jgm/pandoc/issues/4834 View Source def cli_cite ( args ): \"\"\" Main function for the manubot cite command-line interface. Does not allow user to directly specify Pandoc's --to argument, due to inconsistent citation rendering by output format. See https://github.com/jgm/pandoc/issues/4834 \"\"\" citations = Citations ( args . citekeys ) citations . load_manual_references ( paths = args . bibliography ) citations . inspect ( log_level = \"WARNING\" ) csl_items = citations . get_csl_items () # output CSL JSON data , if --render is False if not args . render : write_file = args . output . open ( \"wb\" ) if args . output else sys . stdout . buffer with write_file : write_file . write ( citations . csl_json . encode ()) return # use Pandoc to render references if not args . format and args . output : vars ( args )[ \"format\" ] = extension_to_format . get ( args . output . suffix ) if not args . format : vars ( args )[ \"format\" ] = \"plain\" pandoc_metadata = { \"nocite\" : \"@*\" , \"csl\" : args . csl , \"references\" : csl_items } call_pandoc ( metadata = pandoc_metadata , path = args . output , format = args . format )","title":"cli_cite"},{"location":"reference/manubot/cite/citekey/","text":"Module manubot.cite.citekey Utilities for representing and processing citation keys. View Source \"\"\" Utilities for representing and processing citation keys. \"\"\" import dataclasses import functools import logging import re import typing as tp try : from functools import cached_property except ImportError : from backports . cached_property import cached_property @dataclasses . dataclass class CiteKey : input_id: str aliases : dict = dataclasses . field ( default_factory = dict ) def __ post_init__ ( self ) : self . check_input_id ( self . input_id ) @staticmethod def check_input_id ( input_id ) : if not isinstance ( input_id , str ) : raise TypeError ( \"input_id should be type 'str' not \" f \"{type(input_id).__name__!r}: {input_id!r}\" ) if input_id . startswith ( \"@\" ) : f \"invalid citekey input_id: {input_id!r}\\nstarts with '@'\" @classmethod @functools . lru_cache ( maxsize = None ) def from_input_id ( cls , * args , **kwargs ) : \"\"\"Cached constructor\"\"\" return cls ( * args , **kwargs ) @cached_property def dealiased_id ( self ) -> str : \"\"\" If `self.input_id` is in `self.aliases`, the value specified by `self.aliases`. Otherwise, `self.input_id`. \"\"\" return self . aliases . get ( self . input_id , self . input_id ) def _ set_prefix_accession ( self ) : try : prefix , accession = self . dealiased_id . split ( \":\" , 1 ) except ValueError : prefix , accession = None , None self . _ prefix = prefix self . _ accession = accession @property def prefix ( self ) -> tp . Optional [ str ] : \"\"\" If `self.input_id` contains a colon, the substring up to the first colon. Otherwise, None. \"\"\" if not hasattr ( self , \"_prefix\" ) : self . _ set_prefix_accession () return self . _ prefix @property def prefix_lower ( self ) -> tp . Optional [ str ] : \"\"\" A lowercase version of `self.prefix` or None. \"\"\" if self . prefix is None : return None return self . prefix . lower () @property def accession ( self ) -> tp . Optional [ str ] : \"\"\" If `self.prefix`, the remainder of `self.input_id` following the first colon. \"\"\" if not hasattr ( self , \"_accession\" ) : self . _ set_prefix_accession () return self . _ accession @property def standard_prefix ( self ) -> tp . Optional [ str ] : \"\"\" If the citekey is handled, the standard prefix specified by the handler. Otherwise, None. \"\"\" if not hasattr ( self , \"_standard_prefix\" ) : self . _ standardize () return self . _ standard_prefix @property def standard_accession ( self ) -> tp . Optional [ str ] : \"\"\" If the citekey is handled, the standard accession specified by the handler. Otherwise, None. \"\"\" if not hasattr ( self , \"_standard_accession\" ) : self . _ standardize () return self . _ standard_accession @cached_property def handler ( self ) : from . handlers import Handler , get_handler if self . is_handled_prefix: return get_handler ( self . prefix_lower ) return Handler ( self . prefix_lower ) @cached_property def is_handled_prefix ( self ) -> bool : from . handlers import prefix_to_handler return self . prefix_lower in prefix_to_handler def inspect ( self ) -> tp . Optional [ str ] : \"\"\" Inspect citekey for potential problems. If no problems are found, return None. Otherwise, returns a string describing the problem. \"\"\" return self . handler . inspect ( self ) def _ standardize ( self ) : \"\"\" Set `self._standard_prefix`, `self._standard_accession`, and `self._standard_id`. For citekeys without a prefix or with an unhandled prefix, _standard_prefix and _standard_accession are set to None. \"\"\" if not self . is_handled_prefix: self . _ standard_prefix = None self . _ standard_accession = None self . _ standard_id = self . dealiased_id return fxn = self . handler . standardize_prefix_accession self . _ standard_prefix , self . _ standard_accession = fxn ( self . accession ) self . _ standard_id = f \"{self._standard_prefix}:{self._standard_accession}\" @property def standard_id ( self ) -> str : \"\"\" If the citekey is handled, the standard_id specified by the handler. Otherwise, `self.dealiased_id`. \"\"\" if not hasattr ( self , \"_standard_id\" ) : self . _ standardize () return self . _ standard_id @cached_property def short_id ( self ) -> str : \"\"\" A hashed version of standard_id whose characters are within the ranges 0-9, a-z and A-Z. \"\"\" return shorten_citekey ( self . standard_id ) @cached_property def all_ids ( self ) : ids = [ self . input_id , self . dealiased_id , self . standard_id , self . short_id ] ids = [ x for x in ids if x ] # remove None ids = list ( dict . fromkeys ( ids )) # deduplicate return ids def __ hash__ ( self ) : return hash (( self . input_id , self . dealiased_id )) def __ repr__ ( self ) : return \" --> \" . join ( f \"{getattr(self, key)} ({key})\" for key in ( \"input_id\" , \"dealiased_id\" , \"prefix_lower\" , \"accession\" , \"standard_id\" , \"short_id\" , ) ) @cached_property def csl_item ( self ) : from . csl_item import CSL_Item csl_item = self . handler . get_csl_item ( self ) if not isinstance ( csl_item , CSL_Item ) : csl_item = CSL_Item ( csl_item ) csl_item . set_id ( self . standard_id ) return csl_item def is_pandoc_xnos_prefix ( self , log_case_warning = False ) : from . handlers import _ pandoc_xnos_prefixes if self . prefix in _ pandoc_xnos_prefixes: return True if log_case_warning and self . prefix_lower in _ pandoc_xnos_prefixes: logging . warning ( \"pandoc-xnos prefixes should be all lowercase.\\n\" f'Should {self.input_id!r} use {self.prefix_lower!r} rather than \"{self.prefix!r}\"?' ) return False def shorten_citekey ( standard_citekey: str ) -> str : \"\"\" Return a shortened citekey derived from the input citekey. The input citekey should be standardized prior to this function, since differences in the input citekey will result in different shortened citekeys. Short citekeys are generated by converting the input citekey to a 6 byte hash, and then converting this digest to a base62 ASCII str. Shortened citekeys consist of characters in the following ranges: 0-9, a-z and A-Z. \"\"\" import hashlib import base62 assert not standard_citekey . startswith ( \"@\" ) as_bytes = standard_citekey . encode () blake_hash = hashlib . blake2b ( as_bytes , digest_size = 6 ) digest = blake_hash . digest () short_citekey = base62 . encodebytes ( digest ) return short_citekey def citekey_to_csl_item ( citekey , prune = True , manual_refs= {}, log_level: tp . Union [ str , int ] = \"WARNING\" ) : \"\"\" Generate a CSL_Item for the input citekey. \"\"\" from manubot import __ version__ as manubot_version # https : // stackoverflow . com / a / 35704430 / 4651668 log_level = logging . _ checkLevel ( log_level ) if not isinstance ( citekey , CiteKey ) : citekey = CiteKey ( citekey ) if citekey . standard_id in manual_refs: return manual_refs [ citekey . standard_id ] try : csl_item = citekey . csl_item except Exception as error : logging . log ( log_level , f \"Generating csl_item for {citekey.standard_id!r} failed \" f \"due to a {error.__class__.__name__}:\\n{error}\" , ) logging . info ( error , exc_info = True ) return None # update csl_item with manubot generated metadata note_text = f \"This CSL JSON Item was automatically generated by Manubot v{manubot_version} using citation-by-identifier.\" note_dict = { \"standard_id\" : citekey . standard_id } csl_item . note_append_text ( note_text ) csl_item . note_append_dict ( note_dict ) csl_item . set_id ( citekey . short_id ) csl_item . clean ( prune = prune ) return csl_item def url_to_citekey ( url ) : \"\"\" Convert a HTTP(s) URL into a citekey. For supported sources, convert from url citekey to an alternative source like doi. If citekeys fail inspection, revert alternative sources to URLs. \"\"\" from urllib . parse import urlparse , unquote citekey = None parsed_url = urlparse ( url ) domain_levels = parsed_url . hostname . split ( \".\" ) if domain_levels [ - 2 : ] == [ \"doi\" , \"org\" ] : # DOI URLs doi = unquote ( parsed_url . path . lstrip ( \"/\" )) citekey = f \"doi:{doi}\" if domain_levels [ - 2 ] == \"sci-hub\" : # Sci - Hub domains doi = parsed_url . path . lstrip ( \"/\" ) citekey = f \"doi:{doi}\" if domain_levels [ - 2 : ] == [ \"biorxiv\" , \"org\" ] : # bioRxiv URL to DOI . See https : // git . io / Je9Hq match = re . search ( r \" / ( ? P < biorxiv_id> ([ 0 - 9 ]{ 4 } \\. [ 0 - 9 ]{ 2 } \\. [ 0 - 9 ]{ 2 } \\. ) ? [ 0 - 9 ]{ 6 ,}) \", parsed_url.path, ) if match: citekey = f\" doi : 10.1101 / { match . group ( 'biorxiv_id' )} \" is_ncbi_url = parsed_url.hostname.endswith(\" ncbi . nlm . nih . gov \") if is_ncbi_url and parsed_url.path.startswith(\" / pubmed/ \"): # PubMed URLs try: pmid = parsed_url.path.split(\" / \")[2] citekey = f\" pmid :{ pmid } \" except IndexError: pass if is_ncbi_url and parsed_url.path.startswith(\" / pmc/ \"): # PubMed Central URLs try: pmcid = parsed_url.path.split(\" / \")[3] citekey = f\" pmcid :{ pmcid } \" except IndexError: pass if domain_levels[-2:] == [\" wikidata \", \" org \"] and parsed_url.path.startswith( \" / wiki/ \" ): # Wikidata URLs try: wikidata_id = parsed_url.path.split(\" / \")[2] citekey = f\" wikidata :{ wikidata_id } \" except IndexError: pass if domain_levels[-2:] == [\" arxiv \", \" org \"]: # arXiv identifiers. See https://arxiv.org/help/arxiv_identifier try: arxiv_id = parsed_url.path.split(\" / \", maxsplit=2)[2] if arxiv_id.endswith(\" . pdf \"): arxiv_id = arxiv_id[:-4] citekey = f\" arxiv :{ arxiv_id } \" except IndexError: pass if citekey is None or CiteKey(citekey).inspect() is not None: citekey = f\" url :{ url } \" return citekey Functions citekey_to_csl_item def citekey_to_csl_item ( citekey , prune = True , manual_refs = {}, log_level : Union [ str , int ] = 'WARNING' ) Generate a CSL_Item for the input citekey. View Source def citekey_to_csl_item ( citekey , prune = True , manual_refs = {}, log_level : tp . Union [ str , int ] = \"WARNING\" ): \"\"\" Generate a CSL_Item for the input citekey. \"\"\" from manubot import __version__ as manubot_version # https://stackoverflow.com/a/35704430/4651668 log_level = logging . _checkLevel ( log_level ) if not isinstance ( citekey , CiteKey ): citekey = CiteKey ( citekey ) if citekey . standard_id in manual_refs : return manual_refs [ citekey . standard_id ] try : csl_item = citekey . csl_item except Exception as error : logging . log ( log_level , f \"Generating csl_item for {citekey.standard_id!r} failed \" f \"due to a {error.__class__.__name__}: \\n {error}\" , ) logging . info ( error , exc_info = True ) return None # update csl_item with manubot generated metadata note_text = f \"This CSL JSON Item was automatically generated by Manubot v{manubot_version} using citation-by-identifier.\" note_dict = { \"standard_id\" : citekey . standard_id } csl_item . note_append_text ( note_text ) csl_item . note_append_dict ( note_dict ) csl_item . set_id ( citekey . short_id ) csl_item . clean ( prune = prune ) return csl_item shorten_citekey def shorten_citekey ( standard_citekey : str ) -> str Return a shortened citekey derived from the input citekey. The input citekey should be standardized prior to this function, since differences in the input citekey will result in different shortened citekeys. Short citekeys are generated by converting the input citekey to a 6 byte hash, and then converting this digest to a base62 ASCII str. Shortened citekeys consist of characters in the following ranges: 0-9, a-z and A-Z. View Source def shorten_citekey ( standard_citekey : str ) -> str : \"\"\" Return a shortened citekey derived from the input citekey. The input citekey should be standardized prior to this function, since differences in the input citekey will result in different shortened citekeys. Short citekeys are generated by converting the input citekey to a 6 byte hash, and then converting this digest to a base62 ASCII str. Shortened citekeys consist of characters in the following ranges: 0-9, a-z and A-Z. \"\"\" import hashlib import base62 assert not standard_citekey . startswith ( \"@\" ) as_bytes = standard_citekey . encode () blake_hash = hashlib . blake2b ( as_bytes , digest_size = 6 ) digest = blake_hash . digest () short_citekey = base62 . encodebytes ( digest ) return short_citekey url_to_citekey def url_to_citekey ( url ) Convert a HTTP(s) URL into a citekey. For supported sources, convert from url citekey to an alternative source like doi. If citekeys fail inspection, revert alternative sources to URLs. View Source def url_to_citekey ( url ) : \"\"\" Convert a HTTP(s) URL into a citekey. For supported sources, convert from url citekey to an alternative source like doi. If citekeys fail inspection, revert alternative sources to URLs. \"\"\" from urllib . parse import urlparse , unquote citekey = None parsed_url = urlparse ( url ) domain_levels = parsed_url . hostname . split ( \".\" ) if domain_levels [ - 2 : ] == [ \"doi\" , \"org\" ] : # DOI URLs doi = unquote ( parsed_url . path . lstrip ( \"/\" )) citekey = f \"doi:{doi}\" if domain_levels [ - 2 ] == \"sci-hub\" : # Sci - Hub domains doi = parsed_url . path . lstrip ( \"/\" ) citekey = f \"doi:{doi}\" if domain_levels [ - 2 : ] == [ \"biorxiv\" , \"org\" ] : # bioRxiv URL to DOI . See https : // git . io / Je9Hq match = re . search ( r \" / ( ? P < biorxiv_id> ([ 0 - 9 ]{ 4 } \\. [ 0 - 9 ]{ 2 } \\. [ 0 - 9 ]{ 2 } \\. ) ? [ 0 - 9 ]{ 6 ,}) \", parsed_url.path, ) if match: citekey = f\" doi : 10.1101 / { match . group ( 'biorxiv_id' )} \" is_ncbi_url = parsed_url.hostname.endswith(\" ncbi . nlm . nih . gov \") if is_ncbi_url and parsed_url.path.startswith(\" / pubmed/ \"): # PubMed URLs try: pmid = parsed_url.path.split(\" / \")[2] citekey = f\" pmid :{ pmid } \" except IndexError: pass if is_ncbi_url and parsed_url.path.startswith(\" / pmc/ \"): # PubMed Central URLs try: pmcid = parsed_url.path.split(\" / \")[3] citekey = f\" pmcid :{ pmcid } \" except IndexError: pass if domain_levels[-2:] == [\" wikidata \", \" org \"] and parsed_url.path.startswith( \" / wiki/ \" ): # Wikidata URLs try: wikidata_id = parsed_url.path.split(\" / \")[2] citekey = f\" wikidata :{ wikidata_id } \" except IndexError: pass if domain_levels[-2:] == [\" arxiv \", \" org \"]: # arXiv identifiers. See https://arxiv.org/help/arxiv_identifier try: arxiv_id = parsed_url.path.split(\" / \", maxsplit=2)[2] if arxiv_id.endswith(\" . pdf \"): arxiv_id = arxiv_id[:-4] citekey = f\" arxiv :{ arxiv_id } \" except IndexError: pass if citekey is None or CiteKey(citekey).inspect() is not None: citekey = f\" url :{ url } \" return citekey Classes CiteKey class CiteKey ( input_id : str , aliases : dict = < factory > ) CiteKey(input_id: str, aliases: dict = ) View Source class CiteKey : input_id : str aliases : dict = dataclasses . field ( default_factory = dict ) def __post_init__ ( self ) : self . check_input_id ( self . input_id ) @ staticmethod def check_input_id ( input_id ) : if not isinstance ( input_id , str ) : raise TypeError ( \"input_id should be type 'str' not \" f \"{type(input_id).__name__!r}: {input_id!r}\" ) if input_id . startswith ( \"@\" ) : f \"invalid citekey input_id: {input_id!r} \\n starts with '@'\" @ classmethod @ functools . lru_cache ( maxsize = None ) def from_input_id ( cls , * args , ** kwargs ) : \"\"\"Cached constructor\"\"\" return cls ( * args , ** kwargs ) @ cached_property def dealiased_id ( self ) -> str : \"\"\" If ` self . input_id ` is in ` self . aliases ` , the value specified by ` self . aliases ` . Otherwise , ` self . input_id ` . \"\"\" return self . aliases . get ( self . input_id , self . input_id ) def _set_prefix_accession ( self ) : try : prefix , accession = self . dealiased_id . split ( \":\" , 1 ) except ValueError : prefix , accession = None , None self . _prefix = prefix self . _accession = accession @property def prefix ( self ) -> tp . Optional [ str ] : \"\"\" If ` self . input_id ` contains a colon , the substring up to the first colon . Otherwise , None . \"\"\" if not hasattr ( self , \"_prefix\" ) : self . _set_prefix_accession () return self . _prefix @property def prefix_lower ( self ) -> tp . Optional [ str ] : \"\"\" A lowercase version of ` self . prefix ` or None . \"\"\" if self . prefix is None : return None return self . prefix . lower () @property def accession ( self ) -> tp . Optional [ str ] : \"\"\" If ` self . prefix ` , the remainder of ` self . input_id ` following the first colon . \"\"\" if not hasattr ( self , \"_accession\" ) : self . _set_prefix_accession () return self . _accession @property def standard_prefix ( self ) -> tp . Optional [ str ] : \"\"\" If the citekey is handled , the standard prefix specified by the handler . Otherwise , None . \"\"\" if not hasattr ( self , \"_standard_prefix\" ) : self . _standardize () return self . _standard_prefix @property def standard_accession ( self ) -> tp . Optional [ str ] : \"\"\" If the citekey is handled , the standard accession specified by the handler . Otherwise , None . \"\"\" if not hasattr ( self , \"_standard_accession\" ) : self . _standardize () return self . _standard_accession @ cached_property def handler ( self ) : from . handlers import Handler , get_handler if self . is_handled_prefix : return get_handler ( self . prefix_lower ) return Handler ( self . prefix_lower ) @ cached_property def is_handled_prefix ( self ) -> bool : from . handlers import prefix_to_handler return self . prefix_lower in prefix_to_handler def inspect ( self ) -> tp . Optional [ str ] : \"\"\" Inspect citekey for potential problems . If no problems are found , return None . Otherwise , returns a string describing the problem . \"\"\" return self . handler . inspect ( self ) def _standardize ( self ) : \"\"\" Set ` self . _standard_prefix ` , ` self . _standard_accession ` , and ` self . _standard_id ` . For citekeys without a prefix or with an unhandled prefix , _standard_prefix and _standard_accession are set to None . \"\"\" if not self . is_handled_prefix : self . _standard_prefix = None self . _standard_accession = None self . _standard_id = self . dealiased_id return fxn = self . handler . standardize_prefix_accession self . _standard_prefix , self . _standard_accession = fxn ( self . accession ) self . _standard_id = f \" { self . _standard_prefix } : { self . _standard_accession } \" @property def standard_id ( self ) -> str : \"\"\" If the citekey is handled , the standard_id specified by the handler . Otherwise , ` self . dealiased_id ` . \"\"\" if not hasattr ( self , \"_standard_id\" ) : self . _standardize () return self . _standard_id @ cached_property def short_id ( self ) -> str : \"\"\" A hashed version of standard_id whose characters are within the ranges 0 - 9 , a - z and A - Z . \"\"\" return shorten_citekey ( self . standard_id ) @ cached_property def all_ids ( self ) : ids = [ self . input_id , self . dealiased_id , self . standard_id , self . short_id ] ids = [ x for x in ids if x ] # remove None ids = list ( dict . fromkeys ( ids )) # deduplicate return ids def __hash__ ( self ) : return hash (( self . input_id , self . dealiased_id )) def __repr__ ( self ) : return \" --> \" . join ( f \" { getattr ( self , key )} ({ key }) \" for key in ( \"input_id\" , \"dealiased_id\" , \"prefix_lower\" , \"accession\" , \"standard_id\" , \"short_id\" , ) ) @ cached_property def csl_item ( self ) : from . csl_item import CSL_Item csl_item = self . handler . get_csl_item ( self ) if not isinstance ( csl_item , CSL_Item ) : csl_item = CSL_Item ( csl_item ) csl_item . set_id ( self . standard_id ) return csl_item def is_pandoc_xnos_prefix ( self , log_case_warning = False ) : from . handlers import _pandoc_xnos_prefixes if self . prefix in _pandoc_xnos_prefixes : return True if log_case_warning and self . prefix_lower in _pandoc_xnos_prefixes : logging . warning ( \"pandoc-xnos prefixes should be all lowercase. \\n \" f ' Should { self . input_id ! r } use { self . prefix_lower ! r } rather than \"{self.prefix!r}\" ? ' ) return False Static methods check_input_id def check_input_id ( input_id ) View Source @ staticmethod def check_input_id ( input_id ) : if not isinstance ( input_id , str ) : raise TypeError ( \"input_id should be type 'str' not \" f \"{type(input_id).__name__!r}: {input_id!r}\" ) if input_id . startswith ( \"@\" ) : f \"invalid citekey input_id: {input_id!r} \\n starts with '@'\" from_input_id def from_input_id ( * args , ** kwargs ) Cached constructor View Source @classmethod @functools . lru_cache ( maxsize = None ) def from_input_id ( cls , * args , ** kwargs ) : \"\"\"Cached constructor\"\"\" return cls ( * args , ** kwargs ) Instance variables accession If self.prefix , the remainder of self.input_id following the first colon. prefix If self.input_id contains a colon, the substring up to the first colon. Otherwise, None. prefix_lower A lowercase version of self.prefix or None. standard_accession If the citekey is handled, the standard accession specified by the handler. Otherwise, None. standard_id If the citekey is handled, the standard_id specified by the handler. Otherwise, self.dealiased_id . standard_prefix If the citekey is handled, the standard prefix specified by the handler. Otherwise, None. Methods all_ids def all_ids ( ... ) csl_item def csl_item ( ... ) dealiased_id def dealiased_id ( ... ) If self.input_id is in self.aliases , the value specified by self.aliases . Otherwise, self.input_id . handler def handler ( ... ) inspect def inspect ( self ) -> Union [ str , NoneType ] Inspect citekey for potential problems. If no problems are found, return None. Otherwise, returns a string describing the problem. View Source def inspect ( self ) -> tp . Optional [ str ] : \"\"\" Inspect citekey for potential problems. If no problems are found, return None. Otherwise, returns a string describing the problem. \"\"\" return self . handler . inspect ( self ) is_handled_prefix def is_handled_prefix ( ... ) is_pandoc_xnos_prefix def is_pandoc_xnos_prefix ( self , log_case_warning = False ) View Source def is_pandoc_xnos_prefix ( self , log_case_warning = False ): from .handlers import _pandoc_xnos_prefixes if self . prefix in _pandoc_xnos_prefixes : return True if log_case_warning and self . prefix_lower in _pandoc_xnos_prefixes : logging . warning ( \"pandoc-xnos prefixes should be all lowercase. \\n \" f 'Should {self.input_id!r} use {self.prefix_lower!r} rather than \"{self.prefix!r}\"?' ) return False short_id def short_id ( ... ) A hashed version of standard_id whose characters are within the ranges 0-9, a-z and A-Z.","title":"Citekey"},{"location":"reference/manubot/cite/citekey/#module-manubotcitecitekey","text":"Utilities for representing and processing citation keys. View Source \"\"\" Utilities for representing and processing citation keys. \"\"\" import dataclasses import functools import logging import re import typing as tp try : from functools import cached_property except ImportError : from backports . cached_property import cached_property @dataclasses . dataclass class CiteKey : input_id: str aliases : dict = dataclasses . field ( default_factory = dict ) def __ post_init__ ( self ) : self . check_input_id ( self . input_id ) @staticmethod def check_input_id ( input_id ) : if not isinstance ( input_id , str ) : raise TypeError ( \"input_id should be type 'str' not \" f \"{type(input_id).__name__!r}: {input_id!r}\" ) if input_id . startswith ( \"@\" ) : f \"invalid citekey input_id: {input_id!r}\\nstarts with '@'\" @classmethod @functools . lru_cache ( maxsize = None ) def from_input_id ( cls , * args , **kwargs ) : \"\"\"Cached constructor\"\"\" return cls ( * args , **kwargs ) @cached_property def dealiased_id ( self ) -> str : \"\"\" If `self.input_id` is in `self.aliases`, the value specified by `self.aliases`. Otherwise, `self.input_id`. \"\"\" return self . aliases . get ( self . input_id , self . input_id ) def _ set_prefix_accession ( self ) : try : prefix , accession = self . dealiased_id . split ( \":\" , 1 ) except ValueError : prefix , accession = None , None self . _ prefix = prefix self . _ accession = accession @property def prefix ( self ) -> tp . Optional [ str ] : \"\"\" If `self.input_id` contains a colon, the substring up to the first colon. Otherwise, None. \"\"\" if not hasattr ( self , \"_prefix\" ) : self . _ set_prefix_accession () return self . _ prefix @property def prefix_lower ( self ) -> tp . Optional [ str ] : \"\"\" A lowercase version of `self.prefix` or None. \"\"\" if self . prefix is None : return None return self . prefix . lower () @property def accession ( self ) -> tp . Optional [ str ] : \"\"\" If `self.prefix`, the remainder of `self.input_id` following the first colon. \"\"\" if not hasattr ( self , \"_accession\" ) : self . _ set_prefix_accession () return self . _ accession @property def standard_prefix ( self ) -> tp . Optional [ str ] : \"\"\" If the citekey is handled, the standard prefix specified by the handler. Otherwise, None. \"\"\" if not hasattr ( self , \"_standard_prefix\" ) : self . _ standardize () return self . _ standard_prefix @property def standard_accession ( self ) -> tp . Optional [ str ] : \"\"\" If the citekey is handled, the standard accession specified by the handler. Otherwise, None. \"\"\" if not hasattr ( self , \"_standard_accession\" ) : self . _ standardize () return self . _ standard_accession @cached_property def handler ( self ) : from . handlers import Handler , get_handler if self . is_handled_prefix: return get_handler ( self . prefix_lower ) return Handler ( self . prefix_lower ) @cached_property def is_handled_prefix ( self ) -> bool : from . handlers import prefix_to_handler return self . prefix_lower in prefix_to_handler def inspect ( self ) -> tp . Optional [ str ] : \"\"\" Inspect citekey for potential problems. If no problems are found, return None. Otherwise, returns a string describing the problem. \"\"\" return self . handler . inspect ( self ) def _ standardize ( self ) : \"\"\" Set `self._standard_prefix`, `self._standard_accession`, and `self._standard_id`. For citekeys without a prefix or with an unhandled prefix, _standard_prefix and _standard_accession are set to None. \"\"\" if not self . is_handled_prefix: self . _ standard_prefix = None self . _ standard_accession = None self . _ standard_id = self . dealiased_id return fxn = self . handler . standardize_prefix_accession self . _ standard_prefix , self . _ standard_accession = fxn ( self . accession ) self . _ standard_id = f \"{self._standard_prefix}:{self._standard_accession}\" @property def standard_id ( self ) -> str : \"\"\" If the citekey is handled, the standard_id specified by the handler. Otherwise, `self.dealiased_id`. \"\"\" if not hasattr ( self , \"_standard_id\" ) : self . _ standardize () return self . _ standard_id @cached_property def short_id ( self ) -> str : \"\"\" A hashed version of standard_id whose characters are within the ranges 0-9, a-z and A-Z. \"\"\" return shorten_citekey ( self . standard_id ) @cached_property def all_ids ( self ) : ids = [ self . input_id , self . dealiased_id , self . standard_id , self . short_id ] ids = [ x for x in ids if x ] # remove None ids = list ( dict . fromkeys ( ids )) # deduplicate return ids def __ hash__ ( self ) : return hash (( self . input_id , self . dealiased_id )) def __ repr__ ( self ) : return \" --> \" . join ( f \"{getattr(self, key)} ({key})\" for key in ( \"input_id\" , \"dealiased_id\" , \"prefix_lower\" , \"accession\" , \"standard_id\" , \"short_id\" , ) ) @cached_property def csl_item ( self ) : from . csl_item import CSL_Item csl_item = self . handler . get_csl_item ( self ) if not isinstance ( csl_item , CSL_Item ) : csl_item = CSL_Item ( csl_item ) csl_item . set_id ( self . standard_id ) return csl_item def is_pandoc_xnos_prefix ( self , log_case_warning = False ) : from . handlers import _ pandoc_xnos_prefixes if self . prefix in _ pandoc_xnos_prefixes: return True if log_case_warning and self . prefix_lower in _ pandoc_xnos_prefixes: logging . warning ( \"pandoc-xnos prefixes should be all lowercase.\\n\" f'Should {self.input_id!r} use {self.prefix_lower!r} rather than \"{self.prefix!r}\"?' ) return False def shorten_citekey ( standard_citekey: str ) -> str : \"\"\" Return a shortened citekey derived from the input citekey. The input citekey should be standardized prior to this function, since differences in the input citekey will result in different shortened citekeys. Short citekeys are generated by converting the input citekey to a 6 byte hash, and then converting this digest to a base62 ASCII str. Shortened citekeys consist of characters in the following ranges: 0-9, a-z and A-Z. \"\"\" import hashlib import base62 assert not standard_citekey . startswith ( \"@\" ) as_bytes = standard_citekey . encode () blake_hash = hashlib . blake2b ( as_bytes , digest_size = 6 ) digest = blake_hash . digest () short_citekey = base62 . encodebytes ( digest ) return short_citekey def citekey_to_csl_item ( citekey , prune = True , manual_refs= {}, log_level: tp . Union [ str , int ] = \"WARNING\" ) : \"\"\" Generate a CSL_Item for the input citekey. \"\"\" from manubot import __ version__ as manubot_version # https : // stackoverflow . com / a / 35704430 / 4651668 log_level = logging . _ checkLevel ( log_level ) if not isinstance ( citekey , CiteKey ) : citekey = CiteKey ( citekey ) if citekey . standard_id in manual_refs: return manual_refs [ citekey . standard_id ] try : csl_item = citekey . csl_item except Exception as error : logging . log ( log_level , f \"Generating csl_item for {citekey.standard_id!r} failed \" f \"due to a {error.__class__.__name__}:\\n{error}\" , ) logging . info ( error , exc_info = True ) return None # update csl_item with manubot generated metadata note_text = f \"This CSL JSON Item was automatically generated by Manubot v{manubot_version} using citation-by-identifier.\" note_dict = { \"standard_id\" : citekey . standard_id } csl_item . note_append_text ( note_text ) csl_item . note_append_dict ( note_dict ) csl_item . set_id ( citekey . short_id ) csl_item . clean ( prune = prune ) return csl_item def url_to_citekey ( url ) : \"\"\" Convert a HTTP(s) URL into a citekey. For supported sources, convert from url citekey to an alternative source like doi. If citekeys fail inspection, revert alternative sources to URLs. \"\"\" from urllib . parse import urlparse , unquote citekey = None parsed_url = urlparse ( url ) domain_levels = parsed_url . hostname . split ( \".\" ) if domain_levels [ - 2 : ] == [ \"doi\" , \"org\" ] : # DOI URLs doi = unquote ( parsed_url . path . lstrip ( \"/\" )) citekey = f \"doi:{doi}\" if domain_levels [ - 2 ] == \"sci-hub\" : # Sci - Hub domains doi = parsed_url . path . lstrip ( \"/\" ) citekey = f \"doi:{doi}\" if domain_levels [ - 2 : ] == [ \"biorxiv\" , \"org\" ] : # bioRxiv URL to DOI . See https : // git . io / Je9Hq match = re . search ( r \" / ( ? P < biorxiv_id> ([ 0 - 9 ]{ 4 } \\. [ 0 - 9 ]{ 2 } \\. [ 0 - 9 ]{ 2 } \\. ) ? [ 0 - 9 ]{ 6 ,}) \", parsed_url.path, ) if match: citekey = f\" doi : 10.1101 / { match . group ( 'biorxiv_id' )} \" is_ncbi_url = parsed_url.hostname.endswith(\" ncbi . nlm . nih . gov \") if is_ncbi_url and parsed_url.path.startswith(\" / pubmed/ \"): # PubMed URLs try: pmid = parsed_url.path.split(\" / \")[2] citekey = f\" pmid :{ pmid } \" except IndexError: pass if is_ncbi_url and parsed_url.path.startswith(\" / pmc/ \"): # PubMed Central URLs try: pmcid = parsed_url.path.split(\" / \")[3] citekey = f\" pmcid :{ pmcid } \" except IndexError: pass if domain_levels[-2:] == [\" wikidata \", \" org \"] and parsed_url.path.startswith( \" / wiki/ \" ): # Wikidata URLs try: wikidata_id = parsed_url.path.split(\" / \")[2] citekey = f\" wikidata :{ wikidata_id } \" except IndexError: pass if domain_levels[-2:] == [\" arxiv \", \" org \"]: # arXiv identifiers. See https://arxiv.org/help/arxiv_identifier try: arxiv_id = parsed_url.path.split(\" / \", maxsplit=2)[2] if arxiv_id.endswith(\" . pdf \"): arxiv_id = arxiv_id[:-4] citekey = f\" arxiv :{ arxiv_id } \" except IndexError: pass if citekey is None or CiteKey(citekey).inspect() is not None: citekey = f\" url :{ url } \" return citekey","title":"Module manubot.cite.citekey"},{"location":"reference/manubot/cite/citekey/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/citekey/#citekey_to_csl_item","text":"def citekey_to_csl_item ( citekey , prune = True , manual_refs = {}, log_level : Union [ str , int ] = 'WARNING' ) Generate a CSL_Item for the input citekey. View Source def citekey_to_csl_item ( citekey , prune = True , manual_refs = {}, log_level : tp . Union [ str , int ] = \"WARNING\" ): \"\"\" Generate a CSL_Item for the input citekey. \"\"\" from manubot import __version__ as manubot_version # https://stackoverflow.com/a/35704430/4651668 log_level = logging . _checkLevel ( log_level ) if not isinstance ( citekey , CiteKey ): citekey = CiteKey ( citekey ) if citekey . standard_id in manual_refs : return manual_refs [ citekey . standard_id ] try : csl_item = citekey . csl_item except Exception as error : logging . log ( log_level , f \"Generating csl_item for {citekey.standard_id!r} failed \" f \"due to a {error.__class__.__name__}: \\n {error}\" , ) logging . info ( error , exc_info = True ) return None # update csl_item with manubot generated metadata note_text = f \"This CSL JSON Item was automatically generated by Manubot v{manubot_version} using citation-by-identifier.\" note_dict = { \"standard_id\" : citekey . standard_id } csl_item . note_append_text ( note_text ) csl_item . note_append_dict ( note_dict ) csl_item . set_id ( citekey . short_id ) csl_item . clean ( prune = prune ) return csl_item","title":"citekey_to_csl_item"},{"location":"reference/manubot/cite/citekey/#shorten_citekey","text":"def shorten_citekey ( standard_citekey : str ) -> str Return a shortened citekey derived from the input citekey. The input citekey should be standardized prior to this function, since differences in the input citekey will result in different shortened citekeys. Short citekeys are generated by converting the input citekey to a 6 byte hash, and then converting this digest to a base62 ASCII str. Shortened citekeys consist of characters in the following ranges: 0-9, a-z and A-Z. View Source def shorten_citekey ( standard_citekey : str ) -> str : \"\"\" Return a shortened citekey derived from the input citekey. The input citekey should be standardized prior to this function, since differences in the input citekey will result in different shortened citekeys. Short citekeys are generated by converting the input citekey to a 6 byte hash, and then converting this digest to a base62 ASCII str. Shortened citekeys consist of characters in the following ranges: 0-9, a-z and A-Z. \"\"\" import hashlib import base62 assert not standard_citekey . startswith ( \"@\" ) as_bytes = standard_citekey . encode () blake_hash = hashlib . blake2b ( as_bytes , digest_size = 6 ) digest = blake_hash . digest () short_citekey = base62 . encodebytes ( digest ) return short_citekey","title":"shorten_citekey"},{"location":"reference/manubot/cite/citekey/#url_to_citekey","text":"def url_to_citekey ( url ) Convert a HTTP(s) URL into a citekey. For supported sources, convert from url citekey to an alternative source like doi. If citekeys fail inspection, revert alternative sources to URLs. View Source def url_to_citekey ( url ) : \"\"\" Convert a HTTP(s) URL into a citekey. For supported sources, convert from url citekey to an alternative source like doi. If citekeys fail inspection, revert alternative sources to URLs. \"\"\" from urllib . parse import urlparse , unquote citekey = None parsed_url = urlparse ( url ) domain_levels = parsed_url . hostname . split ( \".\" ) if domain_levels [ - 2 : ] == [ \"doi\" , \"org\" ] : # DOI URLs doi = unquote ( parsed_url . path . lstrip ( \"/\" )) citekey = f \"doi:{doi}\" if domain_levels [ - 2 ] == \"sci-hub\" : # Sci - Hub domains doi = parsed_url . path . lstrip ( \"/\" ) citekey = f \"doi:{doi}\" if domain_levels [ - 2 : ] == [ \"biorxiv\" , \"org\" ] : # bioRxiv URL to DOI . See https : // git . io / Je9Hq match = re . search ( r \" / ( ? P < biorxiv_id> ([ 0 - 9 ]{ 4 } \\. [ 0 - 9 ]{ 2 } \\. [ 0 - 9 ]{ 2 } \\. ) ? [ 0 - 9 ]{ 6 ,}) \", parsed_url.path, ) if match: citekey = f\" doi : 10.1101 / { match . group ( 'biorxiv_id' )} \" is_ncbi_url = parsed_url.hostname.endswith(\" ncbi . nlm . nih . gov \") if is_ncbi_url and parsed_url.path.startswith(\" / pubmed/ \"): # PubMed URLs try: pmid = parsed_url.path.split(\" / \")[2] citekey = f\" pmid :{ pmid } \" except IndexError: pass if is_ncbi_url and parsed_url.path.startswith(\" / pmc/ \"): # PubMed Central URLs try: pmcid = parsed_url.path.split(\" / \")[3] citekey = f\" pmcid :{ pmcid } \" except IndexError: pass if domain_levels[-2:] == [\" wikidata \", \" org \"] and parsed_url.path.startswith( \" / wiki/ \" ): # Wikidata URLs try: wikidata_id = parsed_url.path.split(\" / \")[2] citekey = f\" wikidata :{ wikidata_id } \" except IndexError: pass if domain_levels[-2:] == [\" arxiv \", \" org \"]: # arXiv identifiers. See https://arxiv.org/help/arxiv_identifier try: arxiv_id = parsed_url.path.split(\" / \", maxsplit=2)[2] if arxiv_id.endswith(\" . pdf \"): arxiv_id = arxiv_id[:-4] citekey = f\" arxiv :{ arxiv_id } \" except IndexError: pass if citekey is None or CiteKey(citekey).inspect() is not None: citekey = f\" url :{ url } \" return citekey","title":"url_to_citekey"},{"location":"reference/manubot/cite/citekey/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/citekey/#citekey","text":"class CiteKey ( input_id : str , aliases : dict = < factory > ) CiteKey(input_id: str, aliases: dict = ) View Source class CiteKey : input_id : str aliases : dict = dataclasses . field ( default_factory = dict ) def __post_init__ ( self ) : self . check_input_id ( self . input_id ) @ staticmethod def check_input_id ( input_id ) : if not isinstance ( input_id , str ) : raise TypeError ( \"input_id should be type 'str' not \" f \"{type(input_id).__name__!r}: {input_id!r}\" ) if input_id . startswith ( \"@\" ) : f \"invalid citekey input_id: {input_id!r} \\n starts with '@'\" @ classmethod @ functools . lru_cache ( maxsize = None ) def from_input_id ( cls , * args , ** kwargs ) : \"\"\"Cached constructor\"\"\" return cls ( * args , ** kwargs ) @ cached_property def dealiased_id ( self ) -> str : \"\"\" If ` self . input_id ` is in ` self . aliases ` , the value specified by ` self . aliases ` . Otherwise , ` self . input_id ` . \"\"\" return self . aliases . get ( self . input_id , self . input_id ) def _set_prefix_accession ( self ) : try : prefix , accession = self . dealiased_id . split ( \":\" , 1 ) except ValueError : prefix , accession = None , None self . _prefix = prefix self . _accession = accession @property def prefix ( self ) -> tp . Optional [ str ] : \"\"\" If ` self . input_id ` contains a colon , the substring up to the first colon . Otherwise , None . \"\"\" if not hasattr ( self , \"_prefix\" ) : self . _set_prefix_accession () return self . _prefix @property def prefix_lower ( self ) -> tp . Optional [ str ] : \"\"\" A lowercase version of ` self . prefix ` or None . \"\"\" if self . prefix is None : return None return self . prefix . lower () @property def accession ( self ) -> tp . Optional [ str ] : \"\"\" If ` self . prefix ` , the remainder of ` self . input_id ` following the first colon . \"\"\" if not hasattr ( self , \"_accession\" ) : self . _set_prefix_accession () return self . _accession @property def standard_prefix ( self ) -> tp . Optional [ str ] : \"\"\" If the citekey is handled , the standard prefix specified by the handler . Otherwise , None . \"\"\" if not hasattr ( self , \"_standard_prefix\" ) : self . _standardize () return self . _standard_prefix @property def standard_accession ( self ) -> tp . Optional [ str ] : \"\"\" If the citekey is handled , the standard accession specified by the handler . Otherwise , None . \"\"\" if not hasattr ( self , \"_standard_accession\" ) : self . _standardize () return self . _standard_accession @ cached_property def handler ( self ) : from . handlers import Handler , get_handler if self . is_handled_prefix : return get_handler ( self . prefix_lower ) return Handler ( self . prefix_lower ) @ cached_property def is_handled_prefix ( self ) -> bool : from . handlers import prefix_to_handler return self . prefix_lower in prefix_to_handler def inspect ( self ) -> tp . Optional [ str ] : \"\"\" Inspect citekey for potential problems . If no problems are found , return None . Otherwise , returns a string describing the problem . \"\"\" return self . handler . inspect ( self ) def _standardize ( self ) : \"\"\" Set ` self . _standard_prefix ` , ` self . _standard_accession ` , and ` self . _standard_id ` . For citekeys without a prefix or with an unhandled prefix , _standard_prefix and _standard_accession are set to None . \"\"\" if not self . is_handled_prefix : self . _standard_prefix = None self . _standard_accession = None self . _standard_id = self . dealiased_id return fxn = self . handler . standardize_prefix_accession self . _standard_prefix , self . _standard_accession = fxn ( self . accession ) self . _standard_id = f \" { self . _standard_prefix } : { self . _standard_accession } \" @property def standard_id ( self ) -> str : \"\"\" If the citekey is handled , the standard_id specified by the handler . Otherwise , ` self . dealiased_id ` . \"\"\" if not hasattr ( self , \"_standard_id\" ) : self . _standardize () return self . _standard_id @ cached_property def short_id ( self ) -> str : \"\"\" A hashed version of standard_id whose characters are within the ranges 0 - 9 , a - z and A - Z . \"\"\" return shorten_citekey ( self . standard_id ) @ cached_property def all_ids ( self ) : ids = [ self . input_id , self . dealiased_id , self . standard_id , self . short_id ] ids = [ x for x in ids if x ] # remove None ids = list ( dict . fromkeys ( ids )) # deduplicate return ids def __hash__ ( self ) : return hash (( self . input_id , self . dealiased_id )) def __repr__ ( self ) : return \" --> \" . join ( f \" { getattr ( self , key )} ({ key }) \" for key in ( \"input_id\" , \"dealiased_id\" , \"prefix_lower\" , \"accession\" , \"standard_id\" , \"short_id\" , ) ) @ cached_property def csl_item ( self ) : from . csl_item import CSL_Item csl_item = self . handler . get_csl_item ( self ) if not isinstance ( csl_item , CSL_Item ) : csl_item = CSL_Item ( csl_item ) csl_item . set_id ( self . standard_id ) return csl_item def is_pandoc_xnos_prefix ( self , log_case_warning = False ) : from . handlers import _pandoc_xnos_prefixes if self . prefix in _pandoc_xnos_prefixes : return True if log_case_warning and self . prefix_lower in _pandoc_xnos_prefixes : logging . warning ( \"pandoc-xnos prefixes should be all lowercase. \\n \" f ' Should { self . input_id ! r } use { self . prefix_lower ! r } rather than \"{self.prefix!r}\" ? ' ) return False","title":"CiteKey"},{"location":"reference/manubot/cite/citekey/#static-methods","text":"","title":"Static methods"},{"location":"reference/manubot/cite/citekey/#check_input_id","text":"def check_input_id ( input_id ) View Source @ staticmethod def check_input_id ( input_id ) : if not isinstance ( input_id , str ) : raise TypeError ( \"input_id should be type 'str' not \" f \"{type(input_id).__name__!r}: {input_id!r}\" ) if input_id . startswith ( \"@\" ) : f \"invalid citekey input_id: {input_id!r} \\n starts with '@'\"","title":"check_input_id"},{"location":"reference/manubot/cite/citekey/#from_input_id","text":"def from_input_id ( * args , ** kwargs ) Cached constructor View Source @classmethod @functools . lru_cache ( maxsize = None ) def from_input_id ( cls , * args , ** kwargs ) : \"\"\"Cached constructor\"\"\" return cls ( * args , ** kwargs )","title":"from_input_id"},{"location":"reference/manubot/cite/citekey/#instance-variables","text":"accession If self.prefix , the remainder of self.input_id following the first colon. prefix If self.input_id contains a colon, the substring up to the first colon. Otherwise, None. prefix_lower A lowercase version of self.prefix or None. standard_accession If the citekey is handled, the standard accession specified by the handler. Otherwise, None. standard_id If the citekey is handled, the standard_id specified by the handler. Otherwise, self.dealiased_id . standard_prefix If the citekey is handled, the standard prefix specified by the handler. Otherwise, None.","title":"Instance variables"},{"location":"reference/manubot/cite/citekey/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/citekey/#all_ids","text":"def all_ids ( ... )","title":"all_ids"},{"location":"reference/manubot/cite/citekey/#csl_item","text":"def csl_item ( ... )","title":"csl_item"},{"location":"reference/manubot/cite/citekey/#dealiased_id","text":"def dealiased_id ( ... ) If self.input_id is in self.aliases , the value specified by self.aliases . Otherwise, self.input_id .","title":"dealiased_id"},{"location":"reference/manubot/cite/citekey/#handler","text":"def handler ( ... )","title":"handler"},{"location":"reference/manubot/cite/citekey/#inspect","text":"def inspect ( self ) -> Union [ str , NoneType ] Inspect citekey for potential problems. If no problems are found, return None. Otherwise, returns a string describing the problem. View Source def inspect ( self ) -> tp . Optional [ str ] : \"\"\" Inspect citekey for potential problems. If no problems are found, return None. Otherwise, returns a string describing the problem. \"\"\" return self . handler . inspect ( self )","title":"inspect"},{"location":"reference/manubot/cite/citekey/#is_handled_prefix","text":"def is_handled_prefix ( ... )","title":"is_handled_prefix"},{"location":"reference/manubot/cite/citekey/#is_pandoc_xnos_prefix","text":"def is_pandoc_xnos_prefix ( self , log_case_warning = False ) View Source def is_pandoc_xnos_prefix ( self , log_case_warning = False ): from .handlers import _pandoc_xnos_prefixes if self . prefix in _pandoc_xnos_prefixes : return True if log_case_warning and self . prefix_lower in _pandoc_xnos_prefixes : logging . warning ( \"pandoc-xnos prefixes should be all lowercase. \\n \" f 'Should {self.input_id!r} use {self.prefix_lower!r} rather than \"{self.prefix!r}\"?' ) return False","title":"is_pandoc_xnos_prefix"},{"location":"reference/manubot/cite/citekey/#short_id","text":"def short_id ( ... ) A hashed version of standard_id whose characters are within the ranges 0-9, a-z and A-Z.","title":"short_id"},{"location":"reference/manubot/cite/citeproc/","text":"Module manubot.cite.citeproc Correct or validate CSL item schema. Module naming: citeproc is the generic name for programs that produce formatted bibliographies and citations based on the metadata of the cited objects and the formatting instructions provided by Citation Style Language (CSL) styles. -- https://en.wikipedia.org/wiki/CiteProc View Source \"\"\"Correct or validate CSL item schema. Module naming: citeproc is the generic name for programs that produce formatted bibliographies and citations based on the metadata of the cited objects and the formatting instructions provided by Citation Style Language (CSL) styles. -- https://en.wikipedia.org/wiki/CiteProc \"\"\" import copy import functools import logging @functools.lru_cache () def get_jsonschema_csl_validator (): \"\"\" Return a jsonschema validator for the CSL Item JSON Schema \"\"\" import jsonschema import requests url = \"https://github.com/dhimmel/csl-schema/raw/manubot/csl-data.json\" schema = requests . get ( url ) . json () Validator = jsonschema . validators . validator_for ( schema ) Validator . check_schema ( schema ) return Validator ( schema ) def remove_jsonschema_errors ( instance , recurse_depth = 5 , in_place = False ): \"\"\" Remove fields in CSL Items that produce JSON Schema errors. Should errors be removed, but the JSON instance still fails to validate, recursively call remove_jsonschema_errors until the instance validates or the recursion depth limit is reached. Note that this method may not be work for all types of JSON Schema errors and users looking to adapt it for other applications should write task-specific tests to provide empirical evaluate that it works as intended. The default in_place=False creates a deepcopy of instance before pruning it, such that a new dictionary is returned and instance is not edited. Set in_place=True to edit instance in-place. The inital implementation of remove_jsonschema_errors always deepcopied instance, and it is possible deepcopying is important to prevent malfunction when encountering certain edge cases. Please report if you observe any in_place dependent behaviors. See also: https://github.com/Julian/jsonschema/issues/448 https://stackoverflow.com/questions/44694835 \"\"\" validator = get_jsonschema_csl_validator () errors = list ( validator . iter_errors ( instance )) if not in_place : instance = copy . deepcopy ( instance ) errors = sorted ( errors , key = lambda e : e . path , reverse = True ) for error in errors : _remove_error ( instance , error ) if validator . is_valid ( instance ) or recurse_depth < 1 : return instance return remove_jsonschema_errors ( instance , recurse_depth - 1 , in_place = in_place ) def _delete_elem ( instance , path , absolute_path = None , message = \"\" ): \"\"\" Helper function for remove_jsonschema_errors that deletes an element in the JSON-like input instance at the specified path. absolute_path is relative to the original validated instance for logging purposes. Defaults to path, if not specified. message is an optional string with additional error information to log. \"\"\" if absolute_path is None : absolute_path = path logging . debug ( ( f \"{message} \\n \" if message else message ) + \"_delete_elem deleting CSL element at: \" + \"/\" . join ( map ( str , absolute_path )) ) * head , tail = path try : del _deep_get ( instance , head )[ tail ] except KeyError : pass def _deep_get ( instance , path ): \"\"\" Descend path to return a deep element in the JSON object instance. \"\"\" for key in path : instance = instance [ key ] return instance def _remove_error ( instance , error ): \"\"\" Remove a jsonschema ValidationError from the JSON-like instance. See ValidationError documentation at http://python-jsonschema.readthedocs.io/en/latest/errors/#jsonschema.exceptions.ValidationError \"\"\" sub_errors = error . context if sub_errors : # already_removed_additional was neccessary to workaround # https://github.com/citation-style-language/schema/issues/154 already_removed_additional = False for sub_error in sub_errors : if sub_error . validator == \"additionalProperties\" : if already_removed_additional : continue already_removed_additional = True sub_instance = _deep_get ( instance , error . path ) _remove_error ( sub_instance , sub_error ) elif error . validator == \"additionalProperties\" : extras = set ( error . instance ) - set ( error . schema [ \"properties\" ]) logging . debug ( error . message + f \" \\n Will now remove these {len(extras)} additional properties.\" ) for key in extras : _delete_elem ( instance = instance , path = list ( error . path ) + [ key ], absolute_path = list ( error . absolute_path ) + [ key ], ) elif error . validator in { \"enum\" , \"type\" , \"minItems\" , \"maxItems\" }: _delete_elem ( instance , error . path , error . absolute_path , error . message ) elif error . validator == \"required\" : logging . warning ( ( f \"{error.message} \\n \" if error . message else error . message ) + \"required element missing at: \" + \"/\" . join ( map ( str , error . absolute_path )) ) else : raise NotImplementedError ( f \"{error.validator} is not yet supported\" ) Functions get_jsonschema_csl_validator def get_jsonschema_csl_validator ( ) Return a jsonschema validator for the CSL Item JSON Schema View Source @functools.lru_cache () def get_jsonschema_csl_validator (): \"\"\" Return a jsonschema validator for the CSL Item JSON Schema \"\"\" import jsonschema import requests url = \"https://github.com/dhimmel/csl-schema/raw/manubot/csl-data.json\" schema = requests . get ( url ) . json () Validator = jsonschema . validators . validator_for ( schema ) Validator . check_schema ( schema ) return Validator ( schema ) remove_jsonschema_errors def remove_jsonschema_errors ( instance , recurse_depth = 5 , in_place = False ) Remove fields in CSL Items that produce JSON Schema errors. Should errors be removed, but the JSON instance still fails to validate, recursively call remove_jsonschema_errors until the instance validates or the recursion depth limit is reached. Note that this method may not be work for all types of JSON Schema errors and users looking to adapt it for other applications should write task-specific tests to provide empirical evaluate that it works as intended. The default in_place=False creates a deepcopy of instance before pruning it, such that a new dictionary is returned and instance is not edited. Set in_place=True to edit instance in-place. The inital implementation of remove_jsonschema_errors always deepcopied instance, and it is possible deepcopying is important to prevent malfunction when encountering certain edge cases. Please report if you observe any in_place dependent behaviors. See also: https://github.com/Julian/jsonschema/issues/448 https://stackoverflow.com/questions/44694835 View Source def remove_jsonschema_errors ( instance , recurse_depth = 5 , in_place = False ): \"\"\" Remove fields in CSL Items that produce JSON Schema errors. Should errors be removed, but the JSON instance still fails to validate, recursively call remove_jsonschema_errors until the instance validates or the recursion depth limit is reached. Note that this method may not be work for all types of JSON Schema errors and users looking to adapt it for other applications should write task-specific tests to provide empirical evaluate that it works as intended. The default in_place=False creates a deepcopy of instance before pruning it, such that a new dictionary is returned and instance is not edited. Set in_place=True to edit instance in-place. The inital implementation of remove_jsonschema_errors always deepcopied instance, and it is possible deepcopying is important to prevent malfunction when encountering certain edge cases. Please report if you observe any in_place dependent behaviors. See also: https://github.com/Julian/jsonschema/issues/448 https://stackoverflow.com/questions/44694835 \"\"\" validator = get_jsonschema_csl_validator () errors = list ( validator . iter_errors ( instance )) if not in_place : instance = copy . deepcopy ( instance ) errors = sorted ( errors , key = lambda e : e . path , reverse = True ) for error in errors : _remove_error ( instance , error ) if validator . is_valid ( instance ) or recurse_depth < 1 : return instance return remove_jsonschema_errors ( instance , recurse_depth - 1 , in_place = in_place )","title":"Citeproc"},{"location":"reference/manubot/cite/citeproc/#module-manubotciteciteproc","text":"Correct or validate CSL item schema. Module naming: citeproc is the generic name for programs that produce formatted bibliographies and citations based on the metadata of the cited objects and the formatting instructions provided by Citation Style Language (CSL) styles. -- https://en.wikipedia.org/wiki/CiteProc View Source \"\"\"Correct or validate CSL item schema. Module naming: citeproc is the generic name for programs that produce formatted bibliographies and citations based on the metadata of the cited objects and the formatting instructions provided by Citation Style Language (CSL) styles. -- https://en.wikipedia.org/wiki/CiteProc \"\"\" import copy import functools import logging @functools.lru_cache () def get_jsonschema_csl_validator (): \"\"\" Return a jsonschema validator for the CSL Item JSON Schema \"\"\" import jsonschema import requests url = \"https://github.com/dhimmel/csl-schema/raw/manubot/csl-data.json\" schema = requests . get ( url ) . json () Validator = jsonschema . validators . validator_for ( schema ) Validator . check_schema ( schema ) return Validator ( schema ) def remove_jsonschema_errors ( instance , recurse_depth = 5 , in_place = False ): \"\"\" Remove fields in CSL Items that produce JSON Schema errors. Should errors be removed, but the JSON instance still fails to validate, recursively call remove_jsonschema_errors until the instance validates or the recursion depth limit is reached. Note that this method may not be work for all types of JSON Schema errors and users looking to adapt it for other applications should write task-specific tests to provide empirical evaluate that it works as intended. The default in_place=False creates a deepcopy of instance before pruning it, such that a new dictionary is returned and instance is not edited. Set in_place=True to edit instance in-place. The inital implementation of remove_jsonschema_errors always deepcopied instance, and it is possible deepcopying is important to prevent malfunction when encountering certain edge cases. Please report if you observe any in_place dependent behaviors. See also: https://github.com/Julian/jsonschema/issues/448 https://stackoverflow.com/questions/44694835 \"\"\" validator = get_jsonschema_csl_validator () errors = list ( validator . iter_errors ( instance )) if not in_place : instance = copy . deepcopy ( instance ) errors = sorted ( errors , key = lambda e : e . path , reverse = True ) for error in errors : _remove_error ( instance , error ) if validator . is_valid ( instance ) or recurse_depth < 1 : return instance return remove_jsonschema_errors ( instance , recurse_depth - 1 , in_place = in_place ) def _delete_elem ( instance , path , absolute_path = None , message = \"\" ): \"\"\" Helper function for remove_jsonschema_errors that deletes an element in the JSON-like input instance at the specified path. absolute_path is relative to the original validated instance for logging purposes. Defaults to path, if not specified. message is an optional string with additional error information to log. \"\"\" if absolute_path is None : absolute_path = path logging . debug ( ( f \"{message} \\n \" if message else message ) + \"_delete_elem deleting CSL element at: \" + \"/\" . join ( map ( str , absolute_path )) ) * head , tail = path try : del _deep_get ( instance , head )[ tail ] except KeyError : pass def _deep_get ( instance , path ): \"\"\" Descend path to return a deep element in the JSON object instance. \"\"\" for key in path : instance = instance [ key ] return instance def _remove_error ( instance , error ): \"\"\" Remove a jsonschema ValidationError from the JSON-like instance. See ValidationError documentation at http://python-jsonschema.readthedocs.io/en/latest/errors/#jsonschema.exceptions.ValidationError \"\"\" sub_errors = error . context if sub_errors : # already_removed_additional was neccessary to workaround # https://github.com/citation-style-language/schema/issues/154 already_removed_additional = False for sub_error in sub_errors : if sub_error . validator == \"additionalProperties\" : if already_removed_additional : continue already_removed_additional = True sub_instance = _deep_get ( instance , error . path ) _remove_error ( sub_instance , sub_error ) elif error . validator == \"additionalProperties\" : extras = set ( error . instance ) - set ( error . schema [ \"properties\" ]) logging . debug ( error . message + f \" \\n Will now remove these {len(extras)} additional properties.\" ) for key in extras : _delete_elem ( instance = instance , path = list ( error . path ) + [ key ], absolute_path = list ( error . absolute_path ) + [ key ], ) elif error . validator in { \"enum\" , \"type\" , \"minItems\" , \"maxItems\" }: _delete_elem ( instance , error . path , error . absolute_path , error . message ) elif error . validator == \"required\" : logging . warning ( ( f \"{error.message} \\n \" if error . message else error . message ) + \"required element missing at: \" + \"/\" . join ( map ( str , error . absolute_path )) ) else : raise NotImplementedError ( f \"{error.validator} is not yet supported\" )","title":"Module manubot.cite.citeproc"},{"location":"reference/manubot/cite/citeproc/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/citeproc/#get_jsonschema_csl_validator","text":"def get_jsonschema_csl_validator ( ) Return a jsonschema validator for the CSL Item JSON Schema View Source @functools.lru_cache () def get_jsonschema_csl_validator (): \"\"\" Return a jsonschema validator for the CSL Item JSON Schema \"\"\" import jsonschema import requests url = \"https://github.com/dhimmel/csl-schema/raw/manubot/csl-data.json\" schema = requests . get ( url ) . json () Validator = jsonschema . validators . validator_for ( schema ) Validator . check_schema ( schema ) return Validator ( schema )","title":"get_jsonschema_csl_validator"},{"location":"reference/manubot/cite/citeproc/#remove_jsonschema_errors","text":"def remove_jsonschema_errors ( instance , recurse_depth = 5 , in_place = False ) Remove fields in CSL Items that produce JSON Schema errors. Should errors be removed, but the JSON instance still fails to validate, recursively call remove_jsonschema_errors until the instance validates or the recursion depth limit is reached. Note that this method may not be work for all types of JSON Schema errors and users looking to adapt it for other applications should write task-specific tests to provide empirical evaluate that it works as intended. The default in_place=False creates a deepcopy of instance before pruning it, such that a new dictionary is returned and instance is not edited. Set in_place=True to edit instance in-place. The inital implementation of remove_jsonschema_errors always deepcopied instance, and it is possible deepcopying is important to prevent malfunction when encountering certain edge cases. Please report if you observe any in_place dependent behaviors. See also: https://github.com/Julian/jsonschema/issues/448 https://stackoverflow.com/questions/44694835 View Source def remove_jsonschema_errors ( instance , recurse_depth = 5 , in_place = False ): \"\"\" Remove fields in CSL Items that produce JSON Schema errors. Should errors be removed, but the JSON instance still fails to validate, recursively call remove_jsonschema_errors until the instance validates or the recursion depth limit is reached. Note that this method may not be work for all types of JSON Schema errors and users looking to adapt it for other applications should write task-specific tests to provide empirical evaluate that it works as intended. The default in_place=False creates a deepcopy of instance before pruning it, such that a new dictionary is returned and instance is not edited. Set in_place=True to edit instance in-place. The inital implementation of remove_jsonschema_errors always deepcopied instance, and it is possible deepcopying is important to prevent malfunction when encountering certain edge cases. Please report if you observe any in_place dependent behaviors. See also: https://github.com/Julian/jsonschema/issues/448 https://stackoverflow.com/questions/44694835 \"\"\" validator = get_jsonschema_csl_validator () errors = list ( validator . iter_errors ( instance )) if not in_place : instance = copy . deepcopy ( instance ) errors = sorted ( errors , key = lambda e : e . path , reverse = True ) for error in errors : _remove_error ( instance , error ) if validator . is_valid ( instance ) or recurse_depth < 1 : return instance return remove_jsonschema_errors ( instance , recurse_depth - 1 , in_place = in_place )","title":"remove_jsonschema_errors"},{"location":"reference/manubot/cite/csl_item/","text":"Module manubot.cite.csl_item Represent bibliographic information for a single publication. From the CSL docs: Next up are the bibliographic details of the items you wish to cite : the item metadata . For example , the bibliographic entry for a journal article may show the names of the authors , the year in which the article was published , the article title , the journal title , the volume and issue in which the article appeared , the page numbers of the article , and the article \u2019 s Digital Object Identifier ( DOI ). All these details help the reader identify and find the referenced work . Reference managers make it easy to create a library of items . While many reference managers have their own way of storing item metadata , most support common bibliographic exchange formats such as BibTeX and RIS . The citeproc - js CSL processor introduced a JSON - based format for storing item metadata in a way citeproc - js could understand . Several other CSL processors have since adopted this \u201c CSL JSON \u201d format ( also known as \u201c citeproc JSON \u201d ). -- https://github.com/citation-style-language/documentation/blob/master/primer.txt The terminology we've adopted is csl_data for a list of csl_item dicts, and csl_json for csl_data that is JSON-serialized. View Source \"\"\"Represent bibliographic information for a single publication. From the CSL docs: Next up are the bibliographic details of the items you wish to cite: the item metadata. For example, the bibliographic entry for a journal article may show the names of the authors, the year in which the article was published, the article title, the journal title, the volume and issue in which the article appeared, the page numbers of the article, and the article\u2019s Digital Object Identifier (DOI). All these details help the reader identify and find the referenced work. Reference managers make it easy to create a library of items. While many reference managers have their own way of storing item metadata, most support common bibliographic exchange formats such as BibTeX and RIS. The citeproc-js CSL processor introduced a JSON-based format for storing item metadata in a way citeproc-js could understand. Several other CSL processors have since adopted this \u201cCSL JSON\u201d format (also known as \u201cciteproc JSON\u201d). -- https://github.com/citation-style-language/documentation/blob/master/primer.txt The terminology we've adopted is csl_data for a list of csl_item dicts, and csl_json for csl_data that is JSON-serialized. \"\"\" import copy import logging import re from typing import List , Optional from manubot . cite . citekey import CiteKey class CSL_Item ( dict ) : \"\"\" CSL_Item represents bibliographic information for a single citeable work. On a technical side CSL_Item is a Python dictionary with extra methods that help cleaning and manipulating it. These methods relate to: - adding an `id` key and value for CSL item - correcting bibliographic information and its structure - adding and reading a custom note to CSL item More information on CSL JSON (a list of CSL_Items) is available at: - https://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html - http://docs.citationstyles.org/en/1.0.1/specification.html#standard-variables - https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" # The ideas for CSL_Item methods come from the following parts of code : # - [ ] citekey_to_csl_item ( citekey , prune = True ) # The methods in CSL_Item class provide primitives to reconstruct # functions above . type_mapping = { \"journal-article\" : \"article-journal\" , \"book-chapter\" : \"chapter\" , \"posted-content\" : \"manuscript\" , \"proceedings-article\" : \"paper-conference\" , \"standard\" : \"entry\" , \"reference-entry\" : \"entry\" , } def __init__ ( self , dictionary = None , ** kwargs ) : \"\"\" Can use both a dictionary or keywords to create a CSL_Item object: CSL_Item(title='The Book') CSL_Item({'title': 'The Book'}) csl_dict = {'title': 'The Book', 'ISBN': '321-321'} CSL_Item(csl_dict, type='entry') CSL_Item(title='The Book', ISBN='321-321', type='entry') CSL_Item object is usually provided by bibliographic information API, but constructing small CSL_Item objects is useful for testing. \"\"\" if dictionary is None : dictionary = dict () super (). __init__ ( copy . deepcopy ( dictionary )) self . update ( copy . deepcopy ( kwargs )) def set_id ( self , id_ ) : self [ \"id\" ] = id_ return self def correct_invalid_type ( self ) : \"\"\" Correct invalid CSL item type. Does nothing if `type` not present. For detail see https://github.com/CrossRef/rest-api-doc/issues/187 \"\"\" if \"type\" in self : # Replace a type from in CSL_Item . type_mapping . keys (), # leave type intact in other cases . t = self [ \"type\" ] self [ \"type\" ] = self . type_mapping . get ( t , t ) return self def set_default_type ( self ) : \"\"\" Set type to 'entry', if type not specified. \"\"\" self [ \"type\" ] = self . get ( \"type\" , \"entry\" ) return self def prune_against_schema ( self ) : \"\"\" Remove fields that violate the CSL Item JSON Schema. \"\"\" from . citeproc import remove_jsonschema_errors ( csl_item ,) = remove_jsonschema_errors ( [ self ] , in_place = True ) assert csl_item is self return self def validate_against_schema ( self ) : \"\"\" Confirm that the CSL_Item validates. If not, raises a jsonschema.exceptions.ValidationError. \"\"\" from . citeproc import get_jsonschema_csl_validator validator = get_jsonschema_csl_validator () validator . validate ( [ self ] ) return self def clean ( self , prune : bool = True ) : \"\"\" Sanitize and touch-up a potentially dirty CSL_Item. The following steps are performed: - update incorrect values for \" type \" field when a correct variant is known - remove fields that violate the JSON Schema (if prune=True) - set default value for \" type \" if missing, since CSL JSON requires type - validate against the CSL JSON schema (if prune=True) to ensure output CSL_Item is clean \"\"\" logging . debug ( f \"Starting CSL_Item.clean with{'' if prune else 'out'}\" f \"CSL pruning for id: {self.get('id', 'id not specified')}\" ) self . correct_invalid_type () if prune : self . prune_against_schema () self . set_default_type () if prune : self . validate_against_schema () return self def set_date ( self , date , variable = \"issued\" ) : \"\"\" date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). variable: which variable to assign the date to. \"\"\" date_parts = date_to_date_parts ( date ) if date_parts : self [ variable ] = { \"date-parts\" : [ date_parts ] } return self def get_date ( self , variable = \"issued\" , fill = False ) : \"\"\" Return a CSL date-variable as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). variable: which CSL JSON date variable to retrieve fill: if True, set missing months to January and missing days to the first day of the month. \"\"\" try : date_parts = self [ variable ][ \"date-parts\" ][ 0 ] except ( IndexError , KeyError ) : return None return date_parts_to_string ( date_parts , fill = fill ) @property def note ( self ) -> str : \"\"\" Return the value of the \" note \" field as a string. If \" note \" key is not set, return empty string. \"\"\" return str ( self . get ( \"note\" ) or \"\" ) @note . setter def note ( self , text : str ) : if text : self [ \"note\" ] = text else : # if text is None or an empty string , remove the \"note\" field self . pop ( \"note\" , None ) @property def note_dict ( self ) -> dict : \"\"\" Return a dictionary with key-value pairs encoded by this CSL Item's note. Extracts both forms (line-entry and braced-entry) of key-value pairs from the CSL JSON \" cheater syntax \" https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields Assigning to this dict will not update `self[\" note \"]`. \"\"\" note = self . note line_matches = re . findall ( r \"^(?P<key>[A-Z]+|[-_a-z]+): *(?P<value>.+?) *$\" , note , re . MULTILINE ) braced_matches = re . findall ( r \"{:(?P<key>[A-Z]+|[-_a-z]+): *(?P<value>.+?) *}\" , note ) return dict ( line_matches + braced_matches ) def note_append_text ( self , text : str ) : \"\"\" Append text to the note field of a CSL Item. \"\"\" if not text : return note = self . note if note and not note . endswith ( \"\\n\" ) : note += \"\\n\" note += text self . note = note def note_append_dict ( self , dictionary : dict ) : \"\"\" Append key-value pairs specified by `dictionary` to the note field of a CSL Item. Uses the the [CSL JSON \" cheater syntax \"](https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields) to encode additional values not defined by the CSL JSON schema. \"\"\" for key , value in dictionary . items () : if not re . fullmatch ( r \"[A-Z]+|[-_a-z]+\" , key ) : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"it does not conform to the variable_name syntax as per https://git.io/fjTzW.\" ) continue if \"\\n\" in value : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"the value contains a newline: {value!r}\" ) continue self . note_append_text ( f \"{key}: {value}\" ) def infer_id ( self ) : \"\"\" Detect and set a non-null/empty value for \" id \" or else raise a ValueError. \"\"\" if self . get ( \"standard_citation\" ) : # \"standard_citation\" field is set with a non - null / empty value return self . set_id ( self . pop ( \"standard_citation\" )) if self . note_dict . get ( \"standard_id\" ) : # \"standard_id\" note field is set with a non - null / empty value return self . set_id ( self . note_dict [ \"standard_id\" ] ) if self . get ( \"id\" ) : # \"id\" field exists and is set with a non - null / empty value return self . set_id ( self [ \"id\" ] ) raise ValueError ( \"infer_id could not detect a field with a citation / standard_citation. \" 'Consider setting the CSL Item \"id\" field.' ) def standardize_id ( self ) : \"\"\" Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \" id \" field. The standard_id is extracted from a \" standard_citation \" field, the \" note \" field, or the \" id \" field. The extracted citation is checked for validity and standardized, after which it is the final \" standard_id \". Regarding csl_item modification, the csl_item \" id \" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id and original_id. Note that the Manubot software generally refers to the \" id \" of a CSL Item as a citekey. However, in this context, we use \" id \" rather than \" citekey \" for consistency with CSL's \" id \" field. \"\"\" original_id = self . get ( \"id\" ) self . infer_id () original_standard_id = self [ \"id\" ] citekey = CiteKey ( original_standard_id ) standard_id = citekey . standard_id add_to_note = {} note_dict = self . note_dict if original_id and original_id != standard_id : if original_id != note_dict . get ( \"original_id\" ) : add_to_note [ \"original_id\" ] = original_id if original_standard_id and original_standard_id != standard_id : if original_standard_id != note_dict . get ( \"original_standard_id\" ) : add_to_note [ \"original_standard_id\" ] = original_standard_id if standard_id != note_dict . get ( \"standard_id\" ) : add_to_note [ \"standard_id\" ] = standard_id self . note_append_dict ( dictionary = add_to_note ) self . set_id ( standard_id ) return self def assert_csl_item_type ( x ) : if not isinstance ( x , CSL_Item ) : raise TypeError ( f \"Expected CSL_Item object, got {type(x)}\" ) def date_to_date_parts ( date ) -> Optional [ List[int ] ]: \"\"\" Convert a date string or object to a date parts list. date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). \"\"\" import datetime if date is None : return None if isinstance ( date , ( datetime . date , datetime . datetime )) : date = date . isoformat () if not isinstance ( date , str ) : raise ValueError ( f \"date_to_date_parts: unsupported type for {date}\" ) date = date . strip () re_year = r \"(?P<year>[0-9]{4})\" re_month = r \"(?P<month>1[0-2]|0[1-9])\" re_day = r \"(?P<day>[0-3][0-9])\" patterns = [ f\"{re_year}-{re_month}-{re_day}\", f\"{re_year}-{re_month}\", f\"{re_year}\", f\".*\", # regex to match anything ] for pattern in patterns : match = re . match ( pattern , date ) if match : break date_parts = [] for part in \"year\" , \"month\" , \"day\" : try : value = match . group ( part ) except IndexError : break if not value : break date_parts . append ( int ( value )) if date_parts : return date_parts def date_parts_to_string ( date_parts , fill : bool = False ) -> Optional [ str ] : \"\"\" Return a CSL date-parts list as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). date_parts: list or tuple like [year, month, day] as integers. Also supports [year, month] and [year] for situations where the day or month-and-day are missing. fill: if True, set missing months to January and missing days to the first day of the month. \"\"\" if not date_parts : return None if not isinstance ( date_parts , ( tuple , list )) : raise ValueError ( f \"date_parts must be a tuple or list\" ) while fill and 1 <= len ( date_parts ) < 3 : date_parts . append ( 1 ) widths = 4 , 2 , 2 str_parts = [] for i , part in enumerate ( date_parts [ :3 ] ) : width = widths [ i ] if isinstance ( part , int ) : part = str ( part ) if not isinstance ( part , str ) : break part = part . zfill ( width ) if len ( part ) != width or not part . isdigit () : break str_parts . append ( part ) if not str_parts : return None iso_str = \"-\" . join ( str_parts ) return iso_str Functions assert_csl_item_type def assert_csl_item_type ( x ) View Source def assert_csl_item_type ( x ): if not isinstance ( x , CSL_Item ): raise TypeError ( f \"Expected CSL_Item object, got {type(x)}\" ) date_parts_to_string def date_parts_to_string ( date_parts , fill : bool = False ) -> Union [ str , NoneType ] Return a CSL date-parts list as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). date_parts: list or tuple like [year, month, day] as integers. Also supports [year, month] and [year] for situations where the day or month-and-day are missing. fill: if True, set missing months to January and missing days to the first day of the month. View Source def date_parts_to_string ( date_parts , fill : bool = False ) -> Optional [ str ] : \"\"\" Return a CSL date-parts list as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). date_parts: list or tuple like [year, month, day] as integers. Also supports [year, month] and [year] for situations where the day or month-and-day are missing. fill: if True, set missing months to January and missing days to the first day of the month. \"\"\" if not date_parts : return None if not isinstance ( date_parts , ( tuple , list )) : raise ValueError ( f \"date_parts must be a tuple or list\" ) while fill and 1 <= len ( date_parts ) < 3 : date_parts . append ( 1 ) widths = 4 , 2 , 2 str_parts = [] for i , part in enumerate ( date_parts [ :3 ] ) : width = widths [ i ] if isinstance ( part , int ) : part = str ( part ) if not isinstance ( part , str ) : break part = part . zfill ( width ) if len ( part ) != width or not part . isdigit () : break str_parts . append ( part ) if not str_parts : return None iso_str = \"-\" . join ( str_parts ) return iso_str date_to_date_parts def date_to_date_parts ( date ) -> Union [ List [ int ], NoneType ] Convert a date string or object to a date parts list. date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). View Source def date_to_date_parts ( date ) -> Optional [ List [ int ]]: \"\"\" Convert a date string or object to a date parts list. date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). \"\"\" import datetime if date is None : return None if isinstance ( date , ( datetime . date , datetime . datetime )): date = date . isoformat () if not isinstance ( date , str ): raise ValueError ( f \"date_to_date_parts: unsupported type for {date}\" ) date = date . strip () re_year = r \"(?P<year>[0-9]{4})\" re_month = r \"(?P<month>1[0-2]|0[1-9])\" re_day = r \"(?P<day>[0-3][0-9])\" patterns = [ f \"{re_year}-{re_month}-{re_day}\" , f \"{re_year}-{re_month}\" , f \"{re_year}\" , f \".*\" , # regex to match anything ] for pattern in patterns : match = re . match ( pattern , date ) if match : break date_parts = [] for part in \"year\" , \"month\" , \"day\" : try : value = match . group ( part ) except IndexError : break if not value : break date_parts . append ( int ( value )) if date_parts : return date_parts Classes CSL_Item class CSL_Item ( dictionary = None , ** kwargs ) CSL_Item represents bibliographic information for a single citeable work. On a technical side CSL_Item is a Python dictionary with extra methods that help cleaning and manipulating it. These methods relate to: - adding an id key and value for CSL item - correcting bibliographic information and its structure - adding and reading a custom note to CSL item More information on CSL JSON (a list of CSL_Items) is available at: - https://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html - http://docs.citationstyles.org/en/1.0.1/specification.html#standard-variables - https://github.com/citation-style-language/schema/blob/master/csl-data.json View Source class CSL_Item ( dict ) : \"\"\" CSL_Item represents bibliographic information for a single citeable work. On a technical side CSL_Item is a Python dictionary with extra methods that help cleaning and manipulating it. These methods relate to: - adding an `id` key and value for CSL item - correcting bibliographic information and its structure - adding and reading a custom note to CSL item More information on CSL JSON (a list of CSL_Items) is available at: - https://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html - http://docs.citationstyles.org/en/1.0.1/specification.html#standard-variables - https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" # The ideas for CSL_Item methods come from the following parts of code : # - [ ] citekey_to_csl_item ( citekey , prune = True ) # The methods in CSL_Item class provide primitives to reconstruct # functions above . type_mapping = { \"journal-article\" : \"article-journal\" , \"book-chapter\" : \"chapter\" , \"posted-content\" : \"manuscript\" , \"proceedings-article\" : \"paper-conference\" , \"standard\" : \"entry\" , \"reference-entry\" : \"entry\" , } def __init__ ( self , dictionary = None , ** kwargs ) : \"\"\" Can use both a dictionary or keywords to create a CSL_Item object: CSL_Item(title='The Book') CSL_Item({'title': 'The Book'}) csl_dict = {'title': 'The Book', 'ISBN': '321-321'} CSL_Item(csl_dict, type='entry') CSL_Item(title='The Book', ISBN='321-321', type='entry') CSL_Item object is usually provided by bibliographic information API, but constructing small CSL_Item objects is useful for testing. \"\"\" if dictionary is None : dictionary = dict () super (). __init__ ( copy . deepcopy ( dictionary )) self . update ( copy . deepcopy ( kwargs )) def set_id ( self , id_ ) : self [ \"id\" ] = id_ return self def correct_invalid_type ( self ) : \"\"\" Correct invalid CSL item type. Does nothing if `type` not present. For detail see https://github.com/CrossRef/rest-api-doc/issues/187 \"\"\" if \"type\" in self : # Replace a type from in CSL_Item . type_mapping . keys (), # leave type intact in other cases . t = self [ \"type\" ] self [ \"type\" ] = self . type_mapping . get ( t , t ) return self def set_default_type ( self ) : \"\"\" Set type to 'entry', if type not specified. \"\"\" self [ \"type\" ] = self . get ( \"type\" , \"entry\" ) return self def prune_against_schema ( self ) : \"\"\" Remove fields that violate the CSL Item JSON Schema. \"\"\" from . citeproc import remove_jsonschema_errors ( csl_item ,) = remove_jsonschema_errors ( [ self ] , in_place = True ) assert csl_item is self return self def validate_against_schema ( self ) : \"\"\" Confirm that the CSL_Item validates. If not, raises a jsonschema.exceptions.ValidationError. \"\"\" from . citeproc import get_jsonschema_csl_validator validator = get_jsonschema_csl_validator () validator . validate ( [ self ] ) return self def clean ( self , prune : bool = True ) : \"\"\" Sanitize and touch-up a potentially dirty CSL_Item. The following steps are performed: - update incorrect values for \" type \" field when a correct variant is known - remove fields that violate the JSON Schema (if prune=True) - set default value for \" type \" if missing, since CSL JSON requires type - validate against the CSL JSON schema (if prune=True) to ensure output CSL_Item is clean \"\"\" logging . debug ( f \"Starting CSL_Item.clean with{'' if prune else 'out'}\" f \"CSL pruning for id: {self.get('id', 'id not specified')}\" ) self . correct_invalid_type () if prune : self . prune_against_schema () self . set_default_type () if prune : self . validate_against_schema () return self def set_date ( self , date , variable = \"issued\" ) : \"\"\" date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). variable: which variable to assign the date to. \"\"\" date_parts = date_to_date_parts ( date ) if date_parts : self [ variable ] = { \"date-parts\" : [ date_parts ] } return self def get_date ( self , variable = \"issued\" , fill = False ) : \"\"\" Return a CSL date-variable as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). variable: which CSL JSON date variable to retrieve fill: if True, set missing months to January and missing days to the first day of the month. \"\"\" try : date_parts = self [ variable ][ \"date-parts\" ][ 0 ] except ( IndexError , KeyError ) : return None return date_parts_to_string ( date_parts , fill = fill ) @property def note ( self ) -> str : \"\"\" Return the value of the \" note \" field as a string. If \" note \" key is not set, return empty string. \"\"\" return str ( self . get ( \"note\" ) or \"\" ) @note . setter def note ( self , text : str ) : if text : self [ \"note\" ] = text else : # if text is None or an empty string , remove the \"note\" field self . pop ( \"note\" , None ) @property def note_dict ( self ) -> dict : \"\"\" Return a dictionary with key-value pairs encoded by this CSL Item's note. Extracts both forms (line-entry and braced-entry) of key-value pairs from the CSL JSON \" cheater syntax \" https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields Assigning to this dict will not update `self[\" note \"]`. \"\"\" note = self . note line_matches = re . findall ( r \"^(?P<key>[A-Z]+|[-_a-z]+): *(?P<value>.+?) *$\" , note , re . MULTILINE ) braced_matches = re . findall ( r \"{:(?P<key>[A-Z]+|[-_a-z]+): *(?P<value>.+?) *}\" , note ) return dict ( line_matches + braced_matches ) def note_append_text ( self , text : str ) : \"\"\" Append text to the note field of a CSL Item. \"\"\" if not text : return note = self . note if note and not note . endswith ( \"\\n\" ) : note += \"\\n\" note += text self . note = note def note_append_dict ( self , dictionary : dict ) : \"\"\" Append key-value pairs specified by `dictionary` to the note field of a CSL Item. Uses the the [CSL JSON \" cheater syntax \"](https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields) to encode additional values not defined by the CSL JSON schema. \"\"\" for key , value in dictionary . items () : if not re . fullmatch ( r \"[A-Z]+|[-_a-z]+\" , key ) : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"it does not conform to the variable_name syntax as per https://git.io/fjTzW.\" ) continue if \"\\n\" in value : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"the value contains a newline: {value!r}\" ) continue self . note_append_text ( f \"{key}: {value}\" ) def infer_id ( self ) : \"\"\" Detect and set a non-null/empty value for \" id \" or else raise a ValueError. \"\"\" if self . get ( \"standard_citation\" ) : # \"standard_citation\" field is set with a non - null / empty value return self . set_id ( self . pop ( \"standard_citation\" )) if self . note_dict . get ( \"standard_id\" ) : # \"standard_id\" note field is set with a non - null / empty value return self . set_id ( self . note_dict [ \"standard_id\" ] ) if self . get ( \"id\" ) : # \"id\" field exists and is set with a non - null / empty value return self . set_id ( self [ \"id\" ] ) raise ValueError ( \"infer_id could not detect a field with a citation / standard_citation. \" 'Consider setting the CSL Item \"id\" field.' ) def standardize_id ( self ) : \"\"\" Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \" id \" field. The standard_id is extracted from a \" standard_citation \" field, the \" note \" field, or the \" id \" field. The extracted citation is checked for validity and standardized, after which it is the final \" standard_id \". Regarding csl_item modification, the csl_item \" id \" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id and original_id. Note that the Manubot software generally refers to the \" id \" of a CSL Item as a citekey. However, in this context, we use \" id \" rather than \" citekey \" for consistency with CSL's \" id \" field. \"\"\" original_id = self . get ( \"id\" ) self . infer_id () original_standard_id = self [ \"id\" ] citekey = CiteKey ( original_standard_id ) standard_id = citekey . standard_id add_to_note = {} note_dict = self . note_dict if original_id and original_id != standard_id : if original_id != note_dict . get ( \"original_id\" ) : add_to_note [ \"original_id\" ] = original_id if original_standard_id and original_standard_id != standard_id : if original_standard_id != note_dict . get ( \"original_standard_id\" ) : add_to_note [ \"original_standard_id\" ] = original_standard_id if standard_id != note_dict . get ( \"standard_id\" ) : add_to_note [ \"standard_id\" ] = standard_id self . note_append_dict ( dictionary = add_to_note ) self . set_id ( standard_id ) return self Ancestors (in MRO) builtins.dict Descendants manubot.cite.arxiv.CSL_Item_arXiv Class variables type_mapping Instance variables note Return the value of the \"note\" field as a string. If \"note\" key is not set, return empty string. note_dict Return a dictionary with key-value pairs encoded by this CSL Item's note. Extracts both forms (line-entry and braced-entry) of key-value pairs from the CSL JSON \"cheater syntax\" https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields Assigning to this dict will not update self[\"note\"] . Methods clean def clean ( self , prune : bool = True ) Sanitize and touch-up a potentially dirty CSL_Item. The following steps are performed: - update incorrect values for \"type\" field when a correct variant is known - remove fields that violate the JSON Schema (if prune=True) - set default value for \"type\" if missing, since CSL JSON requires type - validate against the CSL JSON schema (if prune=True) to ensure output CSL_Item is clean View Source def clean ( self , prune : bool = True ): \"\"\" Sanitize and touch-up a potentially dirty CSL_Item. The following steps are performed: - update incorrect values for \" type \" field when a correct variant is known - remove fields that violate the JSON Schema (if prune=True) - set default value for \" type \" if missing, since CSL JSON requires type - validate against the CSL JSON schema (if prune=True) to ensure output CSL_Item is clean \"\"\" logging . debug ( f \"Starting CSL_Item.clean with{'' if prune else 'out'}\" f \"CSL pruning for id: {self.get('id', 'id not specified')}\" ) self . correct_invalid_type () if prune : self . prune_against_schema () self . set_default_type () if prune : self . validate_against_schema () return self clear def clear ( ... ) D.clear() -> None. Remove all items from D. copy def copy ( ... ) D.copy() -> a shallow copy of D correct_invalid_type def correct_invalid_type ( self ) Correct invalid CSL item type. Does nothing if type not present. For detail see https://github.com/CrossRef/rest-api-doc/issues/187 View Source def correct_invalid_type ( self ): \"\"\" Correct invalid CSL item type. Does nothing if `type` not present. For detail see https://github.com/CrossRef/rest-api-doc/issues/187 \"\"\" if \"type\" in self : # Replace a type from in CSL_Item.type_mapping.keys(), # leave type intact in other cases. t = self [ \"type\" ] self [ \"type\" ] = self . type_mapping . get ( t , t ) return self fromkeys def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value. get def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default. get_date def get_date ( self , variable = 'issued' , fill = False ) Return a CSL date-variable as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). variable: which CSL JSON date variable to retrieve fill: if True, set missing months to January and missing days to the first day of the month. View Source def get_date ( self , variable = \"issued\" , fill = False ) : \"\"\" Return a CSL date-variable as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). variable: which CSL JSON date variable to retrieve fill: if True, set missing months to January and missing days to the first day of the month. \"\"\" try : date_parts = self [ variable ][ \"date-parts\" ][ 0 ] except ( IndexError , KeyError ) : return None return date_parts_to_string ( date_parts , fill = fill ) infer_id def infer_id ( self ) Detect and set a non-null/empty value for \"id\" or else raise a ValueError. View Source def infer_id ( self ): \"\"\" Detect and set a non-null/empty value for \" id \" or else raise a ValueError. \"\"\" if self . get ( \"standard_citation\" ): # \"standard_citation\" field is set with a non - null / empty value return self . set_id ( self . pop ( \"standard_citation\" )) if self . note_dict . get ( \"standard_id\" ): # \"standard_id\" note field is set with a non - null / empty value return self . set_id ( self . note_dict [ \"standard_id\" ]) if self . get ( \"id\" ): # \"id\" field exists and is set with a non - null / empty value return self . set_id ( self [ \"id\" ]) raise ValueError ( \"infer_id could not detect a field with a citation / standard_citation. \" 'Consider setting the CSL Item \"id\" field.' ) items def items ( ... ) D.items() -> a set-like object providing a view on D's items keys def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys note_append_dict def note_append_dict ( self , dictionary : dict ) Append key-value pairs specified by dictionary to the note field of a CSL Item. Uses the the CSL JSON \"cheater syntax\" to encode additional values not defined by the CSL JSON schema. View Source def note_append_dict ( self , dictionary : dict ): \"\"\" Append key-value pairs specified by `dictionary` to the note field of a CSL Item. Uses the the [CSL JSON \" cheater syntax \"](https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields) to encode additional values not defined by the CSL JSON schema. \"\"\" for key , value in dictionary . items (): if not re . fullmatch ( r \"[A-Z]+|[-_a-z]+\" , key ): logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"it does not conform to the variable_name syntax as per https://git.io/fjTzW.\" ) continue if \"\\n\" in value : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"the value contains a newline: {value!r}\" ) continue self . note_append_text ( f \"{key}: {value}\" ) note_append_text def note_append_text ( self , text : str ) Append text to the note field of a CSL Item. View Source def note_append_text ( self , text : str ): \"\"\" Append text to the note field of a CSL Item. \"\"\" if not text : return note = self . note if note and not note . endswith ( \"\\n\" ): note += \"\\n\" note += text self . note = note pop def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised popitem def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. prune_against_schema def prune_against_schema ( self ) Remove fields that violate the CSL Item JSON Schema. View Source def prune_against_schema ( self ): \"\"\" Remove fields that violate the CSL Item JSON Schema. \"\"\" from .citeproc import remove_jsonschema_errors ( csl_item ,) = remove_jsonschema_errors ([ self ], in_place = True ) assert csl_item is self return self set_date def set_date ( self , date , variable = 'issued' ) date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). variable: which variable to assign the date to. View Source def set_date ( self , date , variable = \"issued\" ) : \"\"\" date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). variable: which variable to assign the date to. \"\"\" date_parts = date_to_date_parts ( date ) if date_parts : self [ variable ] = { \"date-parts\" : [ date_parts ] } return self set_default_type def set_default_type ( self ) Set type to 'entry', if type not specified. View Source def set_default_type ( self ): \"\"\" Set type to 'entry', if type not specified. \"\"\" self [ \"type\" ] = self . get ( \"type\" , \"entry\" ) return self set_id def set_id ( self , id_ ) View Source def set_id ( self , id_ ): self [ \"id\" ] = id_ return self setdefault def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. standardize_id def standardize_id ( self ) Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \"id\" field. The standard_id is extracted from a \"standard_citation\" field, the \"note\" field, or the \"id\" field. The extracted citation is checked for validity and standardized, after which it is the final \"standard_id\". Regarding csl_item modification, the csl_item \"id\" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id and original_id. Note that the Manubot software generally refers to the \"id\" of a CSL Item as a citekey. However, in this context, we use \"id\" rather than \"citekey\" for consistency with CSL's \"id\" field. View Source def standardize_id ( self ): \"\"\" Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \" id \" field. The standard_id is extracted from a \" standard_citation \" field, the \" note \" field, or the \" id \" field. The extracted citation is checked for validity and standardized, after which it is the final \" standard_id \". Regarding csl_item modification, the csl_item \" id \" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id and original_id. Note that the Manubot software generally refers to the \" id \" of a CSL Item as a citekey. However, in this context, we use \" id \" rather than \" citekey \" for consistency with CSL's \" id \" field. \"\"\" original_id = self . get ( \"id\" ) self . infer_id () original_standard_id = self [ \"id\" ] citekey = CiteKey ( original_standard_id ) standard_id = citekey . standard_id add_to_note = {} note_dict = self . note_dict if original_id and original_id != standard_id : if original_id != note_dict . get ( \"original_id\" ): add_to_note [ \"original_id\" ] = original_id if original_standard_id and original_standard_id != standard_id : if original_standard_id != note_dict . get ( \"original_standard_id\" ): add_to_note [ \"original_standard_id\" ] = original_standard_id if standard_id != note_dict . get ( \"standard_id\" ): add_to_note [ \"standard_id\" ] = standard_id self . note_append_dict ( dictionary = add_to_note ) self . set_id ( standard_id ) return self update def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k] validate_against_schema def validate_against_schema ( self ) Confirm that the CSL_Item validates. If not, raises a jsonschema.exceptions.ValidationError. View Source def validate_against_schema ( self ): \"\"\" Confirm that the CSL_Item validates. If not, raises a jsonschema.exceptions.ValidationError. \"\"\" from .citeproc import get_jsonschema_csl_validator validator = get_jsonschema_csl_validator () validator . validate ([ self ]) return self values def values ( ... ) D.values() -> an object providing a view on D's values","title":"Csl Item"},{"location":"reference/manubot/cite/csl_item/#module-manubotcitecsl_item","text":"Represent bibliographic information for a single publication. From the CSL docs: Next up are the bibliographic details of the items you wish to cite : the item metadata . For example , the bibliographic entry for a journal article may show the names of the authors , the year in which the article was published , the article title , the journal title , the volume and issue in which the article appeared , the page numbers of the article , and the article \u2019 s Digital Object Identifier ( DOI ). All these details help the reader identify and find the referenced work . Reference managers make it easy to create a library of items . While many reference managers have their own way of storing item metadata , most support common bibliographic exchange formats such as BibTeX and RIS . The citeproc - js CSL processor introduced a JSON - based format for storing item metadata in a way citeproc - js could understand . Several other CSL processors have since adopted this \u201c CSL JSON \u201d format ( also known as \u201c citeproc JSON \u201d ). -- https://github.com/citation-style-language/documentation/blob/master/primer.txt The terminology we've adopted is csl_data for a list of csl_item dicts, and csl_json for csl_data that is JSON-serialized. View Source \"\"\"Represent bibliographic information for a single publication. From the CSL docs: Next up are the bibliographic details of the items you wish to cite: the item metadata. For example, the bibliographic entry for a journal article may show the names of the authors, the year in which the article was published, the article title, the journal title, the volume and issue in which the article appeared, the page numbers of the article, and the article\u2019s Digital Object Identifier (DOI). All these details help the reader identify and find the referenced work. Reference managers make it easy to create a library of items. While many reference managers have their own way of storing item metadata, most support common bibliographic exchange formats such as BibTeX and RIS. The citeproc-js CSL processor introduced a JSON-based format for storing item metadata in a way citeproc-js could understand. Several other CSL processors have since adopted this \u201cCSL JSON\u201d format (also known as \u201cciteproc JSON\u201d). -- https://github.com/citation-style-language/documentation/blob/master/primer.txt The terminology we've adopted is csl_data for a list of csl_item dicts, and csl_json for csl_data that is JSON-serialized. \"\"\" import copy import logging import re from typing import List , Optional from manubot . cite . citekey import CiteKey class CSL_Item ( dict ) : \"\"\" CSL_Item represents bibliographic information for a single citeable work. On a technical side CSL_Item is a Python dictionary with extra methods that help cleaning and manipulating it. These methods relate to: - adding an `id` key and value for CSL item - correcting bibliographic information and its structure - adding and reading a custom note to CSL item More information on CSL JSON (a list of CSL_Items) is available at: - https://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html - http://docs.citationstyles.org/en/1.0.1/specification.html#standard-variables - https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" # The ideas for CSL_Item methods come from the following parts of code : # - [ ] citekey_to_csl_item ( citekey , prune = True ) # The methods in CSL_Item class provide primitives to reconstruct # functions above . type_mapping = { \"journal-article\" : \"article-journal\" , \"book-chapter\" : \"chapter\" , \"posted-content\" : \"manuscript\" , \"proceedings-article\" : \"paper-conference\" , \"standard\" : \"entry\" , \"reference-entry\" : \"entry\" , } def __init__ ( self , dictionary = None , ** kwargs ) : \"\"\" Can use both a dictionary or keywords to create a CSL_Item object: CSL_Item(title='The Book') CSL_Item({'title': 'The Book'}) csl_dict = {'title': 'The Book', 'ISBN': '321-321'} CSL_Item(csl_dict, type='entry') CSL_Item(title='The Book', ISBN='321-321', type='entry') CSL_Item object is usually provided by bibliographic information API, but constructing small CSL_Item objects is useful for testing. \"\"\" if dictionary is None : dictionary = dict () super (). __init__ ( copy . deepcopy ( dictionary )) self . update ( copy . deepcopy ( kwargs )) def set_id ( self , id_ ) : self [ \"id\" ] = id_ return self def correct_invalid_type ( self ) : \"\"\" Correct invalid CSL item type. Does nothing if `type` not present. For detail see https://github.com/CrossRef/rest-api-doc/issues/187 \"\"\" if \"type\" in self : # Replace a type from in CSL_Item . type_mapping . keys (), # leave type intact in other cases . t = self [ \"type\" ] self [ \"type\" ] = self . type_mapping . get ( t , t ) return self def set_default_type ( self ) : \"\"\" Set type to 'entry', if type not specified. \"\"\" self [ \"type\" ] = self . get ( \"type\" , \"entry\" ) return self def prune_against_schema ( self ) : \"\"\" Remove fields that violate the CSL Item JSON Schema. \"\"\" from . citeproc import remove_jsonschema_errors ( csl_item ,) = remove_jsonschema_errors ( [ self ] , in_place = True ) assert csl_item is self return self def validate_against_schema ( self ) : \"\"\" Confirm that the CSL_Item validates. If not, raises a jsonschema.exceptions.ValidationError. \"\"\" from . citeproc import get_jsonschema_csl_validator validator = get_jsonschema_csl_validator () validator . validate ( [ self ] ) return self def clean ( self , prune : bool = True ) : \"\"\" Sanitize and touch-up a potentially dirty CSL_Item. The following steps are performed: - update incorrect values for \" type \" field when a correct variant is known - remove fields that violate the JSON Schema (if prune=True) - set default value for \" type \" if missing, since CSL JSON requires type - validate against the CSL JSON schema (if prune=True) to ensure output CSL_Item is clean \"\"\" logging . debug ( f \"Starting CSL_Item.clean with{'' if prune else 'out'}\" f \"CSL pruning for id: {self.get('id', 'id not specified')}\" ) self . correct_invalid_type () if prune : self . prune_against_schema () self . set_default_type () if prune : self . validate_against_schema () return self def set_date ( self , date , variable = \"issued\" ) : \"\"\" date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). variable: which variable to assign the date to. \"\"\" date_parts = date_to_date_parts ( date ) if date_parts : self [ variable ] = { \"date-parts\" : [ date_parts ] } return self def get_date ( self , variable = \"issued\" , fill = False ) : \"\"\" Return a CSL date-variable as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). variable: which CSL JSON date variable to retrieve fill: if True, set missing months to January and missing days to the first day of the month. \"\"\" try : date_parts = self [ variable ][ \"date-parts\" ][ 0 ] except ( IndexError , KeyError ) : return None return date_parts_to_string ( date_parts , fill = fill ) @property def note ( self ) -> str : \"\"\" Return the value of the \" note \" field as a string. If \" note \" key is not set, return empty string. \"\"\" return str ( self . get ( \"note\" ) or \"\" ) @note . setter def note ( self , text : str ) : if text : self [ \"note\" ] = text else : # if text is None or an empty string , remove the \"note\" field self . pop ( \"note\" , None ) @property def note_dict ( self ) -> dict : \"\"\" Return a dictionary with key-value pairs encoded by this CSL Item's note. Extracts both forms (line-entry and braced-entry) of key-value pairs from the CSL JSON \" cheater syntax \" https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields Assigning to this dict will not update `self[\" note \"]`. \"\"\" note = self . note line_matches = re . findall ( r \"^(?P<key>[A-Z]+|[-_a-z]+): *(?P<value>.+?) *$\" , note , re . MULTILINE ) braced_matches = re . findall ( r \"{:(?P<key>[A-Z]+|[-_a-z]+): *(?P<value>.+?) *}\" , note ) return dict ( line_matches + braced_matches ) def note_append_text ( self , text : str ) : \"\"\" Append text to the note field of a CSL Item. \"\"\" if not text : return note = self . note if note and not note . endswith ( \"\\n\" ) : note += \"\\n\" note += text self . note = note def note_append_dict ( self , dictionary : dict ) : \"\"\" Append key-value pairs specified by `dictionary` to the note field of a CSL Item. Uses the the [CSL JSON \" cheater syntax \"](https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields) to encode additional values not defined by the CSL JSON schema. \"\"\" for key , value in dictionary . items () : if not re . fullmatch ( r \"[A-Z]+|[-_a-z]+\" , key ) : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"it does not conform to the variable_name syntax as per https://git.io/fjTzW.\" ) continue if \"\\n\" in value : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"the value contains a newline: {value!r}\" ) continue self . note_append_text ( f \"{key}: {value}\" ) def infer_id ( self ) : \"\"\" Detect and set a non-null/empty value for \" id \" or else raise a ValueError. \"\"\" if self . get ( \"standard_citation\" ) : # \"standard_citation\" field is set with a non - null / empty value return self . set_id ( self . pop ( \"standard_citation\" )) if self . note_dict . get ( \"standard_id\" ) : # \"standard_id\" note field is set with a non - null / empty value return self . set_id ( self . note_dict [ \"standard_id\" ] ) if self . get ( \"id\" ) : # \"id\" field exists and is set with a non - null / empty value return self . set_id ( self [ \"id\" ] ) raise ValueError ( \"infer_id could not detect a field with a citation / standard_citation. \" 'Consider setting the CSL Item \"id\" field.' ) def standardize_id ( self ) : \"\"\" Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \" id \" field. The standard_id is extracted from a \" standard_citation \" field, the \" note \" field, or the \" id \" field. The extracted citation is checked for validity and standardized, after which it is the final \" standard_id \". Regarding csl_item modification, the csl_item \" id \" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id and original_id. Note that the Manubot software generally refers to the \" id \" of a CSL Item as a citekey. However, in this context, we use \" id \" rather than \" citekey \" for consistency with CSL's \" id \" field. \"\"\" original_id = self . get ( \"id\" ) self . infer_id () original_standard_id = self [ \"id\" ] citekey = CiteKey ( original_standard_id ) standard_id = citekey . standard_id add_to_note = {} note_dict = self . note_dict if original_id and original_id != standard_id : if original_id != note_dict . get ( \"original_id\" ) : add_to_note [ \"original_id\" ] = original_id if original_standard_id and original_standard_id != standard_id : if original_standard_id != note_dict . get ( \"original_standard_id\" ) : add_to_note [ \"original_standard_id\" ] = original_standard_id if standard_id != note_dict . get ( \"standard_id\" ) : add_to_note [ \"standard_id\" ] = standard_id self . note_append_dict ( dictionary = add_to_note ) self . set_id ( standard_id ) return self def assert_csl_item_type ( x ) : if not isinstance ( x , CSL_Item ) : raise TypeError ( f \"Expected CSL_Item object, got {type(x)}\" ) def date_to_date_parts ( date ) -> Optional [ List[int ] ]: \"\"\" Convert a date string or object to a date parts list. date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). \"\"\" import datetime if date is None : return None if isinstance ( date , ( datetime . date , datetime . datetime )) : date = date . isoformat () if not isinstance ( date , str ) : raise ValueError ( f \"date_to_date_parts: unsupported type for {date}\" ) date = date . strip () re_year = r \"(?P<year>[0-9]{4})\" re_month = r \"(?P<month>1[0-2]|0[1-9])\" re_day = r \"(?P<day>[0-3][0-9])\" patterns = [ f\"{re_year}-{re_month}-{re_day}\", f\"{re_year}-{re_month}\", f\"{re_year}\", f\".*\", # regex to match anything ] for pattern in patterns : match = re . match ( pattern , date ) if match : break date_parts = [] for part in \"year\" , \"month\" , \"day\" : try : value = match . group ( part ) except IndexError : break if not value : break date_parts . append ( int ( value )) if date_parts : return date_parts def date_parts_to_string ( date_parts , fill : bool = False ) -> Optional [ str ] : \"\"\" Return a CSL date-parts list as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). date_parts: list or tuple like [year, month, day] as integers. Also supports [year, month] and [year] for situations where the day or month-and-day are missing. fill: if True, set missing months to January and missing days to the first day of the month. \"\"\" if not date_parts : return None if not isinstance ( date_parts , ( tuple , list )) : raise ValueError ( f \"date_parts must be a tuple or list\" ) while fill and 1 <= len ( date_parts ) < 3 : date_parts . append ( 1 ) widths = 4 , 2 , 2 str_parts = [] for i , part in enumerate ( date_parts [ :3 ] ) : width = widths [ i ] if isinstance ( part , int ) : part = str ( part ) if not isinstance ( part , str ) : break part = part . zfill ( width ) if len ( part ) != width or not part . isdigit () : break str_parts . append ( part ) if not str_parts : return None iso_str = \"-\" . join ( str_parts ) return iso_str","title":"Module manubot.cite.csl_item"},{"location":"reference/manubot/cite/csl_item/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/csl_item/#assert_csl_item_type","text":"def assert_csl_item_type ( x ) View Source def assert_csl_item_type ( x ): if not isinstance ( x , CSL_Item ): raise TypeError ( f \"Expected CSL_Item object, got {type(x)}\" )","title":"assert_csl_item_type"},{"location":"reference/manubot/cite/csl_item/#date_parts_to_string","text":"def date_parts_to_string ( date_parts , fill : bool = False ) -> Union [ str , NoneType ] Return a CSL date-parts list as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). date_parts: list or tuple like [year, month, day] as integers. Also supports [year, month] and [year] for situations where the day or month-and-day are missing. fill: if True, set missing months to January and missing days to the first day of the month. View Source def date_parts_to_string ( date_parts , fill : bool = False ) -> Optional [ str ] : \"\"\" Return a CSL date-parts list as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). date_parts: list or tuple like [year, month, day] as integers. Also supports [year, month] and [year] for situations where the day or month-and-day are missing. fill: if True, set missing months to January and missing days to the first day of the month. \"\"\" if not date_parts : return None if not isinstance ( date_parts , ( tuple , list )) : raise ValueError ( f \"date_parts must be a tuple or list\" ) while fill and 1 <= len ( date_parts ) < 3 : date_parts . append ( 1 ) widths = 4 , 2 , 2 str_parts = [] for i , part in enumerate ( date_parts [ :3 ] ) : width = widths [ i ] if isinstance ( part , int ) : part = str ( part ) if not isinstance ( part , str ) : break part = part . zfill ( width ) if len ( part ) != width or not part . isdigit () : break str_parts . append ( part ) if not str_parts : return None iso_str = \"-\" . join ( str_parts ) return iso_str","title":"date_parts_to_string"},{"location":"reference/manubot/cite/csl_item/#date_to_date_parts","text":"def date_to_date_parts ( date ) -> Union [ List [ int ], NoneType ] Convert a date string or object to a date parts list. date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). View Source def date_to_date_parts ( date ) -> Optional [ List [ int ]]: \"\"\" Convert a date string or object to a date parts list. date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). \"\"\" import datetime if date is None : return None if isinstance ( date , ( datetime . date , datetime . datetime )): date = date . isoformat () if not isinstance ( date , str ): raise ValueError ( f \"date_to_date_parts: unsupported type for {date}\" ) date = date . strip () re_year = r \"(?P<year>[0-9]{4})\" re_month = r \"(?P<month>1[0-2]|0[1-9])\" re_day = r \"(?P<day>[0-3][0-9])\" patterns = [ f \"{re_year}-{re_month}-{re_day}\" , f \"{re_year}-{re_month}\" , f \"{re_year}\" , f \".*\" , # regex to match anything ] for pattern in patterns : match = re . match ( pattern , date ) if match : break date_parts = [] for part in \"year\" , \"month\" , \"day\" : try : value = match . group ( part ) except IndexError : break if not value : break date_parts . append ( int ( value )) if date_parts : return date_parts","title":"date_to_date_parts"},{"location":"reference/manubot/cite/csl_item/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/csl_item/#csl_item","text":"class CSL_Item ( dictionary = None , ** kwargs ) CSL_Item represents bibliographic information for a single citeable work. On a technical side CSL_Item is a Python dictionary with extra methods that help cleaning and manipulating it. These methods relate to: - adding an id key and value for CSL item - correcting bibliographic information and its structure - adding and reading a custom note to CSL item More information on CSL JSON (a list of CSL_Items) is available at: - https://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html - http://docs.citationstyles.org/en/1.0.1/specification.html#standard-variables - https://github.com/citation-style-language/schema/blob/master/csl-data.json View Source class CSL_Item ( dict ) : \"\"\" CSL_Item represents bibliographic information for a single citeable work. On a technical side CSL_Item is a Python dictionary with extra methods that help cleaning and manipulating it. These methods relate to: - adding an `id` key and value for CSL item - correcting bibliographic information and its structure - adding and reading a custom note to CSL item More information on CSL JSON (a list of CSL_Items) is available at: - https://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html - http://docs.citationstyles.org/en/1.0.1/specification.html#standard-variables - https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" # The ideas for CSL_Item methods come from the following parts of code : # - [ ] citekey_to_csl_item ( citekey , prune = True ) # The methods in CSL_Item class provide primitives to reconstruct # functions above . type_mapping = { \"journal-article\" : \"article-journal\" , \"book-chapter\" : \"chapter\" , \"posted-content\" : \"manuscript\" , \"proceedings-article\" : \"paper-conference\" , \"standard\" : \"entry\" , \"reference-entry\" : \"entry\" , } def __init__ ( self , dictionary = None , ** kwargs ) : \"\"\" Can use both a dictionary or keywords to create a CSL_Item object: CSL_Item(title='The Book') CSL_Item({'title': 'The Book'}) csl_dict = {'title': 'The Book', 'ISBN': '321-321'} CSL_Item(csl_dict, type='entry') CSL_Item(title='The Book', ISBN='321-321', type='entry') CSL_Item object is usually provided by bibliographic information API, but constructing small CSL_Item objects is useful for testing. \"\"\" if dictionary is None : dictionary = dict () super (). __init__ ( copy . deepcopy ( dictionary )) self . update ( copy . deepcopy ( kwargs )) def set_id ( self , id_ ) : self [ \"id\" ] = id_ return self def correct_invalid_type ( self ) : \"\"\" Correct invalid CSL item type. Does nothing if `type` not present. For detail see https://github.com/CrossRef/rest-api-doc/issues/187 \"\"\" if \"type\" in self : # Replace a type from in CSL_Item . type_mapping . keys (), # leave type intact in other cases . t = self [ \"type\" ] self [ \"type\" ] = self . type_mapping . get ( t , t ) return self def set_default_type ( self ) : \"\"\" Set type to 'entry', if type not specified. \"\"\" self [ \"type\" ] = self . get ( \"type\" , \"entry\" ) return self def prune_against_schema ( self ) : \"\"\" Remove fields that violate the CSL Item JSON Schema. \"\"\" from . citeproc import remove_jsonschema_errors ( csl_item ,) = remove_jsonschema_errors ( [ self ] , in_place = True ) assert csl_item is self return self def validate_against_schema ( self ) : \"\"\" Confirm that the CSL_Item validates. If not, raises a jsonschema.exceptions.ValidationError. \"\"\" from . citeproc import get_jsonschema_csl_validator validator = get_jsonschema_csl_validator () validator . validate ( [ self ] ) return self def clean ( self , prune : bool = True ) : \"\"\" Sanitize and touch-up a potentially dirty CSL_Item. The following steps are performed: - update incorrect values for \" type \" field when a correct variant is known - remove fields that violate the JSON Schema (if prune=True) - set default value for \" type \" if missing, since CSL JSON requires type - validate against the CSL JSON schema (if prune=True) to ensure output CSL_Item is clean \"\"\" logging . debug ( f \"Starting CSL_Item.clean with{'' if prune else 'out'}\" f \"CSL pruning for id: {self.get('id', 'id not specified')}\" ) self . correct_invalid_type () if prune : self . prune_against_schema () self . set_default_type () if prune : self . validate_against_schema () return self def set_date ( self , date , variable = \"issued\" ) : \"\"\" date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). variable: which variable to assign the date to. \"\"\" date_parts = date_to_date_parts ( date ) if date_parts : self [ variable ] = { \"date-parts\" : [ date_parts ] } return self def get_date ( self , variable = \"issued\" , fill = False ) : \"\"\" Return a CSL date-variable as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). variable: which CSL JSON date variable to retrieve fill: if True, set missing months to January and missing days to the first day of the month. \"\"\" try : date_parts = self [ variable ][ \"date-parts\" ][ 0 ] except ( IndexError , KeyError ) : return None return date_parts_to_string ( date_parts , fill = fill ) @property def note ( self ) -> str : \"\"\" Return the value of the \" note \" field as a string. If \" note \" key is not set, return empty string. \"\"\" return str ( self . get ( \"note\" ) or \"\" ) @note . setter def note ( self , text : str ) : if text : self [ \"note\" ] = text else : # if text is None or an empty string , remove the \"note\" field self . pop ( \"note\" , None ) @property def note_dict ( self ) -> dict : \"\"\" Return a dictionary with key-value pairs encoded by this CSL Item's note. Extracts both forms (line-entry and braced-entry) of key-value pairs from the CSL JSON \" cheater syntax \" https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields Assigning to this dict will not update `self[\" note \"]`. \"\"\" note = self . note line_matches = re . findall ( r \"^(?P<key>[A-Z]+|[-_a-z]+): *(?P<value>.+?) *$\" , note , re . MULTILINE ) braced_matches = re . findall ( r \"{:(?P<key>[A-Z]+|[-_a-z]+): *(?P<value>.+?) *}\" , note ) return dict ( line_matches + braced_matches ) def note_append_text ( self , text : str ) : \"\"\" Append text to the note field of a CSL Item. \"\"\" if not text : return note = self . note if note and not note . endswith ( \"\\n\" ) : note += \"\\n\" note += text self . note = note def note_append_dict ( self , dictionary : dict ) : \"\"\" Append key-value pairs specified by `dictionary` to the note field of a CSL Item. Uses the the [CSL JSON \" cheater syntax \"](https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields) to encode additional values not defined by the CSL JSON schema. \"\"\" for key , value in dictionary . items () : if not re . fullmatch ( r \"[A-Z]+|[-_a-z]+\" , key ) : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"it does not conform to the variable_name syntax as per https://git.io/fjTzW.\" ) continue if \"\\n\" in value : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"the value contains a newline: {value!r}\" ) continue self . note_append_text ( f \"{key}: {value}\" ) def infer_id ( self ) : \"\"\" Detect and set a non-null/empty value for \" id \" or else raise a ValueError. \"\"\" if self . get ( \"standard_citation\" ) : # \"standard_citation\" field is set with a non - null / empty value return self . set_id ( self . pop ( \"standard_citation\" )) if self . note_dict . get ( \"standard_id\" ) : # \"standard_id\" note field is set with a non - null / empty value return self . set_id ( self . note_dict [ \"standard_id\" ] ) if self . get ( \"id\" ) : # \"id\" field exists and is set with a non - null / empty value return self . set_id ( self [ \"id\" ] ) raise ValueError ( \"infer_id could not detect a field with a citation / standard_citation. \" 'Consider setting the CSL Item \"id\" field.' ) def standardize_id ( self ) : \"\"\" Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \" id \" field. The standard_id is extracted from a \" standard_citation \" field, the \" note \" field, or the \" id \" field. The extracted citation is checked for validity and standardized, after which it is the final \" standard_id \". Regarding csl_item modification, the csl_item \" id \" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id and original_id. Note that the Manubot software generally refers to the \" id \" of a CSL Item as a citekey. However, in this context, we use \" id \" rather than \" citekey \" for consistency with CSL's \" id \" field. \"\"\" original_id = self . get ( \"id\" ) self . infer_id () original_standard_id = self [ \"id\" ] citekey = CiteKey ( original_standard_id ) standard_id = citekey . standard_id add_to_note = {} note_dict = self . note_dict if original_id and original_id != standard_id : if original_id != note_dict . get ( \"original_id\" ) : add_to_note [ \"original_id\" ] = original_id if original_standard_id and original_standard_id != standard_id : if original_standard_id != note_dict . get ( \"original_standard_id\" ) : add_to_note [ \"original_standard_id\" ] = original_standard_id if standard_id != note_dict . get ( \"standard_id\" ) : add_to_note [ \"standard_id\" ] = standard_id self . note_append_dict ( dictionary = add_to_note ) self . set_id ( standard_id ) return self","title":"CSL_Item"},{"location":"reference/manubot/cite/csl_item/#ancestors-in-mro","text":"builtins.dict","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/csl_item/#descendants","text":"manubot.cite.arxiv.CSL_Item_arXiv","title":"Descendants"},{"location":"reference/manubot/cite/csl_item/#class-variables","text":"type_mapping","title":"Class variables"},{"location":"reference/manubot/cite/csl_item/#instance-variables","text":"note Return the value of the \"note\" field as a string. If \"note\" key is not set, return empty string. note_dict Return a dictionary with key-value pairs encoded by this CSL Item's note. Extracts both forms (line-entry and braced-entry) of key-value pairs from the CSL JSON \"cheater syntax\" https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields Assigning to this dict will not update self[\"note\"] .","title":"Instance variables"},{"location":"reference/manubot/cite/csl_item/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/csl_item/#clean","text":"def clean ( self , prune : bool = True ) Sanitize and touch-up a potentially dirty CSL_Item. The following steps are performed: - update incorrect values for \"type\" field when a correct variant is known - remove fields that violate the JSON Schema (if prune=True) - set default value for \"type\" if missing, since CSL JSON requires type - validate against the CSL JSON schema (if prune=True) to ensure output CSL_Item is clean View Source def clean ( self , prune : bool = True ): \"\"\" Sanitize and touch-up a potentially dirty CSL_Item. The following steps are performed: - update incorrect values for \" type \" field when a correct variant is known - remove fields that violate the JSON Schema (if prune=True) - set default value for \" type \" if missing, since CSL JSON requires type - validate against the CSL JSON schema (if prune=True) to ensure output CSL_Item is clean \"\"\" logging . debug ( f \"Starting CSL_Item.clean with{'' if prune else 'out'}\" f \"CSL pruning for id: {self.get('id', 'id not specified')}\" ) self . correct_invalid_type () if prune : self . prune_against_schema () self . set_default_type () if prune : self . validate_against_schema () return self","title":"clean"},{"location":"reference/manubot/cite/csl_item/#clear","text":"def clear ( ... ) D.clear() -> None. Remove all items from D.","title":"clear"},{"location":"reference/manubot/cite/csl_item/#copy","text":"def copy ( ... ) D.copy() -> a shallow copy of D","title":"copy"},{"location":"reference/manubot/cite/csl_item/#correct_invalid_type","text":"def correct_invalid_type ( self ) Correct invalid CSL item type. Does nothing if type not present. For detail see https://github.com/CrossRef/rest-api-doc/issues/187 View Source def correct_invalid_type ( self ): \"\"\" Correct invalid CSL item type. Does nothing if `type` not present. For detail see https://github.com/CrossRef/rest-api-doc/issues/187 \"\"\" if \"type\" in self : # Replace a type from in CSL_Item.type_mapping.keys(), # leave type intact in other cases. t = self [ \"type\" ] self [ \"type\" ] = self . type_mapping . get ( t , t ) return self","title":"correct_invalid_type"},{"location":"reference/manubot/cite/csl_item/#fromkeys","text":"def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value.","title":"fromkeys"},{"location":"reference/manubot/cite/csl_item/#get","text":"def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default.","title":"get"},{"location":"reference/manubot/cite/csl_item/#get_date","text":"def get_date ( self , variable = 'issued' , fill = False ) Return a CSL date-variable as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). variable: which CSL JSON date variable to retrieve fill: if True, set missing months to January and missing days to the first day of the month. View Source def get_date ( self , variable = \"issued\" , fill = False ) : \"\"\" Return a CSL date-variable as ISO formatted string: ('YYYY', 'YYYY-MM', 'YYYY-MM-DD', or None). variable: which CSL JSON date variable to retrieve fill: if True, set missing months to January and missing days to the first day of the month. \"\"\" try : date_parts = self [ variable ][ \"date-parts\" ][ 0 ] except ( IndexError , KeyError ) : return None return date_parts_to_string ( date_parts , fill = fill )","title":"get_date"},{"location":"reference/manubot/cite/csl_item/#infer_id","text":"def infer_id ( self ) Detect and set a non-null/empty value for \"id\" or else raise a ValueError. View Source def infer_id ( self ): \"\"\" Detect and set a non-null/empty value for \" id \" or else raise a ValueError. \"\"\" if self . get ( \"standard_citation\" ): # \"standard_citation\" field is set with a non - null / empty value return self . set_id ( self . pop ( \"standard_citation\" )) if self . note_dict . get ( \"standard_id\" ): # \"standard_id\" note field is set with a non - null / empty value return self . set_id ( self . note_dict [ \"standard_id\" ]) if self . get ( \"id\" ): # \"id\" field exists and is set with a non - null / empty value return self . set_id ( self [ \"id\" ]) raise ValueError ( \"infer_id could not detect a field with a citation / standard_citation. \" 'Consider setting the CSL Item \"id\" field.' )","title":"infer_id"},{"location":"reference/manubot/cite/csl_item/#items","text":"def items ( ... ) D.items() -> a set-like object providing a view on D's items","title":"items"},{"location":"reference/manubot/cite/csl_item/#keys","text":"def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys","title":"keys"},{"location":"reference/manubot/cite/csl_item/#note_append_dict","text":"def note_append_dict ( self , dictionary : dict ) Append key-value pairs specified by dictionary to the note field of a CSL Item. Uses the the CSL JSON \"cheater syntax\" to encode additional values not defined by the CSL JSON schema. View Source def note_append_dict ( self , dictionary : dict ): \"\"\" Append key-value pairs specified by `dictionary` to the note field of a CSL Item. Uses the the [CSL JSON \" cheater syntax \"](https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields) to encode additional values not defined by the CSL JSON schema. \"\"\" for key , value in dictionary . items (): if not re . fullmatch ( r \"[A-Z]+|[-_a-z]+\" , key ): logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"it does not conform to the variable_name syntax as per https://git.io/fjTzW.\" ) continue if \"\\n\" in value : logging . warning ( f \"note_append_dict: skipping adding {key!r} because \" f \"the value contains a newline: {value!r}\" ) continue self . note_append_text ( f \"{key}: {value}\" )","title":"note_append_dict"},{"location":"reference/manubot/cite/csl_item/#note_append_text","text":"def note_append_text ( self , text : str ) Append text to the note field of a CSL Item. View Source def note_append_text ( self , text : str ): \"\"\" Append text to the note field of a CSL Item. \"\"\" if not text : return note = self . note if note and not note . endswith ( \"\\n\" ): note += \"\\n\" note += text self . note = note","title":"note_append_text"},{"location":"reference/manubot/cite/csl_item/#pop","text":"def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised","title":"pop"},{"location":"reference/manubot/cite/csl_item/#popitem","text":"def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty.","title":"popitem"},{"location":"reference/manubot/cite/csl_item/#prune_against_schema","text":"def prune_against_schema ( self ) Remove fields that violate the CSL Item JSON Schema. View Source def prune_against_schema ( self ): \"\"\" Remove fields that violate the CSL Item JSON Schema. \"\"\" from .citeproc import remove_jsonschema_errors ( csl_item ,) = remove_jsonschema_errors ([ self ], in_place = True ) assert csl_item is self return self","title":"prune_against_schema"},{"location":"reference/manubot/cite/csl_item/#set_date","text":"def set_date ( self , date , variable = 'issued' ) date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). variable: which variable to assign the date to. View Source def set_date ( self , date , variable = \"issued\" ) : \"\"\" date: date either as a string (in the form YYYY, YYYY-MM, or YYYY-MM-DD) or as a Python date object (datetime.date or datetime.datetime). variable: which variable to assign the date to. \"\"\" date_parts = date_to_date_parts ( date ) if date_parts : self [ variable ] = { \"date-parts\" : [ date_parts ] } return self","title":"set_date"},{"location":"reference/manubot/cite/csl_item/#set_default_type","text":"def set_default_type ( self ) Set type to 'entry', if type not specified. View Source def set_default_type ( self ): \"\"\" Set type to 'entry', if type not specified. \"\"\" self [ \"type\" ] = self . get ( \"type\" , \"entry\" ) return self","title":"set_default_type"},{"location":"reference/manubot/cite/csl_item/#set_id","text":"def set_id ( self , id_ ) View Source def set_id ( self , id_ ): self [ \"id\" ] = id_ return self","title":"set_id"},{"location":"reference/manubot/cite/csl_item/#setdefault","text":"def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default.","title":"setdefault"},{"location":"reference/manubot/cite/csl_item/#standardize_id","text":"def standardize_id ( self ) Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \"id\" field. The standard_id is extracted from a \"standard_citation\" field, the \"note\" field, or the \"id\" field. The extracted citation is checked for validity and standardized, after which it is the final \"standard_id\". Regarding csl_item modification, the csl_item \"id\" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id and original_id. Note that the Manubot software generally refers to the \"id\" of a CSL Item as a citekey. However, in this context, we use \"id\" rather than \"citekey\" for consistency with CSL's \"id\" field. View Source def standardize_id ( self ): \"\"\" Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \" id \" field. The standard_id is extracted from a \" standard_citation \" field, the \" note \" field, or the \" id \" field. The extracted citation is checked for validity and standardized, after which it is the final \" standard_id \". Regarding csl_item modification, the csl_item \" id \" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id and original_id. Note that the Manubot software generally refers to the \" id \" of a CSL Item as a citekey. However, in this context, we use \" id \" rather than \" citekey \" for consistency with CSL's \" id \" field. \"\"\" original_id = self . get ( \"id\" ) self . infer_id () original_standard_id = self [ \"id\" ] citekey = CiteKey ( original_standard_id ) standard_id = citekey . standard_id add_to_note = {} note_dict = self . note_dict if original_id and original_id != standard_id : if original_id != note_dict . get ( \"original_id\" ): add_to_note [ \"original_id\" ] = original_id if original_standard_id and original_standard_id != standard_id : if original_standard_id != note_dict . get ( \"original_standard_id\" ): add_to_note [ \"original_standard_id\" ] = original_standard_id if standard_id != note_dict . get ( \"standard_id\" ): add_to_note [ \"standard_id\" ] = standard_id self . note_append_dict ( dictionary = add_to_note ) self . set_id ( standard_id ) return self","title":"standardize_id"},{"location":"reference/manubot/cite/csl_item/#update","text":"def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k]","title":"update"},{"location":"reference/manubot/cite/csl_item/#validate_against_schema","text":"def validate_against_schema ( self ) Confirm that the CSL_Item validates. If not, raises a jsonschema.exceptions.ValidationError. View Source def validate_against_schema ( self ): \"\"\" Confirm that the CSL_Item validates. If not, raises a jsonschema.exceptions.ValidationError. \"\"\" from .citeproc import get_jsonschema_csl_validator validator = get_jsonschema_csl_validator () validator . validate ([ self ]) return self","title":"validate_against_schema"},{"location":"reference/manubot/cite/csl_item/#values","text":"def values ( ... ) D.values() -> an object providing a view on D's values","title":"values"},{"location":"reference/manubot/cite/curie/","text":"Module manubot.cite.curie Compact Uniform Resource Identifiers Manubot keeps a local versions of the identifier.org registry. Repository developers can run the following commands to update the Manubot version. # regenerate manubot/cite/curie/namespaces.json python manubot/cite/curie/__init__.py # if namespaces.json has changed, the following test will likely fail: pytest manubot/cite/tests/test_handlers.py::test_prefix_to_handler # copy captured stdout from failed test_prefix_to_handler to # manubot.cite.handlers.prefix_to_handler. Then reformat file: black manubot/cite/handlers.py References: https://en.wikipedia.org/wiki/CURIE https://identifiers.org/ https://github.com/manubot/manubot/issues/218 https://docs.identifiers.org/articles/api.html https://n2t.net/e/compact_ids.html https://n2t.net/e/cdl_ebi_prefixes.yaml https://en.wikipedia.org/wiki/MIRIAM_Registry Identifiers.org and MIRIAM Registry: community resources to provide persistent identification On the road to robust data citation Uniform Resolution of Compact Identifiers for Biomedical Data View Source \"\"\" Compact Uniform Resource Identifiers Manubot keeps a local versions of the identifier.org registry. Repository developers can run the following commands to update the Manubot version. ```shell # regenerate manubot/cite/curie/namespaces.json python manubot/cite/curie/__init__.py # if namespaces.json has changed, the following test will likely fail: pytest manubot/cite/tests/test_handlers.py::test_prefix_to_handler # copy captured stdout from failed test_prefix_to_handler to # manubot.cite.handlers.prefix_to_handler. Then reformat file: black manubot/cite/handlers.py ``` References: - https://en.wikipedia.org/wiki/CURIE - https://identifiers.org/ - https://github.com/manubot/manubot/issues/218 - https://docs.identifiers.org/articles/api.html - https://n2t.net/e/compact_ids.html - https://n2t.net/e/cdl_ebi_prefixes.yaml - https://en.wikipedia.org/wiki/MIRIAM_Registry - [Identifiers.org and MIRIAM Registry: community resources to provide persistent identification](https://doi.org/10.1093/nar/gkr1097) - [On the road to robust data citation](https://doi.org/10.1038/sdata.2018.95) - [Uniform Resolution of Compact Identifiers for Biomedical Data](https://doi.org/10.1038/sdata.2018.29) \"\"\" import dataclasses import functools import json import logging import pathlib import re import typing from manubot . cite . handlers import Handler _keep_namespace_fields = { \"prefix\" , \"mirId\" , # MIRIAM Registry identifier \"name\" , \"pattern\" , # regex pattern \"description\" , \"sampleId\" , # example identifier \"namespaceEmbeddedInLui\" , # whether prefix is included in the local unique identifier \"curiePrefix\" , # a computed field for the actual prefix used by CURIEs including required capitalization } namespace_path = pathlib . Path ( __file__ ). parent . joinpath ( \"namespaces.json\" ) @dataclasses . dataclass class Handler_CURIE ( Handler ) : def __post_init__ ( self ) : prefix_to_namespace = get_prefix_to_namespace () self . namespace = prefix_to_namespace [ self.prefix_lower ] self . standard_prefix = self . namespace [ \"prefix\" ] self . prefixes = sorted ( { self . namespace [ \"prefix\" ] , self . namespace [ \"curiePrefix\" ] . lower () } ) self . accession_pattern = self . namespace [ \"pattern\" ] def get_csl_item ( self , citekey ) : from .. url import get_url_csl_item url = curie_to_url ( citekey . standard_id ) return get_url_csl_item ( url ) def get_curie_handlers () : \"\"\"Get all possible CURIE handlers\"\"\" namespaces = get_namespaces ( compile_patterns = True ) handlers = [ Handler_CURIE(ns[\"prefix\" ] ) for ns in namespaces ] return handlers def _download_namespaces () : \"\"\" Download all namespaces from the Identifiers.org Central Registry. Example of a single namespace JSON data at <https://registry.api.identifiers.org/restApi/namespaces/230> \"\"\" import requests params = dict ( size = 5000 , sort = \"prefix\" ) url = \"https://registry.api.identifiers.org/restApi/namespaces\" response = requests . get ( url , params ) response . raise_for_status () results = response . json () if results [ \"page\" ][ \"totalPages\" ] > 1 : logging . warning ( \"_download_curie_registry does not support multi-page results\\n\" f \"{response.url}\\n{json.dumps(results['page'])}\" ) namespaces = results [ \"_embedded\" ][ \"namespaces\" ] # filter namespace fields to reduce diskspace for namespace in namespaces : namespace [ \"curiePrefix\" ] = get_curie_prefix ( namespace ) for field in set ( namespace ) - _keep_namespace_fields : del namespace [ field ] json_text = json . dumps ( namespaces , indent = 2 , ensure_ascii = False ) namespace_path . write_text ( json_text + \"\\n\" , encoding = \"utf-8\" ) def get_curie_prefix ( namespace ) : \"\"\" The prefix portion of a CURIE is not always the same as the identifiers.org namespace prefix. This occurs when namespaceEmbeddedInLui is true. When namespaceEmbeddedInLui, CURIEs require a specific prefix capitalization. The actual prefix and capitalization is reverse engineered from the regex pattern. References: https://github.com/identifiers-org/identifiers-org.github.io/issues/100#issuecomment-614679142 \"\"\" if not namespace [ \"namespaceEmbeddedInLui\" ] : return namespace [ \"prefix\" ] import exrex example_curie = exrex . getone ( namespace [ \"pattern\" ] ) curie_prefix , _ = example_curie . split ( \":\" , 1 ) return curie_prefix def get_namespaces ( compile_patterns = False ) -> typing . List [ dict ] : with namespace_path . open ( encoding = \"utf-8-sig\" ) as read_file : namespaces = json . load ( read_file ) if compile_patterns : for namespace in namespaces : namespace [ \"compiled_pattern\" ] = re . compile ( namespace [ \"pattern\" ] ) return namespaces @functools . lru_cache () def get_prefix_to_namespace () -> typing . Dict [ str, typing.Dict ] : prefix_to_namespace = dict () for ns in get_namespaces () : for key in \"prefix\" , \"curiePrefix\" : prefix_to_namespace [ ns[key ] . lower () ] = ns return prefix_to_namespace def standardize_curie ( curie ) : \"\"\" Return CURIE with identifiers.org expected capitalization. `curie` should be in `prefix:accession` format. If `curie` is malformed or uses an unrecognized prefix, raise ValueError. \"\"\" if not isinstance ( curie , str ) : raise TypeError ( f \"curie parameter should be string. Received {curie.__class__.__name__} instead for {curie}\" ) try : prefix , accession = curie . split ( \":\" , 1 ) except ValueError : raise ValueError ( f \"curie must be splittable by `:` and formatted like `prefix:accession`. Received {curie}\" ) # do not yet understand capitalization # https : // github . com / identifiers - org / identifiers - org . github . io / issues / 100 prefix_lower = prefix . lower () prefix_to_namespaces = get_prefix_to_namespace () try : namespace = prefix_to_namespaces [ prefix_lower ] except KeyError : raise ValueError ( f \"prefix {prefix_lower} for {curie} is not a recognized prefix\" ) return f \"{namespace['curiePrefix']}:{accession}\" def curie_to_url ( curie ) : \"\"\" `curie` should be in `prefix:accession` format \"\"\" curie = standardize_curie ( curie ) resolver_url = \"https://identifiers.org\" return f \"{resolver_url}/{curie}\" if __name__ == \"__main__\" : _download_namespaces () namespaces = get_namespaces () Variables namespace_path Functions curie_to_url def curie_to_url ( curie ) curie should be in prefix:accession format View Source def curie_to_url ( curie ): \"\"\" `curie` should be in `prefix:accession` format \"\"\" curie = standardize_curie ( curie ) resolver_url = \"https://identifiers.org\" return f \"{resolver_url}/{curie}\" get_curie_handlers def get_curie_handlers ( ) Get all possible CURIE handlers View Source def get_curie_handlers (): \"\"\"Get all possible CURIE handlers\"\"\" namespaces = get_namespaces ( compile_patterns = True ) handlers = [ Handler_CURIE ( ns [ \"prefix\" ]) for ns in namespaces ] return handlers get_curie_prefix def get_curie_prefix ( namespace ) The prefix portion of a CURIE is not always the same as the identifiers.org namespace prefix. This occurs when namespaceEmbeddedInLui is true. When namespaceEmbeddedInLui, CURIEs require a specific prefix capitalization. The actual prefix and capitalization is reverse engineered from the regex pattern. References: https://github.com/identifiers-org/identifiers-org.github.io/issues/100#issuecomment-614679142 View Source def get_curie_prefix ( namespace ): \"\"\" The prefix portion of a CURIE is not always the same as the identifiers.org namespace prefix. This occurs when namespaceEmbeddedInLui is true. When namespaceEmbeddedInLui, CURIEs require a specific prefix capitalization. The actual prefix and capitalization is reverse engineered from the regex pattern. References: https://github.com/identifiers-org/identifiers-org.github.io/issues/100#issuecomment-614679142 \"\"\" if not namespace [ \"namespaceEmbeddedInLui\" ]: return namespace [ \"prefix\" ] import exrex example_curie = exrex . getone ( namespace [ \"pattern\" ]) curie_prefix , _ = example_curie . split ( \":\" , 1 ) return curie_prefix get_namespaces def get_namespaces ( compile_patterns = False ) -> List [ dict ] View Source def get_namespaces ( compile_patterns = False ) -> typing . List [ dict ] : with namespace_path . open ( encoding = \"utf-8-sig\" ) as read_file : namespaces = json . load ( read_file ) if compile_patterns : for namespace in namespaces : namespace [ \"compiled_pattern\" ] = re . compile ( namespace [ \"pattern\" ] ) return namespaces get_prefix_to_namespace def get_prefix_to_namespace ( ) -> Dict [ str , Dict ] View Source @functools . lru_cache () def get_prefix_to_namespace () -> typing . Dict [ str, typing.Dict ] : prefix_to_namespace = dict () for ns in get_namespaces () : for key in \"prefix\" , \"curiePrefix\" : prefix_to_namespace [ ns[key ] . lower () ] = ns return prefix_to_namespace standardize_curie def standardize_curie ( curie ) Return CURIE with identifiers.org expected capitalization. curie should be in prefix:accession format. If curie is malformed or uses an unrecognized prefix, raise ValueError. View Source def standardize_curie ( curie ): \"\"\" Return CURIE with identifiers.org expected capitalization. `curie` should be in `prefix:accession` format. If `curie` is malformed or uses an unrecognized prefix, raise ValueError. \"\"\" if not isinstance ( curie , str ): raise TypeError ( f \"curie parameter should be string. Received {curie.__class__.__name__} instead for {curie}\" ) try : prefix , accession = curie . split ( \":\" , 1 ) except ValueError : raise ValueError ( f \"curie must be splittable by `:` and formatted like `prefix:accession`. Received {curie}\" ) # do not yet understand capitalization # https://github.com/identifiers-org/identifiers-org.github.io/issues/100 prefix_lower = prefix . lower () prefix_to_namespaces = get_prefix_to_namespace () try : namespace = prefix_to_namespaces [ prefix_lower ] except KeyError : raise ValueError ( f \"prefix {prefix_lower} for {curie} is not a recognized prefix\" ) return f \"{namespace['curiePrefix']}:{accession}\" Classes Handler_CURIE class Handler_CURIE ( prefix_lower : str ) Handler_CURIE(prefix_lower: str) View Source class Handler_CURIE ( Handler ): def __post_init__ ( self ): prefix_to_namespace = get_prefix_to_namespace () self . namespace = prefix_to_namespace [ self . prefix_lower ] self . standard_prefix = self . namespace [ \"prefix\" ] self . prefixes = sorted ( { self . namespace [ \"prefix\" ], self . namespace [ \"curiePrefix\" ] . lower ()} ) self . accession_pattern = self . namespace [ \"pattern\" ] def get_csl_item ( self , citekey ): from ..url import get_url_csl_item url = curie_to_url ( citekey . standard_id ) return get_url_csl_item ( url ) Ancestors (in MRO) manubot.cite.handlers.Handler Class variables prefixes Methods get_csl_item def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ): from ..url import get_url_csl_item url = curie_to_url ( citekey . standard_id ) return get_url_csl_item ( url ) inspect def inspect ( self , citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey ): \"\"\" Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. \"\"\" pattern = self . _get_pattern ( \"accession_pattern\" ) if not pattern : return None if not pattern . fullmatch ( citekey . accession ): return f \"{citekey.accession} does not match regex {pattern.pattern}\" standardize_prefix_accession def standardize_prefix_accession ( self , accession ) Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession ): \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession","title":"Curie"},{"location":"reference/manubot/cite/curie/#module-manubotcitecurie","text":"Compact Uniform Resource Identifiers Manubot keeps a local versions of the identifier.org registry. Repository developers can run the following commands to update the Manubot version. # regenerate manubot/cite/curie/namespaces.json python manubot/cite/curie/__init__.py # if namespaces.json has changed, the following test will likely fail: pytest manubot/cite/tests/test_handlers.py::test_prefix_to_handler # copy captured stdout from failed test_prefix_to_handler to # manubot.cite.handlers.prefix_to_handler. Then reformat file: black manubot/cite/handlers.py References: https://en.wikipedia.org/wiki/CURIE https://identifiers.org/ https://github.com/manubot/manubot/issues/218 https://docs.identifiers.org/articles/api.html https://n2t.net/e/compact_ids.html https://n2t.net/e/cdl_ebi_prefixes.yaml https://en.wikipedia.org/wiki/MIRIAM_Registry Identifiers.org and MIRIAM Registry: community resources to provide persistent identification On the road to robust data citation Uniform Resolution of Compact Identifiers for Biomedical Data View Source \"\"\" Compact Uniform Resource Identifiers Manubot keeps a local versions of the identifier.org registry. Repository developers can run the following commands to update the Manubot version. ```shell # regenerate manubot/cite/curie/namespaces.json python manubot/cite/curie/__init__.py # if namespaces.json has changed, the following test will likely fail: pytest manubot/cite/tests/test_handlers.py::test_prefix_to_handler # copy captured stdout from failed test_prefix_to_handler to # manubot.cite.handlers.prefix_to_handler. Then reformat file: black manubot/cite/handlers.py ``` References: - https://en.wikipedia.org/wiki/CURIE - https://identifiers.org/ - https://github.com/manubot/manubot/issues/218 - https://docs.identifiers.org/articles/api.html - https://n2t.net/e/compact_ids.html - https://n2t.net/e/cdl_ebi_prefixes.yaml - https://en.wikipedia.org/wiki/MIRIAM_Registry - [Identifiers.org and MIRIAM Registry: community resources to provide persistent identification](https://doi.org/10.1093/nar/gkr1097) - [On the road to robust data citation](https://doi.org/10.1038/sdata.2018.95) - [Uniform Resolution of Compact Identifiers for Biomedical Data](https://doi.org/10.1038/sdata.2018.29) \"\"\" import dataclasses import functools import json import logging import pathlib import re import typing from manubot . cite . handlers import Handler _keep_namespace_fields = { \"prefix\" , \"mirId\" , # MIRIAM Registry identifier \"name\" , \"pattern\" , # regex pattern \"description\" , \"sampleId\" , # example identifier \"namespaceEmbeddedInLui\" , # whether prefix is included in the local unique identifier \"curiePrefix\" , # a computed field for the actual prefix used by CURIEs including required capitalization } namespace_path = pathlib . Path ( __file__ ). parent . joinpath ( \"namespaces.json\" ) @dataclasses . dataclass class Handler_CURIE ( Handler ) : def __post_init__ ( self ) : prefix_to_namespace = get_prefix_to_namespace () self . namespace = prefix_to_namespace [ self.prefix_lower ] self . standard_prefix = self . namespace [ \"prefix\" ] self . prefixes = sorted ( { self . namespace [ \"prefix\" ] , self . namespace [ \"curiePrefix\" ] . lower () } ) self . accession_pattern = self . namespace [ \"pattern\" ] def get_csl_item ( self , citekey ) : from .. url import get_url_csl_item url = curie_to_url ( citekey . standard_id ) return get_url_csl_item ( url ) def get_curie_handlers () : \"\"\"Get all possible CURIE handlers\"\"\" namespaces = get_namespaces ( compile_patterns = True ) handlers = [ Handler_CURIE(ns[\"prefix\" ] ) for ns in namespaces ] return handlers def _download_namespaces () : \"\"\" Download all namespaces from the Identifiers.org Central Registry. Example of a single namespace JSON data at <https://registry.api.identifiers.org/restApi/namespaces/230> \"\"\" import requests params = dict ( size = 5000 , sort = \"prefix\" ) url = \"https://registry.api.identifiers.org/restApi/namespaces\" response = requests . get ( url , params ) response . raise_for_status () results = response . json () if results [ \"page\" ][ \"totalPages\" ] > 1 : logging . warning ( \"_download_curie_registry does not support multi-page results\\n\" f \"{response.url}\\n{json.dumps(results['page'])}\" ) namespaces = results [ \"_embedded\" ][ \"namespaces\" ] # filter namespace fields to reduce diskspace for namespace in namespaces : namespace [ \"curiePrefix\" ] = get_curie_prefix ( namespace ) for field in set ( namespace ) - _keep_namespace_fields : del namespace [ field ] json_text = json . dumps ( namespaces , indent = 2 , ensure_ascii = False ) namespace_path . write_text ( json_text + \"\\n\" , encoding = \"utf-8\" ) def get_curie_prefix ( namespace ) : \"\"\" The prefix portion of a CURIE is not always the same as the identifiers.org namespace prefix. This occurs when namespaceEmbeddedInLui is true. When namespaceEmbeddedInLui, CURIEs require a specific prefix capitalization. The actual prefix and capitalization is reverse engineered from the regex pattern. References: https://github.com/identifiers-org/identifiers-org.github.io/issues/100#issuecomment-614679142 \"\"\" if not namespace [ \"namespaceEmbeddedInLui\" ] : return namespace [ \"prefix\" ] import exrex example_curie = exrex . getone ( namespace [ \"pattern\" ] ) curie_prefix , _ = example_curie . split ( \":\" , 1 ) return curie_prefix def get_namespaces ( compile_patterns = False ) -> typing . List [ dict ] : with namespace_path . open ( encoding = \"utf-8-sig\" ) as read_file : namespaces = json . load ( read_file ) if compile_patterns : for namespace in namespaces : namespace [ \"compiled_pattern\" ] = re . compile ( namespace [ \"pattern\" ] ) return namespaces @functools . lru_cache () def get_prefix_to_namespace () -> typing . Dict [ str, typing.Dict ] : prefix_to_namespace = dict () for ns in get_namespaces () : for key in \"prefix\" , \"curiePrefix\" : prefix_to_namespace [ ns[key ] . lower () ] = ns return prefix_to_namespace def standardize_curie ( curie ) : \"\"\" Return CURIE with identifiers.org expected capitalization. `curie` should be in `prefix:accession` format. If `curie` is malformed or uses an unrecognized prefix, raise ValueError. \"\"\" if not isinstance ( curie , str ) : raise TypeError ( f \"curie parameter should be string. Received {curie.__class__.__name__} instead for {curie}\" ) try : prefix , accession = curie . split ( \":\" , 1 ) except ValueError : raise ValueError ( f \"curie must be splittable by `:` and formatted like `prefix:accession`. Received {curie}\" ) # do not yet understand capitalization # https : // github . com / identifiers - org / identifiers - org . github . io / issues / 100 prefix_lower = prefix . lower () prefix_to_namespaces = get_prefix_to_namespace () try : namespace = prefix_to_namespaces [ prefix_lower ] except KeyError : raise ValueError ( f \"prefix {prefix_lower} for {curie} is not a recognized prefix\" ) return f \"{namespace['curiePrefix']}:{accession}\" def curie_to_url ( curie ) : \"\"\" `curie` should be in `prefix:accession` format \"\"\" curie = standardize_curie ( curie ) resolver_url = \"https://identifiers.org\" return f \"{resolver_url}/{curie}\" if __name__ == \"__main__\" : _download_namespaces () namespaces = get_namespaces ()","title":"Module manubot.cite.curie"},{"location":"reference/manubot/cite/curie/#variables","text":"namespace_path","title":"Variables"},{"location":"reference/manubot/cite/curie/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/curie/#curie_to_url","text":"def curie_to_url ( curie ) curie should be in prefix:accession format View Source def curie_to_url ( curie ): \"\"\" `curie` should be in `prefix:accession` format \"\"\" curie = standardize_curie ( curie ) resolver_url = \"https://identifiers.org\" return f \"{resolver_url}/{curie}\"","title":"curie_to_url"},{"location":"reference/manubot/cite/curie/#get_curie_handlers","text":"def get_curie_handlers ( ) Get all possible CURIE handlers View Source def get_curie_handlers (): \"\"\"Get all possible CURIE handlers\"\"\" namespaces = get_namespaces ( compile_patterns = True ) handlers = [ Handler_CURIE ( ns [ \"prefix\" ]) for ns in namespaces ] return handlers","title":"get_curie_handlers"},{"location":"reference/manubot/cite/curie/#get_curie_prefix","text":"def get_curie_prefix ( namespace ) The prefix portion of a CURIE is not always the same as the identifiers.org namespace prefix. This occurs when namespaceEmbeddedInLui is true. When namespaceEmbeddedInLui, CURIEs require a specific prefix capitalization. The actual prefix and capitalization is reverse engineered from the regex pattern. References: https://github.com/identifiers-org/identifiers-org.github.io/issues/100#issuecomment-614679142 View Source def get_curie_prefix ( namespace ): \"\"\" The prefix portion of a CURIE is not always the same as the identifiers.org namespace prefix. This occurs when namespaceEmbeddedInLui is true. When namespaceEmbeddedInLui, CURIEs require a specific prefix capitalization. The actual prefix and capitalization is reverse engineered from the regex pattern. References: https://github.com/identifiers-org/identifiers-org.github.io/issues/100#issuecomment-614679142 \"\"\" if not namespace [ \"namespaceEmbeddedInLui\" ]: return namespace [ \"prefix\" ] import exrex example_curie = exrex . getone ( namespace [ \"pattern\" ]) curie_prefix , _ = example_curie . split ( \":\" , 1 ) return curie_prefix","title":"get_curie_prefix"},{"location":"reference/manubot/cite/curie/#get_namespaces","text":"def get_namespaces ( compile_patterns = False ) -> List [ dict ] View Source def get_namespaces ( compile_patterns = False ) -> typing . List [ dict ] : with namespace_path . open ( encoding = \"utf-8-sig\" ) as read_file : namespaces = json . load ( read_file ) if compile_patterns : for namespace in namespaces : namespace [ \"compiled_pattern\" ] = re . compile ( namespace [ \"pattern\" ] ) return namespaces","title":"get_namespaces"},{"location":"reference/manubot/cite/curie/#get_prefix_to_namespace","text":"def get_prefix_to_namespace ( ) -> Dict [ str , Dict ] View Source @functools . lru_cache () def get_prefix_to_namespace () -> typing . Dict [ str, typing.Dict ] : prefix_to_namespace = dict () for ns in get_namespaces () : for key in \"prefix\" , \"curiePrefix\" : prefix_to_namespace [ ns[key ] . lower () ] = ns return prefix_to_namespace","title":"get_prefix_to_namespace"},{"location":"reference/manubot/cite/curie/#standardize_curie","text":"def standardize_curie ( curie ) Return CURIE with identifiers.org expected capitalization. curie should be in prefix:accession format. If curie is malformed or uses an unrecognized prefix, raise ValueError. View Source def standardize_curie ( curie ): \"\"\" Return CURIE with identifiers.org expected capitalization. `curie` should be in `prefix:accession` format. If `curie` is malformed or uses an unrecognized prefix, raise ValueError. \"\"\" if not isinstance ( curie , str ): raise TypeError ( f \"curie parameter should be string. Received {curie.__class__.__name__} instead for {curie}\" ) try : prefix , accession = curie . split ( \":\" , 1 ) except ValueError : raise ValueError ( f \"curie must be splittable by `:` and formatted like `prefix:accession`. Received {curie}\" ) # do not yet understand capitalization # https://github.com/identifiers-org/identifiers-org.github.io/issues/100 prefix_lower = prefix . lower () prefix_to_namespaces = get_prefix_to_namespace () try : namespace = prefix_to_namespaces [ prefix_lower ] except KeyError : raise ValueError ( f \"prefix {prefix_lower} for {curie} is not a recognized prefix\" ) return f \"{namespace['curiePrefix']}:{accession}\"","title":"standardize_curie"},{"location":"reference/manubot/cite/curie/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/curie/#handler_curie","text":"class Handler_CURIE ( prefix_lower : str ) Handler_CURIE(prefix_lower: str) View Source class Handler_CURIE ( Handler ): def __post_init__ ( self ): prefix_to_namespace = get_prefix_to_namespace () self . namespace = prefix_to_namespace [ self . prefix_lower ] self . standard_prefix = self . namespace [ \"prefix\" ] self . prefixes = sorted ( { self . namespace [ \"prefix\" ], self . namespace [ \"curiePrefix\" ] . lower ()} ) self . accession_pattern = self . namespace [ \"pattern\" ] def get_csl_item ( self , citekey ): from ..url import get_url_csl_item url = curie_to_url ( citekey . standard_id ) return get_url_csl_item ( url )","title":"Handler_CURIE"},{"location":"reference/manubot/cite/curie/#ancestors-in-mro","text":"manubot.cite.handlers.Handler","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/curie/#class-variables","text":"prefixes","title":"Class variables"},{"location":"reference/manubot/cite/curie/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/curie/#get_csl_item","text":"def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ): from ..url import get_url_csl_item url = curie_to_url ( citekey . standard_id ) return get_url_csl_item ( url )","title":"get_csl_item"},{"location":"reference/manubot/cite/curie/#inspect","text":"def inspect ( self , citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey ): \"\"\" Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. \"\"\" pattern = self . _get_pattern ( \"accession_pattern\" ) if not pattern : return None if not pattern . fullmatch ( citekey . accession ): return f \"{citekey.accession} does not match regex {pattern.pattern}\"","title":"inspect"},{"location":"reference/manubot/cite/curie/#standardize_prefix_accession","text":"def standardize_prefix_accession ( self , accession ) Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession ): \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession","title":"standardize_prefix_accession"},{"location":"reference/manubot/cite/doi/","text":"Module manubot.cite.doi View Source import json import logging import urllib.parse import urllib.request import requests from manubot.util import get_manubot_user_agent from .handlers import Handler from .pubmed import get_pubmed_ids_for_doi class Handler_DOI ( Handler ): standard_prefix = \"doi\" prefixes = [ \"doi\" , \"shortdoi\" , ] accession_pattern = r \"10\\.[0-9]{4,9}/\\S+\" shortdoi_pattern = r \"10/[a-zA-Z0-9]+\" def inspect ( self , citekey ): identifier = citekey . accession if identifier . startswith ( \"10.\" ): # https://www.crossref.org/blog/dois-and-matching-regular-expressions/ if not self . _get_pattern () . fullmatch ( identifier ): return ( \"Identifier does not conform to the DOI regex. \" \"Double check the DOI.\" ) elif identifier . startswith ( \"10/\" ): # shortDOI, see http://shortdoi.org if not self . _get_pattern ( \"shortdoi_pattern\" ) . fullmatch ( identifier ): return ( \"Identifier does not conform to the shortDOI regex. \" \"Double check the shortDOI.\" ) else : return \"DOIs must start with '10.' (or '10/' for shortDOIs).\" def standardize_prefix_accession ( self , accession ): if accession . startswith ( \"10/\" ): try : accession = expand_short_doi ( accession ) except Exception as error : # If DOI shortening fails, return the unshortened DOI. # DOI metadata lookup will eventually fail somewhere with # appropriate error handling, as opposed to here. logging . error ( f \"Error in expand_short_doi for {accession} \" f \"due to a {error.__class__.__name__}: \\n {error}\" ) logging . info ( error , exc_info = True ) accession = accession . lower () return self . standard_prefix , accession def get_csl_item ( self , citekey ): return get_doi_csl_item ( citekey . standard_accession ) def expand_short_doi ( short_doi ): \"\"\" Convert a shortDOI to a regular DOI. \"\"\" if not short_doi . startswith ( \"10/\" ): raise ValueError ( f \"shortDOIs start with `10/`, but expand_short_doi received: {short_doi}\" ) url = f \"https://doi.org/api/handles/{short_doi.lower()}\" params = { \"type\" : \"HS_ALIAS\" } response = requests . get ( url , params = params ) # response documentation at https://www.handle.net/proxy_servlet.html results = response . json () response_code = results . get ( \"responseCode\" ) # Handle protocol response code if response_code == 100 : raise ValueError ( f \"Handle not found. Double check short_doi: {short_doi}\" ) if response_code == 200 : raise ValueError ( f \"HS_ALIAS values not found. Double check short_doi: {short_doi}\" ) if response_code != 1 : raise ValueError ( f \"Error response code of {response_code} returned by {response.url}\" ) values = results . get ( \"values\" , []) for value in values : if value . get ( \"type\" ) == \"HS_ALIAS\" : doi = value [ \"data\" ][ \"value\" ] return doi . lower () raise RuntimeError ( f 'HS_ALIAS value not found by expand_short_doi(\"{short_doi}\") \\n ' f \"The following JSON was retrieved from {response.url}: \\n \" + json . dumps ( results , indent = 2 ) ) def get_short_doi_url ( doi ): \"\"\" Get the shortDOI URL for a DOI. \"\"\" quoted_doi = urllib . request . quote ( doi ) url = \"http://shortdoi.org/{}?format=json\" . format ( quoted_doi ) headers = { \"User-Agent\" : get_manubot_user_agent ()} try : response = requests . get ( url , headers = headers ) . json () short_doi = response [ \"ShortDOI\" ] short_url = \"https://doi.org/\" + short_doi [ 3 :] # Remove \"10/\" prefix return short_url except Exception : logging . warning ( f \"shortDOI lookup failed for {doi}\" , exc_info = True ) return None \"\"\" Base URL to use for content negotiation. Options include: 1. <https://data.crosscite.org> documented at <https://support.datacite.org/docs/datacite-content-resolver> 2. <https://doi.org/> documented at <https://citation.crosscite.org/docs.html> \"\"\" content_negotiation_url = \"https://data.crosscite.org\" def get_doi_csl_item_crosscite ( doi ): \"\"\" Use Content Negotioation to retrieve the CSL Item metadata for a DOI. \"\"\" url = urllib . parse . urljoin ( content_negotiation_url , urllib . request . quote ( doi )) header = { \"Accept\" : \"application/vnd.citationstyles.csl+json\" , \"User-Agent\" : get_manubot_user_agent (), } response = requests . get ( url , headers = header ) try : return response . json () except Exception as error : logging . error ( f \"Error fetching metadata for doi:{doi}. \\n \" f \"Invalid response from {response.url}: \\n {response.text}\" ) raise error def get_doi_csl_item_zotero ( doi ): \"\"\" Generate CSL JSON Data for a DOI using Zotero's translation-server. \"\"\" from manubot.cite.zotero import get_csl_item return get_csl_item ( f \"doi:{doi}\" ) def augment_get_doi_csl_item ( function ): \"\"\" Decorator providing edits to the csl_item returned by a get_doi_csl_item_* function. \"\"\" def wrapper ( doi : str ): doi = doi . lower () csl_item = function ( doi ) csl_item [ \"DOI\" ] = doi csl_item [ \"URL\" ] = f \"https://doi.org/{doi}\" short_doi_url = get_short_doi_url ( doi ) if short_doi_url : csl_item [ \"URL\" ] = short_doi_url try : csl_item . update ( get_pubmed_ids_for_doi ( doi )) except Exception : logging . warning ( f \"Error calling get_pubmed_ids_for_doi for {doi}\" , exc_info = True ) return csl_item return wrapper @augment_get_doi_csl_item def get_doi_csl_item ( doi ): \"\"\" Generate CSL JSON Data for an DOI. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `doi_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\"\" # FIXME: this function is repetitive with other get_*_csl_item functions. for retriever in doi_retrievers : try : return retriever ( doi ) except Exception as error : logging . warning ( f \"Error in {retriever.__name__} for {doi} \" f \"due to a {error.__class__.__name__}: \\n {error}\" ) logging . info ( error , exc_info = True ) raise Exception ( f \"all get_doi_csl_item methods failed for {doi}\" ) doi_retrievers = [ get_doi_csl_item_crosscite , get_doi_csl_item_zotero ] Variables content_negotiation_url doi_retrievers Functions augment_get_doi_csl_item def augment_get_doi_csl_item ( function ) Decorator providing edits to the csl_item returned by a get_doi_csl_item_* function. View Source def augment_get_doi_csl_item ( function ): \"\"\" Decorator providing edits to the csl_item returned by a get_doi_csl_item_* function. \"\"\" def wrapper ( doi : str ): doi = doi . lower () csl_item = function ( doi ) csl_item [ \"DOI\" ] = doi csl_item [ \"URL\" ] = f \"https://doi.org/{doi}\" short_doi_url = get_short_doi_url ( doi ) if short_doi_url : csl_item [ \"URL\" ] = short_doi_url try : csl_item . update ( get_pubmed_ids_for_doi ( doi )) except Exception : logging . warning ( f \"Error calling get_pubmed_ids_for_doi for {doi}\" , exc_info = True ) return csl_item return wrapper expand_short_doi def expand_short_doi ( short_doi ) Convert a shortDOI to a regular DOI. View Source def expand_short_doi ( short_doi ): \"\"\" Convert a shortDOI to a regular DOI. \"\"\" if not short_doi . startswith ( \"10/\" ): raise ValueError ( f \"shortDOIs start with `10/`, but expand_short_doi received: {short_doi}\" ) url = f \"https://doi.org/api/handles/{short_doi.lower()}\" params = { \"type\" : \"HS_ALIAS\" } response = requests . get ( url , params = params ) # response documentation at https : // www . handle . net / proxy_servlet . html results = response . json () response_code = results . get ( \"responseCode\" ) # Handle protocol response code if response_code == 100 : raise ValueError ( f \"Handle not found. Double check short_doi: {short_doi}\" ) if response_code == 200 : raise ValueError ( f \"HS_ALIAS values not found. Double check short_doi: {short_doi}\" ) if response_code != 1 : raise ValueError ( f \"Error response code of {response_code} returned by {response.url}\" ) values = results . get ( \"values\" , []) for value in values : if value . get ( \"type\" ) == \"HS_ALIAS\" : doi = value [ \"data\" ][ \"value\" ] return doi . lower () raise RuntimeError ( f 'HS_ALIAS value not found by expand_short_doi(\"{short_doi}\")\\n' f \"The following JSON was retrieved from {response.url}:\\n\" + json . dumps ( results , indent = 2 ) ) get_doi_csl_item def get_doi_csl_item ( doi : str ) View Source def wrapper ( doi : str ): doi = doi . lower () csl_item = function ( doi ) csl_item [ \"DOI\" ] = doi csl_item [ \"URL\" ] = f \"https://doi.org/{doi}\" short_doi_url = get_short_doi_url ( doi ) if short_doi_url : csl_item [ \"URL\" ] = short_doi_url try : csl_item . update ( get_pubmed_ids_for_doi ( doi )) except Exception : logging . warning ( f \"Error calling get_pubmed_ids_for_doi for {doi}\" , exc_info = True ) return csl_item get_doi_csl_item_crosscite def get_doi_csl_item_crosscite ( doi ) Use Content Negotioation to retrieve the CSL Item metadata for a DOI. View Source def get_doi_csl_item_crosscite ( doi ): \"\"\" Use Content Negotioation to retrieve the CSL Item metadata for a DOI. \"\"\" url = urllib . parse . urljoin ( content_negotiation_url , urllib . request . quote ( doi )) header = { \"Accept\" : \"application/vnd.citationstyles.csl+json\" , \"User-Agent\" : get_manubot_user_agent (), } response = requests . get ( url , headers = header ) try : return response . json () except Exception as error : logging . error ( f \"Error fetching metadata for doi:{doi}.\\n\" f \"Invalid response from {response.url}:\\n{response.text}\" ) raise error get_doi_csl_item_zotero def get_doi_csl_item_zotero ( doi ) Generate CSL JSON Data for a DOI using Zotero's translation-server. View Source def get_doi_csl_item_zotero ( doi ): \"\"\" Generate CSL JSON Data for a DOI using Zotero's translation-server. \"\"\" from manubot.cite.zotero import get_csl_item return get_csl_item ( f \"doi:{doi}\" ) get_short_doi_url def get_short_doi_url ( doi ) Get the shortDOI URL for a DOI. View Source def get_short_doi_url ( doi ): \"\"\" Get the shortDOI URL for a DOI. \"\"\" quoted_doi = urllib . request . quote ( doi ) url = \"http://shortdoi.org/{}?format=json\" . format ( quoted_doi ) headers = { \"User-Agent\" : get_manubot_user_agent () } try : response = requests . get ( url , headers = headers ). json () short_doi = response [ \"ShortDOI\" ] short_url = \"https://doi.org/\" + short_doi [ 3 :] # Remove \"10/\" prefix return short_url except Exception : logging . warning ( f \"shortDOI lookup failed for {doi}\" , exc_info = True ) return None Classes Handler_DOI class Handler_DOI ( prefix_lower : str ) A Handler is a class that provides support for a certain type of citekey. For example, a Handler subclass could provide support for DOI citekeys. Subclasses enable custom logic for different citekey prefixes, including how to standardize the citekey and how to retrieve CSL Item metadata. View Source class Handler_DOI ( Handler ): standard_prefix = \"doi\" prefixes = [ \"doi\" , \"shortdoi\" , ] accession_pattern = r \"10\\.[0-9]{4,9}/\\S+\" shortdoi_pattern = r \"10/[a-zA-Z0-9]+\" def inspect ( self , citekey ): identifier = citekey . accession if identifier . startswith ( \"10.\" ): # https://www.crossref.org/blog/dois-and-matching-regular-expressions/ if not self . _get_pattern (). fullmatch ( identifier ): return ( \"Identifier does not conform to the DOI regex. \" \"Double check the DOI.\" ) elif identifier . startswith ( \"10/\" ): # shortDOI, see http://shortdoi.org if not self . _get_pattern ( \"shortdoi_pattern\" ). fullmatch ( identifier ): return ( \"Identifier does not conform to the shortDOI regex. \" \"Double check the shortDOI.\" ) else: return \"DOIs must start with '10.' (or '10/' for shortDOIs).\" def standardize_prefix_accession ( self , accession ): if accession . startswith ( \"10/\" ): try: accession = expand_short_doi ( accession ) except Exception as error: # If DOI shortening fails, return the unshortened DOI. # DOI metadata lookup will eventually fail somewhere with # appropriate error handling, as opposed to here. logging . error ( f \"Error in expand_short_doi for {accession} \" f \"due to a {error.__class__.__name__}:\\n{error}\" ) logging . info ( error , exc_info = True ) accession = accession . lower () return self . standard_prefix , accession def get_csl_item ( self , citekey ): return get_doi_csl_item ( citekey . standard_accession ) Ancestors (in MRO) manubot.cite.handlers.Handler Class variables accession_pattern prefixes shortdoi_pattern standard_prefix Methods get_csl_item def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ): return get_doi_csl_item ( citekey . standard_accession ) inspect def inspect ( self , citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey ): identifier = citekey . accession if identifier . startswith ( \"10.\" ): # https : // www . crossref . org / blog / dois - and - matching - regular - expressions / if not self . _get_pattern (). fullmatch ( identifier ): return ( \"Identifier does not conform to the DOI regex. \" \"Double check the DOI.\" ) elif identifier . startswith ( \"10/\" ): # shortDOI , see http : // shortdoi . org if not self . _get_pattern ( \"shortdoi_pattern\" ). fullmatch ( identifier ): return ( \"Identifier does not conform to the shortDOI regex. \" \"Double check the shortDOI.\" ) else : return \"DOIs must start with '10.' (or '10/' for shortDOIs).\" standardize_prefix_accession def standardize_prefix_accession ( self , accession ) Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession ): if accession . startswith ( \"10/\" ): try : accession = expand_short_doi ( accession ) except Exception as error : # If DOI shortening fails , return the unshortened DOI . # DOI metadata lookup will eventually fail somewhere with # appropriate error handling , as opposed to here . logging . error ( f \"Error in expand_short_doi for {accession} \" f \"due to a {error.__class__.__name__}:\\n{error}\" ) logging . info ( error , exc_info = True ) accession = accession . lower () return self . standard_prefix , accession","title":"Doi"},{"location":"reference/manubot/cite/doi/#module-manubotcitedoi","text":"View Source import json import logging import urllib.parse import urllib.request import requests from manubot.util import get_manubot_user_agent from .handlers import Handler from .pubmed import get_pubmed_ids_for_doi class Handler_DOI ( Handler ): standard_prefix = \"doi\" prefixes = [ \"doi\" , \"shortdoi\" , ] accession_pattern = r \"10\\.[0-9]{4,9}/\\S+\" shortdoi_pattern = r \"10/[a-zA-Z0-9]+\" def inspect ( self , citekey ): identifier = citekey . accession if identifier . startswith ( \"10.\" ): # https://www.crossref.org/blog/dois-and-matching-regular-expressions/ if not self . _get_pattern () . fullmatch ( identifier ): return ( \"Identifier does not conform to the DOI regex. \" \"Double check the DOI.\" ) elif identifier . startswith ( \"10/\" ): # shortDOI, see http://shortdoi.org if not self . _get_pattern ( \"shortdoi_pattern\" ) . fullmatch ( identifier ): return ( \"Identifier does not conform to the shortDOI regex. \" \"Double check the shortDOI.\" ) else : return \"DOIs must start with '10.' (or '10/' for shortDOIs).\" def standardize_prefix_accession ( self , accession ): if accession . startswith ( \"10/\" ): try : accession = expand_short_doi ( accession ) except Exception as error : # If DOI shortening fails, return the unshortened DOI. # DOI metadata lookup will eventually fail somewhere with # appropriate error handling, as opposed to here. logging . error ( f \"Error in expand_short_doi for {accession} \" f \"due to a {error.__class__.__name__}: \\n {error}\" ) logging . info ( error , exc_info = True ) accession = accession . lower () return self . standard_prefix , accession def get_csl_item ( self , citekey ): return get_doi_csl_item ( citekey . standard_accession ) def expand_short_doi ( short_doi ): \"\"\" Convert a shortDOI to a regular DOI. \"\"\" if not short_doi . startswith ( \"10/\" ): raise ValueError ( f \"shortDOIs start with `10/`, but expand_short_doi received: {short_doi}\" ) url = f \"https://doi.org/api/handles/{short_doi.lower()}\" params = { \"type\" : \"HS_ALIAS\" } response = requests . get ( url , params = params ) # response documentation at https://www.handle.net/proxy_servlet.html results = response . json () response_code = results . get ( \"responseCode\" ) # Handle protocol response code if response_code == 100 : raise ValueError ( f \"Handle not found. Double check short_doi: {short_doi}\" ) if response_code == 200 : raise ValueError ( f \"HS_ALIAS values not found. Double check short_doi: {short_doi}\" ) if response_code != 1 : raise ValueError ( f \"Error response code of {response_code} returned by {response.url}\" ) values = results . get ( \"values\" , []) for value in values : if value . get ( \"type\" ) == \"HS_ALIAS\" : doi = value [ \"data\" ][ \"value\" ] return doi . lower () raise RuntimeError ( f 'HS_ALIAS value not found by expand_short_doi(\"{short_doi}\") \\n ' f \"The following JSON was retrieved from {response.url}: \\n \" + json . dumps ( results , indent = 2 ) ) def get_short_doi_url ( doi ): \"\"\" Get the shortDOI URL for a DOI. \"\"\" quoted_doi = urllib . request . quote ( doi ) url = \"http://shortdoi.org/{}?format=json\" . format ( quoted_doi ) headers = { \"User-Agent\" : get_manubot_user_agent ()} try : response = requests . get ( url , headers = headers ) . json () short_doi = response [ \"ShortDOI\" ] short_url = \"https://doi.org/\" + short_doi [ 3 :] # Remove \"10/\" prefix return short_url except Exception : logging . warning ( f \"shortDOI lookup failed for {doi}\" , exc_info = True ) return None \"\"\" Base URL to use for content negotiation. Options include: 1. <https://data.crosscite.org> documented at <https://support.datacite.org/docs/datacite-content-resolver> 2. <https://doi.org/> documented at <https://citation.crosscite.org/docs.html> \"\"\" content_negotiation_url = \"https://data.crosscite.org\" def get_doi_csl_item_crosscite ( doi ): \"\"\" Use Content Negotioation to retrieve the CSL Item metadata for a DOI. \"\"\" url = urllib . parse . urljoin ( content_negotiation_url , urllib . request . quote ( doi )) header = { \"Accept\" : \"application/vnd.citationstyles.csl+json\" , \"User-Agent\" : get_manubot_user_agent (), } response = requests . get ( url , headers = header ) try : return response . json () except Exception as error : logging . error ( f \"Error fetching metadata for doi:{doi}. \\n \" f \"Invalid response from {response.url}: \\n {response.text}\" ) raise error def get_doi_csl_item_zotero ( doi ): \"\"\" Generate CSL JSON Data for a DOI using Zotero's translation-server. \"\"\" from manubot.cite.zotero import get_csl_item return get_csl_item ( f \"doi:{doi}\" ) def augment_get_doi_csl_item ( function ): \"\"\" Decorator providing edits to the csl_item returned by a get_doi_csl_item_* function. \"\"\" def wrapper ( doi : str ): doi = doi . lower () csl_item = function ( doi ) csl_item [ \"DOI\" ] = doi csl_item [ \"URL\" ] = f \"https://doi.org/{doi}\" short_doi_url = get_short_doi_url ( doi ) if short_doi_url : csl_item [ \"URL\" ] = short_doi_url try : csl_item . update ( get_pubmed_ids_for_doi ( doi )) except Exception : logging . warning ( f \"Error calling get_pubmed_ids_for_doi for {doi}\" , exc_info = True ) return csl_item return wrapper @augment_get_doi_csl_item def get_doi_csl_item ( doi ): \"\"\" Generate CSL JSON Data for an DOI. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `doi_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\"\" # FIXME: this function is repetitive with other get_*_csl_item functions. for retriever in doi_retrievers : try : return retriever ( doi ) except Exception as error : logging . warning ( f \"Error in {retriever.__name__} for {doi} \" f \"due to a {error.__class__.__name__}: \\n {error}\" ) logging . info ( error , exc_info = True ) raise Exception ( f \"all get_doi_csl_item methods failed for {doi}\" ) doi_retrievers = [ get_doi_csl_item_crosscite , get_doi_csl_item_zotero ]","title":"Module manubot.cite.doi"},{"location":"reference/manubot/cite/doi/#variables","text":"content_negotiation_url doi_retrievers","title":"Variables"},{"location":"reference/manubot/cite/doi/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/doi/#augment_get_doi_csl_item","text":"def augment_get_doi_csl_item ( function ) Decorator providing edits to the csl_item returned by a get_doi_csl_item_* function. View Source def augment_get_doi_csl_item ( function ): \"\"\" Decorator providing edits to the csl_item returned by a get_doi_csl_item_* function. \"\"\" def wrapper ( doi : str ): doi = doi . lower () csl_item = function ( doi ) csl_item [ \"DOI\" ] = doi csl_item [ \"URL\" ] = f \"https://doi.org/{doi}\" short_doi_url = get_short_doi_url ( doi ) if short_doi_url : csl_item [ \"URL\" ] = short_doi_url try : csl_item . update ( get_pubmed_ids_for_doi ( doi )) except Exception : logging . warning ( f \"Error calling get_pubmed_ids_for_doi for {doi}\" , exc_info = True ) return csl_item return wrapper","title":"augment_get_doi_csl_item"},{"location":"reference/manubot/cite/doi/#expand_short_doi","text":"def expand_short_doi ( short_doi ) Convert a shortDOI to a regular DOI. View Source def expand_short_doi ( short_doi ): \"\"\" Convert a shortDOI to a regular DOI. \"\"\" if not short_doi . startswith ( \"10/\" ): raise ValueError ( f \"shortDOIs start with `10/`, but expand_short_doi received: {short_doi}\" ) url = f \"https://doi.org/api/handles/{short_doi.lower()}\" params = { \"type\" : \"HS_ALIAS\" } response = requests . get ( url , params = params ) # response documentation at https : // www . handle . net / proxy_servlet . html results = response . json () response_code = results . get ( \"responseCode\" ) # Handle protocol response code if response_code == 100 : raise ValueError ( f \"Handle not found. Double check short_doi: {short_doi}\" ) if response_code == 200 : raise ValueError ( f \"HS_ALIAS values not found. Double check short_doi: {short_doi}\" ) if response_code != 1 : raise ValueError ( f \"Error response code of {response_code} returned by {response.url}\" ) values = results . get ( \"values\" , []) for value in values : if value . get ( \"type\" ) == \"HS_ALIAS\" : doi = value [ \"data\" ][ \"value\" ] return doi . lower () raise RuntimeError ( f 'HS_ALIAS value not found by expand_short_doi(\"{short_doi}\")\\n' f \"The following JSON was retrieved from {response.url}:\\n\" + json . dumps ( results , indent = 2 ) )","title":"expand_short_doi"},{"location":"reference/manubot/cite/doi/#get_doi_csl_item","text":"def get_doi_csl_item ( doi : str ) View Source def wrapper ( doi : str ): doi = doi . lower () csl_item = function ( doi ) csl_item [ \"DOI\" ] = doi csl_item [ \"URL\" ] = f \"https://doi.org/{doi}\" short_doi_url = get_short_doi_url ( doi ) if short_doi_url : csl_item [ \"URL\" ] = short_doi_url try : csl_item . update ( get_pubmed_ids_for_doi ( doi )) except Exception : logging . warning ( f \"Error calling get_pubmed_ids_for_doi for {doi}\" , exc_info = True ) return csl_item","title":"get_doi_csl_item"},{"location":"reference/manubot/cite/doi/#get_doi_csl_item_crosscite","text":"def get_doi_csl_item_crosscite ( doi ) Use Content Negotioation to retrieve the CSL Item metadata for a DOI. View Source def get_doi_csl_item_crosscite ( doi ): \"\"\" Use Content Negotioation to retrieve the CSL Item metadata for a DOI. \"\"\" url = urllib . parse . urljoin ( content_negotiation_url , urllib . request . quote ( doi )) header = { \"Accept\" : \"application/vnd.citationstyles.csl+json\" , \"User-Agent\" : get_manubot_user_agent (), } response = requests . get ( url , headers = header ) try : return response . json () except Exception as error : logging . error ( f \"Error fetching metadata for doi:{doi}.\\n\" f \"Invalid response from {response.url}:\\n{response.text}\" ) raise error","title":"get_doi_csl_item_crosscite"},{"location":"reference/manubot/cite/doi/#get_doi_csl_item_zotero","text":"def get_doi_csl_item_zotero ( doi ) Generate CSL JSON Data for a DOI using Zotero's translation-server. View Source def get_doi_csl_item_zotero ( doi ): \"\"\" Generate CSL JSON Data for a DOI using Zotero's translation-server. \"\"\" from manubot.cite.zotero import get_csl_item return get_csl_item ( f \"doi:{doi}\" )","title":"get_doi_csl_item_zotero"},{"location":"reference/manubot/cite/doi/#get_short_doi_url","text":"def get_short_doi_url ( doi ) Get the shortDOI URL for a DOI. View Source def get_short_doi_url ( doi ): \"\"\" Get the shortDOI URL for a DOI. \"\"\" quoted_doi = urllib . request . quote ( doi ) url = \"http://shortdoi.org/{}?format=json\" . format ( quoted_doi ) headers = { \"User-Agent\" : get_manubot_user_agent () } try : response = requests . get ( url , headers = headers ). json () short_doi = response [ \"ShortDOI\" ] short_url = \"https://doi.org/\" + short_doi [ 3 :] # Remove \"10/\" prefix return short_url except Exception : logging . warning ( f \"shortDOI lookup failed for {doi}\" , exc_info = True ) return None","title":"get_short_doi_url"},{"location":"reference/manubot/cite/doi/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/doi/#handler_doi","text":"class Handler_DOI ( prefix_lower : str ) A Handler is a class that provides support for a certain type of citekey. For example, a Handler subclass could provide support for DOI citekeys. Subclasses enable custom logic for different citekey prefixes, including how to standardize the citekey and how to retrieve CSL Item metadata. View Source class Handler_DOI ( Handler ): standard_prefix = \"doi\" prefixes = [ \"doi\" , \"shortdoi\" , ] accession_pattern = r \"10\\.[0-9]{4,9}/\\S+\" shortdoi_pattern = r \"10/[a-zA-Z0-9]+\" def inspect ( self , citekey ): identifier = citekey . accession if identifier . startswith ( \"10.\" ): # https://www.crossref.org/blog/dois-and-matching-regular-expressions/ if not self . _get_pattern (). fullmatch ( identifier ): return ( \"Identifier does not conform to the DOI regex. \" \"Double check the DOI.\" ) elif identifier . startswith ( \"10/\" ): # shortDOI, see http://shortdoi.org if not self . _get_pattern ( \"shortdoi_pattern\" ). fullmatch ( identifier ): return ( \"Identifier does not conform to the shortDOI regex. \" \"Double check the shortDOI.\" ) else: return \"DOIs must start with '10.' (or '10/' for shortDOIs).\" def standardize_prefix_accession ( self , accession ): if accession . startswith ( \"10/\" ): try: accession = expand_short_doi ( accession ) except Exception as error: # If DOI shortening fails, return the unshortened DOI. # DOI metadata lookup will eventually fail somewhere with # appropriate error handling, as opposed to here. logging . error ( f \"Error in expand_short_doi for {accession} \" f \"due to a {error.__class__.__name__}:\\n{error}\" ) logging . info ( error , exc_info = True ) accession = accession . lower () return self . standard_prefix , accession def get_csl_item ( self , citekey ): return get_doi_csl_item ( citekey . standard_accession )","title":"Handler_DOI"},{"location":"reference/manubot/cite/doi/#ancestors-in-mro","text":"manubot.cite.handlers.Handler","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/doi/#class-variables","text":"accession_pattern prefixes shortdoi_pattern standard_prefix","title":"Class variables"},{"location":"reference/manubot/cite/doi/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/doi/#get_csl_item","text":"def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ): return get_doi_csl_item ( citekey . standard_accession )","title":"get_csl_item"},{"location":"reference/manubot/cite/doi/#inspect","text":"def inspect ( self , citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey ): identifier = citekey . accession if identifier . startswith ( \"10.\" ): # https : // www . crossref . org / blog / dois - and - matching - regular - expressions / if not self . _get_pattern (). fullmatch ( identifier ): return ( \"Identifier does not conform to the DOI regex. \" \"Double check the DOI.\" ) elif identifier . startswith ( \"10/\" ): # shortDOI , see http : // shortdoi . org if not self . _get_pattern ( \"shortdoi_pattern\" ). fullmatch ( identifier ): return ( \"Identifier does not conform to the shortDOI regex. \" \"Double check the shortDOI.\" ) else : return \"DOIs must start with '10.' (or '10/' for shortDOIs).\"","title":"inspect"},{"location":"reference/manubot/cite/doi/#standardize_prefix_accession","text":"def standardize_prefix_accession ( self , accession ) Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession ): if accession . startswith ( \"10/\" ): try : accession = expand_short_doi ( accession ) except Exception as error : # If DOI shortening fails , return the unshortened DOI . # DOI metadata lookup will eventually fail somewhere with # appropriate error handling , as opposed to here . logging . error ( f \"Error in expand_short_doi for {accession} \" f \"due to a {error.__class__.__name__}:\\n{error}\" ) logging . info ( error , exc_info = True ) accession = accession . lower () return self . standard_prefix , accession","title":"standardize_prefix_accession"},{"location":"reference/manubot/cite/handlers/","text":"Module manubot.cite.handlers View Source import abc import dataclasses import functools import re import typing from manubot.util import import_function \"\"\" Non-citation prefixes used by the pandoc-xnos suite of Pandoc filters, including pandoc-fignos, pandoc-tablenos, and pandoc-eqnos. \"\"\" _pandoc_xnos_prefixes = { \"fig\" , \"tbl\" , \"eq\" } _local_handlers = [ \"manubot.cite.arxiv.Handler_arXiv\" , \"manubot.cite.doi.Handler_DOI\" , \"manubot.cite.isbn.Handler_ISBN\" , \"manubot.cite.pubmed.Handler_PMC\" , \"manubot.cite.pubmed.Handler_PubMed\" , \"manubot.cite.url.Handler_URL\" , \"manubot.cite.wikidata.Handler_Wikidata\" , ] @functools.lru_cache ( maxsize = 10 _000 ) def get_handler ( prefix_lower ): if not isinstance ( prefix_lower , str ): raise TypeError ( f \"prefix_lower should be a str, instead received {prefix_lower.__class__.__name__}\" ) assert prefix_lower == prefix_lower . lower () handler = prefix_to_handler [ prefix_lower ] handler = import_function ( handler )( prefix_lower ) return handler def _generate_prefix_to_handler () -> typing . Dict [ str , str ]: \"\"\" Generate complete dictionary for prefix_to_handler. \"\"\" import inspect from .curie import get_curie_handlers curie_handlers = get_curie_handlers () pth = {} for handler in curie_handlers + _local_handlers : if isinstance ( handler , str ): handler = import_function ( handler )( \"dummy_prefix_lower\" ) for prefix in handler . prefixes : pth [ prefix ] = f \"{inspect.getmodule(handler).__name__}.{handler.__class__.__name__}\" pth = dict ( sorted ( pth . items ())) # sort for clean diffs of serialized dict return pth @dataclasses.dataclass class Handler : \"\"\" A Handler is a class that provides support for a certain type of citekey. For example, a Handler subclass could provide support for DOI citekeys. Subclasses enable custom logic for different citekey prefixes, including how to standardize the citekey and how to retrieve CSL Item metadata. \"\"\" prefix_lower : str prefixes = [] def _get_pattern ( self , attribute = \"accession_pattern\" ) -> typing . Pattern : \"\"\" Return a compiled regex pattern stored by `attribute`. By default, return `self.accession_pattern`, which Handler subclasses can set to provide the expected pattern for `self.accession`. \"\"\" # todo: consider caching compilation pattern = getattr ( self , attribute , None ) if not pattern : return None if not isinstance ( pattern , typing . Pattern ): pattern = re . compile ( pattern ) return pattern def inspect ( self , citekey ): \"\"\" Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. \"\"\" pattern = self . _get_pattern ( \"accession_pattern\" ) if not pattern : return None if not pattern . fullmatch ( citekey . accession ): return f \"{citekey.accession} does not match regex {pattern.pattern}\" def standardize_prefix_accession ( self , accession ): \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession @abc.abstractmethod def get_csl_item ( self , citekey ): \"\"\" Return a CSL_Item with bibliographic details for citekey. \"\"\" raise NotImplementedError ( f \"Manubot does not know how to generate a csl_item for {citekey.standard_id!r}\" ) \"\"\" Mapping from lowercase prefix to Handler class function location as string. This output is automatically generated using `_generate_prefix_to_handler`. Hardcoding this mapping reduces startup time and helps keep imports to a minimum, allowing installations without the full dependencies to function. \"\"\" prefix_to_handler : typing . Dict [ str , str ] = { \"3dmet\" : \"manubot.cite.curie.Handler_CURIE\" , \"abs\" : \"manubot.cite.curie.Handler_CURIE\" , \"aceview.worm\" : \"manubot.cite.curie.Handler_CURIE\" , \"addgene\" : \"manubot.cite.curie.Handler_CURIE\" , \"adw\" : \"manubot.cite.curie.Handler_CURIE\" , \"affy.probeset\" : \"manubot.cite.curie.Handler_CURIE\" , \"aftol.taxonomy\" : \"manubot.cite.curie.Handler_CURIE\" , \"agricola\" : \"manubot.cite.curie.Handler_CURIE\" , \"allergome\" : \"manubot.cite.curie.Handler_CURIE\" , \"amoebadb\" : \"manubot.cite.curie.Handler_CURIE\" , \"antibodyregistry\" : \"manubot.cite.curie.Handler_CURIE\" , \"antweb\" : \"manubot.cite.curie.Handler_CURIE\" , \"aop\" : \"manubot.cite.curie.Handler_CURIE\" , \"aop.events\" : \"manubot.cite.curie.Handler_CURIE\" , \"aop.relationships\" : \"manubot.cite.curie.Handler_CURIE\" , \"aop.stressor\" : \"manubot.cite.curie.Handler_CURIE\" , \"apd\" : \"manubot.cite.curie.Handler_CURIE\" , \"aphidbase.transcript\" : \"manubot.cite.curie.Handler_CURIE\" , \"apid.interactions\" : \"manubot.cite.curie.Handler_CURIE\" , \"arachnoserver\" : \"manubot.cite.curie.Handler_CURIE\" , \"ardb\" : \"manubot.cite.curie.Handler_CURIE\" , \"ark\" : \"manubot.cite.curie.Handler_CURIE\" , \"arrayexpress\" : \"manubot.cite.curie.Handler_CURIE\" , \"arrayexpress.platform\" : \"manubot.cite.curie.Handler_CURIE\" , \"arraymap\" : \"manubot.cite.curie.Handler_CURIE\" , \"arxiv\" : \"manubot.cite.arxiv.Handler_arXiv\" , \"asap\" : \"manubot.cite.curie.Handler_CURIE\" , \"ascl\" : \"manubot.cite.curie.Handler_CURIE\" , \"asin\" : \"manubot.cite.curie.Handler_CURIE\" , \"aspgd.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"aspgd.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"atc\" : \"manubot.cite.curie.Handler_CURIE\" , \"atcc\" : \"manubot.cite.curie.Handler_CURIE\" , \"atcvet\" : \"manubot.cite.curie.Handler_CURIE\" , \"atfdb.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"autdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"bacdive\" : \"manubot.cite.curie.Handler_CURIE\" , \"bacmap.biog\" : \"manubot.cite.curie.Handler_CURIE\" , \"bacmap.map\" : \"manubot.cite.curie.Handler_CURIE\" , \"bao\" : \"manubot.cite.curie.Handler_CURIE\" , \"bdgp.est\" : \"manubot.cite.curie.Handler_CURIE\" , \"bdgp.insertion\" : \"manubot.cite.curie.Handler_CURIE\" , \"bdsc\" : \"manubot.cite.curie.Handler_CURIE\" , \"beetlebase\" : \"manubot.cite.curie.Handler_CURIE\" , \"begdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"bgee.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"bgee.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"bgee.organ\" : \"manubot.cite.curie.Handler_CURIE\" , \"bgee.stage\" : \"manubot.cite.curie.Handler_CURIE\" , \"bigg.compartment\" : \"manubot.cite.curie.Handler_CURIE\" , \"bigg.metabolite\" : \"manubot.cite.curie.Handler_CURIE\" , \"bigg.model\" : \"manubot.cite.curie.Handler_CURIE\" , \"bigg.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"bindingdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"biocarta.pathway\" : \"manubot.cite.curie.Handler_CURIE\" , \"biocatalogue.service\" : \"manubot.cite.curie.Handler_CURIE\" , \"biocyc\" : \"manubot.cite.curie.Handler_CURIE\" , \"biogrid\" : \"manubot.cite.curie.Handler_CURIE\" , \"biolink\" : \"manubot.cite.curie.Handler_CURIE\" , \"biominder\" : \"manubot.cite.curie.Handler_CURIE\" , \"biomodels.db\" : \"manubot.cite.curie.Handler_CURIE\" , \"biomodels.kisao\" : \"manubot.cite.curie.Handler_CURIE\" , \"biomodels.teddy\" : \"manubot.cite.curie.Handler_CURIE\" , \"biomodels.vocabulary\" : \"manubot.cite.curie.Handler_CURIE\" , \"bionumbers\" : \"manubot.cite.curie.Handler_CURIE\" , \"bioportal\" : \"manubot.cite.curie.Handler_CURIE\" , \"bioproject\" : \"manubot.cite.curie.Handler_CURIE\" , \"biosample\" : \"manubot.cite.curie.Handler_CURIE\" , \"biostudies\" : \"manubot.cite.curie.Handler_CURIE\" , \"biosystems\" : \"manubot.cite.curie.Handler_CURIE\" , \"biotools\" : \"manubot.cite.curie.Handler_CURIE\" , \"bitterdb.cpd\" : \"manubot.cite.curie.Handler_CURIE\" , \"bitterdb.rec\" : \"manubot.cite.curie.Handler_CURIE\" , \"bmrb\" : \"manubot.cite.curie.Handler_CURIE\" , \"bold.taxonomy\" : \"manubot.cite.curie.Handler_CURIE\" , \"brenda\" : \"manubot.cite.curie.Handler_CURIE\" , \"broad\" : \"manubot.cite.curie.Handler_CURIE\" , \"bto\" : \"manubot.cite.curie.Handler_CURIE\" , \"bugbase.expt\" : \"manubot.cite.curie.Handler_CURIE\" , \"bugbase.protocol\" : \"manubot.cite.curie.Handler_CURIE\" , \"bykdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"cabri\" : \"manubot.cite.curie.Handler_CURIE\" , \"cadsr\" : \"manubot.cite.curie.Handler_CURIE\" , \"cameo\" : \"manubot.cite.curie.Handler_CURIE\" , \"caps\" : \"manubot.cite.curie.Handler_CURIE\" , \"cas\" : \"manubot.cite.curie.Handler_CURIE\" , \"cath\" : \"manubot.cite.curie.Handler_CURIE\" , \"cath.domain\" : \"manubot.cite.curie.Handler_CURIE\" , \"cath.superfamily\" : \"manubot.cite.curie.Handler_CURIE\" , \"cattleqtldb\" : \"manubot.cite.curie.Handler_CURIE\" , \"cazy\" : \"manubot.cite.curie.Handler_CURIE\" , \"cbioportal\" : \"manubot.cite.curie.Handler_CURIE\" , \"ccds\" : \"manubot.cite.curie.Handler_CURIE\" , \"cco\" : \"manubot.cite.curie.Handler_CURIE\" , \"cdd\" : \"manubot.cite.curie.Handler_CURIE\" , \"cdpd\" : \"manubot.cite.curie.Handler_CURIE\" , \"cellimage\" : \"manubot.cite.curie.Handler_CURIE\" , \"cellosaurus\" : \"manubot.cite.curie.Handler_CURIE\" , \"cgd\" : \"manubot.cite.curie.Handler_CURIE\" , \"cgsc\" : \"manubot.cite.curie.Handler_CURIE\" , \"charprot\" : \"manubot.cite.curie.Handler_CURIE\" , \"chebi\" : \"manubot.cite.curie.Handler_CURIE\" , \"chembl.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"chembl.target\" : \"manubot.cite.curie.Handler_CURIE\" , \"chemdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"chemidplus\" : \"manubot.cite.curie.Handler_CURIE\" , \"chemspider\" : \"manubot.cite.curie.Handler_CURIE\" , \"chickenqtldb\" : \"manubot.cite.curie.Handler_CURIE\" , \"cl\" : \"manubot.cite.curie.Handler_CURIE\" , \"classyfire\" : \"manubot.cite.curie.Handler_CURIE\" , \"cldb\" : \"manubot.cite.curie.Handler_CURIE\" , \"clinicaltrials\" : \"manubot.cite.curie.Handler_CURIE\" , \"clinvar\" : \"manubot.cite.curie.Handler_CURIE\" , \"clinvar.record\" : \"manubot.cite.curie.Handler_CURIE\" , \"clinvar.submission\" : \"manubot.cite.curie.Handler_CURIE\" , \"combine.specifications\" : \"manubot.cite.curie.Handler_CURIE\" , \"complexportal\" : \"manubot.cite.curie.Handler_CURIE\" , \"comptox\" : \"manubot.cite.curie.Handler_CURIE\" , \"compulyeast\" : \"manubot.cite.curie.Handler_CURIE\" , \"conoserver\" : \"manubot.cite.curie.Handler_CURIE\" , \"coriell\" : \"manubot.cite.curie.Handler_CURIE\" , \"corum\" : \"manubot.cite.curie.Handler_CURIE\" , \"cosmic\" : \"manubot.cite.curie.Handler_CURIE\" , \"cpc\" : \"manubot.cite.curie.Handler_CURIE\" , \"crisprdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"cryptodb\" : \"manubot.cite.curie.Handler_CURIE\" , \"csa\" : \"manubot.cite.curie.Handler_CURIE\" , \"cst\" : \"manubot.cite.curie.Handler_CURIE\" , \"cst.ab\" : \"manubot.cite.curie.Handler_CURIE\" , \"ctd.chemical\" : \"manubot.cite.curie.Handler_CURIE\" , \"ctd.disease\" : \"manubot.cite.curie.Handler_CURIE\" , \"ctd.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"cubedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"d1id\" : \"manubot.cite.curie.Handler_CURIE\" , \"dailymed\" : \"manubot.cite.curie.Handler_CURIE\" , \"dandi\" : \"manubot.cite.curie.Handler_CURIE\" , \"darc\" : \"manubot.cite.curie.Handler_CURIE\" , \"dashr\" : \"manubot.cite.curie.Handler_CURIE\" , \"dashr.expression\" : \"manubot.cite.curie.Handler_CURIE\" , \"datf\" : \"manubot.cite.curie.Handler_CURIE\" , \"dbd\" : \"manubot.cite.curie.Handler_CURIE\" , \"dbest\" : \"manubot.cite.curie.Handler_CURIE\" , \"dbg2introns\" : \"manubot.cite.curie.Handler_CURIE\" , \"dbgap\" : \"manubot.cite.curie.Handler_CURIE\" , \"dbprobe\" : \"manubot.cite.curie.Handler_CURIE\" , \"dbsnp\" : \"manubot.cite.curie.Handler_CURIE\" , \"degradome\" : \"manubot.cite.curie.Handler_CURIE\" , \"depod\" : \"manubot.cite.curie.Handler_CURIE\" , \"dev.ga4ghdos\" : \"manubot.cite.curie.Handler_CURIE\" , \"dictybase.est\" : \"manubot.cite.curie.Handler_CURIE\" , \"dictybase.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"did\" : \"manubot.cite.curie.Handler_CURIE\" , \"dip\" : \"manubot.cite.curie.Handler_CURIE\" , \"disprot\" : \"manubot.cite.curie.Handler_CURIE\" , \"disprot.region\" : \"manubot.cite.curie.Handler_CURIE\" , \"dlxb\" : \"manubot.cite.curie.Handler_CURIE\" , \"dlxc\" : \"manubot.cite.curie.Handler_CURIE\" , \"doi\" : \"manubot.cite.doi.Handler_DOI\" , \"doid\" : \"manubot.cite.curie.Handler_CURIE\" , \"dommino\" : \"manubot.cite.curie.Handler_CURIE\" , \"door\" : \"manubot.cite.curie.Handler_CURIE\" , \"doqcs.model\" : \"manubot.cite.curie.Handler_CURIE\" , \"doqcs.pathway\" : \"manubot.cite.curie.Handler_CURIE\" , \"dpv\" : \"manubot.cite.curie.Handler_CURIE\" , \"dragondb.allele\" : \"manubot.cite.curie.Handler_CURIE\" , \"dragondb.dna\" : \"manubot.cite.curie.Handler_CURIE\" , \"dragondb.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"dragondb.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"drsc\" : \"manubot.cite.curie.Handler_CURIE\" , \"drugbank\" : \"manubot.cite.curie.Handler_CURIE\" , \"drugbankv4.target\" : \"manubot.cite.curie.Handler_CURIE\" , \"ec-code\" : \"manubot.cite.curie.Handler_CURIE\" , \"echobase\" : \"manubot.cite.curie.Handler_CURIE\" , \"eco\" : \"manubot.cite.curie.Handler_CURIE\" , \"ecogene\" : \"manubot.cite.curie.Handler_CURIE\" , \"ecoliwiki\" : \"manubot.cite.curie.Handler_CURIE\" , \"ecyano.entity\" : \"manubot.cite.curie.Handler_CURIE\" , \"ecyano.experiment\" : \"manubot.cite.curie.Handler_CURIE\" , \"ecyano.model\" : \"manubot.cite.curie.Handler_CURIE\" , \"ecyano.rule\" : \"manubot.cite.curie.Handler_CURIE\" , \"edam\" : \"manubot.cite.curie.Handler_CURIE\" , \"efo\" : \"manubot.cite.curie.Handler_CURIE\" , \"ega.dataset\" : \"manubot.cite.curie.Handler_CURIE\" , \"ega.study\" : \"manubot.cite.curie.Handler_CURIE\" , \"eggnog\" : \"manubot.cite.curie.Handler_CURIE\" , \"elm\" : \"manubot.cite.curie.Handler_CURIE\" , \"emdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"ena.embl\" : \"manubot.cite.curie.Handler_CURIE\" , \"encode\" : \"manubot.cite.curie.Handler_CURIE\" , \"ensembl\" : \"manubot.cite.curie.Handler_CURIE\" , \"ensembl.bacteria\" : \"manubot.cite.curie.Handler_CURIE\" , \"ensembl.fungi\" : \"manubot.cite.curie.Handler_CURIE\" , \"ensembl.metazoa\" : \"manubot.cite.curie.Handler_CURIE\" , \"ensembl.plant\" : \"manubot.cite.curie.Handler_CURIE\" , \"ensembl.protist\" : \"manubot.cite.curie.Handler_CURIE\" , \"envipath\" : \"manubot.cite.curie.Handler_CURIE\" , \"envo\" : \"manubot.cite.curie.Handler_CURIE\" , \"eo\" : \"manubot.cite.curie.Handler_CURIE\" , \"epd\" : \"manubot.cite.curie.Handler_CURIE\" , \"erm\" : \"manubot.cite.curie.Handler_CURIE\" , \"erv\" : \"manubot.cite.curie.Handler_CURIE\" , \"eu89h\" : \"manubot.cite.curie.Handler_CURIE\" , \"euclinicaltrials\" : \"manubot.cite.curie.Handler_CURIE\" , \"exac.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"exac.transcript\" : \"manubot.cite.curie.Handler_CURIE\" , \"exac.variant\" : \"manubot.cite.curie.Handler_CURIE\" , \"facebase\" : \"manubot.cite.curie.Handler_CURIE\" , \"fairsharing\" : \"manubot.cite.curie.Handler_CURIE\" , \"fb\" : \"manubot.cite.curie.Handler_CURIE\" , \"fbol\" : \"manubot.cite.curie.Handler_CURIE\" , \"flowrepository\" : \"manubot.cite.curie.Handler_CURIE\" , \"fma\" : \"manubot.cite.curie.Handler_CURIE\" , \"foodb.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"foodon\" : \"manubot.cite.curie.Handler_CURIE\" , \"fplx\" : \"manubot.cite.curie.Handler_CURIE\" , \"fsnp\" : \"manubot.cite.curie.Handler_CURIE\" , \"funcbase.fly\" : \"manubot.cite.curie.Handler_CURIE\" , \"funcbase.human\" : \"manubot.cite.curie.Handler_CURIE\" , \"funcbase.mouse\" : \"manubot.cite.curie.Handler_CURIE\" , \"funcbase.yeast\" : \"manubot.cite.curie.Handler_CURIE\" , \"fungidb\" : \"manubot.cite.curie.Handler_CURIE\" , \"ga4ghdos\" : \"manubot.cite.curie.Handler_CURIE\" , \"gabi\" : \"manubot.cite.curie.Handler_CURIE\" , \"gcst\" : \"manubot.cite.curie.Handler_CURIE\" , \"gdc\" : \"manubot.cite.curie.Handler_CURIE\" , \"gdsc\" : \"manubot.cite.curie.Handler_CURIE\" , \"genatlas\" : \"manubot.cite.curie.Handler_CURIE\" , \"genecards\" : \"manubot.cite.curie.Handler_CURIE\" , \"genedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"genefarm\" : \"manubot.cite.curie.Handler_CURIE\" , \"genetree\" : \"manubot.cite.curie.Handler_CURIE\" , \"genewiki\" : \"manubot.cite.curie.Handler_CURIE\" , \"genpept\" : \"manubot.cite.curie.Handler_CURIE\" , \"genprop\" : \"manubot.cite.curie.Handler_CURIE\" , \"geo\" : \"manubot.cite.curie.Handler_CURIE\" , \"giardiadb\" : \"manubot.cite.curie.Handler_CURIE\" , \"glida.gpcr\" : \"manubot.cite.curie.Handler_CURIE\" , \"glida.ligand\" : \"manubot.cite.curie.Handler_CURIE\" , \"glycoepitope\" : \"manubot.cite.curie.Handler_CURIE\" , \"glycomedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"glytoucan\" : \"manubot.cite.curie.Handler_CURIE\" , \"gmd\" : \"manubot.cite.curie.Handler_CURIE\" , \"gmd.analyte\" : \"manubot.cite.curie.Handler_CURIE\" , \"gmd.gcms\" : \"manubot.cite.curie.Handler_CURIE\" , \"gmd.profile\" : \"manubot.cite.curie.Handler_CURIE\" , \"gmd.ref\" : \"manubot.cite.curie.Handler_CURIE\" , \"gnpis\" : \"manubot.cite.curie.Handler_CURIE\" , \"go\" : \"manubot.cite.curie.Handler_CURIE\" , \"go.ref\" : \"manubot.cite.curie.Handler_CURIE\" , \"goa\" : \"manubot.cite.curie.Handler_CURIE\" , \"gold\" : \"manubot.cite.curie.Handler_CURIE\" , \"gold.genome\" : \"manubot.cite.curie.Handler_CURIE\" , \"gold.meta\" : \"manubot.cite.curie.Handler_CURIE\" , \"google.patent\" : \"manubot.cite.curie.Handler_CURIE\" , \"gpcrdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"gpmdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"gramene.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"gramene.growthstage\" : \"manubot.cite.curie.Handler_CURIE\" , \"gramene.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"gramene.qtl\" : \"manubot.cite.curie.Handler_CURIE\" , \"gramene.taxonomy\" : \"manubot.cite.curie.Handler_CURIE\" , \"greengenes\" : \"manubot.cite.curie.Handler_CURIE\" , \"grid\" : \"manubot.cite.curie.Handler_CURIE\" , \"grin.taxonomy\" : \"manubot.cite.curie.Handler_CURIE\" , \"gro\" : \"manubot.cite.curie.Handler_CURIE\" , \"grsdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"gtex\" : \"manubot.cite.curie.Handler_CURIE\" , \"gudmap\" : \"manubot.cite.curie.Handler_CURIE\" , \"gwascentral.marker\" : \"manubot.cite.curie.Handler_CURIE\" , \"gwascentral.phenotype\" : \"manubot.cite.curie.Handler_CURIE\" , \"gwascentral.study\" : \"manubot.cite.curie.Handler_CURIE\" , \"gxa.expt\" : \"manubot.cite.curie.Handler_CURIE\" , \"gxa.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"hamap\" : \"manubot.cite.curie.Handler_CURIE\" , \"hcvdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"hdr\" : \"manubot.cite.curie.Handler_CURIE\" , \"hgmd\" : \"manubot.cite.curie.Handler_CURIE\" , \"hgnc\" : \"manubot.cite.curie.Handler_CURIE\" , \"hgnc.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"hgnc.genefamily\" : \"manubot.cite.curie.Handler_CURIE\" , \"hgnc.symbol\" : \"manubot.cite.curie.Handler_CURIE\" , \"hinv.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"hinv.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"hinv.transcript\" : \"manubot.cite.curie.Handler_CURIE\" , \"hmdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"hogenom\" : \"manubot.cite.curie.Handler_CURIE\" , \"homd.seq\" : \"manubot.cite.curie.Handler_CURIE\" , \"homd.taxon\" : \"manubot.cite.curie.Handler_CURIE\" , \"homologene\" : \"manubot.cite.curie.Handler_CURIE\" , \"hovergen\" : \"manubot.cite.curie.Handler_CURIE\" , \"hp\" : \"manubot.cite.curie.Handler_CURIE\" , \"hpa\" : \"manubot.cite.curie.Handler_CURIE\" , \"hpm.peptide\" : \"manubot.cite.curie.Handler_CURIE\" , \"hpm.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"hprd\" : \"manubot.cite.curie.Handler_CURIE\" , \"hpscreg\" : \"manubot.cite.curie.Handler_CURIE\" , \"hssp\" : \"manubot.cite.curie.Handler_CURIE\" , \"http\" : \"manubot.cite.url.Handler_URL\" , \"https\" : \"manubot.cite.url.Handler_URL\" , \"huge\" : \"manubot.cite.curie.Handler_CURIE\" , \"iao\" : \"manubot.cite.curie.Handler_CURIE\" , \"icd\" : \"manubot.cite.curie.Handler_CURIE\" , \"iceberg.element\" : \"manubot.cite.curie.Handler_CURIE\" , \"iceberg.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"ideal\" : \"manubot.cite.curie.Handler_CURIE\" , \"ido\" : \"manubot.cite.curie.Handler_CURIE\" , \"idoo\" : \"manubot.cite.curie.Handler_CURIE\" , \"idot\" : \"manubot.cite.curie.Handler_CURIE\" , \"idr\" : \"manubot.cite.curie.Handler_CURIE\" , \"imex\" : \"manubot.cite.curie.Handler_CURIE\" , \"img.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"img.taxon\" : \"manubot.cite.curie.Handler_CURIE\" , \"imgt.hla\" : \"manubot.cite.curie.Handler_CURIE\" , \"imgt.ligm\" : \"manubot.cite.curie.Handler_CURIE\" , \"inchi\" : \"manubot.cite.curie.Handler_CURIE\" , \"inchikey\" : \"manubot.cite.curie.Handler_CURIE\" , \"insdc\" : \"manubot.cite.curie.Handler_CURIE\" , \"insdc.cds\" : \"manubot.cite.curie.Handler_CURIE\" , \"insdc.gca\" : \"manubot.cite.curie.Handler_CURIE\" , \"insdc.sra\" : \"manubot.cite.curie.Handler_CURIE\" , \"intact\" : \"manubot.cite.curie.Handler_CURIE\" , \"intact.molecule\" : \"manubot.cite.curie.Handler_CURIE\" , \"interpro\" : \"manubot.cite.curie.Handler_CURIE\" , \"ird.segment\" : \"manubot.cite.curie.Handler_CURIE\" , \"irefweb\" : \"manubot.cite.curie.Handler_CURIE\" , \"isbn\" : \"manubot.cite.isbn.Handler_ISBN\" , \"isfinder\" : \"manubot.cite.curie.Handler_CURIE\" , \"isni\" : \"manubot.cite.curie.Handler_CURIE\" , \"issn\" : \"manubot.cite.curie.Handler_CURIE\" , \"iuphar.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"iuphar.ligand\" : \"manubot.cite.curie.Handler_CURIE\" , \"iuphar.receptor\" : \"manubot.cite.curie.Handler_CURIE\" , \"jaxmice\" : \"manubot.cite.curie.Handler_CURIE\" , \"jcggdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"jcm\" : \"manubot.cite.curie.Handler_CURIE\" , \"jcsd\" : \"manubot.cite.curie.Handler_CURIE\" , \"jstor\" : \"manubot.cite.curie.Handler_CURIE\" , \"jws\" : \"manubot.cite.curie.Handler_CURIE\" , \"kaggle\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.disease\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.drug\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.environ\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.genes\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.genome\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.glycan\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.metagenome\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.module\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.orthology\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.pathway\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"knapsack\" : \"manubot.cite.curie.Handler_CURIE\" , \"lei\" : \"manubot.cite.curie.Handler_CURIE\" , \"lgic\" : \"manubot.cite.curie.Handler_CURIE\" , \"licebase\" : \"manubot.cite.curie.Handler_CURIE\" , \"ligandbook\" : \"manubot.cite.curie.Handler_CURIE\" , \"ligandbox\" : \"manubot.cite.curie.Handler_CURIE\" , \"ligandexpo\" : \"manubot.cite.curie.Handler_CURIE\" , \"lincs.cell\" : \"manubot.cite.curie.Handler_CURIE\" , \"lincs.data\" : \"manubot.cite.curie.Handler_CURIE\" , \"lincs.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"lincs.smallmolecule\" : \"manubot.cite.curie.Handler_CURIE\" , \"lipidbank\" : \"manubot.cite.curie.Handler_CURIE\" , \"lipidmaps\" : \"manubot.cite.curie.Handler_CURIE\" , \"lrg\" : \"manubot.cite.curie.Handler_CURIE\" , \"ma\" : \"manubot.cite.curie.Handler_CURIE\" , \"macie\" : \"manubot.cite.curie.Handler_CURIE\" , \"maizegdb.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"mamo\" : \"manubot.cite.curie.Handler_CURIE\" , \"massbank\" : \"manubot.cite.curie.Handler_CURIE\" , \"massive\" : \"manubot.cite.curie.Handler_CURIE\" , \"matrixdb.association\" : \"manubot.cite.curie.Handler_CURIE\" , \"mdm\" : \"manubot.cite.curie.Handler_CURIE\" , \"meddra\" : \"manubot.cite.curie.Handler_CURIE\" , \"medgen\" : \"manubot.cite.curie.Handler_CURIE\" , \"medlineplus\" : \"manubot.cite.curie.Handler_CURIE\" , \"merops\" : \"manubot.cite.curie.Handler_CURIE\" , \"merops.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"merops.inhibitor\" : \"manubot.cite.curie.Handler_CURIE\" , \"mesh\" : \"manubot.cite.curie.Handler_CURIE\" , \"mesh.2012\" : \"manubot.cite.curie.Handler_CURIE\" , \"mesh.2013\" : \"manubot.cite.curie.Handler_CURIE\" , \"metabolights\" : \"manubot.cite.curie.Handler_CURIE\" , \"metacyc.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"metacyc.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"metanetx.chemical\" : \"manubot.cite.curie.Handler_CURIE\" , \"metanetx.compartment\" : \"manubot.cite.curie.Handler_CURIE\" , \"metanetx.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"metlin\" : \"manubot.cite.curie.Handler_CURIE\" , \"mex\" : \"manubot.cite.curie.Handler_CURIE\" , \"mge\" : \"manubot.cite.curie.Handler_CURIE\" , \"mgi\" : \"manubot.cite.curie.Handler_CURIE\" , \"mgnify.proj\" : \"manubot.cite.curie.Handler_CURIE\" , \"mgnify.samp\" : \"manubot.cite.curie.Handler_CURIE\" , \"mi\" : \"manubot.cite.curie.Handler_CURIE\" , \"microscope\" : \"manubot.cite.curie.Handler_CURIE\" , \"microsporidia\" : \"manubot.cite.curie.Handler_CURIE\" , \"mim\" : \"manubot.cite.curie.Handler_CURIE\" , \"mimodb\" : \"manubot.cite.curie.Handler_CURIE\" , \"minid\" : \"manubot.cite.curie.Handler_CURIE\" , \"minid.test\" : \"manubot.cite.curie.Handler_CURIE\" , \"mint\" : \"manubot.cite.curie.Handler_CURIE\" , \"mipmod\" : \"manubot.cite.curie.Handler_CURIE\" , \"mir\" : \"manubot.cite.curie.Handler_CURIE\" , \"mirbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"mirbase.mature\" : \"manubot.cite.curie.Handler_CURIE\" , \"mirex\" : \"manubot.cite.curie.Handler_CURIE\" , \"miriam.collection\" : \"manubot.cite.curie.Handler_CURIE\" , \"miriam.resource\" : \"manubot.cite.curie.Handler_CURIE\" , \"mirnest\" : \"manubot.cite.curie.Handler_CURIE\" , \"mirtarbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmmp:biomaps\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmp.cat\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmp.db\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmp.fun\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmp.ref\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmrrc\" : \"manubot.cite.curie.Handler_CURIE\" , \"mo\" : \"manubot.cite.curie.Handler_CURIE\" , \"mobidb\" : \"manubot.cite.curie.Handler_CURIE\" , \"mod\" : \"manubot.cite.curie.Handler_CURIE\" , \"modeldb\" : \"manubot.cite.curie.Handler_CURIE\" , \"molbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"mp\" : \"manubot.cite.curie.Handler_CURIE\" , \"mpid\" : \"manubot.cite.curie.Handler_CURIE\" , \"ms\" : \"manubot.cite.curie.Handler_CURIE\" , \"multicellds.cell_line\" : \"manubot.cite.curie.Handler_CURIE\" , \"multicellds.collection\" : \"manubot.cite.curie.Handler_CURIE\" , \"multicellds.snapshot\" : \"manubot.cite.curie.Handler_CURIE\" , \"mw.project\" : \"manubot.cite.curie.Handler_CURIE\" , \"mw.study\" : \"manubot.cite.curie.Handler_CURIE\" , \"myco.lepra\" : \"manubot.cite.curie.Handler_CURIE\" , \"myco.marinum\" : \"manubot.cite.curie.Handler_CURIE\" , \"myco.smeg\" : \"manubot.cite.curie.Handler_CURIE\" , \"myco.tuber\" : \"manubot.cite.curie.Handler_CURIE\" , \"mycobank\" : \"manubot.cite.curie.Handler_CURIE\" , \"mzspec\" : \"manubot.cite.curie.Handler_CURIE\" , \"napdi\" : \"manubot.cite.curie.Handler_CURIE\" , \"napp\" : \"manubot.cite.curie.Handler_CURIE\" , \"narcis\" : \"manubot.cite.curie.Handler_CURIE\" , \"nasc\" : \"manubot.cite.curie.Handler_CURIE\" , \"nbn\" : \"manubot.cite.curie.Handler_CURIE\" , \"nbrc\" : \"manubot.cite.curie.Handler_CURIE\" , \"ncbigene\" : \"manubot.cite.curie.Handler_CURIE\" , \"ncbiprotein\" : \"manubot.cite.curie.Handler_CURIE\" , \"ncim\" : \"manubot.cite.curie.Handler_CURIE\" , \"ncit\" : \"manubot.cite.curie.Handler_CURIE\" , \"ndc\" : \"manubot.cite.curie.Handler_CURIE\" , \"nemo\" : \"manubot.cite.curie.Handler_CURIE\" , \"neurolex\" : \"manubot.cite.curie.Handler_CURIE\" , \"neuromorpho\" : \"manubot.cite.curie.Handler_CURIE\" , \"neurondb\" : \"manubot.cite.curie.Handler_CURIE\" , \"neurovault.collection\" : \"manubot.cite.curie.Handler_CURIE\" , \"neurovault.image\" : \"manubot.cite.curie.Handler_CURIE\" , \"nextdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"nextprot\" : \"manubot.cite.curie.Handler_CURIE\" , \"ngl\" : \"manubot.cite.curie.Handler_CURIE\" , \"niaest\" : \"manubot.cite.curie.Handler_CURIE\" , \"nmr\" : \"manubot.cite.curie.Handler_CURIE\" , \"nmrshiftdb2\" : \"manubot.cite.curie.Handler_CURIE\" , \"noncodev3\" : \"manubot.cite.curie.Handler_CURIE\" , \"noncodev4.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"noncodev4.rna\" : \"manubot.cite.curie.Handler_CURIE\" , \"norine\" : \"manubot.cite.curie.Handler_CURIE\" , \"nuclearbd\" : \"manubot.cite.curie.Handler_CURIE\" , \"obi\" : \"manubot.cite.curie.Handler_CURIE\" , \"occ\" : \"manubot.cite.curie.Handler_CURIE\" , \"oci\" : \"manubot.cite.curie.Handler_CURIE\" , \"ocid\" : \"manubot.cite.curie.Handler_CURIE\" , \"oclc\" : \"manubot.cite.curie.Handler_CURIE\" , \"odor\" : \"manubot.cite.curie.Handler_CURIE\" , \"oid\" : \"manubot.cite.curie.Handler_CURIE\" , \"oma.grp\" : \"manubot.cite.curie.Handler_CURIE\" , \"oma.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"omia\" : \"manubot.cite.curie.Handler_CURIE\" , \"omit\" : \"manubot.cite.curie.Handler_CURIE\" , \"opb\" : \"manubot.cite.curie.Handler_CURIE\" , \"opm\" : \"manubot.cite.curie.Handler_CURIE\" , \"orcid\" : \"manubot.cite.curie.Handler_CURIE\" , \"ordb\" : \"manubot.cite.curie.Handler_CURIE\" , \"oridb.sacch\" : \"manubot.cite.curie.Handler_CURIE\" , \"oridb.schizo\" : \"manubot.cite.curie.Handler_CURIE\" , \"orphanet\" : \"manubot.cite.curie.Handler_CURIE\" , \"orphanet.ordo\" : \"manubot.cite.curie.Handler_CURIE\" , \"orthodb\" : \"manubot.cite.curie.Handler_CURIE\" , \"oryzabase.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"oryzabase.mutant\" : \"manubot.cite.curie.Handler_CURIE\" , \"oryzabase.reference\" : \"manubot.cite.curie.Handler_CURIE\" , \"oryzabase.stage\" : \"manubot.cite.curie.Handler_CURIE\" , \"oryzabase.strain\" : \"manubot.cite.curie.Handler_CURIE\" , \"otl\" : \"manubot.cite.curie.Handler_CURIE\" , \"p3db.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"p3db.site\" : \"manubot.cite.curie.Handler_CURIE\" , \"paleodb\" : \"manubot.cite.curie.Handler_CURIE\" , \"panther.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"panther.node\" : \"manubot.cite.curie.Handler_CURIE\" , \"panther.pathway\" : \"manubot.cite.curie.Handler_CURIE\" , \"panther.pthcmp\" : \"manubot.cite.curie.Handler_CURIE\" , \"pass2\" : \"manubot.cite.curie.Handler_CURIE\" , \"pathwaycommons\" : \"manubot.cite.curie.Handler_CURIE\" , \"pato\" : \"manubot.cite.curie.Handler_CURIE\" , \"paxdb.organism\" : \"manubot.cite.curie.Handler_CURIE\" , \"paxdb.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"pazar\" : \"manubot.cite.curie.Handler_CURIE\" , \"pdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pdb-ccd\" : \"manubot.cite.curie.Handler_CURIE\" , \"pdb.ligand\" : \"manubot.cite.curie.Handler_CURIE\" , \"peptideatlas\" : \"manubot.cite.curie.Handler_CURIE\" , \"peptideatlas.dataset\" : \"manubot.cite.curie.Handler_CURIE\" , \"peroxibase\" : \"manubot.cite.curie.Handler_CURIE\" , \"pfam\" : \"manubot.cite.curie.Handler_CURIE\" , \"pgs\" : \"manubot.cite.curie.Handler_CURIE\" , \"pgx\" : \"manubot.cite.curie.Handler_CURIE\" , \"pharmgkb.disease\" : \"manubot.cite.curie.Handler_CURIE\" , \"pharmgkb.drug\" : \"manubot.cite.curie.Handler_CURIE\" , \"pharmgkb.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"pharmgkb.pathways\" : \"manubot.cite.curie.Handler_CURIE\" , \"phenolexplorer\" : \"manubot.cite.curie.Handler_CURIE\" , \"phosphopoint.kinase\" : \"manubot.cite.curie.Handler_CURIE\" , \"phosphopoint.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"phosphosite.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"phosphosite.residue\" : \"manubot.cite.curie.Handler_CURIE\" , \"phylomedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"phytozome.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"pid.pathway\" : \"manubot.cite.curie.Handler_CURIE\" , \"pigqtldb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pina\" : \"manubot.cite.curie.Handler_CURIE\" , \"piroplasma\" : \"manubot.cite.curie.Handler_CURIE\" , \"pirsf\" : \"manubot.cite.curie.Handler_CURIE\" , \"planttfdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"plasmodb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pmap.cutdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pmap.substratedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pmc\" : \"manubot.cite.pubmed.Handler_PMC\" , \"pmcid\" : \"manubot.cite.pubmed.Handler_PMC\" , \"pmdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pmid\" : \"manubot.cite.pubmed.Handler_PubMed\" , \"pmp\" : \"manubot.cite.curie.Handler_CURIE\" , \"po\" : \"manubot.cite.curie.Handler_CURIE\" , \"pocketome\" : \"manubot.cite.curie.Handler_CURIE\" , \"polbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"pombase\" : \"manubot.cite.curie.Handler_CURIE\" , \"pr\" : \"manubot.cite.curie.Handler_CURIE\" , \"pride\" : \"manubot.cite.curie.Handler_CURIE\" , \"pride.project\" : \"manubot.cite.curie.Handler_CURIE\" , \"prints\" : \"manubot.cite.curie.Handler_CURIE\" , \"probonto\" : \"manubot.cite.curie.Handler_CURIE\" , \"prodom\" : \"manubot.cite.curie.Handler_CURIE\" , \"proglyc\" : \"manubot.cite.curie.Handler_CURIE\" , \"prosite\" : \"manubot.cite.curie.Handler_CURIE\" , \"protclustdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"proteomicsdb.peptide\" : \"manubot.cite.curie.Handler_CURIE\" , \"proteomicsdb.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"protonet.cluster\" : \"manubot.cite.curie.Handler_CURIE\" , \"protonet.proteincard\" : \"manubot.cite.curie.Handler_CURIE\" , \"pscdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pseudomonas\" : \"manubot.cite.curie.Handler_CURIE\" , \"psimi\" : \"manubot.cite.curie.Handler_CURIE\" , \"psipar\" : \"manubot.cite.curie.Handler_CURIE\" , \"pubchem.bioassay\" : \"manubot.cite.curie.Handler_CURIE\" , \"pubchem.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"pubchem.substance\" : \"manubot.cite.curie.Handler_CURIE\" , \"pubmed\" : \"manubot.cite.pubmed.Handler_PubMed\" , \"pw\" : \"manubot.cite.curie.Handler_CURIE\" , \"px\" : \"manubot.cite.curie.Handler_CURIE\" , \"rapdb.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"rapdb.transcript\" : \"manubot.cite.curie.Handler_CURIE\" , \"rbk\" : \"manubot.cite.curie.Handler_CURIE\" , \"reactome\" : \"manubot.cite.curie.Handler_CURIE\" , \"rebase\" : \"manubot.cite.curie.Handler_CURIE\" , \"refseq\" : \"manubot.cite.curie.Handler_CURIE\" , \"resid\" : \"manubot.cite.curie.Handler_CURIE\" , \"rfam\" : \"manubot.cite.curie.Handler_CURIE\" , \"rgd\" : \"manubot.cite.curie.Handler_CURIE\" , \"rgd.qtl\" : \"manubot.cite.curie.Handler_CURIE\" , \"rgd.strain\" : \"manubot.cite.curie.Handler_CURIE\" , \"rhea\" : \"manubot.cite.curie.Handler_CURIE\" , \"ricegap\" : \"manubot.cite.curie.Handler_CURIE\" , \"ricenetdb.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"ricenetdb.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"ricenetdb.mirna\" : \"manubot.cite.curie.Handler_CURIE\" , \"ricenetdb.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"ricenetdb.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"rnacentral\" : \"manubot.cite.curie.Handler_CURIE\" , \"rnamods\" : \"manubot.cite.curie.Handler_CURIE\" , \"ro\" : \"manubot.cite.curie.Handler_CURIE\" , \"rouge\" : \"manubot.cite.curie.Handler_CURIE\" , \"rrid\" : \"manubot.cite.curie.Handler_CURIE\" , \"sabiork.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"sabiork.ec\" : \"manubot.cite.curie.Handler_CURIE\" , \"sabiork.kineticrecord\" : \"manubot.cite.curie.Handler_CURIE\" , \"sabiork.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"sasbdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"sbo\" : \"manubot.cite.curie.Handler_CURIE\" , \"scop\" : \"manubot.cite.curie.Handler_CURIE\" , \"scretf\" : \"manubot.cite.curie.Handler_CURIE\" , \"sdbs\" : \"manubot.cite.curie.Handler_CURIE\" , \"seed\" : \"manubot.cite.curie.Handler_CURIE\" , \"seed.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"seed.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"sgd\" : \"manubot.cite.curie.Handler_CURIE\" , \"sgd.pathways\" : \"manubot.cite.curie.Handler_CURIE\" , \"sgn\" : \"manubot.cite.curie.Handler_CURIE\" , \"sheepqtldb\" : \"manubot.cite.curie.Handler_CURIE\" , \"shortdoi\" : \"manubot.cite.doi.Handler_DOI\" , \"sider.drug\" : \"manubot.cite.curie.Handler_CURIE\" , \"sider.effect\" : \"manubot.cite.curie.Handler_CURIE\" , \"signaling-gateway\" : \"manubot.cite.curie.Handler_CURIE\" , \"sisu\" : \"manubot.cite.curie.Handler_CURIE\" , \"sitex\" : \"manubot.cite.curie.Handler_CURIE\" , \"slm\" : \"manubot.cite.curie.Handler_CURIE\" , \"smart\" : \"manubot.cite.curie.Handler_CURIE\" , \"smpdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"snomedct\" : \"manubot.cite.curie.Handler_CURIE\" , \"snp2tfbs\" : \"manubot.cite.curie.Handler_CURIE\" , \"so\" : \"manubot.cite.curie.Handler_CURIE\" , \"soybase\" : \"manubot.cite.curie.Handler_CURIE\" , \"spdx\" : \"manubot.cite.curie.Handler_CURIE\" , \"spike.map\" : \"manubot.cite.curie.Handler_CURIE\" , \"splash\" : \"manubot.cite.curie.Handler_CURIE\" , \"spp\" : \"manubot.cite.curie.Handler_CURIE\" , \"stap\" : \"manubot.cite.curie.Handler_CURIE\" , \"stitch\" : \"manubot.cite.curie.Handler_CURIE\" , \"storedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"string\" : \"manubot.cite.curie.Handler_CURIE\" , \"subtilist\" : \"manubot.cite.curie.Handler_CURIE\" , \"subtiwiki\" : \"manubot.cite.curie.Handler_CURIE\" , \"sugarbind\" : \"manubot.cite.curie.Handler_CURIE\" , \"supfam\" : \"manubot.cite.curie.Handler_CURIE\" , \"swh\" : \"manubot.cite.curie.Handler_CURIE\" , \"swiss-model\" : \"manubot.cite.curie.Handler_CURIE\" , \"swissregulon\" : \"manubot.cite.curie.Handler_CURIE\" , \"t3db\" : \"manubot.cite.curie.Handler_CURIE\" , \"tair.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"tair.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"tair.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"tarbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"taxonomy\" : \"manubot.cite.curie.Handler_CURIE\" , \"tcdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"tgd\" : \"manubot.cite.curie.Handler_CURIE\" , \"tigrfam\" : \"manubot.cite.curie.Handler_CURIE\" , \"tissuelist\" : \"manubot.cite.curie.Handler_CURIE\" , \"tol\" : \"manubot.cite.curie.Handler_CURIE\" , \"topdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"topfind\" : \"manubot.cite.curie.Handler_CURIE\" , \"toxoplasma\" : \"manubot.cite.curie.Handler_CURIE\" , \"transyt\" : \"manubot.cite.curie.Handler_CURIE\" , \"treebase\" : \"manubot.cite.curie.Handler_CURIE\" , \"treefam\" : \"manubot.cite.curie.Handler_CURIE\" , \"trichdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"tritrypdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"ttd.drug\" : \"manubot.cite.curie.Handler_CURIE\" , \"ttd.target\" : \"manubot.cite.curie.Handler_CURIE\" , \"uberon\" : \"manubot.cite.curie.Handler_CURIE\" , \"ubio.namebank\" : \"manubot.cite.curie.Handler_CURIE\" , \"umbbd.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"umbbd.enzyme\" : \"manubot.cite.curie.Handler_CURIE\" , \"umbbd.pathway\" : \"manubot.cite.curie.Handler_CURIE\" , \"umbbd.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"umbbd.rule\" : \"manubot.cite.curie.Handler_CURIE\" , \"umls\" : \"manubot.cite.curie.Handler_CURIE\" , \"unigene\" : \"manubot.cite.curie.Handler_CURIE\" , \"unii\" : \"manubot.cite.curie.Handler_CURIE\" , \"unimod\" : \"manubot.cite.curie.Handler_CURIE\" , \"uniparc\" : \"manubot.cite.curie.Handler_CURIE\" , \"unipathway.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"unipathway.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"uniprot\" : \"manubot.cite.curie.Handler_CURIE\" , \"uniprot.isoform\" : \"manubot.cite.curie.Handler_CURIE\" , \"unists\" : \"manubot.cite.curie.Handler_CURIE\" , \"unite\" : \"manubot.cite.curie.Handler_CURIE\" , \"uo\" : \"manubot.cite.curie.Handler_CURIE\" , \"url\" : \"manubot.cite.url.Handler_URL\" , \"uspto\" : \"manubot.cite.curie.Handler_CURIE\" , \"validatordb\" : \"manubot.cite.curie.Handler_CURIE\" , \"vario\" : \"manubot.cite.curie.Handler_CURIE\" , \"vbase2\" : \"manubot.cite.curie.Handler_CURIE\" , \"vbrc\" : \"manubot.cite.curie.Handler_CURIE\" , \"vectorbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"vegbank\" : \"manubot.cite.curie.Handler_CURIE\" , \"vfb\" : \"manubot.cite.curie.Handler_CURIE\" , \"vfdb.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"vfdb.genus\" : \"manubot.cite.curie.Handler_CURIE\" , \"vgnc\" : \"manubot.cite.curie.Handler_CURIE\" , \"viaf\" : \"manubot.cite.curie.Handler_CURIE\" , \"vipr\" : \"manubot.cite.curie.Handler_CURIE\" , \"viralzone\" : \"manubot.cite.curie.Handler_CURIE\" , \"virsirna\" : \"manubot.cite.curie.Handler_CURIE\" , \"vmhmetabolite\" : \"manubot.cite.curie.Handler_CURIE\" , \"vmhreaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"wb\" : \"manubot.cite.curie.Handler_CURIE\" , \"wb.rnai\" : \"manubot.cite.curie.Handler_CURIE\" , \"wikidata\" : \"manubot.cite.wikidata.Handler_Wikidata\" , \"wikigenes\" : \"manubot.cite.curie.Handler_CURIE\" , \"wikipathways\" : \"manubot.cite.curie.Handler_CURIE\" , \"wikipedia.en\" : \"manubot.cite.curie.Handler_CURIE\" , \"worfdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"wormpep\" : \"manubot.cite.curie.Handler_CURIE\" , \"worms\" : \"manubot.cite.curie.Handler_CURIE\" , \"xenbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"ydpm\" : \"manubot.cite.curie.Handler_CURIE\" , \"yeastintron\" : \"manubot.cite.curie.Handler_CURIE\" , \"yetfasco\" : \"manubot.cite.curie.Handler_CURIE\" , \"yid\" : \"manubot.cite.curie.Handler_CURIE\" , \"yrcpdr\" : \"manubot.cite.curie.Handler_CURIE\" , \"zfin\" : \"manubot.cite.curie.Handler_CURIE\" , \"zinc\" : \"manubot.cite.curie.Handler_CURIE\" , } Variables prefix_to_handler Functions get_handler def get_handler ( prefix_lower ) View Source @functools . lru_cache ( maxsize = 10 _000 ) def get_handler ( prefix_lower ) : if not isinstance ( prefix_lower , str ) : raise TypeError ( f \"prefix_lower should be a str, instead received {prefix_lower.__class__.__name__}\" ) assert prefix_lower == prefix_lower . lower () handler = prefix_to_handler [ prefix_lower ] handler = import_function ( handler )( prefix_lower ) return handler Classes Handler class Handler ( prefix_lower : str ) A Handler is a class that provides support for a certain type of citekey. For example, a Handler subclass could provide support for DOI citekeys. Subclasses enable custom logic for different citekey prefixes, including how to standardize the citekey and how to retrieve CSL Item metadata. View Source class Handler : \"\"\" A Handler is a class that provides support for a certain type of citekey. For example, a Handler subclass could provide support for DOI citekeys. Subclasses enable custom logic for different citekey prefixes, including how to standardize the citekey and how to retrieve CSL Item metadata. \"\"\" prefix_lower : str prefixes = [] def _get_pattern ( self , attribute = \"accession_pattern\" ) -> typing . Pattern : \"\"\" Return a compiled regex pattern stored by `attribute`. By default, return `self.accession_pattern`, which Handler subclasses can set to provide the expected pattern for `self.accession`. \"\"\" # todo: consider caching compilation pattern = getattr ( self , attribute , None ) if not pattern : return None if not isinstance ( pattern , typing . Pattern ): pattern = re . compile ( pattern ) return pattern def inspect ( self , citekey ): \"\"\" Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. \"\"\" pattern = self . _get_pattern ( \"accession_pattern\" ) if not pattern : return None if not pattern . fullmatch ( citekey . accession ): return f \"{citekey.accession} does not match regex {pattern.pattern}\" def standardize_prefix_accession ( self , accession ): \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession @ abc . abstractmethod def get_csl_item ( self , citekey ): \"\"\" Return a CSL_Item with bibliographic details for citekey. \"\"\" raise NotImplementedError ( f \"Manubot does not know how to generate a csl_item for {citekey.standard_id!r}\" ) Descendants manubot.cite.arxiv.Handler_arXiv manubot.cite.curie.Handler_CURIE manubot.cite.pubmed.Handler_PubMed manubot.cite.pubmed.Handler_PMC manubot.cite.doi.Handler_DOI manubot.cite.isbn.Handler_ISBN manubot.cite.url.Handler_URL manubot.cite.wikidata.Handler_Wikidata Class variables prefixes Methods get_csl_item def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source @abc . abstractmethod def get_csl_item ( self , citekey ) : \"\"\" Return a CSL_Item with bibliographic details for citekey. \"\"\" raise NotImplementedError ( f \"Manubot does not know how to generate a csl_item for {citekey.standard_id!r}\" ) inspect def inspect ( self , citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey ): \"\"\" Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. \"\"\" pattern = self . _get_pattern ( \"accession_pattern\" ) if not pattern : return None if not pattern . fullmatch ( citekey . accession ): return f \"{citekey.accession} does not match regex {pattern.pattern}\" standardize_prefix_accession def standardize_prefix_accession ( self , accession ) Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession ): \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession","title":"Handlers"},{"location":"reference/manubot/cite/handlers/#module-manubotcitehandlers","text":"View Source import abc import dataclasses import functools import re import typing from manubot.util import import_function \"\"\" Non-citation prefixes used by the pandoc-xnos suite of Pandoc filters, including pandoc-fignos, pandoc-tablenos, and pandoc-eqnos. \"\"\" _pandoc_xnos_prefixes = { \"fig\" , \"tbl\" , \"eq\" } _local_handlers = [ \"manubot.cite.arxiv.Handler_arXiv\" , \"manubot.cite.doi.Handler_DOI\" , \"manubot.cite.isbn.Handler_ISBN\" , \"manubot.cite.pubmed.Handler_PMC\" , \"manubot.cite.pubmed.Handler_PubMed\" , \"manubot.cite.url.Handler_URL\" , \"manubot.cite.wikidata.Handler_Wikidata\" , ] @functools.lru_cache ( maxsize = 10 _000 ) def get_handler ( prefix_lower ): if not isinstance ( prefix_lower , str ): raise TypeError ( f \"prefix_lower should be a str, instead received {prefix_lower.__class__.__name__}\" ) assert prefix_lower == prefix_lower . lower () handler = prefix_to_handler [ prefix_lower ] handler = import_function ( handler )( prefix_lower ) return handler def _generate_prefix_to_handler () -> typing . Dict [ str , str ]: \"\"\" Generate complete dictionary for prefix_to_handler. \"\"\" import inspect from .curie import get_curie_handlers curie_handlers = get_curie_handlers () pth = {} for handler in curie_handlers + _local_handlers : if isinstance ( handler , str ): handler = import_function ( handler )( \"dummy_prefix_lower\" ) for prefix in handler . prefixes : pth [ prefix ] = f \"{inspect.getmodule(handler).__name__}.{handler.__class__.__name__}\" pth = dict ( sorted ( pth . items ())) # sort for clean diffs of serialized dict return pth @dataclasses.dataclass class Handler : \"\"\" A Handler is a class that provides support for a certain type of citekey. For example, a Handler subclass could provide support for DOI citekeys. Subclasses enable custom logic for different citekey prefixes, including how to standardize the citekey and how to retrieve CSL Item metadata. \"\"\" prefix_lower : str prefixes = [] def _get_pattern ( self , attribute = \"accession_pattern\" ) -> typing . Pattern : \"\"\" Return a compiled regex pattern stored by `attribute`. By default, return `self.accession_pattern`, which Handler subclasses can set to provide the expected pattern for `self.accession`. \"\"\" # todo: consider caching compilation pattern = getattr ( self , attribute , None ) if not pattern : return None if not isinstance ( pattern , typing . Pattern ): pattern = re . compile ( pattern ) return pattern def inspect ( self , citekey ): \"\"\" Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. \"\"\" pattern = self . _get_pattern ( \"accession_pattern\" ) if not pattern : return None if not pattern . fullmatch ( citekey . accession ): return f \"{citekey.accession} does not match regex {pattern.pattern}\" def standardize_prefix_accession ( self , accession ): \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession @abc.abstractmethod def get_csl_item ( self , citekey ): \"\"\" Return a CSL_Item with bibliographic details for citekey. \"\"\" raise NotImplementedError ( f \"Manubot does not know how to generate a csl_item for {citekey.standard_id!r}\" ) \"\"\" Mapping from lowercase prefix to Handler class function location as string. This output is automatically generated using `_generate_prefix_to_handler`. Hardcoding this mapping reduces startup time and helps keep imports to a minimum, allowing installations without the full dependencies to function. \"\"\" prefix_to_handler : typing . Dict [ str , str ] = { \"3dmet\" : \"manubot.cite.curie.Handler_CURIE\" , \"abs\" : \"manubot.cite.curie.Handler_CURIE\" , \"aceview.worm\" : \"manubot.cite.curie.Handler_CURIE\" , \"addgene\" : \"manubot.cite.curie.Handler_CURIE\" , \"adw\" : \"manubot.cite.curie.Handler_CURIE\" , \"affy.probeset\" : \"manubot.cite.curie.Handler_CURIE\" , \"aftol.taxonomy\" : \"manubot.cite.curie.Handler_CURIE\" , \"agricola\" : \"manubot.cite.curie.Handler_CURIE\" , \"allergome\" : \"manubot.cite.curie.Handler_CURIE\" , \"amoebadb\" : \"manubot.cite.curie.Handler_CURIE\" , \"antibodyregistry\" : \"manubot.cite.curie.Handler_CURIE\" , \"antweb\" : \"manubot.cite.curie.Handler_CURIE\" , \"aop\" : \"manubot.cite.curie.Handler_CURIE\" , \"aop.events\" : \"manubot.cite.curie.Handler_CURIE\" , \"aop.relationships\" : \"manubot.cite.curie.Handler_CURIE\" , \"aop.stressor\" : \"manubot.cite.curie.Handler_CURIE\" , \"apd\" : \"manubot.cite.curie.Handler_CURIE\" , \"aphidbase.transcript\" : \"manubot.cite.curie.Handler_CURIE\" , \"apid.interactions\" : \"manubot.cite.curie.Handler_CURIE\" , \"arachnoserver\" : \"manubot.cite.curie.Handler_CURIE\" , \"ardb\" : \"manubot.cite.curie.Handler_CURIE\" , \"ark\" : \"manubot.cite.curie.Handler_CURIE\" , \"arrayexpress\" : \"manubot.cite.curie.Handler_CURIE\" , \"arrayexpress.platform\" : \"manubot.cite.curie.Handler_CURIE\" , \"arraymap\" : \"manubot.cite.curie.Handler_CURIE\" , \"arxiv\" : \"manubot.cite.arxiv.Handler_arXiv\" , \"asap\" : \"manubot.cite.curie.Handler_CURIE\" , \"ascl\" : \"manubot.cite.curie.Handler_CURIE\" , \"asin\" : \"manubot.cite.curie.Handler_CURIE\" , \"aspgd.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"aspgd.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"atc\" : \"manubot.cite.curie.Handler_CURIE\" , \"atcc\" : \"manubot.cite.curie.Handler_CURIE\" , \"atcvet\" : \"manubot.cite.curie.Handler_CURIE\" , \"atfdb.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"autdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"bacdive\" : \"manubot.cite.curie.Handler_CURIE\" , \"bacmap.biog\" : \"manubot.cite.curie.Handler_CURIE\" , \"bacmap.map\" : \"manubot.cite.curie.Handler_CURIE\" , \"bao\" : \"manubot.cite.curie.Handler_CURIE\" , \"bdgp.est\" : \"manubot.cite.curie.Handler_CURIE\" , \"bdgp.insertion\" : \"manubot.cite.curie.Handler_CURIE\" , \"bdsc\" : \"manubot.cite.curie.Handler_CURIE\" , \"beetlebase\" : \"manubot.cite.curie.Handler_CURIE\" , \"begdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"bgee.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"bgee.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"bgee.organ\" : \"manubot.cite.curie.Handler_CURIE\" , \"bgee.stage\" : \"manubot.cite.curie.Handler_CURIE\" , \"bigg.compartment\" : \"manubot.cite.curie.Handler_CURIE\" , \"bigg.metabolite\" : \"manubot.cite.curie.Handler_CURIE\" , \"bigg.model\" : \"manubot.cite.curie.Handler_CURIE\" , \"bigg.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"bindingdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"biocarta.pathway\" : \"manubot.cite.curie.Handler_CURIE\" , \"biocatalogue.service\" : \"manubot.cite.curie.Handler_CURIE\" , \"biocyc\" : \"manubot.cite.curie.Handler_CURIE\" , \"biogrid\" : \"manubot.cite.curie.Handler_CURIE\" , \"biolink\" : \"manubot.cite.curie.Handler_CURIE\" , \"biominder\" : \"manubot.cite.curie.Handler_CURIE\" , \"biomodels.db\" : \"manubot.cite.curie.Handler_CURIE\" , \"biomodels.kisao\" : \"manubot.cite.curie.Handler_CURIE\" , \"biomodels.teddy\" : \"manubot.cite.curie.Handler_CURIE\" , \"biomodels.vocabulary\" : \"manubot.cite.curie.Handler_CURIE\" , \"bionumbers\" : \"manubot.cite.curie.Handler_CURIE\" , \"bioportal\" : \"manubot.cite.curie.Handler_CURIE\" , \"bioproject\" : \"manubot.cite.curie.Handler_CURIE\" , \"biosample\" : \"manubot.cite.curie.Handler_CURIE\" , \"biostudies\" : \"manubot.cite.curie.Handler_CURIE\" , \"biosystems\" : \"manubot.cite.curie.Handler_CURIE\" , \"biotools\" : \"manubot.cite.curie.Handler_CURIE\" , \"bitterdb.cpd\" : \"manubot.cite.curie.Handler_CURIE\" , \"bitterdb.rec\" : \"manubot.cite.curie.Handler_CURIE\" , \"bmrb\" : \"manubot.cite.curie.Handler_CURIE\" , \"bold.taxonomy\" : \"manubot.cite.curie.Handler_CURIE\" , \"brenda\" : \"manubot.cite.curie.Handler_CURIE\" , \"broad\" : \"manubot.cite.curie.Handler_CURIE\" , \"bto\" : \"manubot.cite.curie.Handler_CURIE\" , \"bugbase.expt\" : \"manubot.cite.curie.Handler_CURIE\" , \"bugbase.protocol\" : \"manubot.cite.curie.Handler_CURIE\" , \"bykdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"cabri\" : \"manubot.cite.curie.Handler_CURIE\" , \"cadsr\" : \"manubot.cite.curie.Handler_CURIE\" , \"cameo\" : \"manubot.cite.curie.Handler_CURIE\" , \"caps\" : \"manubot.cite.curie.Handler_CURIE\" , \"cas\" : \"manubot.cite.curie.Handler_CURIE\" , \"cath\" : \"manubot.cite.curie.Handler_CURIE\" , \"cath.domain\" : \"manubot.cite.curie.Handler_CURIE\" , \"cath.superfamily\" : \"manubot.cite.curie.Handler_CURIE\" , \"cattleqtldb\" : \"manubot.cite.curie.Handler_CURIE\" , \"cazy\" : \"manubot.cite.curie.Handler_CURIE\" , \"cbioportal\" : \"manubot.cite.curie.Handler_CURIE\" , \"ccds\" : \"manubot.cite.curie.Handler_CURIE\" , \"cco\" : \"manubot.cite.curie.Handler_CURIE\" , \"cdd\" : \"manubot.cite.curie.Handler_CURIE\" , \"cdpd\" : \"manubot.cite.curie.Handler_CURIE\" , \"cellimage\" : \"manubot.cite.curie.Handler_CURIE\" , \"cellosaurus\" : \"manubot.cite.curie.Handler_CURIE\" , \"cgd\" : \"manubot.cite.curie.Handler_CURIE\" , \"cgsc\" : \"manubot.cite.curie.Handler_CURIE\" , \"charprot\" : \"manubot.cite.curie.Handler_CURIE\" , \"chebi\" : \"manubot.cite.curie.Handler_CURIE\" , \"chembl.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"chembl.target\" : \"manubot.cite.curie.Handler_CURIE\" , \"chemdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"chemidplus\" : \"manubot.cite.curie.Handler_CURIE\" , \"chemspider\" : \"manubot.cite.curie.Handler_CURIE\" , \"chickenqtldb\" : \"manubot.cite.curie.Handler_CURIE\" , \"cl\" : \"manubot.cite.curie.Handler_CURIE\" , \"classyfire\" : \"manubot.cite.curie.Handler_CURIE\" , \"cldb\" : \"manubot.cite.curie.Handler_CURIE\" , \"clinicaltrials\" : \"manubot.cite.curie.Handler_CURIE\" , \"clinvar\" : \"manubot.cite.curie.Handler_CURIE\" , \"clinvar.record\" : \"manubot.cite.curie.Handler_CURIE\" , \"clinvar.submission\" : \"manubot.cite.curie.Handler_CURIE\" , \"combine.specifications\" : \"manubot.cite.curie.Handler_CURIE\" , \"complexportal\" : \"manubot.cite.curie.Handler_CURIE\" , \"comptox\" : \"manubot.cite.curie.Handler_CURIE\" , \"compulyeast\" : \"manubot.cite.curie.Handler_CURIE\" , \"conoserver\" : \"manubot.cite.curie.Handler_CURIE\" , \"coriell\" : \"manubot.cite.curie.Handler_CURIE\" , \"corum\" : \"manubot.cite.curie.Handler_CURIE\" , \"cosmic\" : \"manubot.cite.curie.Handler_CURIE\" , \"cpc\" : \"manubot.cite.curie.Handler_CURIE\" , \"crisprdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"cryptodb\" : \"manubot.cite.curie.Handler_CURIE\" , \"csa\" : \"manubot.cite.curie.Handler_CURIE\" , \"cst\" : \"manubot.cite.curie.Handler_CURIE\" , \"cst.ab\" : \"manubot.cite.curie.Handler_CURIE\" , \"ctd.chemical\" : \"manubot.cite.curie.Handler_CURIE\" , \"ctd.disease\" : \"manubot.cite.curie.Handler_CURIE\" , \"ctd.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"cubedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"d1id\" : \"manubot.cite.curie.Handler_CURIE\" , \"dailymed\" : \"manubot.cite.curie.Handler_CURIE\" , \"dandi\" : \"manubot.cite.curie.Handler_CURIE\" , \"darc\" : \"manubot.cite.curie.Handler_CURIE\" , \"dashr\" : \"manubot.cite.curie.Handler_CURIE\" , \"dashr.expression\" : \"manubot.cite.curie.Handler_CURIE\" , \"datf\" : \"manubot.cite.curie.Handler_CURIE\" , \"dbd\" : \"manubot.cite.curie.Handler_CURIE\" , \"dbest\" : \"manubot.cite.curie.Handler_CURIE\" , \"dbg2introns\" : \"manubot.cite.curie.Handler_CURIE\" , \"dbgap\" : \"manubot.cite.curie.Handler_CURIE\" , \"dbprobe\" : \"manubot.cite.curie.Handler_CURIE\" , \"dbsnp\" : \"manubot.cite.curie.Handler_CURIE\" , \"degradome\" : \"manubot.cite.curie.Handler_CURIE\" , \"depod\" : \"manubot.cite.curie.Handler_CURIE\" , \"dev.ga4ghdos\" : \"manubot.cite.curie.Handler_CURIE\" , \"dictybase.est\" : \"manubot.cite.curie.Handler_CURIE\" , \"dictybase.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"did\" : \"manubot.cite.curie.Handler_CURIE\" , \"dip\" : \"manubot.cite.curie.Handler_CURIE\" , \"disprot\" : \"manubot.cite.curie.Handler_CURIE\" , \"disprot.region\" : \"manubot.cite.curie.Handler_CURIE\" , \"dlxb\" : \"manubot.cite.curie.Handler_CURIE\" , \"dlxc\" : \"manubot.cite.curie.Handler_CURIE\" , \"doi\" : \"manubot.cite.doi.Handler_DOI\" , \"doid\" : \"manubot.cite.curie.Handler_CURIE\" , \"dommino\" : \"manubot.cite.curie.Handler_CURIE\" , \"door\" : \"manubot.cite.curie.Handler_CURIE\" , \"doqcs.model\" : \"manubot.cite.curie.Handler_CURIE\" , \"doqcs.pathway\" : \"manubot.cite.curie.Handler_CURIE\" , \"dpv\" : \"manubot.cite.curie.Handler_CURIE\" , \"dragondb.allele\" : \"manubot.cite.curie.Handler_CURIE\" , \"dragondb.dna\" : \"manubot.cite.curie.Handler_CURIE\" , \"dragondb.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"dragondb.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"drsc\" : \"manubot.cite.curie.Handler_CURIE\" , \"drugbank\" : \"manubot.cite.curie.Handler_CURIE\" , \"drugbankv4.target\" : \"manubot.cite.curie.Handler_CURIE\" , \"ec-code\" : \"manubot.cite.curie.Handler_CURIE\" , \"echobase\" : \"manubot.cite.curie.Handler_CURIE\" , \"eco\" : \"manubot.cite.curie.Handler_CURIE\" , \"ecogene\" : \"manubot.cite.curie.Handler_CURIE\" , \"ecoliwiki\" : \"manubot.cite.curie.Handler_CURIE\" , \"ecyano.entity\" : \"manubot.cite.curie.Handler_CURIE\" , \"ecyano.experiment\" : \"manubot.cite.curie.Handler_CURIE\" , \"ecyano.model\" : \"manubot.cite.curie.Handler_CURIE\" , \"ecyano.rule\" : \"manubot.cite.curie.Handler_CURIE\" , \"edam\" : \"manubot.cite.curie.Handler_CURIE\" , \"efo\" : \"manubot.cite.curie.Handler_CURIE\" , \"ega.dataset\" : \"manubot.cite.curie.Handler_CURIE\" , \"ega.study\" : \"manubot.cite.curie.Handler_CURIE\" , \"eggnog\" : \"manubot.cite.curie.Handler_CURIE\" , \"elm\" : \"manubot.cite.curie.Handler_CURIE\" , \"emdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"ena.embl\" : \"manubot.cite.curie.Handler_CURIE\" , \"encode\" : \"manubot.cite.curie.Handler_CURIE\" , \"ensembl\" : \"manubot.cite.curie.Handler_CURIE\" , \"ensembl.bacteria\" : \"manubot.cite.curie.Handler_CURIE\" , \"ensembl.fungi\" : \"manubot.cite.curie.Handler_CURIE\" , \"ensembl.metazoa\" : \"manubot.cite.curie.Handler_CURIE\" , \"ensembl.plant\" : \"manubot.cite.curie.Handler_CURIE\" , \"ensembl.protist\" : \"manubot.cite.curie.Handler_CURIE\" , \"envipath\" : \"manubot.cite.curie.Handler_CURIE\" , \"envo\" : \"manubot.cite.curie.Handler_CURIE\" , \"eo\" : \"manubot.cite.curie.Handler_CURIE\" , \"epd\" : \"manubot.cite.curie.Handler_CURIE\" , \"erm\" : \"manubot.cite.curie.Handler_CURIE\" , \"erv\" : \"manubot.cite.curie.Handler_CURIE\" , \"eu89h\" : \"manubot.cite.curie.Handler_CURIE\" , \"euclinicaltrials\" : \"manubot.cite.curie.Handler_CURIE\" , \"exac.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"exac.transcript\" : \"manubot.cite.curie.Handler_CURIE\" , \"exac.variant\" : \"manubot.cite.curie.Handler_CURIE\" , \"facebase\" : \"manubot.cite.curie.Handler_CURIE\" , \"fairsharing\" : \"manubot.cite.curie.Handler_CURIE\" , \"fb\" : \"manubot.cite.curie.Handler_CURIE\" , \"fbol\" : \"manubot.cite.curie.Handler_CURIE\" , \"flowrepository\" : \"manubot.cite.curie.Handler_CURIE\" , \"fma\" : \"manubot.cite.curie.Handler_CURIE\" , \"foodb.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"foodon\" : \"manubot.cite.curie.Handler_CURIE\" , \"fplx\" : \"manubot.cite.curie.Handler_CURIE\" , \"fsnp\" : \"manubot.cite.curie.Handler_CURIE\" , \"funcbase.fly\" : \"manubot.cite.curie.Handler_CURIE\" , \"funcbase.human\" : \"manubot.cite.curie.Handler_CURIE\" , \"funcbase.mouse\" : \"manubot.cite.curie.Handler_CURIE\" , \"funcbase.yeast\" : \"manubot.cite.curie.Handler_CURIE\" , \"fungidb\" : \"manubot.cite.curie.Handler_CURIE\" , \"ga4ghdos\" : \"manubot.cite.curie.Handler_CURIE\" , \"gabi\" : \"manubot.cite.curie.Handler_CURIE\" , \"gcst\" : \"manubot.cite.curie.Handler_CURIE\" , \"gdc\" : \"manubot.cite.curie.Handler_CURIE\" , \"gdsc\" : \"manubot.cite.curie.Handler_CURIE\" , \"genatlas\" : \"manubot.cite.curie.Handler_CURIE\" , \"genecards\" : \"manubot.cite.curie.Handler_CURIE\" , \"genedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"genefarm\" : \"manubot.cite.curie.Handler_CURIE\" , \"genetree\" : \"manubot.cite.curie.Handler_CURIE\" , \"genewiki\" : \"manubot.cite.curie.Handler_CURIE\" , \"genpept\" : \"manubot.cite.curie.Handler_CURIE\" , \"genprop\" : \"manubot.cite.curie.Handler_CURIE\" , \"geo\" : \"manubot.cite.curie.Handler_CURIE\" , \"giardiadb\" : \"manubot.cite.curie.Handler_CURIE\" , \"glida.gpcr\" : \"manubot.cite.curie.Handler_CURIE\" , \"glida.ligand\" : \"manubot.cite.curie.Handler_CURIE\" , \"glycoepitope\" : \"manubot.cite.curie.Handler_CURIE\" , \"glycomedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"glytoucan\" : \"manubot.cite.curie.Handler_CURIE\" , \"gmd\" : \"manubot.cite.curie.Handler_CURIE\" , \"gmd.analyte\" : \"manubot.cite.curie.Handler_CURIE\" , \"gmd.gcms\" : \"manubot.cite.curie.Handler_CURIE\" , \"gmd.profile\" : \"manubot.cite.curie.Handler_CURIE\" , \"gmd.ref\" : \"manubot.cite.curie.Handler_CURIE\" , \"gnpis\" : \"manubot.cite.curie.Handler_CURIE\" , \"go\" : \"manubot.cite.curie.Handler_CURIE\" , \"go.ref\" : \"manubot.cite.curie.Handler_CURIE\" , \"goa\" : \"manubot.cite.curie.Handler_CURIE\" , \"gold\" : \"manubot.cite.curie.Handler_CURIE\" , \"gold.genome\" : \"manubot.cite.curie.Handler_CURIE\" , \"gold.meta\" : \"manubot.cite.curie.Handler_CURIE\" , \"google.patent\" : \"manubot.cite.curie.Handler_CURIE\" , \"gpcrdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"gpmdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"gramene.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"gramene.growthstage\" : \"manubot.cite.curie.Handler_CURIE\" , \"gramene.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"gramene.qtl\" : \"manubot.cite.curie.Handler_CURIE\" , \"gramene.taxonomy\" : \"manubot.cite.curie.Handler_CURIE\" , \"greengenes\" : \"manubot.cite.curie.Handler_CURIE\" , \"grid\" : \"manubot.cite.curie.Handler_CURIE\" , \"grin.taxonomy\" : \"manubot.cite.curie.Handler_CURIE\" , \"gro\" : \"manubot.cite.curie.Handler_CURIE\" , \"grsdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"gtex\" : \"manubot.cite.curie.Handler_CURIE\" , \"gudmap\" : \"manubot.cite.curie.Handler_CURIE\" , \"gwascentral.marker\" : \"manubot.cite.curie.Handler_CURIE\" , \"gwascentral.phenotype\" : \"manubot.cite.curie.Handler_CURIE\" , \"gwascentral.study\" : \"manubot.cite.curie.Handler_CURIE\" , \"gxa.expt\" : \"manubot.cite.curie.Handler_CURIE\" , \"gxa.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"hamap\" : \"manubot.cite.curie.Handler_CURIE\" , \"hcvdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"hdr\" : \"manubot.cite.curie.Handler_CURIE\" , \"hgmd\" : \"manubot.cite.curie.Handler_CURIE\" , \"hgnc\" : \"manubot.cite.curie.Handler_CURIE\" , \"hgnc.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"hgnc.genefamily\" : \"manubot.cite.curie.Handler_CURIE\" , \"hgnc.symbol\" : \"manubot.cite.curie.Handler_CURIE\" , \"hinv.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"hinv.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"hinv.transcript\" : \"manubot.cite.curie.Handler_CURIE\" , \"hmdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"hogenom\" : \"manubot.cite.curie.Handler_CURIE\" , \"homd.seq\" : \"manubot.cite.curie.Handler_CURIE\" , \"homd.taxon\" : \"manubot.cite.curie.Handler_CURIE\" , \"homologene\" : \"manubot.cite.curie.Handler_CURIE\" , \"hovergen\" : \"manubot.cite.curie.Handler_CURIE\" , \"hp\" : \"manubot.cite.curie.Handler_CURIE\" , \"hpa\" : \"manubot.cite.curie.Handler_CURIE\" , \"hpm.peptide\" : \"manubot.cite.curie.Handler_CURIE\" , \"hpm.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"hprd\" : \"manubot.cite.curie.Handler_CURIE\" , \"hpscreg\" : \"manubot.cite.curie.Handler_CURIE\" , \"hssp\" : \"manubot.cite.curie.Handler_CURIE\" , \"http\" : \"manubot.cite.url.Handler_URL\" , \"https\" : \"manubot.cite.url.Handler_URL\" , \"huge\" : \"manubot.cite.curie.Handler_CURIE\" , \"iao\" : \"manubot.cite.curie.Handler_CURIE\" , \"icd\" : \"manubot.cite.curie.Handler_CURIE\" , \"iceberg.element\" : \"manubot.cite.curie.Handler_CURIE\" , \"iceberg.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"ideal\" : \"manubot.cite.curie.Handler_CURIE\" , \"ido\" : \"manubot.cite.curie.Handler_CURIE\" , \"idoo\" : \"manubot.cite.curie.Handler_CURIE\" , \"idot\" : \"manubot.cite.curie.Handler_CURIE\" , \"idr\" : \"manubot.cite.curie.Handler_CURIE\" , \"imex\" : \"manubot.cite.curie.Handler_CURIE\" , \"img.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"img.taxon\" : \"manubot.cite.curie.Handler_CURIE\" , \"imgt.hla\" : \"manubot.cite.curie.Handler_CURIE\" , \"imgt.ligm\" : \"manubot.cite.curie.Handler_CURIE\" , \"inchi\" : \"manubot.cite.curie.Handler_CURIE\" , \"inchikey\" : \"manubot.cite.curie.Handler_CURIE\" , \"insdc\" : \"manubot.cite.curie.Handler_CURIE\" , \"insdc.cds\" : \"manubot.cite.curie.Handler_CURIE\" , \"insdc.gca\" : \"manubot.cite.curie.Handler_CURIE\" , \"insdc.sra\" : \"manubot.cite.curie.Handler_CURIE\" , \"intact\" : \"manubot.cite.curie.Handler_CURIE\" , \"intact.molecule\" : \"manubot.cite.curie.Handler_CURIE\" , \"interpro\" : \"manubot.cite.curie.Handler_CURIE\" , \"ird.segment\" : \"manubot.cite.curie.Handler_CURIE\" , \"irefweb\" : \"manubot.cite.curie.Handler_CURIE\" , \"isbn\" : \"manubot.cite.isbn.Handler_ISBN\" , \"isfinder\" : \"manubot.cite.curie.Handler_CURIE\" , \"isni\" : \"manubot.cite.curie.Handler_CURIE\" , \"issn\" : \"manubot.cite.curie.Handler_CURIE\" , \"iuphar.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"iuphar.ligand\" : \"manubot.cite.curie.Handler_CURIE\" , \"iuphar.receptor\" : \"manubot.cite.curie.Handler_CURIE\" , \"jaxmice\" : \"manubot.cite.curie.Handler_CURIE\" , \"jcggdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"jcm\" : \"manubot.cite.curie.Handler_CURIE\" , \"jcsd\" : \"manubot.cite.curie.Handler_CURIE\" , \"jstor\" : \"manubot.cite.curie.Handler_CURIE\" , \"jws\" : \"manubot.cite.curie.Handler_CURIE\" , \"kaggle\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.disease\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.drug\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.environ\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.genes\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.genome\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.glycan\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.metagenome\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.module\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.orthology\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.pathway\" : \"manubot.cite.curie.Handler_CURIE\" , \"kegg.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"knapsack\" : \"manubot.cite.curie.Handler_CURIE\" , \"lei\" : \"manubot.cite.curie.Handler_CURIE\" , \"lgic\" : \"manubot.cite.curie.Handler_CURIE\" , \"licebase\" : \"manubot.cite.curie.Handler_CURIE\" , \"ligandbook\" : \"manubot.cite.curie.Handler_CURIE\" , \"ligandbox\" : \"manubot.cite.curie.Handler_CURIE\" , \"ligandexpo\" : \"manubot.cite.curie.Handler_CURIE\" , \"lincs.cell\" : \"manubot.cite.curie.Handler_CURIE\" , \"lincs.data\" : \"manubot.cite.curie.Handler_CURIE\" , \"lincs.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"lincs.smallmolecule\" : \"manubot.cite.curie.Handler_CURIE\" , \"lipidbank\" : \"manubot.cite.curie.Handler_CURIE\" , \"lipidmaps\" : \"manubot.cite.curie.Handler_CURIE\" , \"lrg\" : \"manubot.cite.curie.Handler_CURIE\" , \"ma\" : \"manubot.cite.curie.Handler_CURIE\" , \"macie\" : \"manubot.cite.curie.Handler_CURIE\" , \"maizegdb.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"mamo\" : \"manubot.cite.curie.Handler_CURIE\" , \"massbank\" : \"manubot.cite.curie.Handler_CURIE\" , \"massive\" : \"manubot.cite.curie.Handler_CURIE\" , \"matrixdb.association\" : \"manubot.cite.curie.Handler_CURIE\" , \"mdm\" : \"manubot.cite.curie.Handler_CURIE\" , \"meddra\" : \"manubot.cite.curie.Handler_CURIE\" , \"medgen\" : \"manubot.cite.curie.Handler_CURIE\" , \"medlineplus\" : \"manubot.cite.curie.Handler_CURIE\" , \"merops\" : \"manubot.cite.curie.Handler_CURIE\" , \"merops.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"merops.inhibitor\" : \"manubot.cite.curie.Handler_CURIE\" , \"mesh\" : \"manubot.cite.curie.Handler_CURIE\" , \"mesh.2012\" : \"manubot.cite.curie.Handler_CURIE\" , \"mesh.2013\" : \"manubot.cite.curie.Handler_CURIE\" , \"metabolights\" : \"manubot.cite.curie.Handler_CURIE\" , \"metacyc.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"metacyc.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"metanetx.chemical\" : \"manubot.cite.curie.Handler_CURIE\" , \"metanetx.compartment\" : \"manubot.cite.curie.Handler_CURIE\" , \"metanetx.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"metlin\" : \"manubot.cite.curie.Handler_CURIE\" , \"mex\" : \"manubot.cite.curie.Handler_CURIE\" , \"mge\" : \"manubot.cite.curie.Handler_CURIE\" , \"mgi\" : \"manubot.cite.curie.Handler_CURIE\" , \"mgnify.proj\" : \"manubot.cite.curie.Handler_CURIE\" , \"mgnify.samp\" : \"manubot.cite.curie.Handler_CURIE\" , \"mi\" : \"manubot.cite.curie.Handler_CURIE\" , \"microscope\" : \"manubot.cite.curie.Handler_CURIE\" , \"microsporidia\" : \"manubot.cite.curie.Handler_CURIE\" , \"mim\" : \"manubot.cite.curie.Handler_CURIE\" , \"mimodb\" : \"manubot.cite.curie.Handler_CURIE\" , \"minid\" : \"manubot.cite.curie.Handler_CURIE\" , \"minid.test\" : \"manubot.cite.curie.Handler_CURIE\" , \"mint\" : \"manubot.cite.curie.Handler_CURIE\" , \"mipmod\" : \"manubot.cite.curie.Handler_CURIE\" , \"mir\" : \"manubot.cite.curie.Handler_CURIE\" , \"mirbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"mirbase.mature\" : \"manubot.cite.curie.Handler_CURIE\" , \"mirex\" : \"manubot.cite.curie.Handler_CURIE\" , \"miriam.collection\" : \"manubot.cite.curie.Handler_CURIE\" , \"miriam.resource\" : \"manubot.cite.curie.Handler_CURIE\" , \"mirnest\" : \"manubot.cite.curie.Handler_CURIE\" , \"mirtarbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmmp:biomaps\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmp.cat\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmp.db\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmp.fun\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmp.ref\" : \"manubot.cite.curie.Handler_CURIE\" , \"mmrrc\" : \"manubot.cite.curie.Handler_CURIE\" , \"mo\" : \"manubot.cite.curie.Handler_CURIE\" , \"mobidb\" : \"manubot.cite.curie.Handler_CURIE\" , \"mod\" : \"manubot.cite.curie.Handler_CURIE\" , \"modeldb\" : \"manubot.cite.curie.Handler_CURIE\" , \"molbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"mp\" : \"manubot.cite.curie.Handler_CURIE\" , \"mpid\" : \"manubot.cite.curie.Handler_CURIE\" , \"ms\" : \"manubot.cite.curie.Handler_CURIE\" , \"multicellds.cell_line\" : \"manubot.cite.curie.Handler_CURIE\" , \"multicellds.collection\" : \"manubot.cite.curie.Handler_CURIE\" , \"multicellds.snapshot\" : \"manubot.cite.curie.Handler_CURIE\" , \"mw.project\" : \"manubot.cite.curie.Handler_CURIE\" , \"mw.study\" : \"manubot.cite.curie.Handler_CURIE\" , \"myco.lepra\" : \"manubot.cite.curie.Handler_CURIE\" , \"myco.marinum\" : \"manubot.cite.curie.Handler_CURIE\" , \"myco.smeg\" : \"manubot.cite.curie.Handler_CURIE\" , \"myco.tuber\" : \"manubot.cite.curie.Handler_CURIE\" , \"mycobank\" : \"manubot.cite.curie.Handler_CURIE\" , \"mzspec\" : \"manubot.cite.curie.Handler_CURIE\" , \"napdi\" : \"manubot.cite.curie.Handler_CURIE\" , \"napp\" : \"manubot.cite.curie.Handler_CURIE\" , \"narcis\" : \"manubot.cite.curie.Handler_CURIE\" , \"nasc\" : \"manubot.cite.curie.Handler_CURIE\" , \"nbn\" : \"manubot.cite.curie.Handler_CURIE\" , \"nbrc\" : \"manubot.cite.curie.Handler_CURIE\" , \"ncbigene\" : \"manubot.cite.curie.Handler_CURIE\" , \"ncbiprotein\" : \"manubot.cite.curie.Handler_CURIE\" , \"ncim\" : \"manubot.cite.curie.Handler_CURIE\" , \"ncit\" : \"manubot.cite.curie.Handler_CURIE\" , \"ndc\" : \"manubot.cite.curie.Handler_CURIE\" , \"nemo\" : \"manubot.cite.curie.Handler_CURIE\" , \"neurolex\" : \"manubot.cite.curie.Handler_CURIE\" , \"neuromorpho\" : \"manubot.cite.curie.Handler_CURIE\" , \"neurondb\" : \"manubot.cite.curie.Handler_CURIE\" , \"neurovault.collection\" : \"manubot.cite.curie.Handler_CURIE\" , \"neurovault.image\" : \"manubot.cite.curie.Handler_CURIE\" , \"nextdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"nextprot\" : \"manubot.cite.curie.Handler_CURIE\" , \"ngl\" : \"manubot.cite.curie.Handler_CURIE\" , \"niaest\" : \"manubot.cite.curie.Handler_CURIE\" , \"nmr\" : \"manubot.cite.curie.Handler_CURIE\" , \"nmrshiftdb2\" : \"manubot.cite.curie.Handler_CURIE\" , \"noncodev3\" : \"manubot.cite.curie.Handler_CURIE\" , \"noncodev4.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"noncodev4.rna\" : \"manubot.cite.curie.Handler_CURIE\" , \"norine\" : \"manubot.cite.curie.Handler_CURIE\" , \"nuclearbd\" : \"manubot.cite.curie.Handler_CURIE\" , \"obi\" : \"manubot.cite.curie.Handler_CURIE\" , \"occ\" : \"manubot.cite.curie.Handler_CURIE\" , \"oci\" : \"manubot.cite.curie.Handler_CURIE\" , \"ocid\" : \"manubot.cite.curie.Handler_CURIE\" , \"oclc\" : \"manubot.cite.curie.Handler_CURIE\" , \"odor\" : \"manubot.cite.curie.Handler_CURIE\" , \"oid\" : \"manubot.cite.curie.Handler_CURIE\" , \"oma.grp\" : \"manubot.cite.curie.Handler_CURIE\" , \"oma.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"omia\" : \"manubot.cite.curie.Handler_CURIE\" , \"omit\" : \"manubot.cite.curie.Handler_CURIE\" , \"opb\" : \"manubot.cite.curie.Handler_CURIE\" , \"opm\" : \"manubot.cite.curie.Handler_CURIE\" , \"orcid\" : \"manubot.cite.curie.Handler_CURIE\" , \"ordb\" : \"manubot.cite.curie.Handler_CURIE\" , \"oridb.sacch\" : \"manubot.cite.curie.Handler_CURIE\" , \"oridb.schizo\" : \"manubot.cite.curie.Handler_CURIE\" , \"orphanet\" : \"manubot.cite.curie.Handler_CURIE\" , \"orphanet.ordo\" : \"manubot.cite.curie.Handler_CURIE\" , \"orthodb\" : \"manubot.cite.curie.Handler_CURIE\" , \"oryzabase.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"oryzabase.mutant\" : \"manubot.cite.curie.Handler_CURIE\" , \"oryzabase.reference\" : \"manubot.cite.curie.Handler_CURIE\" , \"oryzabase.stage\" : \"manubot.cite.curie.Handler_CURIE\" , \"oryzabase.strain\" : \"manubot.cite.curie.Handler_CURIE\" , \"otl\" : \"manubot.cite.curie.Handler_CURIE\" , \"p3db.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"p3db.site\" : \"manubot.cite.curie.Handler_CURIE\" , \"paleodb\" : \"manubot.cite.curie.Handler_CURIE\" , \"panther.family\" : \"manubot.cite.curie.Handler_CURIE\" , \"panther.node\" : \"manubot.cite.curie.Handler_CURIE\" , \"panther.pathway\" : \"manubot.cite.curie.Handler_CURIE\" , \"panther.pthcmp\" : \"manubot.cite.curie.Handler_CURIE\" , \"pass2\" : \"manubot.cite.curie.Handler_CURIE\" , \"pathwaycommons\" : \"manubot.cite.curie.Handler_CURIE\" , \"pato\" : \"manubot.cite.curie.Handler_CURIE\" , \"paxdb.organism\" : \"manubot.cite.curie.Handler_CURIE\" , \"paxdb.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"pazar\" : \"manubot.cite.curie.Handler_CURIE\" , \"pdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pdb-ccd\" : \"manubot.cite.curie.Handler_CURIE\" , \"pdb.ligand\" : \"manubot.cite.curie.Handler_CURIE\" , \"peptideatlas\" : \"manubot.cite.curie.Handler_CURIE\" , \"peptideatlas.dataset\" : \"manubot.cite.curie.Handler_CURIE\" , \"peroxibase\" : \"manubot.cite.curie.Handler_CURIE\" , \"pfam\" : \"manubot.cite.curie.Handler_CURIE\" , \"pgs\" : \"manubot.cite.curie.Handler_CURIE\" , \"pgx\" : \"manubot.cite.curie.Handler_CURIE\" , \"pharmgkb.disease\" : \"manubot.cite.curie.Handler_CURIE\" , \"pharmgkb.drug\" : \"manubot.cite.curie.Handler_CURIE\" , \"pharmgkb.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"pharmgkb.pathways\" : \"manubot.cite.curie.Handler_CURIE\" , \"phenolexplorer\" : \"manubot.cite.curie.Handler_CURIE\" , \"phosphopoint.kinase\" : \"manubot.cite.curie.Handler_CURIE\" , \"phosphopoint.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"phosphosite.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"phosphosite.residue\" : \"manubot.cite.curie.Handler_CURIE\" , \"phylomedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"phytozome.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"pid.pathway\" : \"manubot.cite.curie.Handler_CURIE\" , \"pigqtldb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pina\" : \"manubot.cite.curie.Handler_CURIE\" , \"piroplasma\" : \"manubot.cite.curie.Handler_CURIE\" , \"pirsf\" : \"manubot.cite.curie.Handler_CURIE\" , \"planttfdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"plasmodb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pmap.cutdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pmap.substratedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pmc\" : \"manubot.cite.pubmed.Handler_PMC\" , \"pmcid\" : \"manubot.cite.pubmed.Handler_PMC\" , \"pmdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pmid\" : \"manubot.cite.pubmed.Handler_PubMed\" , \"pmp\" : \"manubot.cite.curie.Handler_CURIE\" , \"po\" : \"manubot.cite.curie.Handler_CURIE\" , \"pocketome\" : \"manubot.cite.curie.Handler_CURIE\" , \"polbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"pombase\" : \"manubot.cite.curie.Handler_CURIE\" , \"pr\" : \"manubot.cite.curie.Handler_CURIE\" , \"pride\" : \"manubot.cite.curie.Handler_CURIE\" , \"pride.project\" : \"manubot.cite.curie.Handler_CURIE\" , \"prints\" : \"manubot.cite.curie.Handler_CURIE\" , \"probonto\" : \"manubot.cite.curie.Handler_CURIE\" , \"prodom\" : \"manubot.cite.curie.Handler_CURIE\" , \"proglyc\" : \"manubot.cite.curie.Handler_CURIE\" , \"prosite\" : \"manubot.cite.curie.Handler_CURIE\" , \"protclustdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"proteomicsdb.peptide\" : \"manubot.cite.curie.Handler_CURIE\" , \"proteomicsdb.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"protonet.cluster\" : \"manubot.cite.curie.Handler_CURIE\" , \"protonet.proteincard\" : \"manubot.cite.curie.Handler_CURIE\" , \"pscdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"pseudomonas\" : \"manubot.cite.curie.Handler_CURIE\" , \"psimi\" : \"manubot.cite.curie.Handler_CURIE\" , \"psipar\" : \"manubot.cite.curie.Handler_CURIE\" , \"pubchem.bioassay\" : \"manubot.cite.curie.Handler_CURIE\" , \"pubchem.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"pubchem.substance\" : \"manubot.cite.curie.Handler_CURIE\" , \"pubmed\" : \"manubot.cite.pubmed.Handler_PubMed\" , \"pw\" : \"manubot.cite.curie.Handler_CURIE\" , \"px\" : \"manubot.cite.curie.Handler_CURIE\" , \"rapdb.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"rapdb.transcript\" : \"manubot.cite.curie.Handler_CURIE\" , \"rbk\" : \"manubot.cite.curie.Handler_CURIE\" , \"reactome\" : \"manubot.cite.curie.Handler_CURIE\" , \"rebase\" : \"manubot.cite.curie.Handler_CURIE\" , \"refseq\" : \"manubot.cite.curie.Handler_CURIE\" , \"resid\" : \"manubot.cite.curie.Handler_CURIE\" , \"rfam\" : \"manubot.cite.curie.Handler_CURIE\" , \"rgd\" : \"manubot.cite.curie.Handler_CURIE\" , \"rgd.qtl\" : \"manubot.cite.curie.Handler_CURIE\" , \"rgd.strain\" : \"manubot.cite.curie.Handler_CURIE\" , \"rhea\" : \"manubot.cite.curie.Handler_CURIE\" , \"ricegap\" : \"manubot.cite.curie.Handler_CURIE\" , \"ricenetdb.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"ricenetdb.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"ricenetdb.mirna\" : \"manubot.cite.curie.Handler_CURIE\" , \"ricenetdb.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"ricenetdb.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"rnacentral\" : \"manubot.cite.curie.Handler_CURIE\" , \"rnamods\" : \"manubot.cite.curie.Handler_CURIE\" , \"ro\" : \"manubot.cite.curie.Handler_CURIE\" , \"rouge\" : \"manubot.cite.curie.Handler_CURIE\" , \"rrid\" : \"manubot.cite.curie.Handler_CURIE\" , \"sabiork.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"sabiork.ec\" : \"manubot.cite.curie.Handler_CURIE\" , \"sabiork.kineticrecord\" : \"manubot.cite.curie.Handler_CURIE\" , \"sabiork.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"sasbdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"sbo\" : \"manubot.cite.curie.Handler_CURIE\" , \"scop\" : \"manubot.cite.curie.Handler_CURIE\" , \"scretf\" : \"manubot.cite.curie.Handler_CURIE\" , \"sdbs\" : \"manubot.cite.curie.Handler_CURIE\" , \"seed\" : \"manubot.cite.curie.Handler_CURIE\" , \"seed.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"seed.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"sgd\" : \"manubot.cite.curie.Handler_CURIE\" , \"sgd.pathways\" : \"manubot.cite.curie.Handler_CURIE\" , \"sgn\" : \"manubot.cite.curie.Handler_CURIE\" , \"sheepqtldb\" : \"manubot.cite.curie.Handler_CURIE\" , \"shortdoi\" : \"manubot.cite.doi.Handler_DOI\" , \"sider.drug\" : \"manubot.cite.curie.Handler_CURIE\" , \"sider.effect\" : \"manubot.cite.curie.Handler_CURIE\" , \"signaling-gateway\" : \"manubot.cite.curie.Handler_CURIE\" , \"sisu\" : \"manubot.cite.curie.Handler_CURIE\" , \"sitex\" : \"manubot.cite.curie.Handler_CURIE\" , \"slm\" : \"manubot.cite.curie.Handler_CURIE\" , \"smart\" : \"manubot.cite.curie.Handler_CURIE\" , \"smpdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"snomedct\" : \"manubot.cite.curie.Handler_CURIE\" , \"snp2tfbs\" : \"manubot.cite.curie.Handler_CURIE\" , \"so\" : \"manubot.cite.curie.Handler_CURIE\" , \"soybase\" : \"manubot.cite.curie.Handler_CURIE\" , \"spdx\" : \"manubot.cite.curie.Handler_CURIE\" , \"spike.map\" : \"manubot.cite.curie.Handler_CURIE\" , \"splash\" : \"manubot.cite.curie.Handler_CURIE\" , \"spp\" : \"manubot.cite.curie.Handler_CURIE\" , \"stap\" : \"manubot.cite.curie.Handler_CURIE\" , \"stitch\" : \"manubot.cite.curie.Handler_CURIE\" , \"storedb\" : \"manubot.cite.curie.Handler_CURIE\" , \"string\" : \"manubot.cite.curie.Handler_CURIE\" , \"subtilist\" : \"manubot.cite.curie.Handler_CURIE\" , \"subtiwiki\" : \"manubot.cite.curie.Handler_CURIE\" , \"sugarbind\" : \"manubot.cite.curie.Handler_CURIE\" , \"supfam\" : \"manubot.cite.curie.Handler_CURIE\" , \"swh\" : \"manubot.cite.curie.Handler_CURIE\" , \"swiss-model\" : \"manubot.cite.curie.Handler_CURIE\" , \"swissregulon\" : \"manubot.cite.curie.Handler_CURIE\" , \"t3db\" : \"manubot.cite.curie.Handler_CURIE\" , \"tair.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"tair.locus\" : \"manubot.cite.curie.Handler_CURIE\" , \"tair.protein\" : \"manubot.cite.curie.Handler_CURIE\" , \"tarbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"taxonomy\" : \"manubot.cite.curie.Handler_CURIE\" , \"tcdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"tgd\" : \"manubot.cite.curie.Handler_CURIE\" , \"tigrfam\" : \"manubot.cite.curie.Handler_CURIE\" , \"tissuelist\" : \"manubot.cite.curie.Handler_CURIE\" , \"tol\" : \"manubot.cite.curie.Handler_CURIE\" , \"topdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"topfind\" : \"manubot.cite.curie.Handler_CURIE\" , \"toxoplasma\" : \"manubot.cite.curie.Handler_CURIE\" , \"transyt\" : \"manubot.cite.curie.Handler_CURIE\" , \"treebase\" : \"manubot.cite.curie.Handler_CURIE\" , \"treefam\" : \"manubot.cite.curie.Handler_CURIE\" , \"trichdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"tritrypdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"ttd.drug\" : \"manubot.cite.curie.Handler_CURIE\" , \"ttd.target\" : \"manubot.cite.curie.Handler_CURIE\" , \"uberon\" : \"manubot.cite.curie.Handler_CURIE\" , \"ubio.namebank\" : \"manubot.cite.curie.Handler_CURIE\" , \"umbbd.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"umbbd.enzyme\" : \"manubot.cite.curie.Handler_CURIE\" , \"umbbd.pathway\" : \"manubot.cite.curie.Handler_CURIE\" , \"umbbd.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"umbbd.rule\" : \"manubot.cite.curie.Handler_CURIE\" , \"umls\" : \"manubot.cite.curie.Handler_CURIE\" , \"unigene\" : \"manubot.cite.curie.Handler_CURIE\" , \"unii\" : \"manubot.cite.curie.Handler_CURIE\" , \"unimod\" : \"manubot.cite.curie.Handler_CURIE\" , \"uniparc\" : \"manubot.cite.curie.Handler_CURIE\" , \"unipathway.compound\" : \"manubot.cite.curie.Handler_CURIE\" , \"unipathway.reaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"uniprot\" : \"manubot.cite.curie.Handler_CURIE\" , \"uniprot.isoform\" : \"manubot.cite.curie.Handler_CURIE\" , \"unists\" : \"manubot.cite.curie.Handler_CURIE\" , \"unite\" : \"manubot.cite.curie.Handler_CURIE\" , \"uo\" : \"manubot.cite.curie.Handler_CURIE\" , \"url\" : \"manubot.cite.url.Handler_URL\" , \"uspto\" : \"manubot.cite.curie.Handler_CURIE\" , \"validatordb\" : \"manubot.cite.curie.Handler_CURIE\" , \"vario\" : \"manubot.cite.curie.Handler_CURIE\" , \"vbase2\" : \"manubot.cite.curie.Handler_CURIE\" , \"vbrc\" : \"manubot.cite.curie.Handler_CURIE\" , \"vectorbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"vegbank\" : \"manubot.cite.curie.Handler_CURIE\" , \"vfb\" : \"manubot.cite.curie.Handler_CURIE\" , \"vfdb.gene\" : \"manubot.cite.curie.Handler_CURIE\" , \"vfdb.genus\" : \"manubot.cite.curie.Handler_CURIE\" , \"vgnc\" : \"manubot.cite.curie.Handler_CURIE\" , \"viaf\" : \"manubot.cite.curie.Handler_CURIE\" , \"vipr\" : \"manubot.cite.curie.Handler_CURIE\" , \"viralzone\" : \"manubot.cite.curie.Handler_CURIE\" , \"virsirna\" : \"manubot.cite.curie.Handler_CURIE\" , \"vmhmetabolite\" : \"manubot.cite.curie.Handler_CURIE\" , \"vmhreaction\" : \"manubot.cite.curie.Handler_CURIE\" , \"wb\" : \"manubot.cite.curie.Handler_CURIE\" , \"wb.rnai\" : \"manubot.cite.curie.Handler_CURIE\" , \"wikidata\" : \"manubot.cite.wikidata.Handler_Wikidata\" , \"wikigenes\" : \"manubot.cite.curie.Handler_CURIE\" , \"wikipathways\" : \"manubot.cite.curie.Handler_CURIE\" , \"wikipedia.en\" : \"manubot.cite.curie.Handler_CURIE\" , \"worfdb\" : \"manubot.cite.curie.Handler_CURIE\" , \"wormpep\" : \"manubot.cite.curie.Handler_CURIE\" , \"worms\" : \"manubot.cite.curie.Handler_CURIE\" , \"xenbase\" : \"manubot.cite.curie.Handler_CURIE\" , \"ydpm\" : \"manubot.cite.curie.Handler_CURIE\" , \"yeastintron\" : \"manubot.cite.curie.Handler_CURIE\" , \"yetfasco\" : \"manubot.cite.curie.Handler_CURIE\" , \"yid\" : \"manubot.cite.curie.Handler_CURIE\" , \"yrcpdr\" : \"manubot.cite.curie.Handler_CURIE\" , \"zfin\" : \"manubot.cite.curie.Handler_CURIE\" , \"zinc\" : \"manubot.cite.curie.Handler_CURIE\" , }","title":"Module manubot.cite.handlers"},{"location":"reference/manubot/cite/handlers/#variables","text":"prefix_to_handler","title":"Variables"},{"location":"reference/manubot/cite/handlers/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/handlers/#get_handler","text":"def get_handler ( prefix_lower ) View Source @functools . lru_cache ( maxsize = 10 _000 ) def get_handler ( prefix_lower ) : if not isinstance ( prefix_lower , str ) : raise TypeError ( f \"prefix_lower should be a str, instead received {prefix_lower.__class__.__name__}\" ) assert prefix_lower == prefix_lower . lower () handler = prefix_to_handler [ prefix_lower ] handler = import_function ( handler )( prefix_lower ) return handler","title":"get_handler"},{"location":"reference/manubot/cite/handlers/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/handlers/#handler","text":"class Handler ( prefix_lower : str ) A Handler is a class that provides support for a certain type of citekey. For example, a Handler subclass could provide support for DOI citekeys. Subclasses enable custom logic for different citekey prefixes, including how to standardize the citekey and how to retrieve CSL Item metadata. View Source class Handler : \"\"\" A Handler is a class that provides support for a certain type of citekey. For example, a Handler subclass could provide support for DOI citekeys. Subclasses enable custom logic for different citekey prefixes, including how to standardize the citekey and how to retrieve CSL Item metadata. \"\"\" prefix_lower : str prefixes = [] def _get_pattern ( self , attribute = \"accession_pattern\" ) -> typing . Pattern : \"\"\" Return a compiled regex pattern stored by `attribute`. By default, return `self.accession_pattern`, which Handler subclasses can set to provide the expected pattern for `self.accession`. \"\"\" # todo: consider caching compilation pattern = getattr ( self , attribute , None ) if not pattern : return None if not isinstance ( pattern , typing . Pattern ): pattern = re . compile ( pattern ) return pattern def inspect ( self , citekey ): \"\"\" Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. \"\"\" pattern = self . _get_pattern ( \"accession_pattern\" ) if not pattern : return None if not pattern . fullmatch ( citekey . accession ): return f \"{citekey.accession} does not match regex {pattern.pattern}\" def standardize_prefix_accession ( self , accession ): \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession @ abc . abstractmethod def get_csl_item ( self , citekey ): \"\"\" Return a CSL_Item with bibliographic details for citekey. \"\"\" raise NotImplementedError ( f \"Manubot does not know how to generate a csl_item for {citekey.standard_id!r}\" )","title":"Handler"},{"location":"reference/manubot/cite/handlers/#descendants","text":"manubot.cite.arxiv.Handler_arXiv manubot.cite.curie.Handler_CURIE manubot.cite.pubmed.Handler_PubMed manubot.cite.pubmed.Handler_PMC manubot.cite.doi.Handler_DOI manubot.cite.isbn.Handler_ISBN manubot.cite.url.Handler_URL manubot.cite.wikidata.Handler_Wikidata","title":"Descendants"},{"location":"reference/manubot/cite/handlers/#class-variables","text":"prefixes","title":"Class variables"},{"location":"reference/manubot/cite/handlers/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/handlers/#get_csl_item","text":"def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source @abc . abstractmethod def get_csl_item ( self , citekey ) : \"\"\" Return a CSL_Item with bibliographic details for citekey. \"\"\" raise NotImplementedError ( f \"Manubot does not know how to generate a csl_item for {citekey.standard_id!r}\" )","title":"get_csl_item"},{"location":"reference/manubot/cite/handlers/#inspect","text":"def inspect ( self , citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey ): \"\"\" Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. \"\"\" pattern = self . _get_pattern ( \"accession_pattern\" ) if not pattern : return None if not pattern . fullmatch ( citekey . accession ): return f \"{citekey.accession} does not match regex {pattern.pattern}\"","title":"inspect"},{"location":"reference/manubot/cite/handlers/#standardize_prefix_accession","text":"def standardize_prefix_accession ( self , accession ) Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession ): \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession","title":"standardize_prefix_accession"},{"location":"reference/manubot/cite/isbn/","text":"Module manubot.cite.isbn View Source import json import logging import re from .handlers import Handler class Handler_ISBN ( Handler ): standard_prefix = \"isbn\" prefixes = [ \"isbn\" , ] def inspect ( self , citekey ): import isbnlib fail = isbnlib . notisbn ( citekey . accession , level = \"strict\" ) if fail : return f \"identifier violates the ISBN syntax according to isbnlib v{isbnlib.__version__}\" def standardize_prefix_accession ( self , accession ): from isbnlib import to_isbn13 accession = to_isbn13 ( accession ) return self . standard_prefix , accession def get_csl_item ( self , citekey ): return get_isbn_csl_item ( citekey . standard_accession ) def get_isbn_csl_item ( isbn ): \"\"\" Generate CSL JSON Data for an ISBN. Converts all ISBNs to 13-digit format. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `isbn_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\"\" import isbnlib isbn = isbnlib . to_isbn13 ( isbn ) for retriever in isbn_retrievers : try : return retriever ( isbn ) except Exception as error : logging . warning ( f \"Error in {retriever.__name__} for {isbn} \" f \"due to a {error.__class__.__name__}: \\n {error}\" ) logging . info ( error , exc_info = True ) raise Exception ( f \"all get_isbn_csl_item methods failed for {isbn}\" ) def get_isbn_csl_item_zotero ( isbn ): \"\"\" Generate CSL JSON Data for an ISBN using Zotero's translation-server. \"\"\" from manubot.cite.zotero import get_csl_item return get_csl_item ( f \"isbn:{isbn}\" ) def get_isbn_csl_item_citoid ( isbn ): \"\"\" Return CSL JSON Data for an ISBN using the Wikipedia Citoid API. https://en.wikipedia.org/api/rest_v1/#!/Citation/getCitation \"\"\" import requests from manubot.util import get_manubot_user_agent headers = { \"User-Agent\" : get_manubot_user_agent ()} url = f \"https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/{isbn}\" response = requests . get ( url , headers = headers ) result = response . json () if isinstance ( result , dict ): if result [ \"title\" ] == \"Not found.\" : raise KeyError ( f \"Metadata for ISBN {isbn} not found at {url}\" ) else : raise Exception ( f \"Unable to extract CSL from JSON metadata for ISBN {isbn}: \\n \" f \"{json.dumps(result.text)}\" ) ( mediawiki ,) = result csl_item = dict () csl_item [ \"type\" ] = mediawiki . get ( \"itemType\" , \"book\" ) if \"title\" in mediawiki : csl_item [ \"title\" ] = mediawiki [ \"title\" ] if \"author\" in mediawiki : csl_author = list () for last , first in mediawiki [ \"author\" ]: csl_author . append ({ \"given\" : first , \"family\" : last }) if csl_author : csl_item [ \"author\" ] = csl_author if \"date\" in mediawiki : year_pattern = re . compile ( r \"[0-9]{4}\" ) match = year_pattern . search ( mediawiki [ \"date\" ]) if match : year = int ( match . group ()) csl_item [ \"issued\" ] = { \"date-parts\" : [[ year ]]} else : logging . debug ( f \"get_isbn_csl_item_citoid: issue extracting date for ISBN {isbn} \\n \" f \"metadata retrieved from {url} \\n \" f 'unable to extract year from date field: {mediawiki[\"date\"]}' ) if \"publisher\" in mediawiki : csl_item [ \"publisher\" ] = mediawiki [ \"publisher\" ] if \"place\" in mediawiki : csl_item [ \"publisher-place\" ] = mediawiki [ \"place\" ] if \"volume\" in mediawiki : csl_item [ \"volume\" ] = mediawiki [ \"volume\" ] if \"edition\" in mediawiki : csl_item [ \"edition\" ] = mediawiki [ \"edition\" ] if \"abstractNote\" in mediawiki : csl_item [ \"abstract\" ] = mediawiki [ \"abstractNote\" ] csl_item [ \"ISBN\" ] = isbn if \"source\" in mediawiki : csl_item [ \"source\" ] = mediawiki [ \"source\" ][ 0 ] if \"url\" in mediawiki : csl_item [ \"URL\" ] = mediawiki [ \"url\" ] return csl_item def get_isbn_csl_item_isbnlib ( isbn ): \"\"\" Generate CSL JSON Data for an ISBN using isbnlib. \"\"\" import isbnlib metadata = isbnlib . meta ( isbn ) csl_json = isbnlib . registry . bibformatters [ \"csl\" ]( metadata ) csl_data = json . loads ( csl_json ) return csl_data isbn_retrievers = [ get_isbn_csl_item_zotero , get_isbn_csl_item_citoid , get_isbn_csl_item_isbnlib , ] Variables isbn_retrievers Functions get_isbn_csl_item def get_isbn_csl_item ( isbn ) Generate CSL JSON Data for an ISBN. Converts all ISBNs to 13-digit format. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable isbn_retrievers . The methods are attempted in order, with this function returning the metadata from the first non-failing method. View Source def get_isbn_csl_item ( isbn ): \"\"\" Generate CSL JSON Data for an ISBN. Converts all ISBNs to 13-digit format. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `isbn_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\"\" import isbnlib isbn = isbnlib . to_isbn13 ( isbn ) for retriever in isbn_retrievers : try : return retriever ( isbn ) except Exception as error : logging . warning ( f \"Error in {retriever.__name__} for {isbn} \" f \"due to a {error.__class__.__name__}: \\n {error}\" ) logging . info ( error , exc_info = True ) raise Exception ( f \"all get_isbn_csl_item methods failed for {isbn}\" ) get_isbn_csl_item_citoid def get_isbn_csl_item_citoid ( isbn ) Return CSL JSON Data for an ISBN using the Wikipedia Citoid API. https://en.wikipedia.org/api/rest_v1/#!/Citation/getCitation View Source def get_isbn_csl_item_citoid ( isbn ): \"\"\" Return CSL JSON Data for an ISBN using the Wikipedia Citoid API. https://en.wikipedia.org/api/rest_v1/#!/Citation/getCitation \"\"\" import requests from manubot.util import get_manubot_user_agent headers = { \"User-Agent\" : get_manubot_user_agent ()} url = f \"https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/{isbn}\" response = requests . get ( url , headers = headers ) result = response . json () if isinstance ( result , dict ): if result [ \"title\" ] == \"Not found.\" : raise KeyError ( f \"Metadata for ISBN {isbn} not found at {url}\" ) else : raise Exception ( f \"Unable to extract CSL from JSON metadata for ISBN {isbn}: \\n \" f \"{json.dumps(result.text)}\" ) ( mediawiki ,) = result csl_item = dict () csl_item [ \"type\" ] = mediawiki . get ( \"itemType\" , \"book\" ) if \"title\" in mediawiki : csl_item [ \"title\" ] = mediawiki [ \"title\" ] if \"author\" in mediawiki : csl_author = list () for last , first in mediawiki [ \"author\" ]: csl_author . append ({ \"given\" : first , \"family\" : last }) if csl_author : csl_item [ \"author\" ] = csl_author if \"date\" in mediawiki : year_pattern = re . compile ( r \"[0-9]{4}\" ) match = year_pattern . search ( mediawiki [ \"date\" ]) if match : year = int ( match . group ()) csl_item [ \"issued\" ] = { \"date-parts\" : [[ year ]]} else : logging . debug ( f \"get_isbn_csl_item_citoid: issue extracting date for ISBN {isbn} \\n \" f \"metadata retrieved from {url} \\n \" f 'unable to extract year from date field: {mediawiki[\"date\"]}' ) if \"publisher\" in mediawiki : csl_item [ \"publisher\" ] = mediawiki [ \"publisher\" ] if \"place\" in mediawiki : csl_item [ \"publisher-place\" ] = mediawiki [ \"place\" ] if \"volume\" in mediawiki : csl_item [ \"volume\" ] = mediawiki [ \"volume\" ] if \"edition\" in mediawiki : csl_item [ \"edition\" ] = mediawiki [ \"edition\" ] if \"abstractNote\" in mediawiki : csl_item [ \"abstract\" ] = mediawiki [ \"abstractNote\" ] csl_item [ \"ISBN\" ] = isbn if \"source\" in mediawiki : csl_item [ \"source\" ] = mediawiki [ \"source\" ][ 0 ] if \"url\" in mediawiki : csl_item [ \"URL\" ] = mediawiki [ \"url\" ] return csl_item get_isbn_csl_item_isbnlib def get_isbn_csl_item_isbnlib ( isbn ) Generate CSL JSON Data for an ISBN using isbnlib. View Source def get_isbn_csl_item_isbnlib ( isbn ): \"\"\" Generate CSL JSON Data for an ISBN using isbnlib. \"\"\" import isbnlib metadata = isbnlib . meta ( isbn ) csl_json = isbnlib . registry . bibformatters [ \"csl\" ]( metadata ) csl_data = json . loads ( csl_json ) return csl_data get_isbn_csl_item_zotero def get_isbn_csl_item_zotero ( isbn ) Generate CSL JSON Data for an ISBN using Zotero's translation-server. View Source def get_isbn_csl_item_zotero ( isbn ): \"\"\" Generate CSL JSON Data for an ISBN using Zotero's translation-server. \"\"\" from manubot.cite.zotero import get_csl_item return get_csl_item ( f \"isbn:{isbn}\" ) Classes Handler_ISBN class Handler_ISBN ( prefix_lower : str ) A Handler is a class that provides support for a certain type of citekey. For example, a Handler subclass could provide support for DOI citekeys. Subclasses enable custom logic for different citekey prefixes, including how to standardize the citekey and how to retrieve CSL Item metadata. View Source class Handler_ISBN ( Handler ): standard_prefix = \"isbn\" prefixes = [ \"isbn\" , ] def inspect ( self , citekey ): import isbnlib fail = isbnlib . notisbn ( citekey . accession , level = \"strict\" ) if fail : return f \"identifier violates the ISBN syntax according to isbnlib v{isbnlib.__version__}\" def standardize_prefix_accession ( self , accession ): from isbnlib import to_isbn13 accession = to_isbn13 ( accession ) return self . standard_prefix , accession def get_csl_item ( self , citekey ): return get_isbn_csl_item ( citekey . standard_accession ) Ancestors (in MRO) manubot.cite.handlers.Handler Class variables prefixes standard_prefix Methods get_csl_item def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ): return get_isbn_csl_item ( citekey . standard_accession ) inspect def inspect ( self , citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey ): import isbnlib fail = isbnlib . notisbn ( citekey . accession , level = \"strict\" ) if fail : return f \"identifier violates the ISBN syntax according to isbnlib v{isbnlib.__version__}\" standardize_prefix_accession def standardize_prefix_accession ( self , accession ) Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession ): from isbnlib import to_isbn13 accession = to_isbn13 ( accession ) return self . standard_prefix , accession","title":"Isbn"},{"location":"reference/manubot/cite/isbn/#module-manubotciteisbn","text":"View Source import json import logging import re from .handlers import Handler class Handler_ISBN ( Handler ): standard_prefix = \"isbn\" prefixes = [ \"isbn\" , ] def inspect ( self , citekey ): import isbnlib fail = isbnlib . notisbn ( citekey . accession , level = \"strict\" ) if fail : return f \"identifier violates the ISBN syntax according to isbnlib v{isbnlib.__version__}\" def standardize_prefix_accession ( self , accession ): from isbnlib import to_isbn13 accession = to_isbn13 ( accession ) return self . standard_prefix , accession def get_csl_item ( self , citekey ): return get_isbn_csl_item ( citekey . standard_accession ) def get_isbn_csl_item ( isbn ): \"\"\" Generate CSL JSON Data for an ISBN. Converts all ISBNs to 13-digit format. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `isbn_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\"\" import isbnlib isbn = isbnlib . to_isbn13 ( isbn ) for retriever in isbn_retrievers : try : return retriever ( isbn ) except Exception as error : logging . warning ( f \"Error in {retriever.__name__} for {isbn} \" f \"due to a {error.__class__.__name__}: \\n {error}\" ) logging . info ( error , exc_info = True ) raise Exception ( f \"all get_isbn_csl_item methods failed for {isbn}\" ) def get_isbn_csl_item_zotero ( isbn ): \"\"\" Generate CSL JSON Data for an ISBN using Zotero's translation-server. \"\"\" from manubot.cite.zotero import get_csl_item return get_csl_item ( f \"isbn:{isbn}\" ) def get_isbn_csl_item_citoid ( isbn ): \"\"\" Return CSL JSON Data for an ISBN using the Wikipedia Citoid API. https://en.wikipedia.org/api/rest_v1/#!/Citation/getCitation \"\"\" import requests from manubot.util import get_manubot_user_agent headers = { \"User-Agent\" : get_manubot_user_agent ()} url = f \"https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/{isbn}\" response = requests . get ( url , headers = headers ) result = response . json () if isinstance ( result , dict ): if result [ \"title\" ] == \"Not found.\" : raise KeyError ( f \"Metadata for ISBN {isbn} not found at {url}\" ) else : raise Exception ( f \"Unable to extract CSL from JSON metadata for ISBN {isbn}: \\n \" f \"{json.dumps(result.text)}\" ) ( mediawiki ,) = result csl_item = dict () csl_item [ \"type\" ] = mediawiki . get ( \"itemType\" , \"book\" ) if \"title\" in mediawiki : csl_item [ \"title\" ] = mediawiki [ \"title\" ] if \"author\" in mediawiki : csl_author = list () for last , first in mediawiki [ \"author\" ]: csl_author . append ({ \"given\" : first , \"family\" : last }) if csl_author : csl_item [ \"author\" ] = csl_author if \"date\" in mediawiki : year_pattern = re . compile ( r \"[0-9]{4}\" ) match = year_pattern . search ( mediawiki [ \"date\" ]) if match : year = int ( match . group ()) csl_item [ \"issued\" ] = { \"date-parts\" : [[ year ]]} else : logging . debug ( f \"get_isbn_csl_item_citoid: issue extracting date for ISBN {isbn} \\n \" f \"metadata retrieved from {url} \\n \" f 'unable to extract year from date field: {mediawiki[\"date\"]}' ) if \"publisher\" in mediawiki : csl_item [ \"publisher\" ] = mediawiki [ \"publisher\" ] if \"place\" in mediawiki : csl_item [ \"publisher-place\" ] = mediawiki [ \"place\" ] if \"volume\" in mediawiki : csl_item [ \"volume\" ] = mediawiki [ \"volume\" ] if \"edition\" in mediawiki : csl_item [ \"edition\" ] = mediawiki [ \"edition\" ] if \"abstractNote\" in mediawiki : csl_item [ \"abstract\" ] = mediawiki [ \"abstractNote\" ] csl_item [ \"ISBN\" ] = isbn if \"source\" in mediawiki : csl_item [ \"source\" ] = mediawiki [ \"source\" ][ 0 ] if \"url\" in mediawiki : csl_item [ \"URL\" ] = mediawiki [ \"url\" ] return csl_item def get_isbn_csl_item_isbnlib ( isbn ): \"\"\" Generate CSL JSON Data for an ISBN using isbnlib. \"\"\" import isbnlib metadata = isbnlib . meta ( isbn ) csl_json = isbnlib . registry . bibformatters [ \"csl\" ]( metadata ) csl_data = json . loads ( csl_json ) return csl_data isbn_retrievers = [ get_isbn_csl_item_zotero , get_isbn_csl_item_citoid , get_isbn_csl_item_isbnlib , ]","title":"Module manubot.cite.isbn"},{"location":"reference/manubot/cite/isbn/#variables","text":"isbn_retrievers","title":"Variables"},{"location":"reference/manubot/cite/isbn/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/isbn/#get_isbn_csl_item","text":"def get_isbn_csl_item ( isbn ) Generate CSL JSON Data for an ISBN. Converts all ISBNs to 13-digit format. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable isbn_retrievers . The methods are attempted in order, with this function returning the metadata from the first non-failing method. View Source def get_isbn_csl_item ( isbn ): \"\"\" Generate CSL JSON Data for an ISBN. Converts all ISBNs to 13-digit format. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `isbn_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\"\" import isbnlib isbn = isbnlib . to_isbn13 ( isbn ) for retriever in isbn_retrievers : try : return retriever ( isbn ) except Exception as error : logging . warning ( f \"Error in {retriever.__name__} for {isbn} \" f \"due to a {error.__class__.__name__}: \\n {error}\" ) logging . info ( error , exc_info = True ) raise Exception ( f \"all get_isbn_csl_item methods failed for {isbn}\" )","title":"get_isbn_csl_item"},{"location":"reference/manubot/cite/isbn/#get_isbn_csl_item_citoid","text":"def get_isbn_csl_item_citoid ( isbn ) Return CSL JSON Data for an ISBN using the Wikipedia Citoid API. https://en.wikipedia.org/api/rest_v1/#!/Citation/getCitation View Source def get_isbn_csl_item_citoid ( isbn ): \"\"\" Return CSL JSON Data for an ISBN using the Wikipedia Citoid API. https://en.wikipedia.org/api/rest_v1/#!/Citation/getCitation \"\"\" import requests from manubot.util import get_manubot_user_agent headers = { \"User-Agent\" : get_manubot_user_agent ()} url = f \"https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/{isbn}\" response = requests . get ( url , headers = headers ) result = response . json () if isinstance ( result , dict ): if result [ \"title\" ] == \"Not found.\" : raise KeyError ( f \"Metadata for ISBN {isbn} not found at {url}\" ) else : raise Exception ( f \"Unable to extract CSL from JSON metadata for ISBN {isbn}: \\n \" f \"{json.dumps(result.text)}\" ) ( mediawiki ,) = result csl_item = dict () csl_item [ \"type\" ] = mediawiki . get ( \"itemType\" , \"book\" ) if \"title\" in mediawiki : csl_item [ \"title\" ] = mediawiki [ \"title\" ] if \"author\" in mediawiki : csl_author = list () for last , first in mediawiki [ \"author\" ]: csl_author . append ({ \"given\" : first , \"family\" : last }) if csl_author : csl_item [ \"author\" ] = csl_author if \"date\" in mediawiki : year_pattern = re . compile ( r \"[0-9]{4}\" ) match = year_pattern . search ( mediawiki [ \"date\" ]) if match : year = int ( match . group ()) csl_item [ \"issued\" ] = { \"date-parts\" : [[ year ]]} else : logging . debug ( f \"get_isbn_csl_item_citoid: issue extracting date for ISBN {isbn} \\n \" f \"metadata retrieved from {url} \\n \" f 'unable to extract year from date field: {mediawiki[\"date\"]}' ) if \"publisher\" in mediawiki : csl_item [ \"publisher\" ] = mediawiki [ \"publisher\" ] if \"place\" in mediawiki : csl_item [ \"publisher-place\" ] = mediawiki [ \"place\" ] if \"volume\" in mediawiki : csl_item [ \"volume\" ] = mediawiki [ \"volume\" ] if \"edition\" in mediawiki : csl_item [ \"edition\" ] = mediawiki [ \"edition\" ] if \"abstractNote\" in mediawiki : csl_item [ \"abstract\" ] = mediawiki [ \"abstractNote\" ] csl_item [ \"ISBN\" ] = isbn if \"source\" in mediawiki : csl_item [ \"source\" ] = mediawiki [ \"source\" ][ 0 ] if \"url\" in mediawiki : csl_item [ \"URL\" ] = mediawiki [ \"url\" ] return csl_item","title":"get_isbn_csl_item_citoid"},{"location":"reference/manubot/cite/isbn/#get_isbn_csl_item_isbnlib","text":"def get_isbn_csl_item_isbnlib ( isbn ) Generate CSL JSON Data for an ISBN using isbnlib. View Source def get_isbn_csl_item_isbnlib ( isbn ): \"\"\" Generate CSL JSON Data for an ISBN using isbnlib. \"\"\" import isbnlib metadata = isbnlib . meta ( isbn ) csl_json = isbnlib . registry . bibformatters [ \"csl\" ]( metadata ) csl_data = json . loads ( csl_json ) return csl_data","title":"get_isbn_csl_item_isbnlib"},{"location":"reference/manubot/cite/isbn/#get_isbn_csl_item_zotero","text":"def get_isbn_csl_item_zotero ( isbn ) Generate CSL JSON Data for an ISBN using Zotero's translation-server. View Source def get_isbn_csl_item_zotero ( isbn ): \"\"\" Generate CSL JSON Data for an ISBN using Zotero's translation-server. \"\"\" from manubot.cite.zotero import get_csl_item return get_csl_item ( f \"isbn:{isbn}\" )","title":"get_isbn_csl_item_zotero"},{"location":"reference/manubot/cite/isbn/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/isbn/#handler_isbn","text":"class Handler_ISBN ( prefix_lower : str ) A Handler is a class that provides support for a certain type of citekey. For example, a Handler subclass could provide support for DOI citekeys. Subclasses enable custom logic for different citekey prefixes, including how to standardize the citekey and how to retrieve CSL Item metadata. View Source class Handler_ISBN ( Handler ): standard_prefix = \"isbn\" prefixes = [ \"isbn\" , ] def inspect ( self , citekey ): import isbnlib fail = isbnlib . notisbn ( citekey . accession , level = \"strict\" ) if fail : return f \"identifier violates the ISBN syntax according to isbnlib v{isbnlib.__version__}\" def standardize_prefix_accession ( self , accession ): from isbnlib import to_isbn13 accession = to_isbn13 ( accession ) return self . standard_prefix , accession def get_csl_item ( self , citekey ): return get_isbn_csl_item ( citekey . standard_accession )","title":"Handler_ISBN"},{"location":"reference/manubot/cite/isbn/#ancestors-in-mro","text":"manubot.cite.handlers.Handler","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/isbn/#class-variables","text":"prefixes standard_prefix","title":"Class variables"},{"location":"reference/manubot/cite/isbn/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/isbn/#get_csl_item","text":"def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ): return get_isbn_csl_item ( citekey . standard_accession )","title":"get_csl_item"},{"location":"reference/manubot/cite/isbn/#inspect","text":"def inspect ( self , citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey ): import isbnlib fail = isbnlib . notisbn ( citekey . accession , level = \"strict\" ) if fail : return f \"identifier violates the ISBN syntax according to isbnlib v{isbnlib.__version__}\"","title":"inspect"},{"location":"reference/manubot/cite/isbn/#standardize_prefix_accession","text":"def standardize_prefix_accession ( self , accession ) Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession ): from isbnlib import to_isbn13 accession = to_isbn13 ( accession ) return self . standard_prefix , accession","title":"standardize_prefix_accession"},{"location":"reference/manubot/cite/pubmed/","text":"Module manubot.cite.pubmed View Source import functools import json import logging import xml.etree.ElementTree import requests from .handlers import Handler from manubot.util import get_manubot_user_agent class Handler_PubMed ( Handler ): standard_prefix = \"pubmed\" prefixes = [ \"pubmed\" , \"pmid\" , ] accession_pattern = r \"[1-9][0-9]{0,7}\" def inspect ( self , citekey ): identifier = citekey . accession # https://www.nlm.nih.gov/bsd/mms/medlineelements.html#pmid if identifier . startswith ( \"PMC\" ): return ( \"PubMed Identifiers should start with digits rather than PMC. \" f \"Should {citekey.dealiased_id!r} switch the citation source to 'pmc'?\" ) elif not self . _get_pattern () . fullmatch ( identifier ): return \"PubMed Identifiers should be 1-8 digits with no leading zeros.\" def get_csl_item ( self , citekey ): return get_pubmed_csl_item ( citekey . standard_accession ) class Handler_PMC ( Handler ): standard_prefix = \"pmc\" prefixes = [ \"pmc\" , \"pmcid\" , ] accession_pattern = r \"PMC[0-9]+\" def inspect ( self , citekey ): identifier = citekey . accession # https://www.nlm.nih.gov/bsd/mms/medlineelements.html#pmc if not identifier . startswith ( \"PMC\" ): return \"PubMed Central Identifiers must start with 'PMC'.\" elif not self . _get_pattern () . fullmatch ( identifier ): return ( \"Identifier does not conform to the PMCID regex. \" \"Double check the PMCID.\" ) def get_csl_item ( self , citekey ): return get_pmc_csl_item ( citekey . standard_accession ) def get_pmc_csl_item ( pmcid ): \"\"\" Get the CSL Item for a PubMed Central record by its PMID, PMCID, or DOI, using the NCBI Citation Exporter API. https://api.ncbi.nlm.nih.gov/lit/ctxp https://github.com/manubot/manubot/issues/21 https://twitter.com/dhimmel/status/1061787168820092929 \"\"\" assert pmcid . startswith ( \"PMC\" ) csl_item = _get_literature_citation_exporter_csl_item ( \"pmc\" , pmcid [ 3 :]) if \"URL\" not in csl_item : csl_item [ \"URL\" ] = f \"https://www.ncbi.nlm.nih.gov/pmc/articles/{csl_item.get('PMCID', pmcid)}/\" return csl_item def _get_literature_citation_exporter_csl_item ( database , identifier ): \"\"\" https://api.ncbi.nlm.nih.gov/lit/ctxp \"\"\" if database not in { \"pubmed\" , \"pmc\" }: logging . error ( f \"Error calling _get_literature_citation_exporter_csl_item. \\n \" f 'database must be either \"pubmed\" or \"pmc\", not {database}' ) assert False if not identifier : logging . error ( f \"Error calling _get_literature_citation_exporter_csl_item. \\n \" f \"identifier cannot be blank\" ) assert False params = { \"format\" : \"csl\" , \"id\" : identifier } headers = { \"User-Agent\" : get_manubot_user_agent ()} url = f \"https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/{database}/\" response = requests . get ( url , params , headers = headers ) try : csl_item = response . json () except Exception as error : logging . error ( f \"Error fetching {database} metadata for {identifier}. \\n \" f \"Invalid JSON response from {response.url}: \\n {response.text}\" ) raise error assert isinstance ( csl_item , dict ) if csl_item . get ( \"status\" , \"okay\" ) == \"error\" : logging . error ( f \"Error fetching {database} metadata for {identifier}. \\n \" f \"Literature Citation Exporter returned JSON indicating an error for {response.url} \\n \" f \"{json.dumps(csl_item, indent=2)}\" ) assert False return csl_item def get_pubmed_csl_item ( pmid ): \"\"\" Query NCBI E-Utilities to create CSL Items for PubMed IDs. https://github.com/manubot/manubot/issues/21 https://github.com/ncbi/citation-exporter/issues/3#issuecomment-355313143 \"\"\" pmid = str ( pmid ) params = { \"db\" : \"pubmed\" , \"id\" : pmid , \"rettype\" : \"full\" } headers = { \"User-Agent\" : get_manubot_user_agent ()} url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\" with _get_eutils_rate_limiter (): response = requests . get ( url , params , headers = headers ) try : element_tree = xml . etree . ElementTree . fromstring ( response . text ) ( element_tree ,) = list ( element_tree ) except Exception as error : logging . error ( f \"Error fetching PubMed metadata for {pmid}. \\n \" f \"Invalid XML response from {response.url}: \\n {response.text}\" ) raise error try : csl_item = csl_item_from_pubmed_article ( element_tree ) except Exception as error : msg = f \"Error parsing the following PubMed metadata for PMID {pmid}: \\n {response.text}\" logging . error ( msg ) raise error return csl_item def csl_item_from_pubmed_article ( article ): \"\"\" article is a PubmedArticle xml element tree https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" csl_item = dict () if not article . find ( \"MedlineCitation/Article\" ): raise NotImplementedError ( \"Unsupported PubMed record: no <Article> element\" ) title = article . findtext ( \"MedlineCitation/Article/ArticleTitle\" ) if title : csl_item [ \"title\" ] = title volume = article . findtext ( \"MedlineCitation/Article/Journal/JournalIssue/Volume\" ) if volume : csl_item [ \"volume\" ] = volume issue = article . findtext ( \"MedlineCitation/Article/Journal/JournalIssue/Issue\" ) if issue : csl_item [ \"issue\" ] = issue page = article . findtext ( \"MedlineCitation/Article/Pagination/MedlinePgn\" ) if page : csl_item [ \"page\" ] = page journal = article . findtext ( \"MedlineCitation/Article/Journal/Title\" ) if journal : csl_item [ \"container-title\" ] = journal journal_short = article . findtext ( \"MedlineCitation/Article/Journal/ISOAbbreviation\" ) if journal_short : csl_item [ \"container-title-short\" ] = journal_short issn = article . findtext ( \"MedlineCitation/Article/Journal/ISSN\" ) if issn : csl_item [ \"ISSN\" ] = issn date_parts = extract_publication_date_parts ( article ) if date_parts : csl_item [ \"issued\" ] = { \"date-parts\" : [ date_parts ]} authors_csl = list () authors = article . findall ( \"MedlineCitation/Article/AuthorList/Author\" ) for author in authors : author_csl = dict () given = author . findtext ( \"ForeName\" ) if given : author_csl [ \"given\" ] = given family = author . findtext ( \"LastName\" ) if family : author_csl [ \"family\" ] = family authors_csl . append ( author_csl ) if authors_csl : csl_item [ \"author\" ] = authors_csl for id_type , key in ( \"pubmed\" , \"PMID\" ), ( \"pmc\" , \"PMCID\" ), ( \"doi\" , \"DOI\" ): xpath = f \"PubmedData/ArticleIdList/ArticleId[@IdType='{id_type}']\" value = article . findtext ( xpath ) if value : csl_item [ key ] = value . lower () if key == \"DOI\" else value abstract = article . findtext ( \"MedlineCitation/Article/Abstract/AbstractText\" ) if abstract : csl_item [ \"abstract\" ] = abstract csl_item [ \"URL\" ] = f \"https://www.ncbi.nlm.nih.gov/pubmed/{csl_item['PMID']}\" csl_item [ \"type\" ] = \"article-journal\" return csl_item month_abbrev_to_int = { \"Jan\" : 1 , \"Feb\" : 2 , \"Mar\" : 3 , \"Apr\" : 4 , \"May\" : 5 , \"Jun\" : 6 , \"Jul\" : 7 , \"Aug\" : 8 , \"Sep\" : 9 , \"Oct\" : 10 , \"Nov\" : 11 , \"Dec\" : 12 , } def extract_publication_date_parts ( article ): \"\"\" Extract date published from a PubmedArticle xml element tree. \"\"\" date_parts = [] # Electronic articles date = article . find ( \"MedlineCitation/Article/ArticleDate\" ) if date : for part in \"Year\" , \"Month\" , \"Day\" : part = date . findtext ( part ) if not part : break date_parts . append ( int ( part )) return date_parts # Print articles date = article . find ( \"MedlineCitation/Article/Journal/JournalIssue/PubDate\" ) year = date . findtext ( \"Year\" ) if year : date_parts . append ( int ( year )) month = date . findtext ( \"Month\" ) if month : try : date_parts . append ( month_abbrev_to_int [ month ]) except KeyError : date_parts . append ( int ( month )) day = date . findtext ( \"Day\" ) if day : date_parts . append ( int ( day )) return date_parts def get_pmcid_and_pmid_for_doi ( doi ): \"\"\" Query PMC's ID Converter API to retrieve the PMCID and PMID for a DOI. Does not work for DOIs that are in Pubmed but not PubMed Central. https://www.ncbi.nlm.nih.gov/pmc/tools/id-converter-api/ \"\"\" assert isinstance ( doi , str ) assert doi . startswith ( \"10.\" ) params = { \"ids\" : doi , \"tool\" : \"manubot\" } url = \"https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/\" response = requests . get ( url , params ) if not response . ok : logging . warning ( f \"Status code {response.status_code} querying {response.url} \\n \" ) return {} try : element_tree = xml . etree . ElementTree . fromstring ( response . text ) except Exception : logging . warning ( f \"Error fetching PMC ID conversion for {doi}. \\n \" f \"Response from {response.url}: \\n {response.text}\" ) return {} records = element_tree . findall ( \"record\" ) if len ( records ) != 1 : logging . warning ( f \"Expected PubMed Central ID converter to return a single XML record for {doi}. \\n \" f \"Response from {response.url}: \\n {response.text}\" ) return {} ( record ,) = records if record . findtext ( \"status\" , default = \"okay\" ) == \"error\" : return {} id_dict = {} for id_type in \"pmcid\" , \"pmid\" : id_ = record . get ( id_type ) if id_ : id_dict [ id_type . upper ()] = id_ return id_dict def get_pmid_for_doi ( doi ): \"\"\" Query NCBI's E-utilities to retrieve the PMID for a DOI. \"\"\" assert isinstance ( doi , str ) assert doi . startswith ( \"10.\" ) params = { \"db\" : \"pubmed\" , \"term\" : f \"{doi}[DOI]\" } headers = { \"User-Agent\" : get_manubot_user_agent ()} url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\" with _get_eutils_rate_limiter (): response = requests . get ( url , params , headers = headers ) if not response . ok : logging . warning ( f \"Status code {response.status_code} querying {response.url} \\n \" ) return None try : element_tree = xml . etree . ElementTree . fromstring ( response . text ) except Exception : logging . warning ( f \"Error in ESearch XML for DOI: {doi}. \\n \" f \"Response from {response.url}: \\n {response.text}\" ) return None id_elems = element_tree . findall ( \"IdList/Id\" ) if len ( id_elems ) != 1 : logging . debug ( f \"No PMIDs found for {doi}. \\n \" f \"Response from {response.url}: \\n {response.text}\" ) return None ( id_elem ,) = id_elems return id_elem . text def get_pubmed_ids_for_doi ( doi ): \"\"\" Return a dictionary with PMCID and PMID, if they exist, for the specified DOI. See https://github.com/manubot/manubot/issues/45. \"\"\" pubmed_ids = get_pmcid_and_pmid_for_doi ( doi ) if not pubmed_ids : pmid = get_pmid_for_doi ( doi ) if pmid : pubmed_ids [ \"PMID\" ] = pmid return pubmed_ids @functools.lru_cache () def _get_eutils_rate_limiter (): \"\"\" Rate limiter to cap NCBI E-utilities queries to 3 per second as per https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities/ \"\"\" from ratelimiter import RateLimiter return RateLimiter ( max_calls = 2 , period = 1 ) Variables month_abbrev_to_int Functions csl_item_from_pubmed_article def csl_item_from_pubmed_article ( article ) article is a PubmedArticle xml element tree https://github.com/citation-style-language/schema/blob/master/csl-data.json View Source def csl_item_from_pubmed_article ( article ) : \"\"\" article is a PubmedArticle xml element tree https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" csl_item = dict () if not article . find ( \"MedlineCitation/Article\" ) : raise NotImplementedError ( \"Unsupported PubMed record: no <Article> element\" ) title = article . findtext ( \"MedlineCitation/Article/ArticleTitle\" ) if title : csl_item [ \"title\" ] = title volume = article . findtext ( \"MedlineCitation/Article/Journal/JournalIssue/Volume\" ) if volume : csl_item [ \"volume\" ] = volume issue = article . findtext ( \"MedlineCitation/Article/Journal/JournalIssue/Issue\" ) if issue : csl_item [ \"issue\" ] = issue page = article . findtext ( \"MedlineCitation/Article/Pagination/MedlinePgn\" ) if page : csl_item [ \"page\" ] = page journal = article . findtext ( \"MedlineCitation/Article/Journal/Title\" ) if journal : csl_item [ \"container-title\" ] = journal journal_short = article . findtext ( \"MedlineCitation/Article/Journal/ISOAbbreviation\" ) if journal_short : csl_item [ \"container-title-short\" ] = journal_short issn = article . findtext ( \"MedlineCitation/Article/Journal/ISSN\" ) if issn : csl_item [ \"ISSN\" ] = issn date_parts = extract_publication_date_parts ( article ) if date_parts : csl_item [ \"issued\" ] = { \"date-parts\" : [ date_parts ] } authors_csl = list () authors = article . findall ( \"MedlineCitation/Article/AuthorList/Author\" ) for author in authors : author_csl = dict () given = author . findtext ( \"ForeName\" ) if given : author_csl [ \"given\" ] = given family = author . findtext ( \"LastName\" ) if family : author_csl [ \"family\" ] = family authors_csl . append ( author_csl ) if authors_csl : csl_item [ \"author\" ] = authors_csl for id_type , key in ( \"pubmed\" , \"PMID\" ), ( \"pmc\" , \"PMCID\" ), ( \"doi\" , \"DOI\" ) : xpath = f \"PubmedData/ArticleIdList/ArticleId[@IdType='{id_type}']\" value = article . findtext ( xpath ) if value : csl_item [ key ] = value . lower () if key == \"DOI\" else value abstract = article . findtext ( \"MedlineCitation/Article/Abstract/AbstractText\" ) if abstract : csl_item [ \"abstract\" ] = abstract csl_item [ \"URL\" ] = f \"https://www.ncbi.nlm.nih.gov/pubmed/{csl_item['PMID']}\" csl_item [ \"type\" ] = \"article-journal\" return csl_item extract_publication_date_parts def extract_publication_date_parts ( article ) Extract date published from a PubmedArticle xml element tree. View Source def extract_publication_date_parts ( article ) : \"\"\" Extract date published from a PubmedArticle xml element tree. \"\"\" date_parts = [] # Electronic articles date = article . find ( \"MedlineCitation/Article/ArticleDate\" ) if date : for part in \"Year\" , \"Month\" , \"Day\" : part = date . findtext ( part ) if not part : break date_parts . append ( int ( part )) return date_parts # Print articles date = article . find ( \"MedlineCitation/Article/Journal/JournalIssue/PubDate\" ) year = date . findtext ( \"Year\" ) if year : date_parts . append ( int ( year )) month = date . findtext ( \"Month\" ) if month : try : date_parts . append ( month_abbrev_to_int [ month ] ) except KeyError : date_parts . append ( int ( month )) day = date . findtext ( \"Day\" ) if day : date_parts . append ( int ( day )) return date_parts get_pmc_csl_item def get_pmc_csl_item ( pmcid ) Get the CSL Item for a PubMed Central record by its PMID, PMCID, or DOI, using the NCBI Citation Exporter API. https://api.ncbi.nlm.nih.gov/lit/ctxp https://github.com/manubot/manubot/issues/21 https://twitter.com/dhimmel/status/1061787168820092929 View Source def get_pmc_csl_item ( pmcid ): \"\"\" Get the CSL Item for a PubMed Central record by its PMID, PMCID, or DOI, using the NCBI Citation Exporter API. https://api.ncbi.nlm.nih.gov/lit/ctxp https://github.com/manubot/manubot/issues/21 https://twitter.com/dhimmel/status/1061787168820092929 \"\"\" assert pmcid . startswith ( \"PMC\" ) csl_item = _get_literature_citation_exporter_csl_item ( \"pmc\" , pmcid [ 3 :]) if \"URL\" not in csl_item : csl_item [ \"URL\" ] = f \"https://www.ncbi.nlm.nih.gov/pmc/articles/{csl_item.get('PMCID', pmcid)}/\" return csl_item get_pmcid_and_pmid_for_doi def get_pmcid_and_pmid_for_doi ( doi ) Query PMC's ID Converter API to retrieve the PMCID and PMID for a DOI. Does not work for DOIs that are in Pubmed but not PubMed Central. https://www.ncbi.nlm.nih.gov/pmc/tools/id-converter-api/ View Source def get_pmcid_and_pmid_for_doi ( doi ): \"\"\" Query PMC's ID Converter API to retrieve the PMCID and PMID for a DOI. Does not work for DOIs that are in Pubmed but not PubMed Central. https://www.ncbi.nlm.nih.gov/pmc/tools/id-converter-api/ \"\"\" assert isinstance ( doi , str ) assert doi . startswith ( \"10.\" ) params = { \"ids\" : doi , \"tool\" : \"manubot\" } url = \"https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/\" response = requests . get ( url , params ) if not response . ok : logging . warning ( f \"Status code {response.status_code} querying {response.url}\\n\" ) return {} try : element_tree = xml . etree . ElementTree . fromstring ( response . text ) except Exception : logging . warning ( f \"Error fetching PMC ID conversion for {doi}.\\n\" f \"Response from {response.url}:\\n{response.text}\" ) return {} records = element_tree . findall ( \"record\" ) if len ( records ) != 1 : logging . warning ( f \"Expected PubMed Central ID converter to return a single XML record for {doi}.\\n\" f \"Response from {response.url}:\\n{response.text}\" ) return {} ( record ,) = records if record . findtext ( \"status\" , default = \"okay\" ) == \"error\" : return {} id_dict = {} for id_type in \"pmcid\" , \"pmid\" : id_ = record . get ( id_type ) if id_ : id_dict [ id_type . upper ()] = id_ return id_dict get_pmid_for_doi def get_pmid_for_doi ( doi ) Query NCBI's E-utilities to retrieve the PMID for a DOI. View Source def get_pmid_for_doi ( doi ) : \"\"\" Query NCBI's E-utilities to retrieve the PMID for a DOI. \"\"\" assert isinstance ( doi , str ) assert doi . startswith ( \"10.\" ) params = { \"db\" : \"pubmed\" , \"term\" : f \"{doi}[DOI]\" } headers = { \"User-Agent\" : get_manubot_user_agent () } url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\" with _get_eutils_rate_limiter () : response = requests . get ( url , params , headers = headers ) if not response . ok : logging . warning ( f \"Status code {response.status_code} querying {response.url}\\n\" ) return None try : element_tree = xml . etree . ElementTree . fromstring ( response . text ) except Exception : logging . warning ( f \"Error in ESearch XML for DOI: {doi}.\\n\" f \"Response from {response.url}:\\n{response.text}\" ) return None id_elems = element_tree . findall ( \"IdList/Id\" ) if len ( id_elems ) != 1 : logging . debug ( f \"No PMIDs found for {doi}.\\n\" f \"Response from {response.url}:\\n{response.text}\" ) return None ( id_elem ,) = id_elems return id_elem . text get_pubmed_csl_item def get_pubmed_csl_item ( pmid ) Query NCBI E-Utilities to create CSL Items for PubMed IDs. https://github.com/manubot/manubot/issues/21 https://github.com/ncbi/citation-exporter/issues/3#issuecomment-355313143 View Source def get_pubmed_csl_item ( pmid ): \"\"\" Query NCBI E-Utilities to create CSL Items for PubMed IDs. https://github.com/manubot/manubot/issues/21 https://github.com/ncbi/citation-exporter/issues/3#issuecomment-355313143 \"\"\" pmid = str ( pmid ) params = { \"db\" : \"pubmed\" , \"id\" : pmid , \"rettype\" : \"full\" } headers = { \"User-Agent\" : get_manubot_user_agent ()} url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\" with _get_eutils_rate_limiter (): response = requests . get ( url , params , headers= headers ) try : element_tree = xml . etree . ElementTree . fromstring ( response . text ) ( element_tree ,) = list ( element_tree ) except Exception as error : logging . error ( f \"Error fetching PubMed metadata for {pmid}. \\ n\" f \"Invalid XML response from {response.url}: \\ n{response.text}\" ) raise error try : csl_item = csl_item_from_pubmed_article ( element_tree ) except Exception as error : msg = f \"Error parsing the following PubMed metadata for PMID {pmid}: \\ n{response.text}\" logging . error ( msg ) raise error return csl_item get_pubmed_ids_for_doi def get_pubmed_ids_for_doi ( doi ) Return a dictionary with PMCID and PMID, if they exist, for the specified DOI. See https://github.com/manubot/manubot/issues/45. View Source def get_pubmed_ids_for_doi ( doi ): \"\"\" Return a dictionary with PMCID and PMID, if they exist, for the specified DOI. See https://github.com/manubot/manubot/issues/45. \"\"\" pubmed_ids = get_pmcid_and_pmid_for_doi ( doi ) if not pubmed_ids : pmid = get_pmid_for_doi ( doi ) if pmid : pubmed_ids [ \"PMID\" ] = pmid return pubmed_ids Classes Handler_PMC class Handler_PMC ( prefix_lower : str ) A Handler is a class that provides support for a certain type of citekey. For example, a Handler subclass could provide support for DOI citekeys. Subclasses enable custom logic for different citekey prefixes, including how to standardize the citekey and how to retrieve CSL Item metadata. View Source class Handler_PMC ( Handler ): standard_prefix = \"pmc\" prefixes = [ \"pmc\" , \"pmcid\" , ] accession_pattern = r \"PMC[0-9]+\" def inspect ( self , citekey ): identifier = citekey . accession # https://www.nlm.nih.gov/bsd/mms/medlineelements.html#pmc if not identifier . startswith ( \"PMC\" ): return \"PubMed Central Identifiers must start with 'PMC'.\" elif not self . _get_pattern (). fullmatch ( identifier ): return ( \"Identifier does not conform to the PMCID regex. \" \"Double check the PMCID.\" ) def get_csl_item ( self , citekey ): return get_pmc_csl_item ( citekey . standard_accession ) Ancestors (in MRO) manubot.cite.handlers.Handler Class variables accession_pattern prefixes standard_prefix Methods get_csl_item def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ): return get_pmc_csl_item ( citekey . standard_accession ) inspect def inspect ( self , citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey ): identifier = citekey . accession # https : // www . nlm . nih . gov / bsd / mms / medlineelements . html # pmc if not identifier . startswith ( \"PMC\" ): return \"PubMed Central Identifiers must start with 'PMC'.\" elif not self . _get_pattern (). fullmatch ( identifier ): return ( \"Identifier does not conform to the PMCID regex. \" \"Double check the PMCID.\" ) standardize_prefix_accession def standardize_prefix_accession ( self , accession ) Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession ): \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession Handler_PubMed class Handler_PubMed ( prefix_lower : str ) A Handler is a class that provides support for a certain type of citekey. For example, a Handler subclass could provide support for DOI citekeys. Subclasses enable custom logic for different citekey prefixes, including how to standardize the citekey and how to retrieve CSL Item metadata. View Source class Handler_PubMed ( Handler ): standard_prefix = \"pubmed\" prefixes = [ \"pubmed\" , \"pmid\" , ] accession_pattern = r \"[1-9][0-9]{0,7}\" def inspect ( self , citekey ): identifier = citekey . accession # https://www.nlm.nih.gov/bsd/mms/medlineelements.html#pmid if identifier . startswith ( \"PMC\" ): return ( \"PubMed Identifiers should start with digits rather than PMC. \" f \"Should {citekey.dealiased_id!r} switch the citation source to 'pmc'?\" ) elif not self . _get_pattern (). fullmatch ( identifier ): return \"PubMed Identifiers should be 1-8 digits with no leading zeros.\" def get_csl_item ( self , citekey ): return get_pubmed_csl_item ( citekey . standard_accession ) Ancestors (in MRO) manubot.cite.handlers.Handler Class variables accession_pattern prefixes standard_prefix Methods get_csl_item def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ): return get_pubmed_csl_item ( citekey . standard_accession ) inspect def inspect ( self , citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey ): identifier = citekey . accession # https : // www . nlm . nih . gov / bsd / mms / medlineelements . html # pmid if identifier . startswith ( \"PMC\" ): return ( \"PubMed Identifiers should start with digits rather than PMC. \" f \"Should {citekey.dealiased_id!r} switch the citation source to 'pmc'?\" ) elif not self . _get_pattern (). fullmatch ( identifier ): return \"PubMed Identifiers should be 1-8 digits with no leading zeros.\" standardize_prefix_accession def standardize_prefix_accession ( self , accession ) Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession ): \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession","title":"Pubmed"},{"location":"reference/manubot/cite/pubmed/#module-manubotcitepubmed","text":"View Source import functools import json import logging import xml.etree.ElementTree import requests from .handlers import Handler from manubot.util import get_manubot_user_agent class Handler_PubMed ( Handler ): standard_prefix = \"pubmed\" prefixes = [ \"pubmed\" , \"pmid\" , ] accession_pattern = r \"[1-9][0-9]{0,7}\" def inspect ( self , citekey ): identifier = citekey . accession # https://www.nlm.nih.gov/bsd/mms/medlineelements.html#pmid if identifier . startswith ( \"PMC\" ): return ( \"PubMed Identifiers should start with digits rather than PMC. \" f \"Should {citekey.dealiased_id!r} switch the citation source to 'pmc'?\" ) elif not self . _get_pattern () . fullmatch ( identifier ): return \"PubMed Identifiers should be 1-8 digits with no leading zeros.\" def get_csl_item ( self , citekey ): return get_pubmed_csl_item ( citekey . standard_accession ) class Handler_PMC ( Handler ): standard_prefix = \"pmc\" prefixes = [ \"pmc\" , \"pmcid\" , ] accession_pattern = r \"PMC[0-9]+\" def inspect ( self , citekey ): identifier = citekey . accession # https://www.nlm.nih.gov/bsd/mms/medlineelements.html#pmc if not identifier . startswith ( \"PMC\" ): return \"PubMed Central Identifiers must start with 'PMC'.\" elif not self . _get_pattern () . fullmatch ( identifier ): return ( \"Identifier does not conform to the PMCID regex. \" \"Double check the PMCID.\" ) def get_csl_item ( self , citekey ): return get_pmc_csl_item ( citekey . standard_accession ) def get_pmc_csl_item ( pmcid ): \"\"\" Get the CSL Item for a PubMed Central record by its PMID, PMCID, or DOI, using the NCBI Citation Exporter API. https://api.ncbi.nlm.nih.gov/lit/ctxp https://github.com/manubot/manubot/issues/21 https://twitter.com/dhimmel/status/1061787168820092929 \"\"\" assert pmcid . startswith ( \"PMC\" ) csl_item = _get_literature_citation_exporter_csl_item ( \"pmc\" , pmcid [ 3 :]) if \"URL\" not in csl_item : csl_item [ \"URL\" ] = f \"https://www.ncbi.nlm.nih.gov/pmc/articles/{csl_item.get('PMCID', pmcid)}/\" return csl_item def _get_literature_citation_exporter_csl_item ( database , identifier ): \"\"\" https://api.ncbi.nlm.nih.gov/lit/ctxp \"\"\" if database not in { \"pubmed\" , \"pmc\" }: logging . error ( f \"Error calling _get_literature_citation_exporter_csl_item. \\n \" f 'database must be either \"pubmed\" or \"pmc\", not {database}' ) assert False if not identifier : logging . error ( f \"Error calling _get_literature_citation_exporter_csl_item. \\n \" f \"identifier cannot be blank\" ) assert False params = { \"format\" : \"csl\" , \"id\" : identifier } headers = { \"User-Agent\" : get_manubot_user_agent ()} url = f \"https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/{database}/\" response = requests . get ( url , params , headers = headers ) try : csl_item = response . json () except Exception as error : logging . error ( f \"Error fetching {database} metadata for {identifier}. \\n \" f \"Invalid JSON response from {response.url}: \\n {response.text}\" ) raise error assert isinstance ( csl_item , dict ) if csl_item . get ( \"status\" , \"okay\" ) == \"error\" : logging . error ( f \"Error fetching {database} metadata for {identifier}. \\n \" f \"Literature Citation Exporter returned JSON indicating an error for {response.url} \\n \" f \"{json.dumps(csl_item, indent=2)}\" ) assert False return csl_item def get_pubmed_csl_item ( pmid ): \"\"\" Query NCBI E-Utilities to create CSL Items for PubMed IDs. https://github.com/manubot/manubot/issues/21 https://github.com/ncbi/citation-exporter/issues/3#issuecomment-355313143 \"\"\" pmid = str ( pmid ) params = { \"db\" : \"pubmed\" , \"id\" : pmid , \"rettype\" : \"full\" } headers = { \"User-Agent\" : get_manubot_user_agent ()} url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\" with _get_eutils_rate_limiter (): response = requests . get ( url , params , headers = headers ) try : element_tree = xml . etree . ElementTree . fromstring ( response . text ) ( element_tree ,) = list ( element_tree ) except Exception as error : logging . error ( f \"Error fetching PubMed metadata for {pmid}. \\n \" f \"Invalid XML response from {response.url}: \\n {response.text}\" ) raise error try : csl_item = csl_item_from_pubmed_article ( element_tree ) except Exception as error : msg = f \"Error parsing the following PubMed metadata for PMID {pmid}: \\n {response.text}\" logging . error ( msg ) raise error return csl_item def csl_item_from_pubmed_article ( article ): \"\"\" article is a PubmedArticle xml element tree https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" csl_item = dict () if not article . find ( \"MedlineCitation/Article\" ): raise NotImplementedError ( \"Unsupported PubMed record: no <Article> element\" ) title = article . findtext ( \"MedlineCitation/Article/ArticleTitle\" ) if title : csl_item [ \"title\" ] = title volume = article . findtext ( \"MedlineCitation/Article/Journal/JournalIssue/Volume\" ) if volume : csl_item [ \"volume\" ] = volume issue = article . findtext ( \"MedlineCitation/Article/Journal/JournalIssue/Issue\" ) if issue : csl_item [ \"issue\" ] = issue page = article . findtext ( \"MedlineCitation/Article/Pagination/MedlinePgn\" ) if page : csl_item [ \"page\" ] = page journal = article . findtext ( \"MedlineCitation/Article/Journal/Title\" ) if journal : csl_item [ \"container-title\" ] = journal journal_short = article . findtext ( \"MedlineCitation/Article/Journal/ISOAbbreviation\" ) if journal_short : csl_item [ \"container-title-short\" ] = journal_short issn = article . findtext ( \"MedlineCitation/Article/Journal/ISSN\" ) if issn : csl_item [ \"ISSN\" ] = issn date_parts = extract_publication_date_parts ( article ) if date_parts : csl_item [ \"issued\" ] = { \"date-parts\" : [ date_parts ]} authors_csl = list () authors = article . findall ( \"MedlineCitation/Article/AuthorList/Author\" ) for author in authors : author_csl = dict () given = author . findtext ( \"ForeName\" ) if given : author_csl [ \"given\" ] = given family = author . findtext ( \"LastName\" ) if family : author_csl [ \"family\" ] = family authors_csl . append ( author_csl ) if authors_csl : csl_item [ \"author\" ] = authors_csl for id_type , key in ( \"pubmed\" , \"PMID\" ), ( \"pmc\" , \"PMCID\" ), ( \"doi\" , \"DOI\" ): xpath = f \"PubmedData/ArticleIdList/ArticleId[@IdType='{id_type}']\" value = article . findtext ( xpath ) if value : csl_item [ key ] = value . lower () if key == \"DOI\" else value abstract = article . findtext ( \"MedlineCitation/Article/Abstract/AbstractText\" ) if abstract : csl_item [ \"abstract\" ] = abstract csl_item [ \"URL\" ] = f \"https://www.ncbi.nlm.nih.gov/pubmed/{csl_item['PMID']}\" csl_item [ \"type\" ] = \"article-journal\" return csl_item month_abbrev_to_int = { \"Jan\" : 1 , \"Feb\" : 2 , \"Mar\" : 3 , \"Apr\" : 4 , \"May\" : 5 , \"Jun\" : 6 , \"Jul\" : 7 , \"Aug\" : 8 , \"Sep\" : 9 , \"Oct\" : 10 , \"Nov\" : 11 , \"Dec\" : 12 , } def extract_publication_date_parts ( article ): \"\"\" Extract date published from a PubmedArticle xml element tree. \"\"\" date_parts = [] # Electronic articles date = article . find ( \"MedlineCitation/Article/ArticleDate\" ) if date : for part in \"Year\" , \"Month\" , \"Day\" : part = date . findtext ( part ) if not part : break date_parts . append ( int ( part )) return date_parts # Print articles date = article . find ( \"MedlineCitation/Article/Journal/JournalIssue/PubDate\" ) year = date . findtext ( \"Year\" ) if year : date_parts . append ( int ( year )) month = date . findtext ( \"Month\" ) if month : try : date_parts . append ( month_abbrev_to_int [ month ]) except KeyError : date_parts . append ( int ( month )) day = date . findtext ( \"Day\" ) if day : date_parts . append ( int ( day )) return date_parts def get_pmcid_and_pmid_for_doi ( doi ): \"\"\" Query PMC's ID Converter API to retrieve the PMCID and PMID for a DOI. Does not work for DOIs that are in Pubmed but not PubMed Central. https://www.ncbi.nlm.nih.gov/pmc/tools/id-converter-api/ \"\"\" assert isinstance ( doi , str ) assert doi . startswith ( \"10.\" ) params = { \"ids\" : doi , \"tool\" : \"manubot\" } url = \"https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/\" response = requests . get ( url , params ) if not response . ok : logging . warning ( f \"Status code {response.status_code} querying {response.url} \\n \" ) return {} try : element_tree = xml . etree . ElementTree . fromstring ( response . text ) except Exception : logging . warning ( f \"Error fetching PMC ID conversion for {doi}. \\n \" f \"Response from {response.url}: \\n {response.text}\" ) return {} records = element_tree . findall ( \"record\" ) if len ( records ) != 1 : logging . warning ( f \"Expected PubMed Central ID converter to return a single XML record for {doi}. \\n \" f \"Response from {response.url}: \\n {response.text}\" ) return {} ( record ,) = records if record . findtext ( \"status\" , default = \"okay\" ) == \"error\" : return {} id_dict = {} for id_type in \"pmcid\" , \"pmid\" : id_ = record . get ( id_type ) if id_ : id_dict [ id_type . upper ()] = id_ return id_dict def get_pmid_for_doi ( doi ): \"\"\" Query NCBI's E-utilities to retrieve the PMID for a DOI. \"\"\" assert isinstance ( doi , str ) assert doi . startswith ( \"10.\" ) params = { \"db\" : \"pubmed\" , \"term\" : f \"{doi}[DOI]\" } headers = { \"User-Agent\" : get_manubot_user_agent ()} url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\" with _get_eutils_rate_limiter (): response = requests . get ( url , params , headers = headers ) if not response . ok : logging . warning ( f \"Status code {response.status_code} querying {response.url} \\n \" ) return None try : element_tree = xml . etree . ElementTree . fromstring ( response . text ) except Exception : logging . warning ( f \"Error in ESearch XML for DOI: {doi}. \\n \" f \"Response from {response.url}: \\n {response.text}\" ) return None id_elems = element_tree . findall ( \"IdList/Id\" ) if len ( id_elems ) != 1 : logging . debug ( f \"No PMIDs found for {doi}. \\n \" f \"Response from {response.url}: \\n {response.text}\" ) return None ( id_elem ,) = id_elems return id_elem . text def get_pubmed_ids_for_doi ( doi ): \"\"\" Return a dictionary with PMCID and PMID, if they exist, for the specified DOI. See https://github.com/manubot/manubot/issues/45. \"\"\" pubmed_ids = get_pmcid_and_pmid_for_doi ( doi ) if not pubmed_ids : pmid = get_pmid_for_doi ( doi ) if pmid : pubmed_ids [ \"PMID\" ] = pmid return pubmed_ids @functools.lru_cache () def _get_eutils_rate_limiter (): \"\"\" Rate limiter to cap NCBI E-utilities queries to 3 per second as per https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities/ \"\"\" from ratelimiter import RateLimiter return RateLimiter ( max_calls = 2 , period = 1 )","title":"Module manubot.cite.pubmed"},{"location":"reference/manubot/cite/pubmed/#variables","text":"month_abbrev_to_int","title":"Variables"},{"location":"reference/manubot/cite/pubmed/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/pubmed/#csl_item_from_pubmed_article","text":"def csl_item_from_pubmed_article ( article ) article is a PubmedArticle xml element tree https://github.com/citation-style-language/schema/blob/master/csl-data.json View Source def csl_item_from_pubmed_article ( article ) : \"\"\" article is a PubmedArticle xml element tree https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" csl_item = dict () if not article . find ( \"MedlineCitation/Article\" ) : raise NotImplementedError ( \"Unsupported PubMed record: no <Article> element\" ) title = article . findtext ( \"MedlineCitation/Article/ArticleTitle\" ) if title : csl_item [ \"title\" ] = title volume = article . findtext ( \"MedlineCitation/Article/Journal/JournalIssue/Volume\" ) if volume : csl_item [ \"volume\" ] = volume issue = article . findtext ( \"MedlineCitation/Article/Journal/JournalIssue/Issue\" ) if issue : csl_item [ \"issue\" ] = issue page = article . findtext ( \"MedlineCitation/Article/Pagination/MedlinePgn\" ) if page : csl_item [ \"page\" ] = page journal = article . findtext ( \"MedlineCitation/Article/Journal/Title\" ) if journal : csl_item [ \"container-title\" ] = journal journal_short = article . findtext ( \"MedlineCitation/Article/Journal/ISOAbbreviation\" ) if journal_short : csl_item [ \"container-title-short\" ] = journal_short issn = article . findtext ( \"MedlineCitation/Article/Journal/ISSN\" ) if issn : csl_item [ \"ISSN\" ] = issn date_parts = extract_publication_date_parts ( article ) if date_parts : csl_item [ \"issued\" ] = { \"date-parts\" : [ date_parts ] } authors_csl = list () authors = article . findall ( \"MedlineCitation/Article/AuthorList/Author\" ) for author in authors : author_csl = dict () given = author . findtext ( \"ForeName\" ) if given : author_csl [ \"given\" ] = given family = author . findtext ( \"LastName\" ) if family : author_csl [ \"family\" ] = family authors_csl . append ( author_csl ) if authors_csl : csl_item [ \"author\" ] = authors_csl for id_type , key in ( \"pubmed\" , \"PMID\" ), ( \"pmc\" , \"PMCID\" ), ( \"doi\" , \"DOI\" ) : xpath = f \"PubmedData/ArticleIdList/ArticleId[@IdType='{id_type}']\" value = article . findtext ( xpath ) if value : csl_item [ key ] = value . lower () if key == \"DOI\" else value abstract = article . findtext ( \"MedlineCitation/Article/Abstract/AbstractText\" ) if abstract : csl_item [ \"abstract\" ] = abstract csl_item [ \"URL\" ] = f \"https://www.ncbi.nlm.nih.gov/pubmed/{csl_item['PMID']}\" csl_item [ \"type\" ] = \"article-journal\" return csl_item","title":"csl_item_from_pubmed_article"},{"location":"reference/manubot/cite/pubmed/#extract_publication_date_parts","text":"def extract_publication_date_parts ( article ) Extract date published from a PubmedArticle xml element tree. View Source def extract_publication_date_parts ( article ) : \"\"\" Extract date published from a PubmedArticle xml element tree. \"\"\" date_parts = [] # Electronic articles date = article . find ( \"MedlineCitation/Article/ArticleDate\" ) if date : for part in \"Year\" , \"Month\" , \"Day\" : part = date . findtext ( part ) if not part : break date_parts . append ( int ( part )) return date_parts # Print articles date = article . find ( \"MedlineCitation/Article/Journal/JournalIssue/PubDate\" ) year = date . findtext ( \"Year\" ) if year : date_parts . append ( int ( year )) month = date . findtext ( \"Month\" ) if month : try : date_parts . append ( month_abbrev_to_int [ month ] ) except KeyError : date_parts . append ( int ( month )) day = date . findtext ( \"Day\" ) if day : date_parts . append ( int ( day )) return date_parts","title":"extract_publication_date_parts"},{"location":"reference/manubot/cite/pubmed/#get_pmc_csl_item","text":"def get_pmc_csl_item ( pmcid ) Get the CSL Item for a PubMed Central record by its PMID, PMCID, or DOI, using the NCBI Citation Exporter API. https://api.ncbi.nlm.nih.gov/lit/ctxp https://github.com/manubot/manubot/issues/21 https://twitter.com/dhimmel/status/1061787168820092929 View Source def get_pmc_csl_item ( pmcid ): \"\"\" Get the CSL Item for a PubMed Central record by its PMID, PMCID, or DOI, using the NCBI Citation Exporter API. https://api.ncbi.nlm.nih.gov/lit/ctxp https://github.com/manubot/manubot/issues/21 https://twitter.com/dhimmel/status/1061787168820092929 \"\"\" assert pmcid . startswith ( \"PMC\" ) csl_item = _get_literature_citation_exporter_csl_item ( \"pmc\" , pmcid [ 3 :]) if \"URL\" not in csl_item : csl_item [ \"URL\" ] = f \"https://www.ncbi.nlm.nih.gov/pmc/articles/{csl_item.get('PMCID', pmcid)}/\" return csl_item","title":"get_pmc_csl_item"},{"location":"reference/manubot/cite/pubmed/#get_pmcid_and_pmid_for_doi","text":"def get_pmcid_and_pmid_for_doi ( doi ) Query PMC's ID Converter API to retrieve the PMCID and PMID for a DOI. Does not work for DOIs that are in Pubmed but not PubMed Central. https://www.ncbi.nlm.nih.gov/pmc/tools/id-converter-api/ View Source def get_pmcid_and_pmid_for_doi ( doi ): \"\"\" Query PMC's ID Converter API to retrieve the PMCID and PMID for a DOI. Does not work for DOIs that are in Pubmed but not PubMed Central. https://www.ncbi.nlm.nih.gov/pmc/tools/id-converter-api/ \"\"\" assert isinstance ( doi , str ) assert doi . startswith ( \"10.\" ) params = { \"ids\" : doi , \"tool\" : \"manubot\" } url = \"https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/\" response = requests . get ( url , params ) if not response . ok : logging . warning ( f \"Status code {response.status_code} querying {response.url}\\n\" ) return {} try : element_tree = xml . etree . ElementTree . fromstring ( response . text ) except Exception : logging . warning ( f \"Error fetching PMC ID conversion for {doi}.\\n\" f \"Response from {response.url}:\\n{response.text}\" ) return {} records = element_tree . findall ( \"record\" ) if len ( records ) != 1 : logging . warning ( f \"Expected PubMed Central ID converter to return a single XML record for {doi}.\\n\" f \"Response from {response.url}:\\n{response.text}\" ) return {} ( record ,) = records if record . findtext ( \"status\" , default = \"okay\" ) == \"error\" : return {} id_dict = {} for id_type in \"pmcid\" , \"pmid\" : id_ = record . get ( id_type ) if id_ : id_dict [ id_type . upper ()] = id_ return id_dict","title":"get_pmcid_and_pmid_for_doi"},{"location":"reference/manubot/cite/pubmed/#get_pmid_for_doi","text":"def get_pmid_for_doi ( doi ) Query NCBI's E-utilities to retrieve the PMID for a DOI. View Source def get_pmid_for_doi ( doi ) : \"\"\" Query NCBI's E-utilities to retrieve the PMID for a DOI. \"\"\" assert isinstance ( doi , str ) assert doi . startswith ( \"10.\" ) params = { \"db\" : \"pubmed\" , \"term\" : f \"{doi}[DOI]\" } headers = { \"User-Agent\" : get_manubot_user_agent () } url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\" with _get_eutils_rate_limiter () : response = requests . get ( url , params , headers = headers ) if not response . ok : logging . warning ( f \"Status code {response.status_code} querying {response.url}\\n\" ) return None try : element_tree = xml . etree . ElementTree . fromstring ( response . text ) except Exception : logging . warning ( f \"Error in ESearch XML for DOI: {doi}.\\n\" f \"Response from {response.url}:\\n{response.text}\" ) return None id_elems = element_tree . findall ( \"IdList/Id\" ) if len ( id_elems ) != 1 : logging . debug ( f \"No PMIDs found for {doi}.\\n\" f \"Response from {response.url}:\\n{response.text}\" ) return None ( id_elem ,) = id_elems return id_elem . text","title":"get_pmid_for_doi"},{"location":"reference/manubot/cite/pubmed/#get_pubmed_csl_item","text":"def get_pubmed_csl_item ( pmid ) Query NCBI E-Utilities to create CSL Items for PubMed IDs. https://github.com/manubot/manubot/issues/21 https://github.com/ncbi/citation-exporter/issues/3#issuecomment-355313143 View Source def get_pubmed_csl_item ( pmid ): \"\"\" Query NCBI E-Utilities to create CSL Items for PubMed IDs. https://github.com/manubot/manubot/issues/21 https://github.com/ncbi/citation-exporter/issues/3#issuecomment-355313143 \"\"\" pmid = str ( pmid ) params = { \"db\" : \"pubmed\" , \"id\" : pmid , \"rettype\" : \"full\" } headers = { \"User-Agent\" : get_manubot_user_agent ()} url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\" with _get_eutils_rate_limiter (): response = requests . get ( url , params , headers= headers ) try : element_tree = xml . etree . ElementTree . fromstring ( response . text ) ( element_tree ,) = list ( element_tree ) except Exception as error : logging . error ( f \"Error fetching PubMed metadata for {pmid}. \\ n\" f \"Invalid XML response from {response.url}: \\ n{response.text}\" ) raise error try : csl_item = csl_item_from_pubmed_article ( element_tree ) except Exception as error : msg = f \"Error parsing the following PubMed metadata for PMID {pmid}: \\ n{response.text}\" logging . error ( msg ) raise error return csl_item","title":"get_pubmed_csl_item"},{"location":"reference/manubot/cite/pubmed/#get_pubmed_ids_for_doi","text":"def get_pubmed_ids_for_doi ( doi ) Return a dictionary with PMCID and PMID, if they exist, for the specified DOI. See https://github.com/manubot/manubot/issues/45. View Source def get_pubmed_ids_for_doi ( doi ): \"\"\" Return a dictionary with PMCID and PMID, if they exist, for the specified DOI. See https://github.com/manubot/manubot/issues/45. \"\"\" pubmed_ids = get_pmcid_and_pmid_for_doi ( doi ) if not pubmed_ids : pmid = get_pmid_for_doi ( doi ) if pmid : pubmed_ids [ \"PMID\" ] = pmid return pubmed_ids","title":"get_pubmed_ids_for_doi"},{"location":"reference/manubot/cite/pubmed/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/pubmed/#handler_pmc","text":"class Handler_PMC ( prefix_lower : str ) A Handler is a class that provides support for a certain type of citekey. For example, a Handler subclass could provide support for DOI citekeys. Subclasses enable custom logic for different citekey prefixes, including how to standardize the citekey and how to retrieve CSL Item metadata. View Source class Handler_PMC ( Handler ): standard_prefix = \"pmc\" prefixes = [ \"pmc\" , \"pmcid\" , ] accession_pattern = r \"PMC[0-9]+\" def inspect ( self , citekey ): identifier = citekey . accession # https://www.nlm.nih.gov/bsd/mms/medlineelements.html#pmc if not identifier . startswith ( \"PMC\" ): return \"PubMed Central Identifiers must start with 'PMC'.\" elif not self . _get_pattern (). fullmatch ( identifier ): return ( \"Identifier does not conform to the PMCID regex. \" \"Double check the PMCID.\" ) def get_csl_item ( self , citekey ): return get_pmc_csl_item ( citekey . standard_accession )","title":"Handler_PMC"},{"location":"reference/manubot/cite/pubmed/#ancestors-in-mro","text":"manubot.cite.handlers.Handler","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/pubmed/#class-variables","text":"accession_pattern prefixes standard_prefix","title":"Class variables"},{"location":"reference/manubot/cite/pubmed/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/pubmed/#get_csl_item","text":"def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ): return get_pmc_csl_item ( citekey . standard_accession )","title":"get_csl_item"},{"location":"reference/manubot/cite/pubmed/#inspect","text":"def inspect ( self , citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey ): identifier = citekey . accession # https : // www . nlm . nih . gov / bsd / mms / medlineelements . html # pmc if not identifier . startswith ( \"PMC\" ): return \"PubMed Central Identifiers must start with 'PMC'.\" elif not self . _get_pattern (). fullmatch ( identifier ): return ( \"Identifier does not conform to the PMCID regex. \" \"Double check the PMCID.\" )","title":"inspect"},{"location":"reference/manubot/cite/pubmed/#standardize_prefix_accession","text":"def standardize_prefix_accession ( self , accession ) Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession ): \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession","title":"standardize_prefix_accession"},{"location":"reference/manubot/cite/pubmed/#handler_pubmed","text":"class Handler_PubMed ( prefix_lower : str ) A Handler is a class that provides support for a certain type of citekey. For example, a Handler subclass could provide support for DOI citekeys. Subclasses enable custom logic for different citekey prefixes, including how to standardize the citekey and how to retrieve CSL Item metadata. View Source class Handler_PubMed ( Handler ): standard_prefix = \"pubmed\" prefixes = [ \"pubmed\" , \"pmid\" , ] accession_pattern = r \"[1-9][0-9]{0,7}\" def inspect ( self , citekey ): identifier = citekey . accession # https://www.nlm.nih.gov/bsd/mms/medlineelements.html#pmid if identifier . startswith ( \"PMC\" ): return ( \"PubMed Identifiers should start with digits rather than PMC. \" f \"Should {citekey.dealiased_id!r} switch the citation source to 'pmc'?\" ) elif not self . _get_pattern (). fullmatch ( identifier ): return \"PubMed Identifiers should be 1-8 digits with no leading zeros.\" def get_csl_item ( self , citekey ): return get_pubmed_csl_item ( citekey . standard_accession )","title":"Handler_PubMed"},{"location":"reference/manubot/cite/pubmed/#ancestors-in-mro_1","text":"manubot.cite.handlers.Handler","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/pubmed/#class-variables_1","text":"accession_pattern prefixes standard_prefix","title":"Class variables"},{"location":"reference/manubot/cite/pubmed/#methods_1","text":"","title":"Methods"},{"location":"reference/manubot/cite/pubmed/#get_csl_item_1","text":"def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ): return get_pubmed_csl_item ( citekey . standard_accession )","title":"get_csl_item"},{"location":"reference/manubot/cite/pubmed/#inspect_1","text":"def inspect ( self , citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey ): identifier = citekey . accession # https : // www . nlm . nih . gov / bsd / mms / medlineelements . html # pmid if identifier . startswith ( \"PMC\" ): return ( \"PubMed Identifiers should start with digits rather than PMC. \" f \"Should {citekey.dealiased_id!r} switch the citation source to 'pmc'?\" ) elif not self . _get_pattern (). fullmatch ( identifier ): return \"PubMed Identifiers should be 1-8 digits with no leading zeros.\"","title":"inspect"},{"location":"reference/manubot/cite/pubmed/#standardize_prefix_accession_1","text":"def standardize_prefix_accession ( self , accession ) Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession ): \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession","title":"standardize_prefix_accession"},{"location":"reference/manubot/cite/unpaywall/","text":"Module manubot.cite.unpaywall Utilities for accessing https://unpaywall.org/ data to provide access information for DOIs. Also supports identifier sources not directly supported by Unpaywall, such as arXiv IDs. View Source \"\"\" Utilities for accessing <https://unpaywall.org/> data to provide access information for DOIs. Also supports identifier sources not directly supported by Unpaywall, such as arXiv IDs. \"\"\" import abc import requests \"\"\" Unpaywall license choices used by Location.has_open_license. Defaults to licenses that conform to <https://opendefinition.org/>. \"\"\" open_licenses = { \"cc0\" , \"cc-by\" , \"cc-by-sa\" , \"pd\" } class Unpaywall : \"\"\" A class to handle open access locations in the Unpaywall data format. Create new Unpaywall objects using the `from_csl_item` and `from_citekey` methods, or by using __init__ of a subclass like `Unpaywall_DOI`. \"\"\" csl_item = None @abc.abstractmethod def set_oa_locations ( self ): \"\"\" Set `self.oa_locations`, which is a list of `Unpaywall_Location` objects. \"\"\" self . oa_locations = [] @property def best_openly_licensed_pdf ( self ) -> \"Unpaywall_Location\" : for location in self . oa_locations : if location . has_openly_licensed_pdf : return location @property def best_pdf ( self ) -> \"Unpaywall_Location\" : for location in self . oa_locations : if location . has_pdf : return location @staticmethod def from_citekey ( citekey , csl_item = None ): \"\"\" Create an Unpaywall object for `citekey`. `csl_item` is an optional field that can avoid an external web request to generate to a new CSL Item. \"\"\" from .citekey import CiteKey if isinstance ( citekey , str ): citekey = CiteKey ( citekey ) if not isinstance ( citekey , CiteKey ): raise ValueError ( \"citekey must be a str or CiteKey\" ) if citekey . standard_prefix in source_to_unpaywaller : unpaywaller = source_to_unpaywaller [ citekey . standard_prefix ] unpaywall = unpaywaller ( citekey . standard_accession , set_oa_locations = False ) else : raise ValueError ( f \"Cannot Unpaywall {citekey.input_id}. \" f \"Supported citations sources are {', '.join(source_to_unpaywaller)}. \" \"Received {citekey.standard_prefix!r}.\" ) unpaywall . csl_item = csl_item unpaywall . set_oa_locations () return unpaywall @classmethod def from_csl_item ( cls , csl_item ): \"\"\" Create an Unpaywall object for `csl_item`. \"\"\" from .csl_item import CSL_Item csl_item = CSL_Item ( csl_item ) doi = csl_item . get ( \"DOI\" ) if doi : return cls . from_citekey ( f \"doi:{doi}\" , csl_item = csl_item ) csl_item . infer_id () return cls . from_citekey ( csl_item [ \"id\" ], csl_item = csl_item ) class Unpaywall_DOI ( Unpaywall ): \"\"\" From https://unpaywall.org/data-format: > The DOI object is more or less a row in our main database... it's everything we know about a given DOI-assigned resource, including metadata about the resource itself, and information about its OA status. It includes a list of zero or more OA Location Objects, as well as a `best_oa_location` property that's probably the OA Location you'll want to use. \"\"\" def __init__ ( self , doi , set_oa_locations = True ): self . doi = doi . lower () if set_oa_locations : self . set_oa_locations () def set_oa_locations ( self ): from manubot.util import contact_email url = f \"https://api.unpaywall.org/v2/{self.doi}\" params = { \"email\" : contact_email } response = requests . get ( url , params = params ) self . results = response . json () self . oa_locations = [ Unpaywall_Location ( location ) for location in self . results . get ( \"oa_locations\" , []) ] class Unpaywall_arXiv ( Unpaywall ): def __init__ ( self , arxiv_id , set_oa_locations = True , use_doi = True ): from .arxiv import split_arxiv_id_version self . arxiv_id = arxiv_id self . arxiv_id_latest , self . arxiv_id_version = split_arxiv_id_version ( arxiv_id ) self . use_doi = use_doi if set_oa_locations : self . set_oa_locations () def set_oa_locations ( self ): from .arxiv import get_arxiv_csl_item if not self . csl_item : self . csl_item = get_arxiv_csl_item ( self . arxiv_id ) doi = self . csl_item . get ( \"DOI\" ) if self . use_doi and doi : unpaywall_doi = Unpaywall_DOI ( doi ) self . doi = unpaywall_doi . doi self . oa_locations = unpaywall_doi . oa_locations return location = self . location_from_arvix_id () self . oa_locations = [ location ] def location_from_arvix_id ( self ): import datetime url_for_pdf = f \"https://arxiv.org/pdf/{self.arxiv_id}.pdf\" location = Unpaywall_Location ( { \"endpoint_id\" : None , \"evidence\" : \"oa repository\" , \"host_type\" : \"repository\" , \"is_best\" : True , \"license\" : self . get_license (), \"pmh_id\" : f \"oai:arXiv.org:{self.arxiv_id_latest}\" , \"repository_institution\" : \"Cornell University - arXiv\" , \"updated\" : datetime . datetime . now () . isoformat (), \"url\" : url_for_pdf , \"url_for_landing_page\" : f \"https://arxiv.org/abs/{self.arxiv_id}\" , \"url_for_pdf\" : url_for_pdf , \"version\" : \"submittedVersion\" , } ) return location def get_license ( self ): \"\"\" Return license using choices from the Unpaywall data format. Looks for license metadata in the CSL Item. \"\"\" license = self . csl_item . note_dict . get ( \"license\" ) if not license : return # Example licenses from https://arxiv.org/help/license # http://creativecommons.org/publicdomain/zero/1.0/ # http://creativecommons.org/licenses/by/4.0/ # http://creativecommons.org/licenses/by-sa/4.0/ # http://creativecommons.org/licenses/by-nc-sa/4.0/ # http://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html from urllib.parse import urlparse parsed_url = urlparse ( license ) if not parsed_url . scheme . startswith ( \"http\" ): return if parsed_url . hostname . endswith ( \"creativecommons.org\" ): try : abbrev = parsed_url . path . split ( \"/\" )[ 2 ] except IndexError : return if abbrev == \"zero\" : return \"cc0\" return f \"cc-{abbrev}\" source_to_unpaywaller = { \"doi\" : Unpaywall_DOI , \"arxiv\" : Unpaywall_arXiv , } class Unpaywall_Location ( dict ): \"\"\" From https://unpaywall.org/data-format > The OA Location object describes particular place where we found a given OA article. The same article is often available from multiple locations, and there may be differences in format, version, and license depending on the location; the OA Location object describes these key attributes. An OA Location Object is always a Child of a DOI Object. Example oa_locations from the Unpaywall API are: ```json { \"endpoint_id\": null, \"evidence\": \"open (via page says license)\", \"host_type\": \"publisher\", \"is_best\": true, \"license\": \"cc-by\", \"pmh_id\": null, \"repository_institution\": null, \"updated\": \"2020-01-19T08:55:45.548214\", \"url\": \"https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1007250&type=printable\", \"url_for_landing_page\": \"https://doi.org/10.1371/journal.pcbi.1007250\", \"url_for_pdf\": \"https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1007250&type=printable\", \"version\": \"publishedVersion\" }, { \"endpoint_id\": \"ca8f8d56758a80a4f86\", \"evidence\": \"oa repository (via OAI-PMH doi match)\", \"host_type\": \"repository\", \"is_best\": true, \"license\": null, \"pmh_id\": \"oai:arXiv.org:1806.05726\", \"repository_institution\": \"Cornell University - arXiv\", \"updated\": \"2019-11-01T00:28:16.784912\", \"url\": \"http://arxiv.org/pdf/1806.05726\", \"url_for_landing_page\": \"http://arxiv.org/abs/1806.05726\", \"url_for_pdf\": \"http://arxiv.org/pdf/1806.05726\", \"version\": \"submittedVersion\" } ``` \"\"\" @property def has_pdf ( self ): return bool ( self . get ( \"url_for_pdf\" )) @property def has_open_license ( self ): license = self . get ( \"license\" ) return license in open_licenses @property def has_creative_commons_license ( self ): license = self . get ( \"license\" ) if not license : return False return license == \"cc0\" or license . startswith ( \"cc-\" ) @property def has_openly_licensed_pdf ( self ): return self . has_pdf and self . has_open_license Variables open_licenses source_to_unpaywaller Classes Unpaywall class Unpaywall ( / , * args , ** kwargs ) A class to handle open access locations in the Unpaywall data format. Create new Unpaywall objects using the from_csl_item and from_citekey methods, or by using init of a subclass like Unpaywall_DOI . View Source class Unpaywall : \"\"\" A class to handle open access locations in the Unpaywall data format. Create new Unpaywall objects using the `from_csl_item` and `from_citekey` methods, or by using __init__ of a subclass like `Unpaywall_DOI`. \"\"\" csl_item = None @ abc . abstractmethod def set_oa_locations ( self ): \"\"\" Set `self.oa_locations`, which is a list of `Unpaywall_Location` objects. \"\"\" self . oa_locations = [] @ property def best_openly_licensed_pdf ( self ) -> \"Unpaywall_Location\" : for location in self . oa_locations : if location . has_openly_licensed_pdf : return location @ property def best_pdf ( self ) -> \"Unpaywall_Location\" : for location in self . oa_locations : if location . has_pdf : return location @ staticmethod def from_citekey ( citekey , csl_item = None ): \"\"\" Create an Unpaywall object for `citekey`. `csl_item` is an optional field that can avoid an external web request to generate to a new CSL Item. \"\"\" from . citekey import CiteKey if isinstance ( citekey , str ): citekey = CiteKey ( citekey ) if not isinstance ( citekey , CiteKey ): raise ValueError ( \"citekey must be a str or CiteKey\" ) if citekey . standard_prefix in source_to_unpaywaller : unpaywaller = source_to_unpaywaller [ citekey . standard_prefix ] unpaywall = unpaywaller ( citekey . standard_accession , set_oa_locations = False ) else : raise ValueError ( f \"Cannot Unpaywall {citekey.input_id}. \" f \"Supported citations sources are {', '.join(source_to_unpaywaller)}. \" \"Received {citekey.standard_prefix!r}.\" ) unpaywall . csl_item = csl_item unpaywall . set_oa_locations () return unpaywall @ classmethod def from_csl_item ( cls , csl_item ): \"\"\" Create an Unpaywall object for `csl_item`. \"\"\" from . csl_item import CSL_Item csl_item = CSL_Item ( csl_item ) doi = csl_item . get ( \"DOI\" ) if doi : return cls . from_citekey ( f \"doi:{doi}\" , csl_item = csl_item ) csl_item . infer_id () return cls . from_citekey ( csl_item [ \"id\" ], csl_item = csl_item ) Descendants manubot.cite.unpaywall.Unpaywall_DOI manubot.cite.unpaywall.Unpaywall_arXiv Class variables csl_item Static methods from_citekey def from_citekey ( citekey , csl_item = None ) Create an Unpaywall object for citekey . csl_item is an optional field that can avoid an external web request to generate to a new CSL Item. View Source @staticmethod def from_citekey ( citekey , csl_item = None ): \"\"\" Create an Unpaywall object for `citekey`. `csl_item` is an optional field that can avoid an external web request to generate to a new CSL Item. \"\"\" from .citekey import CiteKey if isinstance ( citekey , str ): citekey = CiteKey ( citekey ) if not isinstance ( citekey , CiteKey ): raise ValueError ( \"citekey must be a str or CiteKey\" ) if citekey . standard_prefix in source_to_unpaywaller : unpaywaller = source_to_unpaywaller [ citekey . standard_prefix ] unpaywall = unpaywaller ( citekey . standard_accession , set_oa_locations = False ) else : raise ValueError ( f \"Cannot Unpaywall {citekey.input_id}. \" f \"Supported citations sources are {', '.join(source_to_unpaywaller)}. \" \"Received {citekey.standard_prefix!r}.\" ) unpaywall . csl_item = csl_item unpaywall . set_oa_locations () return unpaywall from_csl_item def from_csl_item ( csl_item ) Create an Unpaywall object for csl_item . View Source @classmethod def from_csl_item ( cls , csl_item ): \"\"\" Create an Unpaywall object for `csl_item`. \"\"\" from .csl_item import CSL_Item csl_item = CSL_Item ( csl_item ) doi = csl_item . get ( \"DOI\" ) if doi : return cls . from_citekey ( f \"doi:{doi}\" , csl_item = csl_item ) csl_item . infer_id () return cls . from_citekey ( csl_item [ \"id\" ], csl_item = csl_item ) Instance variables best_openly_licensed_pdf best_pdf Methods set_oa_locations def set_oa_locations ( self ) Set self.oa_locations , which is a list of Unpaywall_Location objects. View Source @ abc . abstractmethod def set_oa_locations ( self ): \"\"\" Set `self.oa_locations`, which is a list of `Unpaywall_Location` objects. \"\"\" self . oa_locations = [] Unpaywall_DOI class Unpaywall_DOI ( doi , set_oa_locations = True ) From https://unpaywall.org/data-format: The DOI object is more or less a row in our main database... it's everything we know about a given DOI-assigned resource, including metadata about the resource itself, and information about its OA status. It includes a list of zero or more OA Location Objects, as well as a best_oa_location property that's probably the OA Location you'll want to use. View Source class Unpaywall_DOI ( Unpaywall ): \"\"\" From https://unpaywall.org/data-format: > The DOI object is more or less a row in our main database... it's everything we know about a given DOI-assigned resource, including metadata about the resource itself, and information about its OA status. It includes a list of zero or more OA Location Objects, as well as a `best_oa_location` property that's probably the OA Location you'll want to use. \"\"\" def __init__ ( self , doi , set_oa_locations = True ): self . doi = doi . lower () if set_oa_locations : self . set_oa_locations () def set_oa_locations ( self ): from manubot.util import contact_email url = f \"https://api.unpaywall.org/v2/{self.doi}\" params = { \"email\" : contact_email } response = requests . get ( url , params = params ) self . results = response . json () self . oa_locations = [ Unpaywall_Location ( location ) for location in self . results . get ( \"oa_locations\" , []) ] Ancestors (in MRO) manubot.cite.unpaywall.Unpaywall Class variables csl_item Static methods from_citekey def from_citekey ( citekey , csl_item = None ) Create an Unpaywall object for citekey . csl_item is an optional field that can avoid an external web request to generate to a new CSL Item. View Source @staticmethod def from_citekey ( citekey , csl_item = None ): \"\"\" Create an Unpaywall object for `citekey`. `csl_item` is an optional field that can avoid an external web request to generate to a new CSL Item. \"\"\" from .citekey import CiteKey if isinstance ( citekey , str ): citekey = CiteKey ( citekey ) if not isinstance ( citekey , CiteKey ): raise ValueError ( \"citekey must be a str or CiteKey\" ) if citekey . standard_prefix in source_to_unpaywaller : unpaywaller = source_to_unpaywaller [ citekey . standard_prefix ] unpaywall = unpaywaller ( citekey . standard_accession , set_oa_locations = False ) else : raise ValueError ( f \"Cannot Unpaywall {citekey.input_id}. \" f \"Supported citations sources are {', '.join(source_to_unpaywaller)}. \" \"Received {citekey.standard_prefix!r}.\" ) unpaywall . csl_item = csl_item unpaywall . set_oa_locations () return unpaywall from_csl_item def from_csl_item ( csl_item ) Create an Unpaywall object for csl_item . View Source @classmethod def from_csl_item ( cls , csl_item ): \"\"\" Create an Unpaywall object for `csl_item`. \"\"\" from .csl_item import CSL_Item csl_item = CSL_Item ( csl_item ) doi = csl_item . get ( \"DOI\" ) if doi : return cls . from_citekey ( f \"doi:{doi}\" , csl_item = csl_item ) csl_item . infer_id () return cls . from_citekey ( csl_item [ \"id\" ], csl_item = csl_item ) Instance variables best_openly_licensed_pdf best_pdf Methods set_oa_locations def set_oa_locations ( self ) Set self.oa_locations , which is a list of Unpaywall_Location objects. View Source def set_oa_locations ( self ): from manubot.util import contact_email url = f \"https://api.unpaywall.org/v2/{self.doi}\" params = { \"email\" : contact_email } response = requests . get ( url , params = params ) self . results = response . json () self . oa_locations = [ Unpaywall_Location ( location ) for location in self . results . get ( \"oa_locations\" , []) ] Unpaywall_Location class Unpaywall_Location ( / , * args , ** kwargs ) From https://unpaywall.org/data-format The OA Location object describes particular place where we found a given OA article. The same article is often available from multiple locations, and there may be differences in format, version, and license depending on the location; the OA Location object describes these key attributes. An OA Location Object is always a Child of a DOI Object. Example oa_locations from the Unpaywall API are: { \"endpoint_id\" : null , \"evidence\" : \"open (via page says license)\" , \"host_type\" : \"publisher\" , \"is_best\" : true , \"license\" : \"cc-by\" , \"pmh_id\" : null , \"repository_institution\" : null , \"updated\" : \"2020-01-19T08:55:45.548214\" , \"url\" : \"https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1007250&type=printable\" , \"url_for_landing_page\" : \"https://doi.org/10.1371/journal.pcbi.1007250\" , \"url_for_pdf\" : \"https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1007250&type=printable\" , \"version\" : \"publishedVersion\" } , { \"endpoint_id\" : \"ca8f8d56758a80a4f86\" , \"evidence\" : \"oa repository (via OAI-PMH doi match)\" , \"host_type\" : \"repository\" , \"is_best\" : true , \"license\" : null , \"pmh_id\" : \"oai:arXiv.org:1806.05726\" , \"repository_institution\" : \"Cornell University - arXiv\" , \"updated\" : \"2019-11-01T00:28:16.784912\" , \"url\" : \"http://arxiv.org/pdf/1806.05726\" , \"url_for_landing_page\" : \"http://arxiv.org/abs/1806.05726\" , \"url_for_pdf\" : \"http://arxiv.org/pdf/1806.05726\" , \"version\" : \"submittedVersion\" } View Source class Unpaywall_Location ( dict ) : \"\"\" From https://unpaywall.org/data-format > The OA Location object describes particular place where we found a given OA article. The same article is often available from multiple locations, and there may be differences in format, version, and license depending on the location; the OA Location object describes these key attributes. An OA Location Object is always a Child of a DOI Object. Example oa_locations from the Unpaywall API are: ```json { \" endpoint_id \": null, \" evidence \": \" open ( via page says license ) \", \" host_type \": \" publisher \", \" is_best \": true, \" license \": \" cc - by \", \" pmh_id \": null, \" repository_institution \": null, \" updated \": \" 2020 - 01 - 19 T08 : 55 : 45.548214 \", \" url \": \" https : // journals . plos . org / ploscompbiol / article / file ? id = 10.1371 / journal . pcbi .1007250 & type = printable \", \" url_for_landing_page \": \" https : // doi . org / 10.1371 / journal . pcbi .1007250 \", \" url_for_pdf \": \" https : // journals . plos . org / ploscompbiol / article / file ? id = 10.1371 / journal . pcbi .1007250 & type = printable \", \" version \": \" publishedVersion \" }, { \" endpoint_id \": \" ca8f8d56758a80a4f86 \", \" evidence \": \" oa repository ( via OAI - PMH doi match ) \", \" host_type \": \" repository \", \" is_best \": true, \" license \": null, \" pmh_id \": \" oai : arXiv . org : 1806.05726 \", \" repository_institution \": \" Cornell University - arXiv \", \" updated \": \" 2019 - 11 - 01 T00 : 28 : 16.784912 \", \" url \": \" http : // arxiv . org / pdf / 1806.05726 \", \" url_for_landing_page \": \" http : // arxiv . org / abs / 1806.05726 \", \" url_for_pdf \": \" http : // arxiv . org / pdf / 1806.05726 \", \" version \": \" submittedVersion \" } ``` \"\"\" @property def has_pdf ( self ) : return bool ( self . get ( \"url_for_pdf\" )) @property def has_open_license ( self ) : license = self . get ( \"license\" ) return license in open_licenses @property def has_creative_commons_license ( self ) : license = self . get ( \"license\" ) if not license : return False return license == \"cc0\" or license . startswith ( \"cc-\" ) @property def has_openly_licensed_pdf ( self ) : return self . has_pdf and self . has_open_license Ancestors (in MRO) builtins.dict Instance variables has_creative_commons_license has_open_license has_openly_licensed_pdf has_pdf Methods clear def clear ( ... ) D.clear() -> None. Remove all items from D. copy def copy ( ... ) D.copy() -> a shallow copy of D fromkeys def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value. get def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default. items def items ( ... ) D.items() -> a set-like object providing a view on D's items keys def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys pop def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised popitem def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. setdefault def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. update def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k] values def values ( ... ) D.values() -> an object providing a view on D's values Unpaywall_arXiv class Unpaywall_arXiv ( arxiv_id , set_oa_locations = True , use_doi = True ) A class to handle open access locations in the Unpaywall data format. Create new Unpaywall objects using the from_csl_item and from_citekey methods, or by using init of a subclass like Unpaywall_DOI . View Source class Unpaywall_arXiv ( Unpaywall ): def __init__ ( self , arxiv_id , set_oa_locations = True , use_doi = True ): from .arxiv import split_arxiv_id_version self . arxiv_id = arxiv_id self . arxiv_id_latest , self . arxiv_id_version = split_arxiv_id_version ( arxiv_id ) self . use_doi = use_doi if set_oa_locations : self . set_oa_locations () def set_oa_locations ( self ): from .arxiv import get_arxiv_csl_item if not self . csl_item : self . csl_item = get_arxiv_csl_item ( self . arxiv_id ) doi = self . csl_item . get ( \"DOI\" ) if self . use_doi and doi : unpaywall_doi = Unpaywall_DOI ( doi ) self . doi = unpaywall_doi . doi self . oa_locations = unpaywall_doi . oa_locations return location = self . location_from_arvix_id () self . oa_locations = [ location ] def location_from_arvix_id ( self ): import datetime url_for_pdf = f \"https://arxiv.org/pdf/{self.arxiv_id}.pdf\" location = Unpaywall_Location ( { \"endpoint_id\" : None , \"evidence\" : \"oa repository\" , \"host_type\" : \"repository\" , \"is_best\" : True , \"license\" : self . get_license (), \"pmh_id\" : f \"oai:arXiv.org:{self.arxiv_id_latest}\" , \"repository_institution\" : \"Cornell University - arXiv\" , \"updated\" : datetime . datetime . now () . isoformat (), \"url\" : url_for_pdf , \"url_for_landing_page\" : f \"https://arxiv.org/abs/{self.arxiv_id}\" , \"url_for_pdf\" : url_for_pdf , \"version\" : \"submittedVersion\" , } ) return location def get_license ( self ): \"\"\" Return license using choices from the Unpaywall data format. Looks for license metadata in the CSL Item. \"\"\" license = self . csl_item . note_dict . get ( \"license\" ) if not license : return # Example licenses from https://arxiv.org/help/license # http://creativecommons.org/publicdomain/zero/1.0/ # http://creativecommons.org/licenses/by/4.0/ # http://creativecommons.org/licenses/by-sa/4.0/ # http://creativecommons.org/licenses/by-nc-sa/4.0/ # http://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html from urllib.parse import urlparse parsed_url = urlparse ( license ) if not parsed_url . scheme . startswith ( \"http\" ): return if parsed_url . hostname . endswith ( \"creativecommons.org\" ): try : abbrev = parsed_url . path . split ( \"/\" )[ 2 ] except IndexError : return if abbrev == \"zero\" : return \"cc0\" return f \"cc-{abbrev}\" Ancestors (in MRO) manubot.cite.unpaywall.Unpaywall Class variables csl_item Static methods from_citekey def from_citekey ( citekey , csl_item = None ) Create an Unpaywall object for citekey . csl_item is an optional field that can avoid an external web request to generate to a new CSL Item. View Source @staticmethod def from_citekey ( citekey , csl_item = None ): \"\"\" Create an Unpaywall object for `citekey`. `csl_item` is an optional field that can avoid an external web request to generate to a new CSL Item. \"\"\" from .citekey import CiteKey if isinstance ( citekey , str ): citekey = CiteKey ( citekey ) if not isinstance ( citekey , CiteKey ): raise ValueError ( \"citekey must be a str or CiteKey\" ) if citekey . standard_prefix in source_to_unpaywaller : unpaywaller = source_to_unpaywaller [ citekey . standard_prefix ] unpaywall = unpaywaller ( citekey . standard_accession , set_oa_locations = False ) else : raise ValueError ( f \"Cannot Unpaywall {citekey.input_id}. \" f \"Supported citations sources are {', '.join(source_to_unpaywaller)}. \" \"Received {citekey.standard_prefix!r}.\" ) unpaywall . csl_item = csl_item unpaywall . set_oa_locations () return unpaywall from_csl_item def from_csl_item ( csl_item ) Create an Unpaywall object for csl_item . View Source @classmethod def from_csl_item ( cls , csl_item ): \"\"\" Create an Unpaywall object for `csl_item`. \"\"\" from .csl_item import CSL_Item csl_item = CSL_Item ( csl_item ) doi = csl_item . get ( \"DOI\" ) if doi : return cls . from_citekey ( f \"doi:{doi}\" , csl_item = csl_item ) csl_item . infer_id () return cls . from_citekey ( csl_item [ \"id\" ], csl_item = csl_item ) Instance variables best_openly_licensed_pdf best_pdf Methods get_license def get_license ( self ) Return license using choices from the Unpaywall data format. Looks for license metadata in the CSL Item. View Source def get_license ( self ): \"\"\" Return license using choices from the Unpaywall data format. Looks for license metadata in the CSL Item. \"\"\" license = self . csl_item . note_dict . get ( \"license\" ) if not license : return # Example licenses from https://arxiv.org/help/license # http://creativecommons.org/publicdomain/zero/1.0/ # http://creativecommons.org/licenses/by/4.0/ # http://creativecommons.org/licenses/by-sa/4.0/ # http://creativecommons.org/licenses/by-nc-sa/4.0/ # http://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html from urllib.parse import urlparse parsed_url = urlparse ( license ) if not parsed_url . scheme . startswith ( \"http\" ): return if parsed_url . hostname . endswith ( \"creativecommons.org\" ): try : abbrev = parsed_url . path . split ( \"/\" )[ 2 ] except IndexError : return if abbrev == \"zero\" : return \"cc0\" return f \"cc-{abbrev}\" location_from_arvix_id def location_from_arvix_id ( self ) View Source def location_from_arvix_id ( self ): import datetime url_for_pdf = f \"https://arxiv.org/pdf/{self.arxiv_id}.pdf\" location = Unpaywall_Location ( { \"endpoint_id\" : None , \"evidence\" : \"oa repository\" , \"host_type\" : \"repository\" , \"is_best\" : True , \"license\" : self . get_license (), \"pmh_id\" : f \"oai:arXiv.org:{self.arxiv_id_latest}\" , \"repository_institution\" : \"Cornell University - arXiv\" , \"updated\" : datetime . datetime . now () . isoformat (), \"url\" : url_for_pdf , \"url_for_landing_page\" : f \"https://arxiv.org/abs/{self.arxiv_id}\" , \"url_for_pdf\" : url_for_pdf , \"version\" : \"submittedVersion\" , } ) return location set_oa_locations def set_oa_locations ( self ) Set self.oa_locations , which is a list of Unpaywall_Location objects. View Source def set_oa_locations ( self ): from .arxiv import get_arxiv_csl_item if not self . csl_item : self . csl_item = get_arxiv_csl_item ( self . arxiv_id ) doi = self . csl_item . get ( \"DOI\" ) if self . use_doi and doi : unpaywall_doi = Unpaywall_DOI ( doi ) self . doi = unpaywall_doi . doi self . oa_locations = unpaywall_doi . oa_locations return location = self . location_from_arvix_id () self . oa_locations = [ location ]","title":"Unpaywall"},{"location":"reference/manubot/cite/unpaywall/#module-manubotciteunpaywall","text":"Utilities for accessing https://unpaywall.org/ data to provide access information for DOIs. Also supports identifier sources not directly supported by Unpaywall, such as arXiv IDs. View Source \"\"\" Utilities for accessing <https://unpaywall.org/> data to provide access information for DOIs. Also supports identifier sources not directly supported by Unpaywall, such as arXiv IDs. \"\"\" import abc import requests \"\"\" Unpaywall license choices used by Location.has_open_license. Defaults to licenses that conform to <https://opendefinition.org/>. \"\"\" open_licenses = { \"cc0\" , \"cc-by\" , \"cc-by-sa\" , \"pd\" } class Unpaywall : \"\"\" A class to handle open access locations in the Unpaywall data format. Create new Unpaywall objects using the `from_csl_item` and `from_citekey` methods, or by using __init__ of a subclass like `Unpaywall_DOI`. \"\"\" csl_item = None @abc.abstractmethod def set_oa_locations ( self ): \"\"\" Set `self.oa_locations`, which is a list of `Unpaywall_Location` objects. \"\"\" self . oa_locations = [] @property def best_openly_licensed_pdf ( self ) -> \"Unpaywall_Location\" : for location in self . oa_locations : if location . has_openly_licensed_pdf : return location @property def best_pdf ( self ) -> \"Unpaywall_Location\" : for location in self . oa_locations : if location . has_pdf : return location @staticmethod def from_citekey ( citekey , csl_item = None ): \"\"\" Create an Unpaywall object for `citekey`. `csl_item` is an optional field that can avoid an external web request to generate to a new CSL Item. \"\"\" from .citekey import CiteKey if isinstance ( citekey , str ): citekey = CiteKey ( citekey ) if not isinstance ( citekey , CiteKey ): raise ValueError ( \"citekey must be a str or CiteKey\" ) if citekey . standard_prefix in source_to_unpaywaller : unpaywaller = source_to_unpaywaller [ citekey . standard_prefix ] unpaywall = unpaywaller ( citekey . standard_accession , set_oa_locations = False ) else : raise ValueError ( f \"Cannot Unpaywall {citekey.input_id}. \" f \"Supported citations sources are {', '.join(source_to_unpaywaller)}. \" \"Received {citekey.standard_prefix!r}.\" ) unpaywall . csl_item = csl_item unpaywall . set_oa_locations () return unpaywall @classmethod def from_csl_item ( cls , csl_item ): \"\"\" Create an Unpaywall object for `csl_item`. \"\"\" from .csl_item import CSL_Item csl_item = CSL_Item ( csl_item ) doi = csl_item . get ( \"DOI\" ) if doi : return cls . from_citekey ( f \"doi:{doi}\" , csl_item = csl_item ) csl_item . infer_id () return cls . from_citekey ( csl_item [ \"id\" ], csl_item = csl_item ) class Unpaywall_DOI ( Unpaywall ): \"\"\" From https://unpaywall.org/data-format: > The DOI object is more or less a row in our main database... it's everything we know about a given DOI-assigned resource, including metadata about the resource itself, and information about its OA status. It includes a list of zero or more OA Location Objects, as well as a `best_oa_location` property that's probably the OA Location you'll want to use. \"\"\" def __init__ ( self , doi , set_oa_locations = True ): self . doi = doi . lower () if set_oa_locations : self . set_oa_locations () def set_oa_locations ( self ): from manubot.util import contact_email url = f \"https://api.unpaywall.org/v2/{self.doi}\" params = { \"email\" : contact_email } response = requests . get ( url , params = params ) self . results = response . json () self . oa_locations = [ Unpaywall_Location ( location ) for location in self . results . get ( \"oa_locations\" , []) ] class Unpaywall_arXiv ( Unpaywall ): def __init__ ( self , arxiv_id , set_oa_locations = True , use_doi = True ): from .arxiv import split_arxiv_id_version self . arxiv_id = arxiv_id self . arxiv_id_latest , self . arxiv_id_version = split_arxiv_id_version ( arxiv_id ) self . use_doi = use_doi if set_oa_locations : self . set_oa_locations () def set_oa_locations ( self ): from .arxiv import get_arxiv_csl_item if not self . csl_item : self . csl_item = get_arxiv_csl_item ( self . arxiv_id ) doi = self . csl_item . get ( \"DOI\" ) if self . use_doi and doi : unpaywall_doi = Unpaywall_DOI ( doi ) self . doi = unpaywall_doi . doi self . oa_locations = unpaywall_doi . oa_locations return location = self . location_from_arvix_id () self . oa_locations = [ location ] def location_from_arvix_id ( self ): import datetime url_for_pdf = f \"https://arxiv.org/pdf/{self.arxiv_id}.pdf\" location = Unpaywall_Location ( { \"endpoint_id\" : None , \"evidence\" : \"oa repository\" , \"host_type\" : \"repository\" , \"is_best\" : True , \"license\" : self . get_license (), \"pmh_id\" : f \"oai:arXiv.org:{self.arxiv_id_latest}\" , \"repository_institution\" : \"Cornell University - arXiv\" , \"updated\" : datetime . datetime . now () . isoformat (), \"url\" : url_for_pdf , \"url_for_landing_page\" : f \"https://arxiv.org/abs/{self.arxiv_id}\" , \"url_for_pdf\" : url_for_pdf , \"version\" : \"submittedVersion\" , } ) return location def get_license ( self ): \"\"\" Return license using choices from the Unpaywall data format. Looks for license metadata in the CSL Item. \"\"\" license = self . csl_item . note_dict . get ( \"license\" ) if not license : return # Example licenses from https://arxiv.org/help/license # http://creativecommons.org/publicdomain/zero/1.0/ # http://creativecommons.org/licenses/by/4.0/ # http://creativecommons.org/licenses/by-sa/4.0/ # http://creativecommons.org/licenses/by-nc-sa/4.0/ # http://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html from urllib.parse import urlparse parsed_url = urlparse ( license ) if not parsed_url . scheme . startswith ( \"http\" ): return if parsed_url . hostname . endswith ( \"creativecommons.org\" ): try : abbrev = parsed_url . path . split ( \"/\" )[ 2 ] except IndexError : return if abbrev == \"zero\" : return \"cc0\" return f \"cc-{abbrev}\" source_to_unpaywaller = { \"doi\" : Unpaywall_DOI , \"arxiv\" : Unpaywall_arXiv , } class Unpaywall_Location ( dict ): \"\"\" From https://unpaywall.org/data-format > The OA Location object describes particular place where we found a given OA article. The same article is often available from multiple locations, and there may be differences in format, version, and license depending on the location; the OA Location object describes these key attributes. An OA Location Object is always a Child of a DOI Object. Example oa_locations from the Unpaywall API are: ```json { \"endpoint_id\": null, \"evidence\": \"open (via page says license)\", \"host_type\": \"publisher\", \"is_best\": true, \"license\": \"cc-by\", \"pmh_id\": null, \"repository_institution\": null, \"updated\": \"2020-01-19T08:55:45.548214\", \"url\": \"https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1007250&type=printable\", \"url_for_landing_page\": \"https://doi.org/10.1371/journal.pcbi.1007250\", \"url_for_pdf\": \"https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1007250&type=printable\", \"version\": \"publishedVersion\" }, { \"endpoint_id\": \"ca8f8d56758a80a4f86\", \"evidence\": \"oa repository (via OAI-PMH doi match)\", \"host_type\": \"repository\", \"is_best\": true, \"license\": null, \"pmh_id\": \"oai:arXiv.org:1806.05726\", \"repository_institution\": \"Cornell University - arXiv\", \"updated\": \"2019-11-01T00:28:16.784912\", \"url\": \"http://arxiv.org/pdf/1806.05726\", \"url_for_landing_page\": \"http://arxiv.org/abs/1806.05726\", \"url_for_pdf\": \"http://arxiv.org/pdf/1806.05726\", \"version\": \"submittedVersion\" } ``` \"\"\" @property def has_pdf ( self ): return bool ( self . get ( \"url_for_pdf\" )) @property def has_open_license ( self ): license = self . get ( \"license\" ) return license in open_licenses @property def has_creative_commons_license ( self ): license = self . get ( \"license\" ) if not license : return False return license == \"cc0\" or license . startswith ( \"cc-\" ) @property def has_openly_licensed_pdf ( self ): return self . has_pdf and self . has_open_license","title":"Module manubot.cite.unpaywall"},{"location":"reference/manubot/cite/unpaywall/#variables","text":"open_licenses source_to_unpaywaller","title":"Variables"},{"location":"reference/manubot/cite/unpaywall/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/unpaywall/#unpaywall","text":"class Unpaywall ( / , * args , ** kwargs ) A class to handle open access locations in the Unpaywall data format. Create new Unpaywall objects using the from_csl_item and from_citekey methods, or by using init of a subclass like Unpaywall_DOI . View Source class Unpaywall : \"\"\" A class to handle open access locations in the Unpaywall data format. Create new Unpaywall objects using the `from_csl_item` and `from_citekey` methods, or by using __init__ of a subclass like `Unpaywall_DOI`. \"\"\" csl_item = None @ abc . abstractmethod def set_oa_locations ( self ): \"\"\" Set `self.oa_locations`, which is a list of `Unpaywall_Location` objects. \"\"\" self . oa_locations = [] @ property def best_openly_licensed_pdf ( self ) -> \"Unpaywall_Location\" : for location in self . oa_locations : if location . has_openly_licensed_pdf : return location @ property def best_pdf ( self ) -> \"Unpaywall_Location\" : for location in self . oa_locations : if location . has_pdf : return location @ staticmethod def from_citekey ( citekey , csl_item = None ): \"\"\" Create an Unpaywall object for `citekey`. `csl_item` is an optional field that can avoid an external web request to generate to a new CSL Item. \"\"\" from . citekey import CiteKey if isinstance ( citekey , str ): citekey = CiteKey ( citekey ) if not isinstance ( citekey , CiteKey ): raise ValueError ( \"citekey must be a str or CiteKey\" ) if citekey . standard_prefix in source_to_unpaywaller : unpaywaller = source_to_unpaywaller [ citekey . standard_prefix ] unpaywall = unpaywaller ( citekey . standard_accession , set_oa_locations = False ) else : raise ValueError ( f \"Cannot Unpaywall {citekey.input_id}. \" f \"Supported citations sources are {', '.join(source_to_unpaywaller)}. \" \"Received {citekey.standard_prefix!r}.\" ) unpaywall . csl_item = csl_item unpaywall . set_oa_locations () return unpaywall @ classmethod def from_csl_item ( cls , csl_item ): \"\"\" Create an Unpaywall object for `csl_item`. \"\"\" from . csl_item import CSL_Item csl_item = CSL_Item ( csl_item ) doi = csl_item . get ( \"DOI\" ) if doi : return cls . from_citekey ( f \"doi:{doi}\" , csl_item = csl_item ) csl_item . infer_id () return cls . from_citekey ( csl_item [ \"id\" ], csl_item = csl_item )","title":"Unpaywall"},{"location":"reference/manubot/cite/unpaywall/#descendants","text":"manubot.cite.unpaywall.Unpaywall_DOI manubot.cite.unpaywall.Unpaywall_arXiv","title":"Descendants"},{"location":"reference/manubot/cite/unpaywall/#class-variables","text":"csl_item","title":"Class variables"},{"location":"reference/manubot/cite/unpaywall/#static-methods","text":"","title":"Static methods"},{"location":"reference/manubot/cite/unpaywall/#from_citekey","text":"def from_citekey ( citekey , csl_item = None ) Create an Unpaywall object for citekey . csl_item is an optional field that can avoid an external web request to generate to a new CSL Item. View Source @staticmethod def from_citekey ( citekey , csl_item = None ): \"\"\" Create an Unpaywall object for `citekey`. `csl_item` is an optional field that can avoid an external web request to generate to a new CSL Item. \"\"\" from .citekey import CiteKey if isinstance ( citekey , str ): citekey = CiteKey ( citekey ) if not isinstance ( citekey , CiteKey ): raise ValueError ( \"citekey must be a str or CiteKey\" ) if citekey . standard_prefix in source_to_unpaywaller : unpaywaller = source_to_unpaywaller [ citekey . standard_prefix ] unpaywall = unpaywaller ( citekey . standard_accession , set_oa_locations = False ) else : raise ValueError ( f \"Cannot Unpaywall {citekey.input_id}. \" f \"Supported citations sources are {', '.join(source_to_unpaywaller)}. \" \"Received {citekey.standard_prefix!r}.\" ) unpaywall . csl_item = csl_item unpaywall . set_oa_locations () return unpaywall","title":"from_citekey"},{"location":"reference/manubot/cite/unpaywall/#from_csl_item","text":"def from_csl_item ( csl_item ) Create an Unpaywall object for csl_item . View Source @classmethod def from_csl_item ( cls , csl_item ): \"\"\" Create an Unpaywall object for `csl_item`. \"\"\" from .csl_item import CSL_Item csl_item = CSL_Item ( csl_item ) doi = csl_item . get ( \"DOI\" ) if doi : return cls . from_citekey ( f \"doi:{doi}\" , csl_item = csl_item ) csl_item . infer_id () return cls . from_citekey ( csl_item [ \"id\" ], csl_item = csl_item )","title":"from_csl_item"},{"location":"reference/manubot/cite/unpaywall/#instance-variables","text":"best_openly_licensed_pdf best_pdf","title":"Instance variables"},{"location":"reference/manubot/cite/unpaywall/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/unpaywall/#set_oa_locations","text":"def set_oa_locations ( self ) Set self.oa_locations , which is a list of Unpaywall_Location objects. View Source @ abc . abstractmethod def set_oa_locations ( self ): \"\"\" Set `self.oa_locations`, which is a list of `Unpaywall_Location` objects. \"\"\" self . oa_locations = []","title":"set_oa_locations"},{"location":"reference/manubot/cite/unpaywall/#unpaywall_doi","text":"class Unpaywall_DOI ( doi , set_oa_locations = True ) From https://unpaywall.org/data-format: The DOI object is more or less a row in our main database... it's everything we know about a given DOI-assigned resource, including metadata about the resource itself, and information about its OA status. It includes a list of zero or more OA Location Objects, as well as a best_oa_location property that's probably the OA Location you'll want to use. View Source class Unpaywall_DOI ( Unpaywall ): \"\"\" From https://unpaywall.org/data-format: > The DOI object is more or less a row in our main database... it's everything we know about a given DOI-assigned resource, including metadata about the resource itself, and information about its OA status. It includes a list of zero or more OA Location Objects, as well as a `best_oa_location` property that's probably the OA Location you'll want to use. \"\"\" def __init__ ( self , doi , set_oa_locations = True ): self . doi = doi . lower () if set_oa_locations : self . set_oa_locations () def set_oa_locations ( self ): from manubot.util import contact_email url = f \"https://api.unpaywall.org/v2/{self.doi}\" params = { \"email\" : contact_email } response = requests . get ( url , params = params ) self . results = response . json () self . oa_locations = [ Unpaywall_Location ( location ) for location in self . results . get ( \"oa_locations\" , []) ]","title":"Unpaywall_DOI"},{"location":"reference/manubot/cite/unpaywall/#ancestors-in-mro","text":"manubot.cite.unpaywall.Unpaywall","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/unpaywall/#class-variables_1","text":"csl_item","title":"Class variables"},{"location":"reference/manubot/cite/unpaywall/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/manubot/cite/unpaywall/#from_citekey_1","text":"def from_citekey ( citekey , csl_item = None ) Create an Unpaywall object for citekey . csl_item is an optional field that can avoid an external web request to generate to a new CSL Item. View Source @staticmethod def from_citekey ( citekey , csl_item = None ): \"\"\" Create an Unpaywall object for `citekey`. `csl_item` is an optional field that can avoid an external web request to generate to a new CSL Item. \"\"\" from .citekey import CiteKey if isinstance ( citekey , str ): citekey = CiteKey ( citekey ) if not isinstance ( citekey , CiteKey ): raise ValueError ( \"citekey must be a str or CiteKey\" ) if citekey . standard_prefix in source_to_unpaywaller : unpaywaller = source_to_unpaywaller [ citekey . standard_prefix ] unpaywall = unpaywaller ( citekey . standard_accession , set_oa_locations = False ) else : raise ValueError ( f \"Cannot Unpaywall {citekey.input_id}. \" f \"Supported citations sources are {', '.join(source_to_unpaywaller)}. \" \"Received {citekey.standard_prefix!r}.\" ) unpaywall . csl_item = csl_item unpaywall . set_oa_locations () return unpaywall","title":"from_citekey"},{"location":"reference/manubot/cite/unpaywall/#from_csl_item_1","text":"def from_csl_item ( csl_item ) Create an Unpaywall object for csl_item . View Source @classmethod def from_csl_item ( cls , csl_item ): \"\"\" Create an Unpaywall object for `csl_item`. \"\"\" from .csl_item import CSL_Item csl_item = CSL_Item ( csl_item ) doi = csl_item . get ( \"DOI\" ) if doi : return cls . from_citekey ( f \"doi:{doi}\" , csl_item = csl_item ) csl_item . infer_id () return cls . from_citekey ( csl_item [ \"id\" ], csl_item = csl_item )","title":"from_csl_item"},{"location":"reference/manubot/cite/unpaywall/#instance-variables_1","text":"best_openly_licensed_pdf best_pdf","title":"Instance variables"},{"location":"reference/manubot/cite/unpaywall/#methods_1","text":"","title":"Methods"},{"location":"reference/manubot/cite/unpaywall/#set_oa_locations_1","text":"def set_oa_locations ( self ) Set self.oa_locations , which is a list of Unpaywall_Location objects. View Source def set_oa_locations ( self ): from manubot.util import contact_email url = f \"https://api.unpaywall.org/v2/{self.doi}\" params = { \"email\" : contact_email } response = requests . get ( url , params = params ) self . results = response . json () self . oa_locations = [ Unpaywall_Location ( location ) for location in self . results . get ( \"oa_locations\" , []) ]","title":"set_oa_locations"},{"location":"reference/manubot/cite/unpaywall/#unpaywall_location","text":"class Unpaywall_Location ( / , * args , ** kwargs ) From https://unpaywall.org/data-format The OA Location object describes particular place where we found a given OA article. The same article is often available from multiple locations, and there may be differences in format, version, and license depending on the location; the OA Location object describes these key attributes. An OA Location Object is always a Child of a DOI Object. Example oa_locations from the Unpaywall API are: { \"endpoint_id\" : null , \"evidence\" : \"open (via page says license)\" , \"host_type\" : \"publisher\" , \"is_best\" : true , \"license\" : \"cc-by\" , \"pmh_id\" : null , \"repository_institution\" : null , \"updated\" : \"2020-01-19T08:55:45.548214\" , \"url\" : \"https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1007250&type=printable\" , \"url_for_landing_page\" : \"https://doi.org/10.1371/journal.pcbi.1007250\" , \"url_for_pdf\" : \"https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1007250&type=printable\" , \"version\" : \"publishedVersion\" } , { \"endpoint_id\" : \"ca8f8d56758a80a4f86\" , \"evidence\" : \"oa repository (via OAI-PMH doi match)\" , \"host_type\" : \"repository\" , \"is_best\" : true , \"license\" : null , \"pmh_id\" : \"oai:arXiv.org:1806.05726\" , \"repository_institution\" : \"Cornell University - arXiv\" , \"updated\" : \"2019-11-01T00:28:16.784912\" , \"url\" : \"http://arxiv.org/pdf/1806.05726\" , \"url_for_landing_page\" : \"http://arxiv.org/abs/1806.05726\" , \"url_for_pdf\" : \"http://arxiv.org/pdf/1806.05726\" , \"version\" : \"submittedVersion\" } View Source class Unpaywall_Location ( dict ) : \"\"\" From https://unpaywall.org/data-format > The OA Location object describes particular place where we found a given OA article. The same article is often available from multiple locations, and there may be differences in format, version, and license depending on the location; the OA Location object describes these key attributes. An OA Location Object is always a Child of a DOI Object. Example oa_locations from the Unpaywall API are: ```json { \" endpoint_id \": null, \" evidence \": \" open ( via page says license ) \", \" host_type \": \" publisher \", \" is_best \": true, \" license \": \" cc - by \", \" pmh_id \": null, \" repository_institution \": null, \" updated \": \" 2020 - 01 - 19 T08 : 55 : 45.548214 \", \" url \": \" https : // journals . plos . org / ploscompbiol / article / file ? id = 10.1371 / journal . pcbi .1007250 & type = printable \", \" url_for_landing_page \": \" https : // doi . org / 10.1371 / journal . pcbi .1007250 \", \" url_for_pdf \": \" https : // journals . plos . org / ploscompbiol / article / file ? id = 10.1371 / journal . pcbi .1007250 & type = printable \", \" version \": \" publishedVersion \" }, { \" endpoint_id \": \" ca8f8d56758a80a4f86 \", \" evidence \": \" oa repository ( via OAI - PMH doi match ) \", \" host_type \": \" repository \", \" is_best \": true, \" license \": null, \" pmh_id \": \" oai : arXiv . org : 1806.05726 \", \" repository_institution \": \" Cornell University - arXiv \", \" updated \": \" 2019 - 11 - 01 T00 : 28 : 16.784912 \", \" url \": \" http : // arxiv . org / pdf / 1806.05726 \", \" url_for_landing_page \": \" http : // arxiv . org / abs / 1806.05726 \", \" url_for_pdf \": \" http : // arxiv . org / pdf / 1806.05726 \", \" version \": \" submittedVersion \" } ``` \"\"\" @property def has_pdf ( self ) : return bool ( self . get ( \"url_for_pdf\" )) @property def has_open_license ( self ) : license = self . get ( \"license\" ) return license in open_licenses @property def has_creative_commons_license ( self ) : license = self . get ( \"license\" ) if not license : return False return license == \"cc0\" or license . startswith ( \"cc-\" ) @property def has_openly_licensed_pdf ( self ) : return self . has_pdf and self . has_open_license","title":"Unpaywall_Location"},{"location":"reference/manubot/cite/unpaywall/#ancestors-in-mro_1","text":"builtins.dict","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/unpaywall/#instance-variables_2","text":"has_creative_commons_license has_open_license has_openly_licensed_pdf has_pdf","title":"Instance variables"},{"location":"reference/manubot/cite/unpaywall/#methods_2","text":"","title":"Methods"},{"location":"reference/manubot/cite/unpaywall/#clear","text":"def clear ( ... ) D.clear() -> None. Remove all items from D.","title":"clear"},{"location":"reference/manubot/cite/unpaywall/#copy","text":"def copy ( ... ) D.copy() -> a shallow copy of D","title":"copy"},{"location":"reference/manubot/cite/unpaywall/#fromkeys","text":"def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value.","title":"fromkeys"},{"location":"reference/manubot/cite/unpaywall/#get","text":"def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default.","title":"get"},{"location":"reference/manubot/cite/unpaywall/#items","text":"def items ( ... ) D.items() -> a set-like object providing a view on D's items","title":"items"},{"location":"reference/manubot/cite/unpaywall/#keys","text":"def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys","title":"keys"},{"location":"reference/manubot/cite/unpaywall/#pop","text":"def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised","title":"pop"},{"location":"reference/manubot/cite/unpaywall/#popitem","text":"def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty.","title":"popitem"},{"location":"reference/manubot/cite/unpaywall/#setdefault","text":"def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default.","title":"setdefault"},{"location":"reference/manubot/cite/unpaywall/#update","text":"def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k]","title":"update"},{"location":"reference/manubot/cite/unpaywall/#values","text":"def values ( ... ) D.values() -> an object providing a view on D's values","title":"values"},{"location":"reference/manubot/cite/unpaywall/#unpaywall_arxiv","text":"class Unpaywall_arXiv ( arxiv_id , set_oa_locations = True , use_doi = True ) A class to handle open access locations in the Unpaywall data format. Create new Unpaywall objects using the from_csl_item and from_citekey methods, or by using init of a subclass like Unpaywall_DOI . View Source class Unpaywall_arXiv ( Unpaywall ): def __init__ ( self , arxiv_id , set_oa_locations = True , use_doi = True ): from .arxiv import split_arxiv_id_version self . arxiv_id = arxiv_id self . arxiv_id_latest , self . arxiv_id_version = split_arxiv_id_version ( arxiv_id ) self . use_doi = use_doi if set_oa_locations : self . set_oa_locations () def set_oa_locations ( self ): from .arxiv import get_arxiv_csl_item if not self . csl_item : self . csl_item = get_arxiv_csl_item ( self . arxiv_id ) doi = self . csl_item . get ( \"DOI\" ) if self . use_doi and doi : unpaywall_doi = Unpaywall_DOI ( doi ) self . doi = unpaywall_doi . doi self . oa_locations = unpaywall_doi . oa_locations return location = self . location_from_arvix_id () self . oa_locations = [ location ] def location_from_arvix_id ( self ): import datetime url_for_pdf = f \"https://arxiv.org/pdf/{self.arxiv_id}.pdf\" location = Unpaywall_Location ( { \"endpoint_id\" : None , \"evidence\" : \"oa repository\" , \"host_type\" : \"repository\" , \"is_best\" : True , \"license\" : self . get_license (), \"pmh_id\" : f \"oai:arXiv.org:{self.arxiv_id_latest}\" , \"repository_institution\" : \"Cornell University - arXiv\" , \"updated\" : datetime . datetime . now () . isoformat (), \"url\" : url_for_pdf , \"url_for_landing_page\" : f \"https://arxiv.org/abs/{self.arxiv_id}\" , \"url_for_pdf\" : url_for_pdf , \"version\" : \"submittedVersion\" , } ) return location def get_license ( self ): \"\"\" Return license using choices from the Unpaywall data format. Looks for license metadata in the CSL Item. \"\"\" license = self . csl_item . note_dict . get ( \"license\" ) if not license : return # Example licenses from https://arxiv.org/help/license # http://creativecommons.org/publicdomain/zero/1.0/ # http://creativecommons.org/licenses/by/4.0/ # http://creativecommons.org/licenses/by-sa/4.0/ # http://creativecommons.org/licenses/by-nc-sa/4.0/ # http://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html from urllib.parse import urlparse parsed_url = urlparse ( license ) if not parsed_url . scheme . startswith ( \"http\" ): return if parsed_url . hostname . endswith ( \"creativecommons.org\" ): try : abbrev = parsed_url . path . split ( \"/\" )[ 2 ] except IndexError : return if abbrev == \"zero\" : return \"cc0\" return f \"cc-{abbrev}\"","title":"Unpaywall_arXiv"},{"location":"reference/manubot/cite/unpaywall/#ancestors-in-mro_2","text":"manubot.cite.unpaywall.Unpaywall","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/unpaywall/#class-variables_2","text":"csl_item","title":"Class variables"},{"location":"reference/manubot/cite/unpaywall/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/manubot/cite/unpaywall/#from_citekey_2","text":"def from_citekey ( citekey , csl_item = None ) Create an Unpaywall object for citekey . csl_item is an optional field that can avoid an external web request to generate to a new CSL Item. View Source @staticmethod def from_citekey ( citekey , csl_item = None ): \"\"\" Create an Unpaywall object for `citekey`. `csl_item` is an optional field that can avoid an external web request to generate to a new CSL Item. \"\"\" from .citekey import CiteKey if isinstance ( citekey , str ): citekey = CiteKey ( citekey ) if not isinstance ( citekey , CiteKey ): raise ValueError ( \"citekey must be a str or CiteKey\" ) if citekey . standard_prefix in source_to_unpaywaller : unpaywaller = source_to_unpaywaller [ citekey . standard_prefix ] unpaywall = unpaywaller ( citekey . standard_accession , set_oa_locations = False ) else : raise ValueError ( f \"Cannot Unpaywall {citekey.input_id}. \" f \"Supported citations sources are {', '.join(source_to_unpaywaller)}. \" \"Received {citekey.standard_prefix!r}.\" ) unpaywall . csl_item = csl_item unpaywall . set_oa_locations () return unpaywall","title":"from_citekey"},{"location":"reference/manubot/cite/unpaywall/#from_csl_item_2","text":"def from_csl_item ( csl_item ) Create an Unpaywall object for csl_item . View Source @classmethod def from_csl_item ( cls , csl_item ): \"\"\" Create an Unpaywall object for `csl_item`. \"\"\" from .csl_item import CSL_Item csl_item = CSL_Item ( csl_item ) doi = csl_item . get ( \"DOI\" ) if doi : return cls . from_citekey ( f \"doi:{doi}\" , csl_item = csl_item ) csl_item . infer_id () return cls . from_citekey ( csl_item [ \"id\" ], csl_item = csl_item )","title":"from_csl_item"},{"location":"reference/manubot/cite/unpaywall/#instance-variables_3","text":"best_openly_licensed_pdf best_pdf","title":"Instance variables"},{"location":"reference/manubot/cite/unpaywall/#methods_3","text":"","title":"Methods"},{"location":"reference/manubot/cite/unpaywall/#get_license","text":"def get_license ( self ) Return license using choices from the Unpaywall data format. Looks for license metadata in the CSL Item. View Source def get_license ( self ): \"\"\" Return license using choices from the Unpaywall data format. Looks for license metadata in the CSL Item. \"\"\" license = self . csl_item . note_dict . get ( \"license\" ) if not license : return # Example licenses from https://arxiv.org/help/license # http://creativecommons.org/publicdomain/zero/1.0/ # http://creativecommons.org/licenses/by/4.0/ # http://creativecommons.org/licenses/by-sa/4.0/ # http://creativecommons.org/licenses/by-nc-sa/4.0/ # http://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html from urllib.parse import urlparse parsed_url = urlparse ( license ) if not parsed_url . scheme . startswith ( \"http\" ): return if parsed_url . hostname . endswith ( \"creativecommons.org\" ): try : abbrev = parsed_url . path . split ( \"/\" )[ 2 ] except IndexError : return if abbrev == \"zero\" : return \"cc0\" return f \"cc-{abbrev}\"","title":"get_license"},{"location":"reference/manubot/cite/unpaywall/#location_from_arvix_id","text":"def location_from_arvix_id ( self ) View Source def location_from_arvix_id ( self ): import datetime url_for_pdf = f \"https://arxiv.org/pdf/{self.arxiv_id}.pdf\" location = Unpaywall_Location ( { \"endpoint_id\" : None , \"evidence\" : \"oa repository\" , \"host_type\" : \"repository\" , \"is_best\" : True , \"license\" : self . get_license (), \"pmh_id\" : f \"oai:arXiv.org:{self.arxiv_id_latest}\" , \"repository_institution\" : \"Cornell University - arXiv\" , \"updated\" : datetime . datetime . now () . isoformat (), \"url\" : url_for_pdf , \"url_for_landing_page\" : f \"https://arxiv.org/abs/{self.arxiv_id}\" , \"url_for_pdf\" : url_for_pdf , \"version\" : \"submittedVersion\" , } ) return location","title":"location_from_arvix_id"},{"location":"reference/manubot/cite/unpaywall/#set_oa_locations_2","text":"def set_oa_locations ( self ) Set self.oa_locations , which is a list of Unpaywall_Location objects. View Source def set_oa_locations ( self ): from .arxiv import get_arxiv_csl_item if not self . csl_item : self . csl_item = get_arxiv_csl_item ( self . arxiv_id ) doi = self . csl_item . get ( \"DOI\" ) if self . use_doi and doi : unpaywall_doi = Unpaywall_DOI ( doi ) self . doi = unpaywall_doi . doi self . oa_locations = unpaywall_doi . oa_locations return location = self . location_from_arvix_id () self . oa_locations = [ location ]","title":"set_oa_locations"},{"location":"reference/manubot/cite/url/","text":"Module manubot.cite.url View Source import json import logging import re from .handlers import Handler class Handler_URL ( Handler ): standard_prefix = \"url\" prefixes = [ \"url\" , \"http\" , \"https\" , ] def standardize_prefix_accession ( self , accession ): if self . prefix_lower != \"url\" : accession = f \"{self.prefix_lower}:{accession}\" return self . standard_prefix , accession def get_csl_item ( self , citekey ): return get_url_csl_item ( citekey . standard_accession ) def get_url_csl_item ( url ): \"\"\" Get csl_item for a URL trying a sequence of strategies. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `url_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\"\" for retriever in url_retrievers : try : return retriever ( url ) except Exception as error : logging . warning ( f \"Error in {retriever.__name__} for {url} \" f \"due to a {error.__class__.__name__}: \\n {error}\" ) logging . info ( error , exc_info = True ) raise Exception ( f \"all get_url_csl_item methods failed for {url}\" ) def get_url_csl_item_zotero ( url ): \"\"\" Use Zotero's translation-server to generate a CSL Item for the specified URL. \"\"\" from manubot.cite.zotero import export_as_csl , web_query zotero_data = web_query ( url ) csl_data = export_as_csl ( zotero_data ) ( csl_item ,) = csl_data return csl_item def get_url_csl_item_greycite ( url ): \"\"\" Uses Greycite which has experiened uptime problems in the past. API calls seem to take at least 15 seconds. Browser requests are much faster. Setting header did not have an effect. Consider mimicking browser using selenium. More information on Greycite at: http://greycite.knowledgeblog.org/ http://knowledgeblog.org/greycite https://arxiv.org/abs/1304.7151 https://git.io/v9N2C \"\"\" import requests from manubot.util import get_manubot_user_agent headers = { \"Connection\" : \"close\" , # https://github.com/kennethreitz/requests/issues/4023 \"User-Agent\" : get_manubot_user_agent (), } response = requests . get ( \"http://greycite.knowledgeblog.org/json\" , params = { \"uri\" : url }, headers = headers ) # Some Greycite responses were valid JSON besides for an error appended # like \"<p>*** Date set from uri<p>\" or \"<p>*** fetch error : 404<p>\". pattern = re . compile ( r \"<p>\\*\\*\\*.*<p>\" ) text = pattern . sub ( \"\" , response . text ) csl_item = json . loads ( text ) csl_item [ \"type\" ] = \"webpage\" return csl_item def get_url_csl_item_manual ( url ): \"\"\" Manually create csl_item for a URL. \"\"\" return { \"URL\" : url , \"type\" : \"webpage\" } url_retrievers = [ get_url_csl_item_zotero , get_url_csl_item_greycite , get_url_csl_item_manual , ] Variables url_retrievers Functions get_url_csl_item def get_url_csl_item ( url ) Get csl_item for a URL trying a sequence of strategies. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable url_retrievers . The methods are attempted in order, with this function returning the metadata from the first non-failing method. View Source def get_url_csl_item ( url ): \"\"\" Get csl_item for a URL trying a sequence of strategies. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `url_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\"\" for retriever in url_retrievers : try : return retriever ( url ) except Exception as error : logging . warning ( f \"Error in {retriever.__name__} for {url} \" f \"due to a {error.__class__.__name__}:\\n{error}\" ) logging . info ( error , exc_info = True ) raise Exception ( f \"all get_url_csl_item methods failed for {url}\" ) get_url_csl_item_greycite def get_url_csl_item_greycite ( url ) Uses Greycite which has experiened uptime problems in the past. API calls seem to take at least 15 seconds. Browser requests are much faster. Setting header did not have an effect. Consider mimicking browser using selenium. More information on Greycite at: http://greycite.knowledgeblog.org/ http://knowledgeblog.org/greycite https://arxiv.org/abs/1304.7151 https://git.io/v9N2C View Source def get_url_csl_item_greycite ( url ): \"\"\" Uses Greycite which has experiened uptime problems in the past. API calls seem to take at least 15 seconds. Browser requests are much faster. Setting header did not have an effect. Consider mimicking browser using selenium. More information on Greycite at: http://greycite.knowledgeblog.org/ http://knowledgeblog.org/greycite https://arxiv.org/abs/1304.7151 https://git.io/v9N2C \"\"\" import requests from manubot.util import get_manubot_user_agent headers = { \"Connection\" : \"close\" , # https://github.com/kennethreitz/requests/issues/4023 \"User-Agent\" : get_manubot_user_agent (), } response = requests . get ( \"http://greycite.knowledgeblog.org/json\" , params = { \"uri\" : url }, headers = headers ) # Some Greycite responses were valid JSON besides for an error appended # like \"<p>*** Date set from uri<p>\" or \"<p>*** fetch error : 404<p>\". pattern = re . compile ( r \"<p>\\*\\*\\*.*<p>\" ) text = pattern . sub ( \"\" , response . text ) csl_item = json . loads ( text ) csl_item [ \"type\" ] = \"webpage\" return csl_item get_url_csl_item_manual def get_url_csl_item_manual ( url ) Manually create csl_item for a URL. View Source def get_url_csl_item_manual ( url ): \"\"\" Manually create csl_item for a URL. \"\"\" return { \"URL\" : url , \"type\" : \"webpage\" } get_url_csl_item_zotero def get_url_csl_item_zotero ( url ) Use Zotero's translation-server to generate a CSL Item for the specified URL. View Source def get_url_csl_item_zotero ( url ): \"\"\" Use Zotero's translation-server to generate a CSL Item for the specified URL. \"\"\" from manubot.cite.zotero import export_as_csl , web_query zotero_data = web_query ( url ) csl_data = export_as_csl ( zotero_data ) ( csl_item ,) = csl_data return csl_item Classes Handler_URL class Handler_URL ( prefix_lower : str ) A Handler is a class that provides support for a certain type of citekey. For example, a Handler subclass could provide support for DOI citekeys. Subclasses enable custom logic for different citekey prefixes, including how to standardize the citekey and how to retrieve CSL Item metadata. View Source class Handler_URL ( Handler ): standard_prefix = \"url\" prefixes = [ \"url\" , \"http\" , \"https\" , ] def standardize_prefix_accession ( self , accession ): if self . prefix_lower != \"url\" : accession = f \"{self.prefix_lower}:{accession}\" return self . standard_prefix , accession def get_csl_item ( self , citekey ): return get_url_csl_item ( citekey . standard_accession ) Ancestors (in MRO) manubot.cite.handlers.Handler Class variables prefixes standard_prefix Methods get_csl_item def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ): return get_url_csl_item ( citekey . standard_accession ) inspect def inspect ( self , citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey ): \"\"\" Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. \"\"\" pattern = self . _get_pattern ( \"accession_pattern\" ) if not pattern : return None if not pattern . fullmatch ( citekey . accession ): return f \"{citekey.accession} does not match regex {pattern.pattern}\" standardize_prefix_accession def standardize_prefix_accession ( self , accession ) Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession ): if self . prefix_lower != \"url\" : accession = f \"{self.prefix_lower}:{accession}\" return self . standard_prefix , accession","title":"Url"},{"location":"reference/manubot/cite/url/#module-manubotciteurl","text":"View Source import json import logging import re from .handlers import Handler class Handler_URL ( Handler ): standard_prefix = \"url\" prefixes = [ \"url\" , \"http\" , \"https\" , ] def standardize_prefix_accession ( self , accession ): if self . prefix_lower != \"url\" : accession = f \"{self.prefix_lower}:{accession}\" return self . standard_prefix , accession def get_csl_item ( self , citekey ): return get_url_csl_item ( citekey . standard_accession ) def get_url_csl_item ( url ): \"\"\" Get csl_item for a URL trying a sequence of strategies. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `url_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\"\" for retriever in url_retrievers : try : return retriever ( url ) except Exception as error : logging . warning ( f \"Error in {retriever.__name__} for {url} \" f \"due to a {error.__class__.__name__}: \\n {error}\" ) logging . info ( error , exc_info = True ) raise Exception ( f \"all get_url_csl_item methods failed for {url}\" ) def get_url_csl_item_zotero ( url ): \"\"\" Use Zotero's translation-server to generate a CSL Item for the specified URL. \"\"\" from manubot.cite.zotero import export_as_csl , web_query zotero_data = web_query ( url ) csl_data = export_as_csl ( zotero_data ) ( csl_item ,) = csl_data return csl_item def get_url_csl_item_greycite ( url ): \"\"\" Uses Greycite which has experiened uptime problems in the past. API calls seem to take at least 15 seconds. Browser requests are much faster. Setting header did not have an effect. Consider mimicking browser using selenium. More information on Greycite at: http://greycite.knowledgeblog.org/ http://knowledgeblog.org/greycite https://arxiv.org/abs/1304.7151 https://git.io/v9N2C \"\"\" import requests from manubot.util import get_manubot_user_agent headers = { \"Connection\" : \"close\" , # https://github.com/kennethreitz/requests/issues/4023 \"User-Agent\" : get_manubot_user_agent (), } response = requests . get ( \"http://greycite.knowledgeblog.org/json\" , params = { \"uri\" : url }, headers = headers ) # Some Greycite responses were valid JSON besides for an error appended # like \"<p>*** Date set from uri<p>\" or \"<p>*** fetch error : 404<p>\". pattern = re . compile ( r \"<p>\\*\\*\\*.*<p>\" ) text = pattern . sub ( \"\" , response . text ) csl_item = json . loads ( text ) csl_item [ \"type\" ] = \"webpage\" return csl_item def get_url_csl_item_manual ( url ): \"\"\" Manually create csl_item for a URL. \"\"\" return { \"URL\" : url , \"type\" : \"webpage\" } url_retrievers = [ get_url_csl_item_zotero , get_url_csl_item_greycite , get_url_csl_item_manual , ]","title":"Module manubot.cite.url"},{"location":"reference/manubot/cite/url/#variables","text":"url_retrievers","title":"Variables"},{"location":"reference/manubot/cite/url/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/url/#get_url_csl_item","text":"def get_url_csl_item ( url ) Get csl_item for a URL trying a sequence of strategies. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable url_retrievers . The methods are attempted in order, with this function returning the metadata from the first non-failing method. View Source def get_url_csl_item ( url ): \"\"\" Get csl_item for a URL trying a sequence of strategies. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `url_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\"\" for retriever in url_retrievers : try : return retriever ( url ) except Exception as error : logging . warning ( f \"Error in {retriever.__name__} for {url} \" f \"due to a {error.__class__.__name__}:\\n{error}\" ) logging . info ( error , exc_info = True ) raise Exception ( f \"all get_url_csl_item methods failed for {url}\" )","title":"get_url_csl_item"},{"location":"reference/manubot/cite/url/#get_url_csl_item_greycite","text":"def get_url_csl_item_greycite ( url ) Uses Greycite which has experiened uptime problems in the past. API calls seem to take at least 15 seconds. Browser requests are much faster. Setting header did not have an effect. Consider mimicking browser using selenium. More information on Greycite at: http://greycite.knowledgeblog.org/ http://knowledgeblog.org/greycite https://arxiv.org/abs/1304.7151 https://git.io/v9N2C View Source def get_url_csl_item_greycite ( url ): \"\"\" Uses Greycite which has experiened uptime problems in the past. API calls seem to take at least 15 seconds. Browser requests are much faster. Setting header did not have an effect. Consider mimicking browser using selenium. More information on Greycite at: http://greycite.knowledgeblog.org/ http://knowledgeblog.org/greycite https://arxiv.org/abs/1304.7151 https://git.io/v9N2C \"\"\" import requests from manubot.util import get_manubot_user_agent headers = { \"Connection\" : \"close\" , # https://github.com/kennethreitz/requests/issues/4023 \"User-Agent\" : get_manubot_user_agent (), } response = requests . get ( \"http://greycite.knowledgeblog.org/json\" , params = { \"uri\" : url }, headers = headers ) # Some Greycite responses were valid JSON besides for an error appended # like \"<p>*** Date set from uri<p>\" or \"<p>*** fetch error : 404<p>\". pattern = re . compile ( r \"<p>\\*\\*\\*.*<p>\" ) text = pattern . sub ( \"\" , response . text ) csl_item = json . loads ( text ) csl_item [ \"type\" ] = \"webpage\" return csl_item","title":"get_url_csl_item_greycite"},{"location":"reference/manubot/cite/url/#get_url_csl_item_manual","text":"def get_url_csl_item_manual ( url ) Manually create csl_item for a URL. View Source def get_url_csl_item_manual ( url ): \"\"\" Manually create csl_item for a URL. \"\"\" return { \"URL\" : url , \"type\" : \"webpage\" }","title":"get_url_csl_item_manual"},{"location":"reference/manubot/cite/url/#get_url_csl_item_zotero","text":"def get_url_csl_item_zotero ( url ) Use Zotero's translation-server to generate a CSL Item for the specified URL. View Source def get_url_csl_item_zotero ( url ): \"\"\" Use Zotero's translation-server to generate a CSL Item for the specified URL. \"\"\" from manubot.cite.zotero import export_as_csl , web_query zotero_data = web_query ( url ) csl_data = export_as_csl ( zotero_data ) ( csl_item ,) = csl_data return csl_item","title":"get_url_csl_item_zotero"},{"location":"reference/manubot/cite/url/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/url/#handler_url","text":"class Handler_URL ( prefix_lower : str ) A Handler is a class that provides support for a certain type of citekey. For example, a Handler subclass could provide support for DOI citekeys. Subclasses enable custom logic for different citekey prefixes, including how to standardize the citekey and how to retrieve CSL Item metadata. View Source class Handler_URL ( Handler ): standard_prefix = \"url\" prefixes = [ \"url\" , \"http\" , \"https\" , ] def standardize_prefix_accession ( self , accession ): if self . prefix_lower != \"url\" : accession = f \"{self.prefix_lower}:{accession}\" return self . standard_prefix , accession def get_csl_item ( self , citekey ): return get_url_csl_item ( citekey . standard_accession )","title":"Handler_URL"},{"location":"reference/manubot/cite/url/#ancestors-in-mro","text":"manubot.cite.handlers.Handler","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/url/#class-variables","text":"prefixes standard_prefix","title":"Class variables"},{"location":"reference/manubot/cite/url/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/url/#get_csl_item","text":"def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ): return get_url_csl_item ( citekey . standard_accession )","title":"get_csl_item"},{"location":"reference/manubot/cite/url/#inspect","text":"def inspect ( self , citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect ( self , citekey ): \"\"\" Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. \"\"\" pattern = self . _get_pattern ( \"accession_pattern\" ) if not pattern : return None if not pattern . fullmatch ( citekey . accession ): return f \"{citekey.accession} does not match regex {pattern.pattern}\"","title":"inspect"},{"location":"reference/manubot/cite/url/#standardize_prefix_accession","text":"def standardize_prefix_accession ( self , accession ) Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession ): if self . prefix_lower != \"url\" : accession = f \"{self.prefix_lower}:{accession}\" return self . standard_prefix , accession","title":"standardize_prefix_accession"},{"location":"reference/manubot/cite/wikidata/","text":"Module manubot.cite.wikidata View Source from .handlers import Handler class Handler_Wikidata ( Handler ): prefixes = [ \"wikidata\" ] standard_prefix = \"wikidata\" accession_pattern = r \"Q[0-9]+\" def inspect ( self , citekey ): \"\"\" https://www.wikidata.org/wiki/Wikidata:Identifiers \"\"\" accession = citekey . accession if not accession . startswith ( \"Q\" ): return \"Wikidata item IDs must start with 'Q'.\" elif not self . _get_pattern () . fullmatch ( accession ): return ( \"Accession does not conform to the Wikidata regex. \" \"Double check the entity ID.\" ) def get_csl_item ( self , citekey ): return get_wikidata_csl_item ( citekey . standard_accession ) def get_wikidata_csl_item ( identifier ): \"\"\" Get a CSL JSON item with the citation metadata for a Wikidata item. identifier should be a Wikidata item ID corresponding to a citeable work, such as Q50051684. \"\"\" url = f \"https://www.wikidata.org/wiki/{identifier}\" from manubot.cite.url import get_url_csl_item_zotero csl_item = get_url_csl_item_zotero ( url ) if \"DOI\" in csl_item : csl_item [ \"DOI\" ] = csl_item [ \"DOI\" ] . lower () if \"URL\" not in csl_item : csl_item [ \"URL\" ] = url return csl_item Functions get_wikidata_csl_item def get_wikidata_csl_item ( identifier ) Get a CSL JSON item with the citation metadata for a Wikidata item. identifier should be a Wikidata item ID corresponding to a citeable work, such as Q50051684. View Source def get_wikidata_csl_item ( identifier ): \"\"\" Get a CSL JSON item with the citation metadata for a Wikidata item. identifier should be a Wikidata item ID corresponding to a citeable work, such as Q50051684. \"\"\" url = f \"https://www.wikidata.org/wiki/{identifier}\" from manubot.cite.url import get_url_csl_item_zotero csl_item = get_url_csl_item_zotero ( url ) if \"DOI\" in csl_item : csl_item [ \"DOI\" ] = csl_item [ \"DOI\" ] . lower () if \"URL\" not in csl_item : csl_item [ \"URL\" ] = url return csl_item Classes Handler_Wikidata class Handler_Wikidata ( prefix_lower : str ) A Handler is a class that provides support for a certain type of citekey. For example, a Handler subclass could provide support for DOI citekeys. Subclasses enable custom logic for different citekey prefixes, including how to standardize the citekey and how to retrieve CSL Item metadata. View Source class Handler_Wikidata ( Handler ): prefixes = [ \"wikidata\" ] standard_prefix = \"wikidata\" accession_pattern = r \"Q[0-9]+\" def inspect ( self , citekey ): \"\"\" https://www.wikidata.org/wiki/Wikidata:Identifiers \"\"\" accession = citekey . accession if not accession . startswith ( \"Q\" ): return \"Wikidata item IDs must start with 'Q'.\" elif not self . _get_pattern (). fullmatch ( accession ): return ( \"Accession does not conform to the Wikidata regex. \" \"Double check the entity ID.\" ) def get_csl_item ( self , citekey ): return get_wikidata_csl_item ( citekey . standard_accession ) Ancestors (in MRO) manubot.cite.handlers.Handler Class variables accession_pattern prefixes standard_prefix Methods get_csl_item def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ): return get_wikidata_csl_item ( citekey . standard_accession ) inspect def inspect ( self , citekey ) https://www.wikidata.org/wiki/Wikidata:Identifiers View Source def inspect ( self , citekey ): \"\"\" https://www.wikidata.org/wiki/Wikidata:Identifiers \"\"\" accession = citekey . accession if not accession . startswith ( \"Q\" ): return \"Wikidata item IDs must start with 'Q'.\" elif not self . _get_pattern (). fullmatch ( accession ): return ( \"Accession does not conform to the Wikidata regex. \" \"Double check the entity ID.\" ) standardize_prefix_accession def standardize_prefix_accession ( self , accession ) Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession ): \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession","title":"Wikidata"},{"location":"reference/manubot/cite/wikidata/#module-manubotcitewikidata","text":"View Source from .handlers import Handler class Handler_Wikidata ( Handler ): prefixes = [ \"wikidata\" ] standard_prefix = \"wikidata\" accession_pattern = r \"Q[0-9]+\" def inspect ( self , citekey ): \"\"\" https://www.wikidata.org/wiki/Wikidata:Identifiers \"\"\" accession = citekey . accession if not accession . startswith ( \"Q\" ): return \"Wikidata item IDs must start with 'Q'.\" elif not self . _get_pattern () . fullmatch ( accession ): return ( \"Accession does not conform to the Wikidata regex. \" \"Double check the entity ID.\" ) def get_csl_item ( self , citekey ): return get_wikidata_csl_item ( citekey . standard_accession ) def get_wikidata_csl_item ( identifier ): \"\"\" Get a CSL JSON item with the citation metadata for a Wikidata item. identifier should be a Wikidata item ID corresponding to a citeable work, such as Q50051684. \"\"\" url = f \"https://www.wikidata.org/wiki/{identifier}\" from manubot.cite.url import get_url_csl_item_zotero csl_item = get_url_csl_item_zotero ( url ) if \"DOI\" in csl_item : csl_item [ \"DOI\" ] = csl_item [ \"DOI\" ] . lower () if \"URL\" not in csl_item : csl_item [ \"URL\" ] = url return csl_item","title":"Module manubot.cite.wikidata"},{"location":"reference/manubot/cite/wikidata/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/wikidata/#get_wikidata_csl_item","text":"def get_wikidata_csl_item ( identifier ) Get a CSL JSON item with the citation metadata for a Wikidata item. identifier should be a Wikidata item ID corresponding to a citeable work, such as Q50051684. View Source def get_wikidata_csl_item ( identifier ): \"\"\" Get a CSL JSON item with the citation metadata for a Wikidata item. identifier should be a Wikidata item ID corresponding to a citeable work, such as Q50051684. \"\"\" url = f \"https://www.wikidata.org/wiki/{identifier}\" from manubot.cite.url import get_url_csl_item_zotero csl_item = get_url_csl_item_zotero ( url ) if \"DOI\" in csl_item : csl_item [ \"DOI\" ] = csl_item [ \"DOI\" ] . lower () if \"URL\" not in csl_item : csl_item [ \"URL\" ] = url return csl_item","title":"get_wikidata_csl_item"},{"location":"reference/manubot/cite/wikidata/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/wikidata/#handler_wikidata","text":"class Handler_Wikidata ( prefix_lower : str ) A Handler is a class that provides support for a certain type of citekey. For example, a Handler subclass could provide support for DOI citekeys. Subclasses enable custom logic for different citekey prefixes, including how to standardize the citekey and how to retrieve CSL Item metadata. View Source class Handler_Wikidata ( Handler ): prefixes = [ \"wikidata\" ] standard_prefix = \"wikidata\" accession_pattern = r \"Q[0-9]+\" def inspect ( self , citekey ): \"\"\" https://www.wikidata.org/wiki/Wikidata:Identifiers \"\"\" accession = citekey . accession if not accession . startswith ( \"Q\" ): return \"Wikidata item IDs must start with 'Q'.\" elif not self . _get_pattern (). fullmatch ( accession ): return ( \"Accession does not conform to the Wikidata regex. \" \"Double check the entity ID.\" ) def get_csl_item ( self , citekey ): return get_wikidata_csl_item ( citekey . standard_accession )","title":"Handler_Wikidata"},{"location":"reference/manubot/cite/wikidata/#ancestors-in-mro","text":"manubot.cite.handlers.Handler","title":"Ancestors (in MRO)"},{"location":"reference/manubot/cite/wikidata/#class-variables","text":"accession_pattern prefixes standard_prefix","title":"Class variables"},{"location":"reference/manubot/cite/wikidata/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/wikidata/#get_csl_item","text":"def get_csl_item ( self , citekey ) Return a CSL_Item with bibliographic details for citekey. View Source def get_csl_item ( self , citekey ): return get_wikidata_csl_item ( citekey . standard_accession )","title":"get_csl_item"},{"location":"reference/manubot/cite/wikidata/#inspect","text":"def inspect ( self , citekey ) https://www.wikidata.org/wiki/Wikidata:Identifiers View Source def inspect ( self , citekey ): \"\"\" https://www.wikidata.org/wiki/Wikidata:Identifiers \"\"\" accession = citekey . accession if not accession . startswith ( \"Q\" ): return \"Wikidata item IDs must start with 'Q'.\" elif not self . _get_pattern (). fullmatch ( accession ): return ( \"Accession does not conform to the Wikidata regex. \" \"Double check the entity ID.\" )","title":"inspect"},{"location":"reference/manubot/cite/wikidata/#standardize_prefix_accession","text":"def standardize_prefix_accession ( self , accession ) Return (prefix, accession) in standardized form. This method defaults to returning self.standard_prefix (or self.prefix_lower if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. View Source def standardize_prefix_accession ( self , accession ): \"\"\" Return (prefix, accession) in standardized form. This method defaults to returning `self.standard_prefix` (or `self.prefix_lower` if standard_prefix is not defined). Subclasses can override this method with more specific standardization logic. \"\"\" standard_prefix = getattr ( self , \"standard_prefix\" , self . prefix_lower ) standard_accession = accession return standard_prefix , standard_accession","title":"standardize_prefix_accession"},{"location":"reference/manubot/cite/zotero/","text":"Module manubot.cite.zotero Methods to interact with a Zotero translation-server. https://github.com/zotero/translation-server The Manubot team currently hosts a public translation server at https://translate.manubot.org. More information on this instance at https://github.com/manubot/manubot/issues/82. View Source \"\"\" Methods to interact with a Zotero translation-server. https://github.com/zotero/translation-server The Manubot team currently hosts a public translation server at https://translate.manubot.org. More information on this instance at https://github.com/manubot/manubot/issues/82. \"\"\" import json import logging import requests from manubot.util import get_manubot_user_agent , is_http_url base_url = \"https://translate.manubot.org\" def web_query ( url ): \"\"\" Return Zotero citation metadata for a URL as a list containing a single element that is a dictionary with the URL's metadata. \"\"\" headers = { \"User-Agent\" : get_manubot_user_agent (), \"Content-Type\" : \"text/plain\" } params = { \"single\" : 1 } api_url = f \"{base_url}/web\" response = requests . post ( api_url , params = params , headers = headers , data = str ( url )) try : zotero_data = response . json () except Exception as error : logging . warning ( f \"Error parsing web_query output as JSON for {url}: \\n {response.text}\" ) raise error if response . status_code == 300 : # When single=1 is specified, multiple results should never be returned logging . warning ( f \"web_query returned multiple results for {url}: \\n \" + json . dumps ( zotero_data , indent = 2 ) ) raise ValueError ( f \"multiple results for {url}\" ) zotero_data = _passthrough_zotero_data ( zotero_data ) return zotero_data def search_query ( identifier ): \"\"\" Retrive Zotero metadata for a DOI, ISBN, PMID, or arXiv ID. Example usage: ```shell curl --silent --data '10.2307/4486062' \\ --header 'Content-Type: text/plain' \\ http://127.0.0.1:1969/search ``` \"\"\" api_url = f \"{base_url}/search\" headers = { \"User-Agent\" : get_manubot_user_agent (), \"Content-Type\" : \"text/plain\" } response = requests . post ( api_url , headers = headers , data = str ( identifier )) try : zotero_data = response . json () except Exception as error : logging . warning ( f \"Error parsing search_query output as JSON for {identifier}: \\n {response.text}\" ) raise error zotero_data = _passthrough_zotero_data ( zotero_data ) return zotero_data def _passthrough_zotero_data ( zotero_data ): \"\"\" Address known issues with Zotero metadata. Assumes zotero data should contain a single bibliographic record. \"\"\" if not isinstance ( zotero_data , list ): raise ValueError ( \"_passthrough_zotero_data: zotero_data should be a list\" ) if len ( zotero_data ) > 1 : # Sometimes translation-server creates multiple data items for a single record. # If so, keep only the parent item, and remove child items (such as notes). # https://github.com/zotero/translation-server/issues/67 zotero_data = zotero_data [: 1 ] return zotero_data def export_as_csl ( zotero_data ): \"\"\" Export Zotero JSON data to CSL JSON using a translation-server /export query. Performs a similar query to the following curl command: ``` curl --verbose \\ --data @items.json \\ --header 'Content-Type: application/json' \\ 'https://translate.manubot.org/export?format=csljson' ``` \"\"\" api_url = f \"{base_url}/export\" params = { \"format\" : \"csljson\" } headers = { \"User-Agent\" : get_manubot_user_agent ()} response = requests . post ( api_url , params = params , headers = headers , json = zotero_data ) if not response . ok : message = f \"export_as_csl: translation-server returned status code {response.status_code}\" logging . warning ( f \"{message} with the following output: \\n {response.text}\" ) raise requests . HTTPError ( message ) try : csl_json = response . json () except Exception as error : logging . warning ( f \"Error parsing export_as_csl output as JSON: \\n {response.text}\" ) raise error return csl_json def get_csl_item ( identifier : str ): \"\"\" Use a translation-server search query followed by an export query to return a CSL Item (the first & only record of the returned CSL JSON). \"\"\" zotero_data = search_query ( identifier ) csl_data = export_as_csl ( zotero_data ) ( csl_item ,) = csl_data return csl_item def search_or_web_query ( identifier : str ) -> list : \"\"\" Detect whether `identifier` is a URL. If so, retrieve zotero metadata using a /web query. Otherwise, retrieve zotero metadata using a /search query. \"\"\" if is_http_url ( identifier ): zotero_data = web_query ( identifier ) else : zotero_data = search_query ( identifier ) return zotero_data Variables base_url Functions export_as_csl def export_as_csl ( zotero_data ) Export Zotero JSON data to CSL JSON using a translation-server /export query. Performs a similar query to the following curl command: curl --verbose --data @items.json --header 'Content-Type: application/json' 'https://translate.manubot.org/export?format=csljson' View Source def export_as_csl ( zotero_data ) : \"\"\" Export Zotero JSON data to CSL JSON using a translation-server /export query. Performs a similar query to the following curl command: ``` curl --verbose \\ --data @items.json \\ --header 'Content-Type: application/json' \\ 'https://translate.manubot.org/export?format=csljson' ``` \"\"\" api_url = f \"{base_url}/export\" params = { \"format\" : \"csljson\" } headers = { \"User-Agent\" : get_manubot_user_agent () } response = requests . post ( api_url , params = params , headers = headers , json = zotero_data ) if not response . ok : message = f \"export_as_csl: translation-server returned status code {response.status_code}\" logging . warning ( f \"{message} with the following output:\\n{response.text}\" ) raise requests . HTTPError ( message ) try : csl_json = response . json () except Exception as error : logging . warning ( f \"Error parsing export_as_csl output as JSON:\\n{response.text}\" ) raise error return csl_json get_csl_item def get_csl_item ( identifier : str ) Use a translation-server search query followed by an export query to return a CSL Item (the first & only record of the returned CSL JSON). View Source def get_csl_item ( identifier : str ): \"\"\" Use a translation-server search query followed by an export query to return a CSL Item (the first & only record of the returned CSL JSON). \"\"\" zotero_data = search_query ( identifier ) csl_data = export_as_csl ( zotero_data ) ( csl_item ,) = csl_data return csl_item search_or_web_query def search_or_web_query ( identifier : str ) -> list Detect whether identifier is a URL. If so, retrieve zotero metadata using a /web query. Otherwise, retrieve zotero metadata using a /search query. View Source def search_or_web_query ( identifier : str ) -> list : \"\"\" Detect whether `identifier` is a URL. If so, retrieve zotero metadata using a /web query. Otherwise, retrieve zotero metadata using a /search query. \"\"\" if is_http_url ( identifier ): zotero_data = web_query ( identifier ) else : zotero_data = search_query ( identifier ) return zotero_data search_query def search_query ( identifier ) Retrive Zotero metadata for a DOI, ISBN, PMID, or arXiv ID. Example usage: curl --silent --data '10.2307/4486062' --header 'Content-Type: text/plain' http://127.0.0.1:1969/search View Source def search_query ( identifier ): \"\"\" Retrive Zotero metadata for a DOI, ISBN, PMID, or arXiv ID. Example usage: ```shell curl --silent --data '10.2307/4486062' \\ --header 'Content-Type: text/plain' \\ http://127.0.0.1:1969/search ``` \"\"\" api_url = f \"{base_url}/search\" headers = { \"User-Agent\" : get_manubot_user_agent (), \"Content-Type\" : \"text/plain\" } response = requests . post ( api_url , headers= headers , data= str ( identifier )) try : zotero_data = response . json () except Exception as error : logging . warning ( f \"Error parsing search_query output as JSON for {identifier}: \\ n{response.text}\" ) raise error zotero_data = _passthrough_zotero_data ( zotero_data ) return zotero_data web_query def web_query ( url ) Return Zotero citation metadata for a URL as a list containing a single element that is a dictionary with the URL's metadata. View Source def web_query ( url ): \"\"\" Return Zotero citation metadata for a URL as a list containing a single element that is a dictionary with the URL's metadata. \"\"\" headers = { \"User-Agent\" : get_manubot_user_agent (), \"Content-Type\" : \"text/plain\" } params = { \"single\" : 1 } api_url = f \"{base_url}/web\" response = requests . post ( api_url , params= params , headers= headers , data= str ( url )) try : zotero_data = response . json () except Exception as error : logging . warning ( f \"Error parsing web_query output as JSON for {url}: \\ n{response.text}\" ) raise error if response . status_code = = 300 : # When single=1 is specified, multiple results should never be returned logging . warning ( f \"web_query returned multiple results for {url}: \\ n\" + json . dumps ( zotero_data , indent= 2 ) ) raise ValueError ( f \"multiple results for {url}\" ) zotero_data = _passthrough_zotero_data ( zotero_data ) return zotero_data","title":"Zotero"},{"location":"reference/manubot/cite/zotero/#module-manubotcitezotero","text":"Methods to interact with a Zotero translation-server. https://github.com/zotero/translation-server The Manubot team currently hosts a public translation server at https://translate.manubot.org. More information on this instance at https://github.com/manubot/manubot/issues/82. View Source \"\"\" Methods to interact with a Zotero translation-server. https://github.com/zotero/translation-server The Manubot team currently hosts a public translation server at https://translate.manubot.org. More information on this instance at https://github.com/manubot/manubot/issues/82. \"\"\" import json import logging import requests from manubot.util import get_manubot_user_agent , is_http_url base_url = \"https://translate.manubot.org\" def web_query ( url ): \"\"\" Return Zotero citation metadata for a URL as a list containing a single element that is a dictionary with the URL's metadata. \"\"\" headers = { \"User-Agent\" : get_manubot_user_agent (), \"Content-Type\" : \"text/plain\" } params = { \"single\" : 1 } api_url = f \"{base_url}/web\" response = requests . post ( api_url , params = params , headers = headers , data = str ( url )) try : zotero_data = response . json () except Exception as error : logging . warning ( f \"Error parsing web_query output as JSON for {url}: \\n {response.text}\" ) raise error if response . status_code == 300 : # When single=1 is specified, multiple results should never be returned logging . warning ( f \"web_query returned multiple results for {url}: \\n \" + json . dumps ( zotero_data , indent = 2 ) ) raise ValueError ( f \"multiple results for {url}\" ) zotero_data = _passthrough_zotero_data ( zotero_data ) return zotero_data def search_query ( identifier ): \"\"\" Retrive Zotero metadata for a DOI, ISBN, PMID, or arXiv ID. Example usage: ```shell curl --silent --data '10.2307/4486062' \\ --header 'Content-Type: text/plain' \\ http://127.0.0.1:1969/search ``` \"\"\" api_url = f \"{base_url}/search\" headers = { \"User-Agent\" : get_manubot_user_agent (), \"Content-Type\" : \"text/plain\" } response = requests . post ( api_url , headers = headers , data = str ( identifier )) try : zotero_data = response . json () except Exception as error : logging . warning ( f \"Error parsing search_query output as JSON for {identifier}: \\n {response.text}\" ) raise error zotero_data = _passthrough_zotero_data ( zotero_data ) return zotero_data def _passthrough_zotero_data ( zotero_data ): \"\"\" Address known issues with Zotero metadata. Assumes zotero data should contain a single bibliographic record. \"\"\" if not isinstance ( zotero_data , list ): raise ValueError ( \"_passthrough_zotero_data: zotero_data should be a list\" ) if len ( zotero_data ) > 1 : # Sometimes translation-server creates multiple data items for a single record. # If so, keep only the parent item, and remove child items (such as notes). # https://github.com/zotero/translation-server/issues/67 zotero_data = zotero_data [: 1 ] return zotero_data def export_as_csl ( zotero_data ): \"\"\" Export Zotero JSON data to CSL JSON using a translation-server /export query. Performs a similar query to the following curl command: ``` curl --verbose \\ --data @items.json \\ --header 'Content-Type: application/json' \\ 'https://translate.manubot.org/export?format=csljson' ``` \"\"\" api_url = f \"{base_url}/export\" params = { \"format\" : \"csljson\" } headers = { \"User-Agent\" : get_manubot_user_agent ()} response = requests . post ( api_url , params = params , headers = headers , json = zotero_data ) if not response . ok : message = f \"export_as_csl: translation-server returned status code {response.status_code}\" logging . warning ( f \"{message} with the following output: \\n {response.text}\" ) raise requests . HTTPError ( message ) try : csl_json = response . json () except Exception as error : logging . warning ( f \"Error parsing export_as_csl output as JSON: \\n {response.text}\" ) raise error return csl_json def get_csl_item ( identifier : str ): \"\"\" Use a translation-server search query followed by an export query to return a CSL Item (the first & only record of the returned CSL JSON). \"\"\" zotero_data = search_query ( identifier ) csl_data = export_as_csl ( zotero_data ) ( csl_item ,) = csl_data return csl_item def search_or_web_query ( identifier : str ) -> list : \"\"\" Detect whether `identifier` is a URL. If so, retrieve zotero metadata using a /web query. Otherwise, retrieve zotero metadata using a /search query. \"\"\" if is_http_url ( identifier ): zotero_data = web_query ( identifier ) else : zotero_data = search_query ( identifier ) return zotero_data","title":"Module manubot.cite.zotero"},{"location":"reference/manubot/cite/zotero/#variables","text":"base_url","title":"Variables"},{"location":"reference/manubot/cite/zotero/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/zotero/#export_as_csl","text":"def export_as_csl ( zotero_data ) Export Zotero JSON data to CSL JSON using a translation-server /export query. Performs a similar query to the following curl command: curl --verbose --data @items.json --header 'Content-Type: application/json' 'https://translate.manubot.org/export?format=csljson' View Source def export_as_csl ( zotero_data ) : \"\"\" Export Zotero JSON data to CSL JSON using a translation-server /export query. Performs a similar query to the following curl command: ``` curl --verbose \\ --data @items.json \\ --header 'Content-Type: application/json' \\ 'https://translate.manubot.org/export?format=csljson' ``` \"\"\" api_url = f \"{base_url}/export\" params = { \"format\" : \"csljson\" } headers = { \"User-Agent\" : get_manubot_user_agent () } response = requests . post ( api_url , params = params , headers = headers , json = zotero_data ) if not response . ok : message = f \"export_as_csl: translation-server returned status code {response.status_code}\" logging . warning ( f \"{message} with the following output:\\n{response.text}\" ) raise requests . HTTPError ( message ) try : csl_json = response . json () except Exception as error : logging . warning ( f \"Error parsing export_as_csl output as JSON:\\n{response.text}\" ) raise error return csl_json","title":"export_as_csl"},{"location":"reference/manubot/cite/zotero/#get_csl_item","text":"def get_csl_item ( identifier : str ) Use a translation-server search query followed by an export query to return a CSL Item (the first & only record of the returned CSL JSON). View Source def get_csl_item ( identifier : str ): \"\"\" Use a translation-server search query followed by an export query to return a CSL Item (the first & only record of the returned CSL JSON). \"\"\" zotero_data = search_query ( identifier ) csl_data = export_as_csl ( zotero_data ) ( csl_item ,) = csl_data return csl_item","title":"get_csl_item"},{"location":"reference/manubot/cite/zotero/#search_or_web_query","text":"def search_or_web_query ( identifier : str ) -> list Detect whether identifier is a URL. If so, retrieve zotero metadata using a /web query. Otherwise, retrieve zotero metadata using a /search query. View Source def search_or_web_query ( identifier : str ) -> list : \"\"\" Detect whether `identifier` is a URL. If so, retrieve zotero metadata using a /web query. Otherwise, retrieve zotero metadata using a /search query. \"\"\" if is_http_url ( identifier ): zotero_data = web_query ( identifier ) else : zotero_data = search_query ( identifier ) return zotero_data","title":"search_or_web_query"},{"location":"reference/manubot/cite/zotero/#search_query","text":"def search_query ( identifier ) Retrive Zotero metadata for a DOI, ISBN, PMID, or arXiv ID. Example usage: curl --silent --data '10.2307/4486062' --header 'Content-Type: text/plain' http://127.0.0.1:1969/search View Source def search_query ( identifier ): \"\"\" Retrive Zotero metadata for a DOI, ISBN, PMID, or arXiv ID. Example usage: ```shell curl --silent --data '10.2307/4486062' \\ --header 'Content-Type: text/plain' \\ http://127.0.0.1:1969/search ``` \"\"\" api_url = f \"{base_url}/search\" headers = { \"User-Agent\" : get_manubot_user_agent (), \"Content-Type\" : \"text/plain\" } response = requests . post ( api_url , headers= headers , data= str ( identifier )) try : zotero_data = response . json () except Exception as error : logging . warning ( f \"Error parsing search_query output as JSON for {identifier}: \\ n{response.text}\" ) raise error zotero_data = _passthrough_zotero_data ( zotero_data ) return zotero_data","title":"search_query"},{"location":"reference/manubot/cite/zotero/#web_query","text":"def web_query ( url ) Return Zotero citation metadata for a URL as a list containing a single element that is a dictionary with the URL's metadata. View Source def web_query ( url ): \"\"\" Return Zotero citation metadata for a URL as a list containing a single element that is a dictionary with the URL's metadata. \"\"\" headers = { \"User-Agent\" : get_manubot_user_agent (), \"Content-Type\" : \"text/plain\" } params = { \"single\" : 1 } api_url = f \"{base_url}/web\" response = requests . post ( api_url , params= params , headers= headers , data= str ( url )) try : zotero_data = response . json () except Exception as error : logging . warning ( f \"Error parsing web_query output as JSON for {url}: \\ n{response.text}\" ) raise error if response . status_code = = 300 : # When single=1 is specified, multiple results should never be returned logging . warning ( f \"web_query returned multiple results for {url}: \\ n\" + json . dumps ( zotero_data , indent= 2 ) ) raise ValueError ( f \"multiple results for {url}\" ) zotero_data = _passthrough_zotero_data ( zotero_data ) return zotero_data","title":"web_query"},{"location":"reference/manubot/cite/tests/","text":"Module manubot.cite.tests Sub-modules manubot.cite.tests.test_arxiv manubot.cite.tests.test_citations manubot.cite.tests.test_cite_command manubot.cite.tests.test_citekey manubot.cite.tests.test_citekey_api manubot.cite.tests.test_citeproc manubot.cite.tests.test_csl_item manubot.cite.tests.test_curie manubot.cite.tests.test_doi manubot.cite.tests.test_handlers manubot.cite.tests.test_isbn manubot.cite.tests.test_pubmed manubot.cite.tests.test_unpaywall manubot.cite.tests.test_url manubot.cite.tests.test_wikidata manubot.cite.tests.test_zotero","title":"Index"},{"location":"reference/manubot/cite/tests/#module-manubotcitetests","text":"","title":"Module manubot.cite.tests"},{"location":"reference/manubot/cite/tests/#sub-modules","text":"manubot.cite.tests.test_arxiv manubot.cite.tests.test_citations manubot.cite.tests.test_cite_command manubot.cite.tests.test_citekey manubot.cite.tests.test_citekey_api manubot.cite.tests.test_citeproc manubot.cite.tests.test_csl_item manubot.cite.tests.test_curie manubot.cite.tests.test_doi manubot.cite.tests.test_handlers manubot.cite.tests.test_isbn manubot.cite.tests.test_pubmed manubot.cite.tests.test_unpaywall manubot.cite.tests.test_url manubot.cite.tests.test_wikidata manubot.cite.tests.test_zotero","title":"Sub-modules"},{"location":"reference/manubot/cite/tests/test_arxiv/","text":"Module manubot.cite.tests.test_arxiv View Source from ..arxiv import get_arxiv_csl_item_export_api , get_arxiv_csl_item_oai def test_get_arxiv_csl_item_abstract_whitespace (): \"\"\" Test wrapping newlines are properly removed from the abstract, while preserving paragraph breaks. From https://arxiv.org/help/prep#abstracts: > Carriage returns will be stripped unless they are followed by leading white spaces. So if you want a new paragraph or a table of contents, be sure to indent the lines after the carriage return. When the abstract is formatted for email announcement, it will be wrapped to 80 characters. \"\"\" csl_item = get_arxiv_csl_item_export_api ( \"1908.00936v2\" ) assert csl_item [ \"title\" ] == \"Multi-Scale Learned Iterative Reconstruction\" assert csl_item [ \"URL\" ] == \"https://arxiv.org/abs/1908.00936v2\" assert csl_item [ \"version\" ] == \"v2\" abstract = ( \"Model-based learned iterative reconstruction methods have recently been shown to outperform classical reconstruction algorithms. Applicability of these methods to large scale inverse problems is however limited by the available memory for training and extensive training times due to computationally expensive forward models. As a possible solution to these restrictions we propose a multi-scale learned iterative reconstruction scheme that computes iterates on discretisations of increasing resolution. This procedure does not only reduce memory requirements, it also considerably speeds up reconstruction and training times, but most importantly is scalable to large scale inverse problems with non-trivial forward operators, such as those that arise in many 3D tomographic applications. In particular, we propose a hybrid network that combines the multi-scale iterative approach with a particularly expressive network architecture which in combination exhibits excellent scalability in 3D.\" \" \\n Applicability of the algorithm is demonstrated for 3D cone beam computed tomography from real measurement data of an organic phantom. Additionally, we examine scalability and reconstruction quality in comparison to established learned reconstruction methods in two dimensions for low dose computed tomography on human phantoms.\" ) assert csl_item [ \"abstract\" ] == abstract def test_get_arxiv_csl_item_oai (): \"\"\" https://export.arxiv.org/oai2?verb=GetRecord&identifier=oai:arXiv.org:1912.04616&metadataPrefix=arXiv \"\"\" csl_item = get_arxiv_csl_item_oai ( \"1912.04616\" ) assert ( csl_item [ \"title\" ] == \"OpenBioLink: A benchmarking framework for large-scale biomedical link prediction\" ) assert csl_item [ \"URL\" ] == \"https://arxiv.org/abs/1912.04616\" assert csl_item . note_dict [ \"license\" ] assert csl_item [ \"author\" ][ 0 ][ \"family\" ] == \"Breit\" Functions test_get_arxiv_csl_item_abstract_whitespace def test_get_arxiv_csl_item_abstract_whitespace ( ) Test wrapping newlines are properly removed from the abstract, while preserving paragraph breaks. From https://arxiv.org/help/prep#abstracts: Carriage returns will be stripped unless they are followed by leading white spaces. So if you want a new paragraph or a table of contents, be sure to indent the lines after the carriage return. When the abstract is formatted for email announcement, it will be wrapped to 80 characters. View Source def test_get_arxiv_csl_item_abstract_whitespace (): \"\"\" Test wrapping newlines are properly removed from the abstract, while preserving paragraph breaks. From https://arxiv.org/help/prep#abstracts: > Carriage returns will be stripped unless they are followed by leading white spaces. So if you want a new paragraph or a table of contents, be sure to indent the lines after the carriage return. When the abstract is formatted for email announcement, it will be wrapped to 80 characters. \"\"\" csl_item = get_arxiv_csl_item_export_api ( \"1908.00936v2\" ) assert csl_item [ \"title\" ] == \"Multi-Scale Learned Iterative Reconstruction\" assert csl_item [ \"URL\" ] == \"https://arxiv.org/abs/1908.00936v2\" assert csl_item [ \"version\" ] == \"v2\" abstract = ( \"Model-based learned iterative reconstruction methods have recently been shown to outperform classical reconstruction algorithms. Applicability of these methods to large scale inverse problems is however limited by the available memory for training and extensive training times due to computationally expensive forward models. As a possible solution to these restrictions we propose a multi-scale learned iterative reconstruction scheme that computes iterates on discretisations of increasing resolution. This procedure does not only reduce memory requirements, it also considerably speeds up reconstruction and training times, but most importantly is scalable to large scale inverse problems with non-trivial forward operators, such as those that arise in many 3D tomographic applications. In particular, we propose a hybrid network that combines the multi-scale iterative approach with a particularly expressive network architecture which in combination exhibits excellent scalability in 3D.\" \"\\n Applicability of the algorithm is demonstrated for 3D cone beam computed tomography from real measurement data of an organic phantom. Additionally, we examine scalability and reconstruction quality in comparison to established learned reconstruction methods in two dimensions for low dose computed tomography on human phantoms.\" ) assert csl_item [ \"abstract\" ] == abstract test_get_arxiv_csl_item_oai def test_get_arxiv_csl_item_oai ( ) https://export.arxiv.org/oai2?verb=GetRecord&identifier=oai:arXiv.org:1912.04616&metadataPrefix=arXiv View Source def test_get_arxiv_csl_item_oai (): \"\"\" https://export.arxiv.org/oai2?verb=GetRecord&identifier=oai:arXiv.org:1912.04616&metadataPrefix=arXiv \"\"\" csl_item = get_arxiv_csl_item_oai ( \"1912.04616\" ) assert ( csl_item [ \"title\" ] == \"OpenBioLink: A benchmarking framework for large-scale biomedical link prediction\" ) assert csl_item [ \"URL\" ] == \"https://arxiv.org/abs/1912.04616\" assert csl_item . note_dict [ \"license\" ] assert csl_item [ \"author\" ][ 0 ][ \"family\" ] == \"Breit\"","title":"Test Arxiv"},{"location":"reference/manubot/cite/tests/test_arxiv/#module-manubotciteteststest_arxiv","text":"View Source from ..arxiv import get_arxiv_csl_item_export_api , get_arxiv_csl_item_oai def test_get_arxiv_csl_item_abstract_whitespace (): \"\"\" Test wrapping newlines are properly removed from the abstract, while preserving paragraph breaks. From https://arxiv.org/help/prep#abstracts: > Carriage returns will be stripped unless they are followed by leading white spaces. So if you want a new paragraph or a table of contents, be sure to indent the lines after the carriage return. When the abstract is formatted for email announcement, it will be wrapped to 80 characters. \"\"\" csl_item = get_arxiv_csl_item_export_api ( \"1908.00936v2\" ) assert csl_item [ \"title\" ] == \"Multi-Scale Learned Iterative Reconstruction\" assert csl_item [ \"URL\" ] == \"https://arxiv.org/abs/1908.00936v2\" assert csl_item [ \"version\" ] == \"v2\" abstract = ( \"Model-based learned iterative reconstruction methods have recently been shown to outperform classical reconstruction algorithms. Applicability of these methods to large scale inverse problems is however limited by the available memory for training and extensive training times due to computationally expensive forward models. As a possible solution to these restrictions we propose a multi-scale learned iterative reconstruction scheme that computes iterates on discretisations of increasing resolution. This procedure does not only reduce memory requirements, it also considerably speeds up reconstruction and training times, but most importantly is scalable to large scale inverse problems with non-trivial forward operators, such as those that arise in many 3D tomographic applications. In particular, we propose a hybrid network that combines the multi-scale iterative approach with a particularly expressive network architecture which in combination exhibits excellent scalability in 3D.\" \" \\n Applicability of the algorithm is demonstrated for 3D cone beam computed tomography from real measurement data of an organic phantom. Additionally, we examine scalability and reconstruction quality in comparison to established learned reconstruction methods in two dimensions for low dose computed tomography on human phantoms.\" ) assert csl_item [ \"abstract\" ] == abstract def test_get_arxiv_csl_item_oai (): \"\"\" https://export.arxiv.org/oai2?verb=GetRecord&identifier=oai:arXiv.org:1912.04616&metadataPrefix=arXiv \"\"\" csl_item = get_arxiv_csl_item_oai ( \"1912.04616\" ) assert ( csl_item [ \"title\" ] == \"OpenBioLink: A benchmarking framework for large-scale biomedical link prediction\" ) assert csl_item [ \"URL\" ] == \"https://arxiv.org/abs/1912.04616\" assert csl_item . note_dict [ \"license\" ] assert csl_item [ \"author\" ][ 0 ][ \"family\" ] == \"Breit\"","title":"Module manubot.cite.tests.test_arxiv"},{"location":"reference/manubot/cite/tests/test_arxiv/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_arxiv/#test_get_arxiv_csl_item_abstract_whitespace","text":"def test_get_arxiv_csl_item_abstract_whitespace ( ) Test wrapping newlines are properly removed from the abstract, while preserving paragraph breaks. From https://arxiv.org/help/prep#abstracts: Carriage returns will be stripped unless they are followed by leading white spaces. So if you want a new paragraph or a table of contents, be sure to indent the lines after the carriage return. When the abstract is formatted for email announcement, it will be wrapped to 80 characters. View Source def test_get_arxiv_csl_item_abstract_whitespace (): \"\"\" Test wrapping newlines are properly removed from the abstract, while preserving paragraph breaks. From https://arxiv.org/help/prep#abstracts: > Carriage returns will be stripped unless they are followed by leading white spaces. So if you want a new paragraph or a table of contents, be sure to indent the lines after the carriage return. When the abstract is formatted for email announcement, it will be wrapped to 80 characters. \"\"\" csl_item = get_arxiv_csl_item_export_api ( \"1908.00936v2\" ) assert csl_item [ \"title\" ] == \"Multi-Scale Learned Iterative Reconstruction\" assert csl_item [ \"URL\" ] == \"https://arxiv.org/abs/1908.00936v2\" assert csl_item [ \"version\" ] == \"v2\" abstract = ( \"Model-based learned iterative reconstruction methods have recently been shown to outperform classical reconstruction algorithms. Applicability of these methods to large scale inverse problems is however limited by the available memory for training and extensive training times due to computationally expensive forward models. As a possible solution to these restrictions we propose a multi-scale learned iterative reconstruction scheme that computes iterates on discretisations of increasing resolution. This procedure does not only reduce memory requirements, it also considerably speeds up reconstruction and training times, but most importantly is scalable to large scale inverse problems with non-trivial forward operators, such as those that arise in many 3D tomographic applications. In particular, we propose a hybrid network that combines the multi-scale iterative approach with a particularly expressive network architecture which in combination exhibits excellent scalability in 3D.\" \"\\n Applicability of the algorithm is demonstrated for 3D cone beam computed tomography from real measurement data of an organic phantom. Additionally, we examine scalability and reconstruction quality in comparison to established learned reconstruction methods in two dimensions for low dose computed tomography on human phantoms.\" ) assert csl_item [ \"abstract\" ] == abstract","title":"test_get_arxiv_csl_item_abstract_whitespace"},{"location":"reference/manubot/cite/tests/test_arxiv/#test_get_arxiv_csl_item_oai","text":"def test_get_arxiv_csl_item_oai ( ) https://export.arxiv.org/oai2?verb=GetRecord&identifier=oai:arXiv.org:1912.04616&metadataPrefix=arXiv View Source def test_get_arxiv_csl_item_oai (): \"\"\" https://export.arxiv.org/oai2?verb=GetRecord&identifier=oai:arXiv.org:1912.04616&metadataPrefix=arXiv \"\"\" csl_item = get_arxiv_csl_item_oai ( \"1912.04616\" ) assert ( csl_item [ \"title\" ] == \"OpenBioLink: A benchmarking framework for large-scale biomedical link prediction\" ) assert csl_item [ \"URL\" ] == \"https://arxiv.org/abs/1912.04616\" assert csl_item . note_dict [ \"license\" ] assert csl_item [ \"author\" ][ 0 ][ \"family\" ] == \"Breit\"","title":"test_get_arxiv_csl_item_oai"},{"location":"reference/manubot/cite/tests/test_citations/","text":"Module manubot.cite.tests.test_citations View Source from manubot.cite.citations import Citations def test_citations_filter_pandoc_xnos (): input_ids = [ \"fig:pandoc-fignos-key\" , # should filter \"eq:pandoc-eqnos-key\" , # should filter \"tbl:pandoc-tablenos-key\" , # should filter \"not-pandoc-xnos-key\" , # should keep ] citations = Citations ( input_ids ) citations . filter_pandoc_xnos () assert len ( citations . citekeys ) == 1 assert citations . citekeys [ 0 ] . input_id == \"not-pandoc-xnos-key\" def test_citations_filter_unhandled (): input_ids = [ \"citekey-with-no-prefix\" , \"bad-prefix:citekey\" , \":empty-prefix\" , \"doi:handled-prefix\" , ] citations = Citations ( input_ids ) citations . filter_unhandled () assert len ( citations . citekeys ) == 1 assert citations . citekeys [ 0 ] . input_id == \"doi:handled-prefix\" def test_citations_check_collisions ( caplog ): input_ids = [ \"citekey-1\" , \"citekey-1\" , \"citekey-2\" , \"Citekey-2\" , ] citations = Citations ( input_ids ) citations . check_collisions () assert not caplog . records def test_citations_check_multiple_input_ids ( caplog ): input_ids = [ \"doi:10/b6vnmd\" , \"DOI:10/B6VNMD\" , \"doi:10.1016/s0933-3657(96)00367-3\" , \"ugly-doi-alias\" , \"other-citekey\" , ] citekey_aliases = { \"ugly-doi-alias\" : \"DOI:10.1016/s0933-3657(96)00367-3\" } citations = Citations ( input_ids , citekey_aliases ) citations . check_multiple_input_ids () expected = \"Multiple citekey input_ids refer to the same standard_id doi:10.1016/s0933-3657(96)00367-3:\" \"['doi:10/b6vnmd', 'DOI:10/B6VNMD', 'doi:10.1016/s0933-3657(96)00367-3', 'ugly-doi-alias']\" assert expected in caplog . text def test_citations_citekeys_tsv (): input_ids = [ \"citekey-1\" , \"arXiv:1806.05726v1\" , \"DOI:10.7717/peerj.338\" , \"pmid:29618526\" , ] citations = Citations ( input_ids ) citekeys_tsv = citations . citekeys_tsv assert isinstance ( citekeys_tsv , str ) assert \"arxiv:1806.05726v1\" in citekeys_tsv . splitlines ()[ 2 ] . split ( \" \\t \" ) def test_citations_inspect (): input_ids = [ \"citekey-1\" , # passes inspection \"arXiv:1806.05726v1\" , # passes inspection \"arXiv:bad-id\" , \"DOI:bad-id\" , \"pmid:bad-id\" , \"DOID:not-disease-ontology-id\" , ] citations = Citations ( input_ids ) report = citations . inspect ( log_level = \"INFO\" ) print ( report ) assert len ( report . splitlines ()) == 4 assert \"pmid:bad-id -- PubMed Identifiers should be 1-8 digits\" in report Functions test_citations_check_collisions def test_citations_check_collisions ( caplog ) View Source def test_citations_check_collisions ( caplog ): input_ids = [ \"citekey-1\" , \"citekey-1\" , \"citekey-2\" , \"Citekey-2\" , ] citations = Citations ( input_ids ) citations . check_collisions () assert not caplog . records test_citations_check_multiple_input_ids def test_citations_check_multiple_input_ids ( caplog ) View Source def test_citations_check_multiple_input_ids ( caplog ): input_ids = [ \"doi:10/b6vnmd\" , \"DOI:10/B6VNMD\" , \"doi:10.1016/s0933-3657(96)00367-3\" , \"ugly-doi-alias\" , \"other-citekey\" , ] citekey_aliases = { \"ugly-doi-alias\" : \"DOI:10.1016/s0933-3657(96)00367-3\" } citations = Citations ( input_ids , citekey_aliases ) citations . check_multiple_input_ids () expected = \"Multiple citekey input_ids refer to the same standard_id doi:10.1016/s0933-3657(96)00367-3:\" \"['doi:10/b6vnmd', 'DOI:10/B6VNMD', 'doi:10.1016/s0933-3657(96)00367-3', 'ugly-doi-alias']\" assert expected in caplog . text test_citations_citekeys_tsv def test_citations_citekeys_tsv ( ) View Source def test_citations_citekeys_tsv (): input_ids = [ \"citekey-1\" , \"arXiv:1806.05726v1\" , \"DOI:10.7717/peerj.338\" , \"pmid:29618526\" , ] citations = Citations ( input_ids ) citekeys_tsv = citations . citekeys_tsv assert isinstance ( citekeys_tsv , str ) assert \"arxiv:1806.05726v1\" in citekeys_tsv . splitlines ()[ 2 ]. split ( \"\\t\" ) test_citations_filter_pandoc_xnos def test_citations_filter_pandoc_xnos ( ) View Source def test_citations_filter_pandoc_xnos (): input_ids = [ \"fig:pandoc-fignos-key\" , # should filter \"eq:pandoc-eqnos-key\" , # should filter \"tbl:pandoc-tablenos-key\" , # should filter \"not-pandoc-xnos-key\" , # should keep ] citations = Citations ( input_ids ) citations . filter_pandoc_xnos () assert len ( citations . citekeys ) == 1 assert citations . citekeys [ 0 ]. input_id == \"not-pandoc-xnos-key\" test_citations_filter_unhandled def test_citations_filter_unhandled ( ) View Source def test_citations_filter_unhandled (): input_ids = [ \"citekey-with-no-prefix\" , \"bad-prefix:citekey\" , \":empty-prefix\" , \"doi:handled-prefix\" , ] citations = Citations ( input_ids ) citations . filter_unhandled () assert len ( citations . citekeys ) == 1 assert citations . citekeys [ 0 ]. input_id == \"doi:handled-prefix\" test_citations_inspect def test_citations_inspect ( ) View Source def test_citations_inspect (): input_ids = [ \"citekey-1\" , # passes inspection \"arXiv:1806.05726v1\" , # passes inspection \"arXiv:bad-id\" , \"DOI:bad-id\" , \"pmid:bad-id\" , \"DOID:not-disease-ontology-id\" , ] citations = Citations ( input_ids ) report = citations . inspect ( log_level = \"INFO\" ) print ( report ) assert len ( report . splitlines ()) == 4 assert \"pmid:bad-id -- PubMed Identifiers should be 1-8 digits\" in report","title":"Test Citations"},{"location":"reference/manubot/cite/tests/test_citations/#module-manubotciteteststest_citations","text":"View Source from manubot.cite.citations import Citations def test_citations_filter_pandoc_xnos (): input_ids = [ \"fig:pandoc-fignos-key\" , # should filter \"eq:pandoc-eqnos-key\" , # should filter \"tbl:pandoc-tablenos-key\" , # should filter \"not-pandoc-xnos-key\" , # should keep ] citations = Citations ( input_ids ) citations . filter_pandoc_xnos () assert len ( citations . citekeys ) == 1 assert citations . citekeys [ 0 ] . input_id == \"not-pandoc-xnos-key\" def test_citations_filter_unhandled (): input_ids = [ \"citekey-with-no-prefix\" , \"bad-prefix:citekey\" , \":empty-prefix\" , \"doi:handled-prefix\" , ] citations = Citations ( input_ids ) citations . filter_unhandled () assert len ( citations . citekeys ) == 1 assert citations . citekeys [ 0 ] . input_id == \"doi:handled-prefix\" def test_citations_check_collisions ( caplog ): input_ids = [ \"citekey-1\" , \"citekey-1\" , \"citekey-2\" , \"Citekey-2\" , ] citations = Citations ( input_ids ) citations . check_collisions () assert not caplog . records def test_citations_check_multiple_input_ids ( caplog ): input_ids = [ \"doi:10/b6vnmd\" , \"DOI:10/B6VNMD\" , \"doi:10.1016/s0933-3657(96)00367-3\" , \"ugly-doi-alias\" , \"other-citekey\" , ] citekey_aliases = { \"ugly-doi-alias\" : \"DOI:10.1016/s0933-3657(96)00367-3\" } citations = Citations ( input_ids , citekey_aliases ) citations . check_multiple_input_ids () expected = \"Multiple citekey input_ids refer to the same standard_id doi:10.1016/s0933-3657(96)00367-3:\" \"['doi:10/b6vnmd', 'DOI:10/B6VNMD', 'doi:10.1016/s0933-3657(96)00367-3', 'ugly-doi-alias']\" assert expected in caplog . text def test_citations_citekeys_tsv (): input_ids = [ \"citekey-1\" , \"arXiv:1806.05726v1\" , \"DOI:10.7717/peerj.338\" , \"pmid:29618526\" , ] citations = Citations ( input_ids ) citekeys_tsv = citations . citekeys_tsv assert isinstance ( citekeys_tsv , str ) assert \"arxiv:1806.05726v1\" in citekeys_tsv . splitlines ()[ 2 ] . split ( \" \\t \" ) def test_citations_inspect (): input_ids = [ \"citekey-1\" , # passes inspection \"arXiv:1806.05726v1\" , # passes inspection \"arXiv:bad-id\" , \"DOI:bad-id\" , \"pmid:bad-id\" , \"DOID:not-disease-ontology-id\" , ] citations = Citations ( input_ids ) report = citations . inspect ( log_level = \"INFO\" ) print ( report ) assert len ( report . splitlines ()) == 4 assert \"pmid:bad-id -- PubMed Identifiers should be 1-8 digits\" in report","title":"Module manubot.cite.tests.test_citations"},{"location":"reference/manubot/cite/tests/test_citations/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_citations/#test_citations_check_collisions","text":"def test_citations_check_collisions ( caplog ) View Source def test_citations_check_collisions ( caplog ): input_ids = [ \"citekey-1\" , \"citekey-1\" , \"citekey-2\" , \"Citekey-2\" , ] citations = Citations ( input_ids ) citations . check_collisions () assert not caplog . records","title":"test_citations_check_collisions"},{"location":"reference/manubot/cite/tests/test_citations/#test_citations_check_multiple_input_ids","text":"def test_citations_check_multiple_input_ids ( caplog ) View Source def test_citations_check_multiple_input_ids ( caplog ): input_ids = [ \"doi:10/b6vnmd\" , \"DOI:10/B6VNMD\" , \"doi:10.1016/s0933-3657(96)00367-3\" , \"ugly-doi-alias\" , \"other-citekey\" , ] citekey_aliases = { \"ugly-doi-alias\" : \"DOI:10.1016/s0933-3657(96)00367-3\" } citations = Citations ( input_ids , citekey_aliases ) citations . check_multiple_input_ids () expected = \"Multiple citekey input_ids refer to the same standard_id doi:10.1016/s0933-3657(96)00367-3:\" \"['doi:10/b6vnmd', 'DOI:10/B6VNMD', 'doi:10.1016/s0933-3657(96)00367-3', 'ugly-doi-alias']\" assert expected in caplog . text","title":"test_citations_check_multiple_input_ids"},{"location":"reference/manubot/cite/tests/test_citations/#test_citations_citekeys_tsv","text":"def test_citations_citekeys_tsv ( ) View Source def test_citations_citekeys_tsv (): input_ids = [ \"citekey-1\" , \"arXiv:1806.05726v1\" , \"DOI:10.7717/peerj.338\" , \"pmid:29618526\" , ] citations = Citations ( input_ids ) citekeys_tsv = citations . citekeys_tsv assert isinstance ( citekeys_tsv , str ) assert \"arxiv:1806.05726v1\" in citekeys_tsv . splitlines ()[ 2 ]. split ( \"\\t\" )","title":"test_citations_citekeys_tsv"},{"location":"reference/manubot/cite/tests/test_citations/#test_citations_filter_pandoc_xnos","text":"def test_citations_filter_pandoc_xnos ( ) View Source def test_citations_filter_pandoc_xnos (): input_ids = [ \"fig:pandoc-fignos-key\" , # should filter \"eq:pandoc-eqnos-key\" , # should filter \"tbl:pandoc-tablenos-key\" , # should filter \"not-pandoc-xnos-key\" , # should keep ] citations = Citations ( input_ids ) citations . filter_pandoc_xnos () assert len ( citations . citekeys ) == 1 assert citations . citekeys [ 0 ]. input_id == \"not-pandoc-xnos-key\"","title":"test_citations_filter_pandoc_xnos"},{"location":"reference/manubot/cite/tests/test_citations/#test_citations_filter_unhandled","text":"def test_citations_filter_unhandled ( ) View Source def test_citations_filter_unhandled (): input_ids = [ \"citekey-with-no-prefix\" , \"bad-prefix:citekey\" , \":empty-prefix\" , \"doi:handled-prefix\" , ] citations = Citations ( input_ids ) citations . filter_unhandled () assert len ( citations . citekeys ) == 1 assert citations . citekeys [ 0 ]. input_id == \"doi:handled-prefix\"","title":"test_citations_filter_unhandled"},{"location":"reference/manubot/cite/tests/test_citations/#test_citations_inspect","text":"def test_citations_inspect ( ) View Source def test_citations_inspect (): input_ids = [ \"citekey-1\" , # passes inspection \"arXiv:1806.05726v1\" , # passes inspection \"arXiv:bad-id\" , \"DOI:bad-id\" , \"pmid:bad-id\" , \"DOID:not-disease-ontology-id\" , ] citations = Citations ( input_ids ) report = citations . inspect ( log_level = \"INFO\" ) print ( report ) assert len ( report . splitlines ()) == 4 assert \"pmid:bad-id -- PubMed Identifiers should be 1-8 digits\" in report","title":"test_citations_inspect"},{"location":"reference/manubot/cite/tests/test_cite_command/","text":"Module manubot.cite.tests.test_cite_command View Source import json import pathlib import shutil import subprocess import pytest from manubot.util import shlex_join from manubot.pandoc.util import get_pandoc_version def test_cite_command_empty (): process = subprocess . run ( [ \"manubot\" , \"cite\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ) print ( process . stderr ) assert process . returncode == 2 assert \"the following arguments are required: citekeys\" in process . stderr def test_cite_command_stdout (): process = subprocess . run ( [ \"manubot\" , \"cite\" , \"arxiv:1806.05726v1\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ) print ( process . stderr ) assert process . returncode == 0 ( csl ,) = json . loads ( process . stdout ) assert csl [ \"URL\" ] == \"https://arxiv.org/abs/1806.05726v1\" def test_cite_command_file ( tmpdir ): path = pathlib . Path ( tmpdir ) / \"csl-items.json\" process = subprocess . run ( [ \"manubot\" , \"cite\" , \"--output\" , str ( path ), \"arxiv:1806.05726v1\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , ) print ( process . stderr . decode ()) assert process . returncode == 0 with path . open () as read_file : ( csl ,) = json . load ( read_file ) assert csl [ \"URL\" ] == \"https://arxiv.org/abs/1806.05726v1\" @pytest.mark.parametrize ( [ \"args\" , \"filename\" ], [ pytest . param ([], \"references-plain-{}.txt\" , id = \"no-args\" ), pytest . param ( [ \"--format\" , \"plain\" ], \"references-plain-{}.txt\" , id = \"--format=plain\" ), pytest . param ( [ \"--format\" , \"markdown\" ], \"references-markdown-{}.md\" , id = \"--format=markdown\" , ), pytest . param ( [ \"--format\" , \"html\" ], \"references-html-{}.html\" , id = \"--format=html\" ), pytest . param ( [ \"--format\" , \"jats\" ], \"references-jats-{}.xml\" , id = \"--format=jats\" ), ], ) @pytest.mark.skipif ( not shutil . which ( \"pandoc\" ), reason = \"pandoc installation not found on system\" ) @pytest.mark.skipif ( not shutil . which ( \"pandoc-citeproc\" ), reason = \"pandoc-citeproc installation not found on system\" , ) @pytest.mark.pandoc_version_sensitive def test_cite_command_render_stdout ( args , filename ): \"\"\" Test the stdout output of `manubot cite --render` with various formats. The output is sensitive to the version of Pandoc used, so expected output files include the pandoc version stamp in their filename. When the expected version is missing, the test fails but writes the command output to that file. Therefore, subsequent runs of the same test will pass. Before committing the auto-generated output, do look to ensure its integrity. This test uses --bibliography to avoid slow network calls. Regenerate the CSL JSON using: ```shell manubot cite \\ --output=manubot/cite/tests/cite-command-rendered/input-references.json \\ arxiv:1806.05726v1 doi:10.7717/peerj.338 pmid:29618526 ``` \"\"\" # get pandoc version info pandoc_version = get_pandoc_version () pandoc_stamp = \".\" . join ( map ( str , pandoc_version )) data_dir = pathlib . Path ( __file__ ) . parent . joinpath ( \"cite-command-rendered\" ) path = data_dir . joinpath ( filename . format ( pandoc_stamp )) # skip test on old pandoc versions for output in \"markdown\" , \"html\" , \"jats\" : if output in args and pandoc_version < ( 2 , 5 ): pytest . skip ( f \"Test {output} output assumes pandoc >= 2.5\" ) if pandoc_version < ( 2 , 0 ): pytest . skip ( \"Test requires pandoc >= 2.0 to support --lua-filter and --csl=URL\" ) args = [ \"manubot\" , \"cite\" , \"--bibliography=input-references.json\" , \"--render\" , \"--csl=https://github.com/greenelab/manubot-rootstock/raw/e83e51dcd89256403bb787c3d9a46e4ee8d04a9e/build/assets/style.csl\" , \"arxiv:1806.05726v1\" , \"doi:10.7717/peerj.338\" , \"pmid:29618526\" , ] + args process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , cwd = data_dir , ) print ( shlex_join ( process . args )) if not path . exists (): # https://github.com/manubot/manubot/pull/146#discussion_r333132261 print ( f \"Missing expected output at {path} \\n \" \"Writing output to file such that future tests will pass.\" ) path . write_text ( process . stdout , encoding = \"utf-8\" ) assert False print ( process . stdout ) print ( process . stderr ) expected = path . read_text ( encoding = \"utf-8-sig\" ) assert process . stdout == expected def test_cite_command_bibliography (): bib_dir = pathlib . Path ( __file__ ) . parent . parent . parent . joinpath ( \"pandoc/tests/bibliographies\" ) args = [ \"manubot\" , \"cite\" , \"--bibliography=bibliography.json\" , \"DOI:10.7554/elife.32822\" , ] csl_json = subprocess . check_output ( args , encoding = \"utf-8\" , cwd = bib_dir ) csl_items = json . loads ( csl_json ) assert len ( csl_items ) == 1 csl_item = csl_items [ 0 ] assert \"manual_reference_filename: bibliography.json\" in csl_item [ \"note\" ] def teardown_module ( module ): \"\"\" Avoid too many requests to NCBI E-Utils in the test_pubmed.py, which is executed following this module. E-Utility requests are capped at 3 per second, which is usually controled by _get_eutils_rate_limiter, but this does not seem to work across test modules. \"\"\" import time time . sleep ( 1 ) Functions teardown_module def teardown_module ( module ) Avoid too many requests to NCBI E-Utils in the test_pubmed.py, which is executed following this module. E-Utility requests are capped at 3 per second, which is usually controled by _get_eutils_rate_limiter, but this does not seem to work across test modules. View Source def teardown_module ( module ): \"\"\" Avoid too many requests to NCBI E-Utils in the test_pubmed.py, which is executed following this module. E-Utility requests are capped at 3 per second, which is usually controled by _get_eutils_rate_limiter, but this does not seem to work across test modules. \"\"\" import time time . sleep ( 1 ) test_cite_command_bibliography def test_cite_command_bibliography ( ) View Source def test_cite_command_bibliography (): bib_dir = pathlib . Path ( __file__ ). parent . parent . parent . joinpath ( \"pandoc/tests/bibliographies\" ) args = [ \"manubot\" , \"cite\" , \"--bibliography=bibliography.json\" , \"DOI:10.7554/elife.32822\" , ] csl_json = subprocess . check_output ( args , encoding = \"utf-8\" , cwd = bib_dir ) csl_items = json . loads ( csl_json ) assert len ( csl_items ) == 1 csl_item = csl_items [ 0 ] assert \"manual_reference_filename: bibliography.json\" in csl_item [ \"note\" ] test_cite_command_empty def test_cite_command_empty ( ) View Source def test_cite_command_empty (): process = subprocess . run ( [ \"manubot\" , \"cite\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ) print ( process . stderr ) assert process . returncode == 2 assert \"the following arguments are required: citekeys\" in process . stderr test_cite_command_file def test_cite_command_file ( tmpdir ) View Source def test_cite_command_file ( tmpdir ): path = pathlib . Path ( tmpdir ) / \"csl-items.json\" process = subprocess . run ( [ \"manubot\" , \"cite\" , \"--output\" , str ( path ), \"arxiv:1806.05726v1\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , ) print ( process . stderr . decode ()) assert process . returncode == 0 with path . open () as read_file : ( csl ,) = json . load ( read_file ) assert csl [ \"URL\" ] == \"https://arxiv.org/abs/1806.05726v1\" test_cite_command_render_stdout def test_cite_command_render_stdout ( args , filename ) Test the stdout output of manubot cite --render with various formats. The output is sensitive to the version of Pandoc used, so expected output files include the pandoc version stamp in their filename. When the expected version is missing, the test fails but writes the command output to that file. Therefore, subsequent runs of the same test will pass. Before committing the auto-generated output, do look to ensure its integrity. This test uses --bibliography to avoid slow network calls. Regenerate the CSL JSON using: manubot cite --output = manubot/cite/tests/cite-command-rendered/input-references.json arxiv:1806.05726v1 doi:10.7717/peerj.338 pmid:29618526 View Source @pytest . mark . parametrize ( [ \"args\", \"filename\" ] , [ pytest.param([ ] , \"references-plain-{}.txt\" , id = \"no-args\" ), pytest . param ( [ \"--format\", \"plain\" ] , \"references-plain-{}.txt\" , id = \"--format=plain\" ), pytest . param ( [ \"--format\", \"markdown\" ] , \"references-markdown-{}.md\" , id = \"--format=markdown\" , ), pytest . param ( [ \"--format\", \"html\" ] , \"references-html-{}.html\" , id = \"--format=html\" ), pytest . param ( [ \"--format\", \"jats\" ] , \"references-jats-{}.xml\" , id = \"--format=jats\" ), ] , ) @pytest . mark . skipif ( not shutil . which ( \"pandoc\" ), reason = \"pandoc installation not found on system\" ) @pytest . mark . skipif ( not shutil . which ( \"pandoc-citeproc\" ), reason = \"pandoc-citeproc installation not found on system\" , ) @pytest . mark . pandoc_version_sensitive def test_cite_command_render_stdout ( args , filename ) : \"\"\" Test the stdout output of `manubot cite --render` with various formats. The output is sensitive to the version of Pandoc used, so expected output files include the pandoc version stamp in their filename. When the expected version is missing, the test fails but writes the command output to that file. Therefore, subsequent runs of the same test will pass. Before committing the auto-generated output, do look to ensure its integrity. This test uses --bibliography to avoid slow network calls. Regenerate the CSL JSON using: ```shell manubot cite \\ --output=manubot/cite/tests/cite-command-rendered/input-references.json \\ arxiv:1806.05726v1 doi:10.7717/peerj.338 pmid:29618526 ``` \"\"\" # get pandoc version info pandoc_version = get_pandoc_version () pandoc_stamp = \".\" . join ( map ( str , pandoc_version )) data_dir = pathlib . Path ( __file__ ). parent . joinpath ( \"cite-command-rendered\" ) path = data_dir . joinpath ( filename . format ( pandoc_stamp )) # skip test on old pandoc versions for output in \"markdown\" , \"html\" , \"jats\" : if output in args and pandoc_version < ( 2 , 5 ) : pytest . skip ( f \"Test {output} output assumes pandoc >= 2.5\" ) if pandoc_version < ( 2 , 0 ) : pytest . skip ( \"Test requires pandoc >= 2.0 to support --lua-filter and --csl=URL\" ) args = [ \"manubot\", \"cite\", \"--bibliography=input-references.json\", \"--render\", \"--csl=https://github.com/greenelab/manubot-rootstock/raw/e83e51dcd89256403bb787c3d9a46e4ee8d04a9e/build/assets/style.csl\", \"arxiv:1806.05726v1\", \"doi:10.7717/peerj.338\", \"pmid:29618526\", ] + args process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , cwd = data_dir , ) print ( shlex_join ( process . args )) if not path . exists () : # https : // github . com / manubot / manubot / pull / 146 #discussion_r333132261 print ( f \"Missing expected output at {path}\\n\" \"Writing output to file such that future tests will pass.\" ) path . write_text ( process . stdout , encoding = \"utf-8\" ) assert False print ( process . stdout ) print ( process . stderr ) expected = path . read_text ( encoding = \"utf-8-sig\" ) assert process . stdout == expected test_cite_command_stdout def test_cite_command_stdout ( ) View Source def test_cite_command_stdout (): process = subprocess . run ( [ \"manubot\" , \"cite\" , \"arxiv:1806.05726v1\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ) print ( process . stderr ) assert process . returncode == 0 ( csl ,) = json . loads ( process . stdout ) assert csl [ \"URL\" ] == \"https://arxiv.org/abs/1806.05726v1\"","title":"Test Cite Command"},{"location":"reference/manubot/cite/tests/test_cite_command/#module-manubotciteteststest_cite_command","text":"View Source import json import pathlib import shutil import subprocess import pytest from manubot.util import shlex_join from manubot.pandoc.util import get_pandoc_version def test_cite_command_empty (): process = subprocess . run ( [ \"manubot\" , \"cite\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ) print ( process . stderr ) assert process . returncode == 2 assert \"the following arguments are required: citekeys\" in process . stderr def test_cite_command_stdout (): process = subprocess . run ( [ \"manubot\" , \"cite\" , \"arxiv:1806.05726v1\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ) print ( process . stderr ) assert process . returncode == 0 ( csl ,) = json . loads ( process . stdout ) assert csl [ \"URL\" ] == \"https://arxiv.org/abs/1806.05726v1\" def test_cite_command_file ( tmpdir ): path = pathlib . Path ( tmpdir ) / \"csl-items.json\" process = subprocess . run ( [ \"manubot\" , \"cite\" , \"--output\" , str ( path ), \"arxiv:1806.05726v1\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , ) print ( process . stderr . decode ()) assert process . returncode == 0 with path . open () as read_file : ( csl ,) = json . load ( read_file ) assert csl [ \"URL\" ] == \"https://arxiv.org/abs/1806.05726v1\" @pytest.mark.parametrize ( [ \"args\" , \"filename\" ], [ pytest . param ([], \"references-plain-{}.txt\" , id = \"no-args\" ), pytest . param ( [ \"--format\" , \"plain\" ], \"references-plain-{}.txt\" , id = \"--format=plain\" ), pytest . param ( [ \"--format\" , \"markdown\" ], \"references-markdown-{}.md\" , id = \"--format=markdown\" , ), pytest . param ( [ \"--format\" , \"html\" ], \"references-html-{}.html\" , id = \"--format=html\" ), pytest . param ( [ \"--format\" , \"jats\" ], \"references-jats-{}.xml\" , id = \"--format=jats\" ), ], ) @pytest.mark.skipif ( not shutil . which ( \"pandoc\" ), reason = \"pandoc installation not found on system\" ) @pytest.mark.skipif ( not shutil . which ( \"pandoc-citeproc\" ), reason = \"pandoc-citeproc installation not found on system\" , ) @pytest.mark.pandoc_version_sensitive def test_cite_command_render_stdout ( args , filename ): \"\"\" Test the stdout output of `manubot cite --render` with various formats. The output is sensitive to the version of Pandoc used, so expected output files include the pandoc version stamp in their filename. When the expected version is missing, the test fails but writes the command output to that file. Therefore, subsequent runs of the same test will pass. Before committing the auto-generated output, do look to ensure its integrity. This test uses --bibliography to avoid slow network calls. Regenerate the CSL JSON using: ```shell manubot cite \\ --output=manubot/cite/tests/cite-command-rendered/input-references.json \\ arxiv:1806.05726v1 doi:10.7717/peerj.338 pmid:29618526 ``` \"\"\" # get pandoc version info pandoc_version = get_pandoc_version () pandoc_stamp = \".\" . join ( map ( str , pandoc_version )) data_dir = pathlib . Path ( __file__ ) . parent . joinpath ( \"cite-command-rendered\" ) path = data_dir . joinpath ( filename . format ( pandoc_stamp )) # skip test on old pandoc versions for output in \"markdown\" , \"html\" , \"jats\" : if output in args and pandoc_version < ( 2 , 5 ): pytest . skip ( f \"Test {output} output assumes pandoc >= 2.5\" ) if pandoc_version < ( 2 , 0 ): pytest . skip ( \"Test requires pandoc >= 2.0 to support --lua-filter and --csl=URL\" ) args = [ \"manubot\" , \"cite\" , \"--bibliography=input-references.json\" , \"--render\" , \"--csl=https://github.com/greenelab/manubot-rootstock/raw/e83e51dcd89256403bb787c3d9a46e4ee8d04a9e/build/assets/style.csl\" , \"arxiv:1806.05726v1\" , \"doi:10.7717/peerj.338\" , \"pmid:29618526\" , ] + args process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , cwd = data_dir , ) print ( shlex_join ( process . args )) if not path . exists (): # https://github.com/manubot/manubot/pull/146#discussion_r333132261 print ( f \"Missing expected output at {path} \\n \" \"Writing output to file such that future tests will pass.\" ) path . write_text ( process . stdout , encoding = \"utf-8\" ) assert False print ( process . stdout ) print ( process . stderr ) expected = path . read_text ( encoding = \"utf-8-sig\" ) assert process . stdout == expected def test_cite_command_bibliography (): bib_dir = pathlib . Path ( __file__ ) . parent . parent . parent . joinpath ( \"pandoc/tests/bibliographies\" ) args = [ \"manubot\" , \"cite\" , \"--bibliography=bibliography.json\" , \"DOI:10.7554/elife.32822\" , ] csl_json = subprocess . check_output ( args , encoding = \"utf-8\" , cwd = bib_dir ) csl_items = json . loads ( csl_json ) assert len ( csl_items ) == 1 csl_item = csl_items [ 0 ] assert \"manual_reference_filename: bibliography.json\" in csl_item [ \"note\" ] def teardown_module ( module ): \"\"\" Avoid too many requests to NCBI E-Utils in the test_pubmed.py, which is executed following this module. E-Utility requests are capped at 3 per second, which is usually controled by _get_eutils_rate_limiter, but this does not seem to work across test modules. \"\"\" import time time . sleep ( 1 )","title":"Module manubot.cite.tests.test_cite_command"},{"location":"reference/manubot/cite/tests/test_cite_command/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_cite_command/#teardown_module","text":"def teardown_module ( module ) Avoid too many requests to NCBI E-Utils in the test_pubmed.py, which is executed following this module. E-Utility requests are capped at 3 per second, which is usually controled by _get_eutils_rate_limiter, but this does not seem to work across test modules. View Source def teardown_module ( module ): \"\"\" Avoid too many requests to NCBI E-Utils in the test_pubmed.py, which is executed following this module. E-Utility requests are capped at 3 per second, which is usually controled by _get_eutils_rate_limiter, but this does not seem to work across test modules. \"\"\" import time time . sleep ( 1 )","title":"teardown_module"},{"location":"reference/manubot/cite/tests/test_cite_command/#test_cite_command_bibliography","text":"def test_cite_command_bibliography ( ) View Source def test_cite_command_bibliography (): bib_dir = pathlib . Path ( __file__ ). parent . parent . parent . joinpath ( \"pandoc/tests/bibliographies\" ) args = [ \"manubot\" , \"cite\" , \"--bibliography=bibliography.json\" , \"DOI:10.7554/elife.32822\" , ] csl_json = subprocess . check_output ( args , encoding = \"utf-8\" , cwd = bib_dir ) csl_items = json . loads ( csl_json ) assert len ( csl_items ) == 1 csl_item = csl_items [ 0 ] assert \"manual_reference_filename: bibliography.json\" in csl_item [ \"note\" ]","title":"test_cite_command_bibliography"},{"location":"reference/manubot/cite/tests/test_cite_command/#test_cite_command_empty","text":"def test_cite_command_empty ( ) View Source def test_cite_command_empty (): process = subprocess . run ( [ \"manubot\" , \"cite\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ) print ( process . stderr ) assert process . returncode == 2 assert \"the following arguments are required: citekeys\" in process . stderr","title":"test_cite_command_empty"},{"location":"reference/manubot/cite/tests/test_cite_command/#test_cite_command_file","text":"def test_cite_command_file ( tmpdir ) View Source def test_cite_command_file ( tmpdir ): path = pathlib . Path ( tmpdir ) / \"csl-items.json\" process = subprocess . run ( [ \"manubot\" , \"cite\" , \"--output\" , str ( path ), \"arxiv:1806.05726v1\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , ) print ( process . stderr . decode ()) assert process . returncode == 0 with path . open () as read_file : ( csl ,) = json . load ( read_file ) assert csl [ \"URL\" ] == \"https://arxiv.org/abs/1806.05726v1\"","title":"test_cite_command_file"},{"location":"reference/manubot/cite/tests/test_cite_command/#test_cite_command_render_stdout","text":"def test_cite_command_render_stdout ( args , filename ) Test the stdout output of manubot cite --render with various formats. The output is sensitive to the version of Pandoc used, so expected output files include the pandoc version stamp in their filename. When the expected version is missing, the test fails but writes the command output to that file. Therefore, subsequent runs of the same test will pass. Before committing the auto-generated output, do look to ensure its integrity. This test uses --bibliography to avoid slow network calls. Regenerate the CSL JSON using: manubot cite --output = manubot/cite/tests/cite-command-rendered/input-references.json arxiv:1806.05726v1 doi:10.7717/peerj.338 pmid:29618526 View Source @pytest . mark . parametrize ( [ \"args\", \"filename\" ] , [ pytest.param([ ] , \"references-plain-{}.txt\" , id = \"no-args\" ), pytest . param ( [ \"--format\", \"plain\" ] , \"references-plain-{}.txt\" , id = \"--format=plain\" ), pytest . param ( [ \"--format\", \"markdown\" ] , \"references-markdown-{}.md\" , id = \"--format=markdown\" , ), pytest . param ( [ \"--format\", \"html\" ] , \"references-html-{}.html\" , id = \"--format=html\" ), pytest . param ( [ \"--format\", \"jats\" ] , \"references-jats-{}.xml\" , id = \"--format=jats\" ), ] , ) @pytest . mark . skipif ( not shutil . which ( \"pandoc\" ), reason = \"pandoc installation not found on system\" ) @pytest . mark . skipif ( not shutil . which ( \"pandoc-citeproc\" ), reason = \"pandoc-citeproc installation not found on system\" , ) @pytest . mark . pandoc_version_sensitive def test_cite_command_render_stdout ( args , filename ) : \"\"\" Test the stdout output of `manubot cite --render` with various formats. The output is sensitive to the version of Pandoc used, so expected output files include the pandoc version stamp in their filename. When the expected version is missing, the test fails but writes the command output to that file. Therefore, subsequent runs of the same test will pass. Before committing the auto-generated output, do look to ensure its integrity. This test uses --bibliography to avoid slow network calls. Regenerate the CSL JSON using: ```shell manubot cite \\ --output=manubot/cite/tests/cite-command-rendered/input-references.json \\ arxiv:1806.05726v1 doi:10.7717/peerj.338 pmid:29618526 ``` \"\"\" # get pandoc version info pandoc_version = get_pandoc_version () pandoc_stamp = \".\" . join ( map ( str , pandoc_version )) data_dir = pathlib . Path ( __file__ ). parent . joinpath ( \"cite-command-rendered\" ) path = data_dir . joinpath ( filename . format ( pandoc_stamp )) # skip test on old pandoc versions for output in \"markdown\" , \"html\" , \"jats\" : if output in args and pandoc_version < ( 2 , 5 ) : pytest . skip ( f \"Test {output} output assumes pandoc >= 2.5\" ) if pandoc_version < ( 2 , 0 ) : pytest . skip ( \"Test requires pandoc >= 2.0 to support --lua-filter and --csl=URL\" ) args = [ \"manubot\", \"cite\", \"--bibliography=input-references.json\", \"--render\", \"--csl=https://github.com/greenelab/manubot-rootstock/raw/e83e51dcd89256403bb787c3d9a46e4ee8d04a9e/build/assets/style.csl\", \"arxiv:1806.05726v1\", \"doi:10.7717/peerj.338\", \"pmid:29618526\", ] + args process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , cwd = data_dir , ) print ( shlex_join ( process . args )) if not path . exists () : # https : // github . com / manubot / manubot / pull / 146 #discussion_r333132261 print ( f \"Missing expected output at {path}\\n\" \"Writing output to file such that future tests will pass.\" ) path . write_text ( process . stdout , encoding = \"utf-8\" ) assert False print ( process . stdout ) print ( process . stderr ) expected = path . read_text ( encoding = \"utf-8-sig\" ) assert process . stdout == expected","title":"test_cite_command_render_stdout"},{"location":"reference/manubot/cite/tests/test_cite_command/#test_cite_command_stdout","text":"def test_cite_command_stdout ( ) View Source def test_cite_command_stdout (): process = subprocess . run ( [ \"manubot\" , \"cite\" , \"arxiv:1806.05726v1\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ) print ( process . stderr ) assert process . returncode == 0 ( csl ,) = json . loads ( process . stdout ) assert csl [ \"URL\" ] == \"https://arxiv.org/abs/1806.05726v1\"","title":"test_cite_command_stdout"},{"location":"reference/manubot/cite/tests/test_citekey/","text":"Module manubot.cite.tests.test_citekey Tests rest of functions in manubot.cite, not covered by test_citekey_api.py. View Source \"\"\"Tests rest of functions in manubot.cite, not covered by test_citekey_api.py.\"\"\" import pytest from manubot.cite.citekey import ( CiteKey , shorten_citekey , url_to_citekey , ) @pytest.mark.parametrize ( [ \"input_id\" , \"citekey_attrs\" ], [ ( \"DOI:10.5061/DRYad.q447c/1\" , dict ( prefix_lower = \"doi\" , standard_accession = \"10.5061/dryad.q447c/1\" , standard_id = \"doi:10.5061/dryad.q447c/1\" , ), ), ( \"arXiv:1407.3561v1\" , dict ( prefix_lower = \"arxiv\" , standard_id = \"arxiv:1407.3561v1\" ,), ), ( \"pmid:24159271\" , dict ( standard_id = \"pubmed:24159271\" ,),), ( \"pmcid:PMC4304851\" , dict ( standard_id = \"pmc:PMC4304851\" ,),), ( \"https://greenelab.github.io/manubot-rootstock/\" , dict ( standard_id = \"url:https://greenelab.github.io/manubot-rootstock/\" ,), ), ( \"isbn:1-339-91988-5\" , dict ( standard_id = \"isbn:9781339919881\" ,),), ( \"DOID:14330\" , dict ( standard_id = \"doid:14330\" ,),), ( \"PubChem.substance:100101\" , dict ( standard_id = \"pubchem.substance:100101\" ,),), ( \"Wikidata:Q50051684\" , dict ( standard_id = \"wikidata:Q50051684\" ,),), ], ) def test_citekey_class ( input_id , citekey_attrs ): citekey = CiteKey ( input_id ) print ( citekey ) for key , value in citekey_attrs . items (): assert getattr ( citekey , key ) == value assert citekey . short_id @pytest.mark.parametrize ( \"standard_citekey,expected\" , [ ( \"doi:10.5061/dryad.q447c/1\" , \"kQFQ8EaO\" ), ( \"arxiv:1407.3561v1\" , \"16kozZ9Ys\" ), ( \"pmid:24159271\" , \"11sli93ov\" ), ( \"url:http://blog.dhimmel.com/irreproducible-timestamps/\" , \"QBWMEuxW\" ), ], ) def test_shorten_citekey ( standard_citekey , expected ): short_citekey = shorten_citekey ( standard_citekey ) assert short_citekey == expected @pytest.mark.parametrize ( \"citekey\" , [ \"doi:10.7717/peerj.705\" , \"doi:10/b6vnmd\" , \"pmcid:PMC4304851\" , \"pmid:25648772\" , \"arxiv:1407.3561\" , \"arxiv:1407.3561v1\" , \"arxiv:math.GT/0309136\" , \"arxiv:math.GT/0309136v1\" , \"arxiv:hep-th/9305059\" , \"arxiv:hep-th/9305059v2\" , \"isbn:978-1-339-91988-1\" , \"isbn:1-339-91988-5\" , \"wikidata:Q1\" , \"wikidata:Q50051684\" , \"url:https://peerj.com/articles/705/\" , ], ) def test_inspect_citekey_passes ( citekey ): \"\"\" These citekeys should pass inspection by inspect_citekey. \"\"\" report = CiteKey ( citekey ) . inspect () assert report is None @pytest.mark.parametrize ( [ \"citekey\" , \"contains\" ], [ ( \"doi:10.771/peerj.705\" , \"Double check the DOI\" ), ( \"doi:10/b6v_nmd\" , \"Double check the shortDOI\" ), ( \"doi:7717/peerj.705\" , \"must start with '10.'\" ), ( \"doi:b6vnmd\" , \"must start with '10.'\" ), ( \"pmcid:25648772\" , \"must start with 'PMC'\" ), ( \"pmid:PMC4304851\" , \"Should 'pmid:PMC4304851' switch the citation source to 'pmc'?\" , ), ( \"isbn:1-339-91988-X\" , \"identifier violates the ISBN syntax\" ), ( \"wikidata:P212\" , \"item IDs must start with 'Q'\" ), ( \"wikidata:QABCD\" , \"does not conform to the Wikidata regex\" ), ( \"arxiv:YYMM.number\" , \"must conform to syntax\" ), ], ) def test_inspect_citekey_fails ( citekey , contains ): \"\"\" These citekeys should fail inspection by inspect_citekey. \"\"\" report = CiteKey ( citekey ) . inspect () assert report is not None assert isinstance ( report , str ) assert contains in report @pytest.mark.parametrize ( [ \"url\" , \"citekey\" ], [ ( \"https://www.doi.org/\" , \"url:https://www.doi.org/\" ,), ( \"https://www.doi.org/factsheets/Identifier_Interoper.html\" , \"url:https://www.doi.org/factsheets/Identifier_Interoper.html\" , ), ( \"https://doi.org/10.1097 %2F 00004032-200403000-00012\" , \"doi:10.1097/00004032-200403000-00012\" , ), ( \"http://dx.doi.org/10.7554/eLife.46574\" , \"doi:10.7554/eLife.46574\" ), ( \"https://doi.org/10/b6vnmd#anchor\" , \"doi:10/b6vnmd\" ), # ShortDOI URLs without `10/` prefix not yet supported` ( \"https://doi.org/b6vnmd\" , \"url:https://doi.org/b6vnmd\" ), ( \"https://www.biorxiv.org/about-biorxiv\" , \"url:https://www.biorxiv.org/about-biorxiv\" , ), ( \"https://sci-hub.tw/10.1038/nature19057\" , \"doi:10.1038/nature19057\" ), ( \"https://www.biorxiv.org/content/10.1101/087619v3\" , \"doi:10.1101/087619\" ), ( \"https://www.biorxiv.org/content/10.1101/087619v3.full\" , \"doi:10.1101/087619\" ), ( \"https://www.biorxiv.org/content/early/2017/08/31/087619.full.pdf\" , \"doi:10.1101/087619\" , ), ( \"https://www.biorxiv.org/content/10.1101/2019.12.11.872580v1\" , \"doi:10.1101/2019.12.11.872580\" , ), ( \"https://www.biorxiv.org/content/10.1101/2019.12.11.872580v1.full.pdf+html\" , \"doi:10.1101/2019.12.11.872580\" , ), ( \"https://www.biorxiv.org/content/10.1101/2019.12.11.872580v1.full.pdf\" , \"doi:10.1101/2019.12.11.872580\" , ), ( \"https://www.ncbi.nlm.nih.gov\" , \"url:https://www.ncbi.nlm.nih.gov\" ), ( \"https://www.ncbi.nlm.nih.gov/pubmed\" , \"url:https://www.ncbi.nlm.nih.gov/pubmed\" , ), ( \"https://www.ncbi.nlm.nih.gov/pubmed/31233491\" , \"pmid:31233491\" ), ( \"https://www.ncbi.nlm.nih.gov/pmc/\" , \"url:https://www.ncbi.nlm.nih.gov/pmc/\" ), ( \"https://www.ncbi.nlm.nih.gov/pmc/about/intro/\" , \"url:https://www.ncbi.nlm.nih.gov/pmc/about/intro/\" , ), ( \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4304851/\" , \"pmcid:PMC4304851\" ), ( \"https://www.wikidata.org/wiki/Wikidata:Main_Page\" , \"url:https://www.wikidata.org/wiki/Wikidata:Main_Page\" , ), ( \"https://www.wikidata.org/wiki/Q50051684\" , \"wikidata:Q50051684\" ), ( \"https://arxiv.org/\" , \"url:https://arxiv.org/\" ), ( \"https://arxiv.org/list/q-fin/recent\" , \"url:https://arxiv.org/list/q-fin/recent\" , ), ( \"https://arxiv.org/abs/1912.03529v1\" , \"arxiv:1912.03529v1\" ), ( \"https://arxiv.org/pdf/1912.03529v1.pdf\" , \"arxiv:1912.03529v1\" ), ( \"https://arxiv.org/ps/1912.03529v1\" , \"arxiv:1912.03529v1\" ), ( \"https://arxiv.org/abs/math.GT/0309136\" , \"arxiv:math.GT/0309136\" ), ( \"https://arxiv.org/abs/hep-th/9305059\" , \"arxiv:hep-th/9305059\" ), ( \"https://arxiv.org/pdf/hep-th/9305059.pdf\" , \"arxiv:hep-th/9305059\" ), ], ) def test_url_to_citekey ( url , citekey ): assert url_to_citekey ( url ) == citekey Functions test_citekey_class def test_citekey_class ( input_id , citekey_attrs ) View Source @pytest . mark . parametrize ( [ \"input_id\", \"citekey_attrs\" ] , [ ( \"DOI:10.5061/DRYad.q447c/1\", dict( prefix_lower=\"doi\", standard_accession=\"10.5061/dryad.q447c/1\", standard_id=\"doi:10.5061/dryad.q447c/1\", ), ), ( \"arXiv:1407.3561v1\", dict(prefix_lower=\"arxiv\", standard_id=\"arxiv:1407.3561v1\",), ), (\"pmid:24159271\", dict(standard_id=\"pubmed:24159271\",),), (\"pmcid:PMC4304851\", dict(standard_id=\"pmc:PMC4304851\",),), ( \"https://greenelab.github.io/manubot-rootstock/\", dict(standard_id=\"url:https://greenelab.github.io/manubot-rootstock/\",), ), (\"isbn:1-339-91988-5\", dict(standard_id=\"isbn:9781339919881\",),), (\"DOID:14330\", dict(standard_id=\"doid:14330\",),), (\"PubChem.substance:100101\", dict(standard_id=\"pubchem.substance:100101\",),), (\"Wikidata:Q50051684\", dict(standard_id=\"wikidata:Q50051684\",),), ] , ) def test_citekey_class ( input_id , citekey_attrs ) : citekey = CiteKey ( input_id ) print ( citekey ) for key , value in citekey_attrs . items () : assert getattr ( citekey , key ) == value assert citekey . short_id test_inspect_citekey_fails def test_inspect_citekey_fails ( citekey , contains ) These citekeys should fail inspection by inspect_citekey. View Source @pytest . mark . parametrize ( [ \"citekey\", \"contains\" ] , [ (\"doi:10.771/peerj.705\", \"Double check the DOI\"), (\"doi:10/b6v_nmd\", \"Double check the shortDOI\"), (\"doi:7717/peerj.705\", \"must start with '10.'\"), (\"doi:b6vnmd\", \"must start with '10.'\"), (\"pmcid:25648772\", \"must start with 'PMC'\"), ( \"pmid:PMC4304851\", \"Should 'pmid:PMC4304851' switch the citation source to 'pmc'?\", ), (\"isbn:1-339-91988-X\", \"identifier violates the ISBN syntax\"), (\"wikidata:P212\", \"item IDs must start with 'Q'\"), (\"wikidata:QABCD\", \"does not conform to the Wikidata regex\"), (\"arxiv:YYMM.number\", \"must conform to syntax\"), ] , ) def test_inspect_citekey_fails ( citekey , contains ) : \"\"\" These citekeys should fail inspection by inspect_citekey. \"\"\" report = CiteKey ( citekey ). inspect () assert report is not None assert isinstance ( report , str ) assert contains in report test_inspect_citekey_passes def test_inspect_citekey_passes ( citekey ) These citekeys should pass inspection by inspect_citekey. View Source @pytest . mark . parametrize ( \"citekey\" , [ \"doi:10.7717/peerj.705\", \"doi:10/b6vnmd\", \"pmcid:PMC4304851\", \"pmid:25648772\", \"arxiv:1407.3561\", \"arxiv:1407.3561v1\", \"arxiv:math.GT/0309136\", \"arxiv:math.GT/0309136v1\", \"arxiv:hep-th/9305059\", \"arxiv:hep-th/9305059v2\", \"isbn:978-1-339-91988-1\", \"isbn:1-339-91988-5\", \"wikidata:Q1\", \"wikidata:Q50051684\", \"url:https://peerj.com/articles/705/\", ] , ) def test_inspect_citekey_passes ( citekey ) : \"\"\" These citekeys should pass inspection by inspect_citekey. \"\"\" report = CiteKey ( citekey ). inspect () assert report is None test_shorten_citekey def test_shorten_citekey ( standard_citekey , expected ) View Source @pytest . mark . parametrize ( \"standard_citekey,expected\" , [ (\"doi:10.5061/dryad.q447c/1\", \"kQFQ8EaO\"), (\"arxiv:1407.3561v1\", \"16kozZ9Ys\"), (\"pmid:24159271\", \"11sli93ov\"), (\"url:http://blog.dhimmel.com/irreproducible-timestamps/\", \"QBWMEuxW\"), ] , ) def test_shorten_citekey ( standard_citekey , expected ) : short_citekey = shorten_citekey ( standard_citekey ) assert short_citekey == expected test_url_to_citekey def test_url_to_citekey ( url , citekey ) View Source @pytest . mark . parametrize ( [ \"url\", \"citekey\" ] , [ (\"https://www.doi.org/\", \"url:https://www.doi.org/\",), ( \"https://www.doi.org/factsheets/Identifier_Interoper.html\", \"url:https://www.doi.org/factsheets/Identifier_Interoper.html\", ), ( \"https://doi.org/10.1097%2F00004032-200403000-00012\", \"doi:10.1097/00004032-200403000-00012\", ), (\"http://dx.doi.org/10.7554/eLife.46574\", \"doi:10.7554/eLife.46574\"), (\"https://doi.org/10/b6vnmd#anchor\", \"doi:10/b6vnmd\"), # ShortDOI URLs without `10/` prefix not yet supported` (\"https://doi.org/b6vnmd\", \"url:https://doi.org/b6vnmd\"), ( \"https://www.biorxiv.org/about-biorxiv\", \"url:https://www.biorxiv.org/about-biorxiv\", ), (\"https://sci-hub.tw/10.1038/nature19057\", \"doi:10.1038/nature19057\"), (\"https://www.biorxiv.org/content/10.1101/087619v3\", \"doi:10.1101/087619\"), (\"https://www.biorxiv.org/content/10.1101/087619v3.full\", \"doi:10.1101/087619\"), ( \"https://www.biorxiv.org/content/early/2017/08/31/087619.full.pdf\", \"doi:10.1101/087619\", ), ( \"https://www.biorxiv.org/content/10.1101/2019.12.11.872580v1\", \"doi:10.1101/2019.12.11.872580\", ), ( \"https://www.biorxiv.org/content/10.1101/2019.12.11.872580v1.full.pdf+html\", \"doi:10.1101/2019.12.11.872580\", ), ( \"https://www.biorxiv.org/content/10.1101/2019.12.11.872580v1.full.pdf\", \"doi:10.1101/2019.12.11.872580\", ), (\"https://www.ncbi.nlm.nih.gov\", \"url:https://www.ncbi.nlm.nih.gov\"), ( \"https://www.ncbi.nlm.nih.gov/pubmed\", \"url:https://www.ncbi.nlm.nih.gov/pubmed\", ), (\"https://www.ncbi.nlm.nih.gov/pubmed/31233491\", \"pmid:31233491\"), (\"https://www.ncbi.nlm.nih.gov/pmc/\", \"url:https://www.ncbi.nlm.nih.gov/pmc/\"), ( \"https://www.ncbi.nlm.nih.gov/pmc/about/intro/\", \"url:https://www.ncbi.nlm.nih.gov/pmc/about/intro/\", ), (\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4304851/\", \"pmcid:PMC4304851\"), ( \"https://www.wikidata.org/wiki/Wikidata:Main_Page\", \"url:https://www.wikidata.org/wiki/Wikidata:Main_Page\", ), (\"https://www.wikidata.org/wiki/Q50051684\", \"wikidata:Q50051684\"), (\"https://arxiv.org/\", \"url:https://arxiv.org/\"), ( \"https://arxiv.org/list/q-fin/recent\", \"url:https://arxiv.org/list/q-fin/recent\", ), (\"https://arxiv.org/abs/1912.03529v1\", \"arxiv:1912.03529v1\"), (\"https://arxiv.org/pdf/1912.03529v1.pdf\", \"arxiv:1912.03529v1\"), (\"https://arxiv.org/ps/1912.03529v1\", \"arxiv:1912.03529v1\"), (\"https://arxiv.org/abs/math.GT/0309136\", \"arxiv:math.GT/0309136\"), (\"https://arxiv.org/abs/hep-th/9305059\", \"arxiv:hep-th/9305059\"), (\"https://arxiv.org/pdf/hep-th/9305059.pdf\", \"arxiv:hep-th/9305059\"), ] , ) def test_url_to_citekey ( url , citekey ) : assert url_to_citekey ( url ) == citekey","title":"Test Citekey"},{"location":"reference/manubot/cite/tests/test_citekey/#module-manubotciteteststest_citekey","text":"Tests rest of functions in manubot.cite, not covered by test_citekey_api.py. View Source \"\"\"Tests rest of functions in manubot.cite, not covered by test_citekey_api.py.\"\"\" import pytest from manubot.cite.citekey import ( CiteKey , shorten_citekey , url_to_citekey , ) @pytest.mark.parametrize ( [ \"input_id\" , \"citekey_attrs\" ], [ ( \"DOI:10.5061/DRYad.q447c/1\" , dict ( prefix_lower = \"doi\" , standard_accession = \"10.5061/dryad.q447c/1\" , standard_id = \"doi:10.5061/dryad.q447c/1\" , ), ), ( \"arXiv:1407.3561v1\" , dict ( prefix_lower = \"arxiv\" , standard_id = \"arxiv:1407.3561v1\" ,), ), ( \"pmid:24159271\" , dict ( standard_id = \"pubmed:24159271\" ,),), ( \"pmcid:PMC4304851\" , dict ( standard_id = \"pmc:PMC4304851\" ,),), ( \"https://greenelab.github.io/manubot-rootstock/\" , dict ( standard_id = \"url:https://greenelab.github.io/manubot-rootstock/\" ,), ), ( \"isbn:1-339-91988-5\" , dict ( standard_id = \"isbn:9781339919881\" ,),), ( \"DOID:14330\" , dict ( standard_id = \"doid:14330\" ,),), ( \"PubChem.substance:100101\" , dict ( standard_id = \"pubchem.substance:100101\" ,),), ( \"Wikidata:Q50051684\" , dict ( standard_id = \"wikidata:Q50051684\" ,),), ], ) def test_citekey_class ( input_id , citekey_attrs ): citekey = CiteKey ( input_id ) print ( citekey ) for key , value in citekey_attrs . items (): assert getattr ( citekey , key ) == value assert citekey . short_id @pytest.mark.parametrize ( \"standard_citekey,expected\" , [ ( \"doi:10.5061/dryad.q447c/1\" , \"kQFQ8EaO\" ), ( \"arxiv:1407.3561v1\" , \"16kozZ9Ys\" ), ( \"pmid:24159271\" , \"11sli93ov\" ), ( \"url:http://blog.dhimmel.com/irreproducible-timestamps/\" , \"QBWMEuxW\" ), ], ) def test_shorten_citekey ( standard_citekey , expected ): short_citekey = shorten_citekey ( standard_citekey ) assert short_citekey == expected @pytest.mark.parametrize ( \"citekey\" , [ \"doi:10.7717/peerj.705\" , \"doi:10/b6vnmd\" , \"pmcid:PMC4304851\" , \"pmid:25648772\" , \"arxiv:1407.3561\" , \"arxiv:1407.3561v1\" , \"arxiv:math.GT/0309136\" , \"arxiv:math.GT/0309136v1\" , \"arxiv:hep-th/9305059\" , \"arxiv:hep-th/9305059v2\" , \"isbn:978-1-339-91988-1\" , \"isbn:1-339-91988-5\" , \"wikidata:Q1\" , \"wikidata:Q50051684\" , \"url:https://peerj.com/articles/705/\" , ], ) def test_inspect_citekey_passes ( citekey ): \"\"\" These citekeys should pass inspection by inspect_citekey. \"\"\" report = CiteKey ( citekey ) . inspect () assert report is None @pytest.mark.parametrize ( [ \"citekey\" , \"contains\" ], [ ( \"doi:10.771/peerj.705\" , \"Double check the DOI\" ), ( \"doi:10/b6v_nmd\" , \"Double check the shortDOI\" ), ( \"doi:7717/peerj.705\" , \"must start with '10.'\" ), ( \"doi:b6vnmd\" , \"must start with '10.'\" ), ( \"pmcid:25648772\" , \"must start with 'PMC'\" ), ( \"pmid:PMC4304851\" , \"Should 'pmid:PMC4304851' switch the citation source to 'pmc'?\" , ), ( \"isbn:1-339-91988-X\" , \"identifier violates the ISBN syntax\" ), ( \"wikidata:P212\" , \"item IDs must start with 'Q'\" ), ( \"wikidata:QABCD\" , \"does not conform to the Wikidata regex\" ), ( \"arxiv:YYMM.number\" , \"must conform to syntax\" ), ], ) def test_inspect_citekey_fails ( citekey , contains ): \"\"\" These citekeys should fail inspection by inspect_citekey. \"\"\" report = CiteKey ( citekey ) . inspect () assert report is not None assert isinstance ( report , str ) assert contains in report @pytest.mark.parametrize ( [ \"url\" , \"citekey\" ], [ ( \"https://www.doi.org/\" , \"url:https://www.doi.org/\" ,), ( \"https://www.doi.org/factsheets/Identifier_Interoper.html\" , \"url:https://www.doi.org/factsheets/Identifier_Interoper.html\" , ), ( \"https://doi.org/10.1097 %2F 00004032-200403000-00012\" , \"doi:10.1097/00004032-200403000-00012\" , ), ( \"http://dx.doi.org/10.7554/eLife.46574\" , \"doi:10.7554/eLife.46574\" ), ( \"https://doi.org/10/b6vnmd#anchor\" , \"doi:10/b6vnmd\" ), # ShortDOI URLs without `10/` prefix not yet supported` ( \"https://doi.org/b6vnmd\" , \"url:https://doi.org/b6vnmd\" ), ( \"https://www.biorxiv.org/about-biorxiv\" , \"url:https://www.biorxiv.org/about-biorxiv\" , ), ( \"https://sci-hub.tw/10.1038/nature19057\" , \"doi:10.1038/nature19057\" ), ( \"https://www.biorxiv.org/content/10.1101/087619v3\" , \"doi:10.1101/087619\" ), ( \"https://www.biorxiv.org/content/10.1101/087619v3.full\" , \"doi:10.1101/087619\" ), ( \"https://www.biorxiv.org/content/early/2017/08/31/087619.full.pdf\" , \"doi:10.1101/087619\" , ), ( \"https://www.biorxiv.org/content/10.1101/2019.12.11.872580v1\" , \"doi:10.1101/2019.12.11.872580\" , ), ( \"https://www.biorxiv.org/content/10.1101/2019.12.11.872580v1.full.pdf+html\" , \"doi:10.1101/2019.12.11.872580\" , ), ( \"https://www.biorxiv.org/content/10.1101/2019.12.11.872580v1.full.pdf\" , \"doi:10.1101/2019.12.11.872580\" , ), ( \"https://www.ncbi.nlm.nih.gov\" , \"url:https://www.ncbi.nlm.nih.gov\" ), ( \"https://www.ncbi.nlm.nih.gov/pubmed\" , \"url:https://www.ncbi.nlm.nih.gov/pubmed\" , ), ( \"https://www.ncbi.nlm.nih.gov/pubmed/31233491\" , \"pmid:31233491\" ), ( \"https://www.ncbi.nlm.nih.gov/pmc/\" , \"url:https://www.ncbi.nlm.nih.gov/pmc/\" ), ( \"https://www.ncbi.nlm.nih.gov/pmc/about/intro/\" , \"url:https://www.ncbi.nlm.nih.gov/pmc/about/intro/\" , ), ( \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4304851/\" , \"pmcid:PMC4304851\" ), ( \"https://www.wikidata.org/wiki/Wikidata:Main_Page\" , \"url:https://www.wikidata.org/wiki/Wikidata:Main_Page\" , ), ( \"https://www.wikidata.org/wiki/Q50051684\" , \"wikidata:Q50051684\" ), ( \"https://arxiv.org/\" , \"url:https://arxiv.org/\" ), ( \"https://arxiv.org/list/q-fin/recent\" , \"url:https://arxiv.org/list/q-fin/recent\" , ), ( \"https://arxiv.org/abs/1912.03529v1\" , \"arxiv:1912.03529v1\" ), ( \"https://arxiv.org/pdf/1912.03529v1.pdf\" , \"arxiv:1912.03529v1\" ), ( \"https://arxiv.org/ps/1912.03529v1\" , \"arxiv:1912.03529v1\" ), ( \"https://arxiv.org/abs/math.GT/0309136\" , \"arxiv:math.GT/0309136\" ), ( \"https://arxiv.org/abs/hep-th/9305059\" , \"arxiv:hep-th/9305059\" ), ( \"https://arxiv.org/pdf/hep-th/9305059.pdf\" , \"arxiv:hep-th/9305059\" ), ], ) def test_url_to_citekey ( url , citekey ): assert url_to_citekey ( url ) == citekey","title":"Module manubot.cite.tests.test_citekey"},{"location":"reference/manubot/cite/tests/test_citekey/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_citekey/#test_citekey_class","text":"def test_citekey_class ( input_id , citekey_attrs ) View Source @pytest . mark . parametrize ( [ \"input_id\", \"citekey_attrs\" ] , [ ( \"DOI:10.5061/DRYad.q447c/1\", dict( prefix_lower=\"doi\", standard_accession=\"10.5061/dryad.q447c/1\", standard_id=\"doi:10.5061/dryad.q447c/1\", ), ), ( \"arXiv:1407.3561v1\", dict(prefix_lower=\"arxiv\", standard_id=\"arxiv:1407.3561v1\",), ), (\"pmid:24159271\", dict(standard_id=\"pubmed:24159271\",),), (\"pmcid:PMC4304851\", dict(standard_id=\"pmc:PMC4304851\",),), ( \"https://greenelab.github.io/manubot-rootstock/\", dict(standard_id=\"url:https://greenelab.github.io/manubot-rootstock/\",), ), (\"isbn:1-339-91988-5\", dict(standard_id=\"isbn:9781339919881\",),), (\"DOID:14330\", dict(standard_id=\"doid:14330\",),), (\"PubChem.substance:100101\", dict(standard_id=\"pubchem.substance:100101\",),), (\"Wikidata:Q50051684\", dict(standard_id=\"wikidata:Q50051684\",),), ] , ) def test_citekey_class ( input_id , citekey_attrs ) : citekey = CiteKey ( input_id ) print ( citekey ) for key , value in citekey_attrs . items () : assert getattr ( citekey , key ) == value assert citekey . short_id","title":"test_citekey_class"},{"location":"reference/manubot/cite/tests/test_citekey/#test_inspect_citekey_fails","text":"def test_inspect_citekey_fails ( citekey , contains ) These citekeys should fail inspection by inspect_citekey. View Source @pytest . mark . parametrize ( [ \"citekey\", \"contains\" ] , [ (\"doi:10.771/peerj.705\", \"Double check the DOI\"), (\"doi:10/b6v_nmd\", \"Double check the shortDOI\"), (\"doi:7717/peerj.705\", \"must start with '10.'\"), (\"doi:b6vnmd\", \"must start with '10.'\"), (\"pmcid:25648772\", \"must start with 'PMC'\"), ( \"pmid:PMC4304851\", \"Should 'pmid:PMC4304851' switch the citation source to 'pmc'?\", ), (\"isbn:1-339-91988-X\", \"identifier violates the ISBN syntax\"), (\"wikidata:P212\", \"item IDs must start with 'Q'\"), (\"wikidata:QABCD\", \"does not conform to the Wikidata regex\"), (\"arxiv:YYMM.number\", \"must conform to syntax\"), ] , ) def test_inspect_citekey_fails ( citekey , contains ) : \"\"\" These citekeys should fail inspection by inspect_citekey. \"\"\" report = CiteKey ( citekey ). inspect () assert report is not None assert isinstance ( report , str ) assert contains in report","title":"test_inspect_citekey_fails"},{"location":"reference/manubot/cite/tests/test_citekey/#test_inspect_citekey_passes","text":"def test_inspect_citekey_passes ( citekey ) These citekeys should pass inspection by inspect_citekey. View Source @pytest . mark . parametrize ( \"citekey\" , [ \"doi:10.7717/peerj.705\", \"doi:10/b6vnmd\", \"pmcid:PMC4304851\", \"pmid:25648772\", \"arxiv:1407.3561\", \"arxiv:1407.3561v1\", \"arxiv:math.GT/0309136\", \"arxiv:math.GT/0309136v1\", \"arxiv:hep-th/9305059\", \"arxiv:hep-th/9305059v2\", \"isbn:978-1-339-91988-1\", \"isbn:1-339-91988-5\", \"wikidata:Q1\", \"wikidata:Q50051684\", \"url:https://peerj.com/articles/705/\", ] , ) def test_inspect_citekey_passes ( citekey ) : \"\"\" These citekeys should pass inspection by inspect_citekey. \"\"\" report = CiteKey ( citekey ). inspect () assert report is None","title":"test_inspect_citekey_passes"},{"location":"reference/manubot/cite/tests/test_citekey/#test_shorten_citekey","text":"def test_shorten_citekey ( standard_citekey , expected ) View Source @pytest . mark . parametrize ( \"standard_citekey,expected\" , [ (\"doi:10.5061/dryad.q447c/1\", \"kQFQ8EaO\"), (\"arxiv:1407.3561v1\", \"16kozZ9Ys\"), (\"pmid:24159271\", \"11sli93ov\"), (\"url:http://blog.dhimmel.com/irreproducible-timestamps/\", \"QBWMEuxW\"), ] , ) def test_shorten_citekey ( standard_citekey , expected ) : short_citekey = shorten_citekey ( standard_citekey ) assert short_citekey == expected","title":"test_shorten_citekey"},{"location":"reference/manubot/cite/tests/test_citekey/#test_url_to_citekey","text":"def test_url_to_citekey ( url , citekey ) View Source @pytest . mark . parametrize ( [ \"url\", \"citekey\" ] , [ (\"https://www.doi.org/\", \"url:https://www.doi.org/\",), ( \"https://www.doi.org/factsheets/Identifier_Interoper.html\", \"url:https://www.doi.org/factsheets/Identifier_Interoper.html\", ), ( \"https://doi.org/10.1097%2F00004032-200403000-00012\", \"doi:10.1097/00004032-200403000-00012\", ), (\"http://dx.doi.org/10.7554/eLife.46574\", \"doi:10.7554/eLife.46574\"), (\"https://doi.org/10/b6vnmd#anchor\", \"doi:10/b6vnmd\"), # ShortDOI URLs without `10/` prefix not yet supported` (\"https://doi.org/b6vnmd\", \"url:https://doi.org/b6vnmd\"), ( \"https://www.biorxiv.org/about-biorxiv\", \"url:https://www.biorxiv.org/about-biorxiv\", ), (\"https://sci-hub.tw/10.1038/nature19057\", \"doi:10.1038/nature19057\"), (\"https://www.biorxiv.org/content/10.1101/087619v3\", \"doi:10.1101/087619\"), (\"https://www.biorxiv.org/content/10.1101/087619v3.full\", \"doi:10.1101/087619\"), ( \"https://www.biorxiv.org/content/early/2017/08/31/087619.full.pdf\", \"doi:10.1101/087619\", ), ( \"https://www.biorxiv.org/content/10.1101/2019.12.11.872580v1\", \"doi:10.1101/2019.12.11.872580\", ), ( \"https://www.biorxiv.org/content/10.1101/2019.12.11.872580v1.full.pdf+html\", \"doi:10.1101/2019.12.11.872580\", ), ( \"https://www.biorxiv.org/content/10.1101/2019.12.11.872580v1.full.pdf\", \"doi:10.1101/2019.12.11.872580\", ), (\"https://www.ncbi.nlm.nih.gov\", \"url:https://www.ncbi.nlm.nih.gov\"), ( \"https://www.ncbi.nlm.nih.gov/pubmed\", \"url:https://www.ncbi.nlm.nih.gov/pubmed\", ), (\"https://www.ncbi.nlm.nih.gov/pubmed/31233491\", \"pmid:31233491\"), (\"https://www.ncbi.nlm.nih.gov/pmc/\", \"url:https://www.ncbi.nlm.nih.gov/pmc/\"), ( \"https://www.ncbi.nlm.nih.gov/pmc/about/intro/\", \"url:https://www.ncbi.nlm.nih.gov/pmc/about/intro/\", ), (\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4304851/\", \"pmcid:PMC4304851\"), ( \"https://www.wikidata.org/wiki/Wikidata:Main_Page\", \"url:https://www.wikidata.org/wiki/Wikidata:Main_Page\", ), (\"https://www.wikidata.org/wiki/Q50051684\", \"wikidata:Q50051684\"), (\"https://arxiv.org/\", \"url:https://arxiv.org/\"), ( \"https://arxiv.org/list/q-fin/recent\", \"url:https://arxiv.org/list/q-fin/recent\", ), (\"https://arxiv.org/abs/1912.03529v1\", \"arxiv:1912.03529v1\"), (\"https://arxiv.org/pdf/1912.03529v1.pdf\", \"arxiv:1912.03529v1\"), (\"https://arxiv.org/ps/1912.03529v1\", \"arxiv:1912.03529v1\"), (\"https://arxiv.org/abs/math.GT/0309136\", \"arxiv:math.GT/0309136\"), (\"https://arxiv.org/abs/hep-th/9305059\", \"arxiv:hep-th/9305059\"), (\"https://arxiv.org/pdf/hep-th/9305059.pdf\", \"arxiv:hep-th/9305059\"), ] , ) def test_url_to_citekey ( url , citekey ) : assert url_to_citekey ( url ) == citekey","title":"test_url_to_citekey"},{"location":"reference/manubot/cite/tests/test_citekey_api/","text":"Module manubot.cite.tests.test_citekey_api Tests API-level functions in manubot.cite. Both functions are found in citekey.py View Source \"\"\"Tests API-level functions in manubot.cite. Both functions are found in citekey.py\"\"\" import pytest from manubot.cite import citekey_to_csl_item from manubot.cite.citekey import CiteKey @pytest.mark.parametrize ( \"input_id,expected\" , [ ( \"doi:10.5061/DRYAD.q447c/1\" , \"doi:10.5061/dryad.q447c/1\" ), ( \"doi:10.5061/dryad.q447c/1\" , \"doi:10.5061/dryad.q447c/1\" ), ( \"doi:10/b6vnmd\" , \"doi:10.1016/s0933-3657(96)00367-3\" ), ( \"doi:10/B6VNMD\" , \"doi:10.1016/s0933-3657(96)00367-3\" ), ( \"doi:10/xxxxxxxxxxxxxYY\" , \"doi:10/xxxxxxxxxxxxxyy\" , ), # passthrough non-existent shortDOI ( \"pmid:24159271\" , \"pubmed:24159271\" ), ( \"isbn:1339919885\" , \"isbn:9781339919881\" ), ( \"isbn:1-339-91988-5\" , \"isbn:9781339919881\" ), ( \"isbn:978-0-387-95069-3\" , \"isbn:9780387950693\" ), ( \"isbn:9780387950938\" , \"isbn:9780387950938\" ), ( \"isbn:1-55860-510-X\" , \"isbn:9781558605107\" ), ( \"isbn:1-55860-510-x\" , \"isbn:9781558605107\" ), ], ) def test_citekey_standard_id ( input_id , expected ): \"\"\" Test CiteKey.standard_id property for common prefixes. \"\"\" citekey = CiteKey ( input_id ) assert citekey . standard_id == expected @pytest.mark.xfail ( reason = \"https://twitter.com/dhimmel/status/950443969313419264\" ) def test_citekey_to_csl_item_doi_datacite (): citekey = \"doi:10.7287/peerj.preprints.3100v1\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"11cb5HXoY\" assert csl_item [ \"URL\" ] == \"https://doi.org/10.7287/peerj.preprints.3100v1\" assert csl_item [ \"DOI\" ] == \"10.7287/peerj.preprints.3100v1\" assert csl_item [ \"type\" ] == \"report\" assert ( csl_item [ \"title\" ] == \"Sci-Hub provides access to nearly all scholarly literature\" ) authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"family\" ] == \"Himmelstein\" assert authors [ - 1 ][ \"family\" ] == \"Greene\" def test_citekey_to_csl_item_arxiv (): citekey = \"arxiv:cond-mat/0703470v2\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"ES92tcdg\" assert csl_item [ \"URL\" ] == \"https://arxiv.org/abs/cond-mat/0703470v2\" assert csl_item [ \"number\" ] == \"cond-mat/0703470v2\" assert csl_item [ \"version\" ] == \"v2\" assert csl_item [ \"type\" ] == \"report\" assert csl_item [ \"container-title\" ] == \"arXiv\" assert csl_item [ \"title\" ] == \"Portraits of Complex Networks\" authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"literal\" ] == \"J. P. Bagrow\" assert csl_item [ \"DOI\" ] == \"10.1209/0295-5075/81/68004\" def test_citekey_to_csl_item_pmc (): \"\"\" https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/pmc/?format=csl&id=3041534 \"\"\" citekey = f \"pmc:PMC3041534\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"1CGP1eifE\" assert csl_item [ \"URL\" ] == \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041534/\" assert csl_item [ \"container-title-short\" ] == \"Summit Transl Bioinform\" assert ( csl_item [ \"title\" ] == \"Secondary Use of EHR: Data Quality Issues and Informatics Opportunities\" ) authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"family\" ] == \"Botsis\" assert csl_item [ \"PMID\" ] == \"21347133\" assert csl_item [ \"PMCID\" ] == \"PMC3041534\" assert \"generated by Manubot\" in csl_item [ \"note\" ] assert \"standard_id: pmc:PMC3041534\" in csl_item [ \"note\" ] def test_citekey_to_csl_item_pubmed_1 (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=21347133&rettype=full \"\"\" citekey = \"pubmed:21347133\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"o7hs4FTC\" assert csl_item [ \"type\" ] == \"article-journal\" assert csl_item [ \"URL\" ] == \"https://www.ncbi.nlm.nih.gov/pubmed/21347133\" assert csl_item [ \"container-title\" ] == \"Summit on translational bioinformatics\" assert ( csl_item [ \"title\" ] == \"Secondary Use of EHR: Data Quality Issues and Informatics Opportunities.\" ) assert csl_item [ \"issued\" ][ \"date-parts\" ] == [[ 2010 , 3 , 1 ]] authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"given\" ] == \"Taxiarchis\" assert authors [ 0 ][ \"family\" ] == \"Botsis\" assert csl_item [ \"PMID\" ] == \"21347133\" assert csl_item [ \"PMCID\" ] == \"PMC3041534\" def test_citekey_to_csl_item_pubmed_2 (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=27094199&rettype=full \"\"\" citekey = \"pubmed:27094199\" csl_item = citekey_to_csl_item ( citekey ) print ( csl_item ) assert csl_item [ \"id\" ] == \"5v0vabZu\" assert csl_item [ \"type\" ] == \"article-journal\" assert csl_item [ \"URL\" ] == \"https://www.ncbi.nlm.nih.gov/pubmed/27094199\" assert csl_item [ \"container-title\" ] == \"Circulation. Cardiovascular genetics\" assert csl_item [ \"container-title-short\" ] == \"Circ Cardiovasc Genet\" assert csl_item [ \"page\" ] == \"179-84\" assert ( csl_item [ \"title\" ] == \"Genetic Association-Guided Analysis of Gene Networks for the Study of Complex Traits.\" ) assert csl_item [ \"issued\" ][ \"date-parts\" ] == [[ 2016 , 4 ]] authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"given\" ] == \"Casey S\" assert authors [ 0 ][ \"family\" ] == \"Greene\" assert csl_item [ \"PMID\" ] == \"27094199\" assert csl_item [ \"DOI\" ] == \"10.1161/circgenetics.115.001181\" def test_citekey_to_csl_item_pubmed_with_numeric_month (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29028984&rettype=full See https://github.com/manubot/manubot/issues/69 \"\"\" citekey = \"pmid:29028984\" csl_item = citekey_to_csl_item ( citekey ) print ( csl_item ) assert csl_item [ \"issued\" ][ \"date-parts\" ] == [[ 2018 , 3 , 15 ]] def test_citekey_to_csl_item_pubmed_book ( caplog ): \"\"\" Extracting CSL metadata from books in PubMed is not supported. Logic not implemented to parse XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29227604&rettype=full \"\"\" csl_item = citekey_to_csl_item ( \"pmid:29227604\" , log_level = \"ERROR\" ) assert csl_item is None assert \"Unsupported PubMed record: no <Article> element\" in caplog . text def test_citekey_to_csl_item_isbn (): csl_item = citekey_to_csl_item ( \"isbn:9780387950693\" ) assert csl_item [ \"type\" ] == \"book\" assert csl_item [ \"title\" ] == \"Complex analysis\" def test_citekey_to_csl_item_clinical_trial (): \"\"\" Test clinicaltrials.gov citation support using CURIEs. https://github.com/manubot/manubot/issues/216 \"\"\" csl_item = citekey_to_csl_item ( \"clinicaltrials:NCT04292899\" ) assert csl_item [ \"title\" ] . startswith ( \"A Phase 3 Randomized Study\" ) assert csl_item [ \"source\" ] . startswith ( \"clinicaltrials.gov\" ) assert csl_item [ \"URL\" ] == \"https://clinicaltrials.gov/ct2/show/NCT04292899\" Functions test_citekey_standard_id def test_citekey_standard_id ( input_id , expected ) Test CiteKey.standard_id property for common prefixes. View Source @pytest . mark . parametrize ( \"input_id,expected\" , [ (\"doi:10.5061/DRYAD.q447c/1\", \"doi:10.5061/dryad.q447c/1\"), (\"doi:10.5061/dryad.q447c/1\", \"doi:10.5061/dryad.q447c/1\"), (\"doi:10/b6vnmd\", \"doi:10.1016/s0933-3657(96)00367-3\"), (\"doi:10/B6VNMD\", \"doi:10.1016/s0933-3657(96)00367-3\"), ( \"doi:10/xxxxxxxxxxxxxYY\", \"doi:10/xxxxxxxxxxxxxyy\", ), # passthrough non-existent shortDOI (\"pmid:24159271\", \"pubmed:24159271\"), (\"isbn:1339919885\", \"isbn:9781339919881\"), (\"isbn:1-339-91988-5\", \"isbn:9781339919881\"), (\"isbn:978-0-387-95069-3\", \"isbn:9780387950693\"), (\"isbn:9780387950938\", \"isbn:9780387950938\"), (\"isbn:1-55860-510-X\", \"isbn:9781558605107\"), (\"isbn:1-55860-510-x\", \"isbn:9781558605107\"), ] , ) def test_citekey_standard_id ( input_id , expected ) : \"\"\" Test CiteKey.standard_id property for common prefixes. \"\"\" citekey = CiteKey ( input_id ) assert citekey . standard_id == expected test_citekey_to_csl_item_arxiv def test_citekey_to_csl_item_arxiv ( ) View Source def test_citekey_to_csl_item_arxiv (): citekey = \"arxiv:cond-mat/0703470v2\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"ES92tcdg\" assert csl_item [ \"URL\" ] == \"https://arxiv.org/abs/cond-mat/0703470v2\" assert csl_item [ \"number\" ] == \"cond-mat/0703470v2\" assert csl_item [ \"version\" ] == \"v2\" assert csl_item [ \"type\" ] == \"report\" assert csl_item [ \"container-title\" ] == \"arXiv\" assert csl_item [ \"title\" ] == \"Portraits of Complex Networks\" authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"literal\" ] == \"J. P. Bagrow\" assert csl_item [ \"DOI\" ] == \"10.1209/0295-5075/81/68004\" test_citekey_to_csl_item_clinical_trial def test_citekey_to_csl_item_clinical_trial ( ) Test clinicaltrials.gov citation support using CURIEs. https://github.com/manubot/manubot/issues/216 View Source def test_citekey_to_csl_item_clinical_trial (): \"\"\" Test clinicaltrials.gov citation support using CURIEs. https://github.com/manubot/manubot/issues/216 \"\"\" csl_item = citekey_to_csl_item ( \"clinicaltrials:NCT04292899\" ) assert csl_item [ \"title\" ]. startswith ( \"A Phase 3 Randomized Study\" ) assert csl_item [ \"source\" ]. startswith ( \"clinicaltrials.gov\" ) assert csl_item [ \"URL\" ] == \"https://clinicaltrials.gov/ct2/show/NCT04292899\" test_citekey_to_csl_item_doi_datacite def test_citekey_to_csl_item_doi_datacite ( ) View Source @pytest . mark . xfail ( reason = \"https://twitter.com/dhimmel/status/950443969313419264\" ) def test_citekey_to_csl_item_doi_datacite () : citekey = \"doi:10.7287/peerj.preprints.3100v1\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"11cb5HXoY\" assert csl_item [ \"URL\" ] == \"https://doi.org/10.7287/peerj.preprints.3100v1\" assert csl_item [ \"DOI\" ] == \"10.7287/peerj.preprints.3100v1\" assert csl_item [ \"type\" ] == \"report\" assert ( csl_item [ \"title\" ] == \"Sci-Hub provides access to nearly all scholarly literature\" ) authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"family\" ] == \"Himmelstein\" assert authors [ -1 ][ \"family\" ] == \"Greene\" test_citekey_to_csl_item_isbn def test_citekey_to_csl_item_isbn ( ) View Source def test_citekey_to_csl_item_isbn (): csl_item = citekey_to_csl_item ( \"isbn:9780387950693\" ) assert csl_item [ \"type\" ] == \"book\" assert csl_item [ \"title\" ] == \"Complex analysis\" test_citekey_to_csl_item_pmc def test_citekey_to_csl_item_pmc ( ) https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/pmc/?format=csl&id=3041534 View Source def test_citekey_to_csl_item_pmc (): \"\"\" https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/pmc/?format=csl&id=3041534 \"\"\" citekey = f \"pmc:PMC3041534\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"1CGP1eifE\" assert csl_item [ \"URL\" ] == \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041534/\" assert csl_item [ \"container-title-short\" ] == \"Summit Transl Bioinform\" assert ( csl_item [ \"title\" ] == \"Secondary Use of EHR: Data Quality Issues and Informatics Opportunities\" ) authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"family\" ] == \"Botsis\" assert csl_item [ \"PMID\" ] == \"21347133\" assert csl_item [ \"PMCID\" ] == \"PMC3041534\" assert \"generated by Manubot\" in csl_item [ \"note\" ] assert \"standard_id: pmc:PMC3041534\" in csl_item [ \"note\" ] test_citekey_to_csl_item_pubmed_1 def test_citekey_to_csl_item_pubmed_1 ( ) Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=21347133&rettype=full View Source def test_citekey_to_csl_item_pubmed_1 (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=21347133&rettype=full \"\"\" citekey = \"pubmed:21347133\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"o7hs4FTC\" assert csl_item [ \"type\" ] == \"article-journal\" assert csl_item [ \"URL\" ] == \"https://www.ncbi.nlm.nih.gov/pubmed/21347133\" assert csl_item [ \"container-title\" ] == \"Summit on translational bioinformatics\" assert ( csl_item [ \"title\" ] == \"Secondary Use of EHR: Data Quality Issues and Informatics Opportunities.\" ) assert csl_item [ \"issued\" ][ \"date-parts\" ] == [[ 2010 , 3 , 1 ]] authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"given\" ] == \"Taxiarchis\" assert authors [ 0 ][ \"family\" ] == \"Botsis\" assert csl_item [ \"PMID\" ] == \"21347133\" assert csl_item [ \"PMCID\" ] == \"PMC3041534\" test_citekey_to_csl_item_pubmed_2 def test_citekey_to_csl_item_pubmed_2 ( ) Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=27094199&rettype=full View Source def test_citekey_to_csl_item_pubmed_2 (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=27094199&rettype=full \"\"\" citekey = \"pubmed:27094199\" csl_item = citekey_to_csl_item ( citekey ) print ( csl_item ) assert csl_item [ \"id\" ] == \"5v0vabZu\" assert csl_item [ \"type\" ] == \"article-journal\" assert csl_item [ \"URL\" ] == \"https://www.ncbi.nlm.nih.gov/pubmed/27094199\" assert csl_item [ \"container-title\" ] == \"Circulation. Cardiovascular genetics\" assert csl_item [ \"container-title-short\" ] == \"Circ Cardiovasc Genet\" assert csl_item [ \"page\" ] == \"179-84\" assert ( csl_item [ \"title\" ] == \"Genetic Association-Guided Analysis of Gene Networks for the Study of Complex Traits.\" ) assert csl_item [ \"issued\" ][ \"date-parts\" ] == [[ 2016 , 4 ]] authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"given\" ] == \"Casey S\" assert authors [ 0 ][ \"family\" ] == \"Greene\" assert csl_item [ \"PMID\" ] == \"27094199\" assert csl_item [ \"DOI\" ] == \"10.1161/circgenetics.115.001181\" test_citekey_to_csl_item_pubmed_book def test_citekey_to_csl_item_pubmed_book ( caplog ) Extracting CSL metadata from books in PubMed is not supported. Logic not implemented to parse XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29227604&rettype=full View Source def test_citekey_to_csl_item_pubmed_book ( caplog ): \"\"\" Extracting CSL metadata from books in PubMed is not supported. Logic not implemented to parse XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29227604&rettype=full \"\"\" csl_item = citekey_to_csl_item ( \"pmid:29227604\" , log_level = \"ERROR\" ) assert csl_item is None assert \"Unsupported PubMed record: no <Article> element\" in caplog . text test_citekey_to_csl_item_pubmed_with_numeric_month def test_citekey_to_csl_item_pubmed_with_numeric_month ( ) Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29028984&rettype=full See https://github.com/manubot/manubot/issues/69 View Source def test_citekey_to_csl_item_pubmed_with_numeric_month (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29028984&rettype=full See https://github.com/manubot/manubot/issues/69 \"\"\" citekey = \"pmid:29028984\" csl_item = citekey_to_csl_item ( citekey ) print ( csl_item ) assert csl_item [ \"issued\" ][ \"date-parts\" ] == [[ 2018 , 3 , 15 ]]","title":"Test Citekey Api"},{"location":"reference/manubot/cite/tests/test_citekey_api/#module-manubotciteteststest_citekey_api","text":"Tests API-level functions in manubot.cite. Both functions are found in citekey.py View Source \"\"\"Tests API-level functions in manubot.cite. Both functions are found in citekey.py\"\"\" import pytest from manubot.cite import citekey_to_csl_item from manubot.cite.citekey import CiteKey @pytest.mark.parametrize ( \"input_id,expected\" , [ ( \"doi:10.5061/DRYAD.q447c/1\" , \"doi:10.5061/dryad.q447c/1\" ), ( \"doi:10.5061/dryad.q447c/1\" , \"doi:10.5061/dryad.q447c/1\" ), ( \"doi:10/b6vnmd\" , \"doi:10.1016/s0933-3657(96)00367-3\" ), ( \"doi:10/B6VNMD\" , \"doi:10.1016/s0933-3657(96)00367-3\" ), ( \"doi:10/xxxxxxxxxxxxxYY\" , \"doi:10/xxxxxxxxxxxxxyy\" , ), # passthrough non-existent shortDOI ( \"pmid:24159271\" , \"pubmed:24159271\" ), ( \"isbn:1339919885\" , \"isbn:9781339919881\" ), ( \"isbn:1-339-91988-5\" , \"isbn:9781339919881\" ), ( \"isbn:978-0-387-95069-3\" , \"isbn:9780387950693\" ), ( \"isbn:9780387950938\" , \"isbn:9780387950938\" ), ( \"isbn:1-55860-510-X\" , \"isbn:9781558605107\" ), ( \"isbn:1-55860-510-x\" , \"isbn:9781558605107\" ), ], ) def test_citekey_standard_id ( input_id , expected ): \"\"\" Test CiteKey.standard_id property for common prefixes. \"\"\" citekey = CiteKey ( input_id ) assert citekey . standard_id == expected @pytest.mark.xfail ( reason = \"https://twitter.com/dhimmel/status/950443969313419264\" ) def test_citekey_to_csl_item_doi_datacite (): citekey = \"doi:10.7287/peerj.preprints.3100v1\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"11cb5HXoY\" assert csl_item [ \"URL\" ] == \"https://doi.org/10.7287/peerj.preprints.3100v1\" assert csl_item [ \"DOI\" ] == \"10.7287/peerj.preprints.3100v1\" assert csl_item [ \"type\" ] == \"report\" assert ( csl_item [ \"title\" ] == \"Sci-Hub provides access to nearly all scholarly literature\" ) authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"family\" ] == \"Himmelstein\" assert authors [ - 1 ][ \"family\" ] == \"Greene\" def test_citekey_to_csl_item_arxiv (): citekey = \"arxiv:cond-mat/0703470v2\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"ES92tcdg\" assert csl_item [ \"URL\" ] == \"https://arxiv.org/abs/cond-mat/0703470v2\" assert csl_item [ \"number\" ] == \"cond-mat/0703470v2\" assert csl_item [ \"version\" ] == \"v2\" assert csl_item [ \"type\" ] == \"report\" assert csl_item [ \"container-title\" ] == \"arXiv\" assert csl_item [ \"title\" ] == \"Portraits of Complex Networks\" authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"literal\" ] == \"J. P. Bagrow\" assert csl_item [ \"DOI\" ] == \"10.1209/0295-5075/81/68004\" def test_citekey_to_csl_item_pmc (): \"\"\" https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/pmc/?format=csl&id=3041534 \"\"\" citekey = f \"pmc:PMC3041534\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"1CGP1eifE\" assert csl_item [ \"URL\" ] == \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041534/\" assert csl_item [ \"container-title-short\" ] == \"Summit Transl Bioinform\" assert ( csl_item [ \"title\" ] == \"Secondary Use of EHR: Data Quality Issues and Informatics Opportunities\" ) authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"family\" ] == \"Botsis\" assert csl_item [ \"PMID\" ] == \"21347133\" assert csl_item [ \"PMCID\" ] == \"PMC3041534\" assert \"generated by Manubot\" in csl_item [ \"note\" ] assert \"standard_id: pmc:PMC3041534\" in csl_item [ \"note\" ] def test_citekey_to_csl_item_pubmed_1 (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=21347133&rettype=full \"\"\" citekey = \"pubmed:21347133\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"o7hs4FTC\" assert csl_item [ \"type\" ] == \"article-journal\" assert csl_item [ \"URL\" ] == \"https://www.ncbi.nlm.nih.gov/pubmed/21347133\" assert csl_item [ \"container-title\" ] == \"Summit on translational bioinformatics\" assert ( csl_item [ \"title\" ] == \"Secondary Use of EHR: Data Quality Issues and Informatics Opportunities.\" ) assert csl_item [ \"issued\" ][ \"date-parts\" ] == [[ 2010 , 3 , 1 ]] authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"given\" ] == \"Taxiarchis\" assert authors [ 0 ][ \"family\" ] == \"Botsis\" assert csl_item [ \"PMID\" ] == \"21347133\" assert csl_item [ \"PMCID\" ] == \"PMC3041534\" def test_citekey_to_csl_item_pubmed_2 (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=27094199&rettype=full \"\"\" citekey = \"pubmed:27094199\" csl_item = citekey_to_csl_item ( citekey ) print ( csl_item ) assert csl_item [ \"id\" ] == \"5v0vabZu\" assert csl_item [ \"type\" ] == \"article-journal\" assert csl_item [ \"URL\" ] == \"https://www.ncbi.nlm.nih.gov/pubmed/27094199\" assert csl_item [ \"container-title\" ] == \"Circulation. Cardiovascular genetics\" assert csl_item [ \"container-title-short\" ] == \"Circ Cardiovasc Genet\" assert csl_item [ \"page\" ] == \"179-84\" assert ( csl_item [ \"title\" ] == \"Genetic Association-Guided Analysis of Gene Networks for the Study of Complex Traits.\" ) assert csl_item [ \"issued\" ][ \"date-parts\" ] == [[ 2016 , 4 ]] authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"given\" ] == \"Casey S\" assert authors [ 0 ][ \"family\" ] == \"Greene\" assert csl_item [ \"PMID\" ] == \"27094199\" assert csl_item [ \"DOI\" ] == \"10.1161/circgenetics.115.001181\" def test_citekey_to_csl_item_pubmed_with_numeric_month (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29028984&rettype=full See https://github.com/manubot/manubot/issues/69 \"\"\" citekey = \"pmid:29028984\" csl_item = citekey_to_csl_item ( citekey ) print ( csl_item ) assert csl_item [ \"issued\" ][ \"date-parts\" ] == [[ 2018 , 3 , 15 ]] def test_citekey_to_csl_item_pubmed_book ( caplog ): \"\"\" Extracting CSL metadata from books in PubMed is not supported. Logic not implemented to parse XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29227604&rettype=full \"\"\" csl_item = citekey_to_csl_item ( \"pmid:29227604\" , log_level = \"ERROR\" ) assert csl_item is None assert \"Unsupported PubMed record: no <Article> element\" in caplog . text def test_citekey_to_csl_item_isbn (): csl_item = citekey_to_csl_item ( \"isbn:9780387950693\" ) assert csl_item [ \"type\" ] == \"book\" assert csl_item [ \"title\" ] == \"Complex analysis\" def test_citekey_to_csl_item_clinical_trial (): \"\"\" Test clinicaltrials.gov citation support using CURIEs. https://github.com/manubot/manubot/issues/216 \"\"\" csl_item = citekey_to_csl_item ( \"clinicaltrials:NCT04292899\" ) assert csl_item [ \"title\" ] . startswith ( \"A Phase 3 Randomized Study\" ) assert csl_item [ \"source\" ] . startswith ( \"clinicaltrials.gov\" ) assert csl_item [ \"URL\" ] == \"https://clinicaltrials.gov/ct2/show/NCT04292899\"","title":"Module manubot.cite.tests.test_citekey_api"},{"location":"reference/manubot/cite/tests/test_citekey_api/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_standard_id","text":"def test_citekey_standard_id ( input_id , expected ) Test CiteKey.standard_id property for common prefixes. View Source @pytest . mark . parametrize ( \"input_id,expected\" , [ (\"doi:10.5061/DRYAD.q447c/1\", \"doi:10.5061/dryad.q447c/1\"), (\"doi:10.5061/dryad.q447c/1\", \"doi:10.5061/dryad.q447c/1\"), (\"doi:10/b6vnmd\", \"doi:10.1016/s0933-3657(96)00367-3\"), (\"doi:10/B6VNMD\", \"doi:10.1016/s0933-3657(96)00367-3\"), ( \"doi:10/xxxxxxxxxxxxxYY\", \"doi:10/xxxxxxxxxxxxxyy\", ), # passthrough non-existent shortDOI (\"pmid:24159271\", \"pubmed:24159271\"), (\"isbn:1339919885\", \"isbn:9781339919881\"), (\"isbn:1-339-91988-5\", \"isbn:9781339919881\"), (\"isbn:978-0-387-95069-3\", \"isbn:9780387950693\"), (\"isbn:9780387950938\", \"isbn:9780387950938\"), (\"isbn:1-55860-510-X\", \"isbn:9781558605107\"), (\"isbn:1-55860-510-x\", \"isbn:9781558605107\"), ] , ) def test_citekey_standard_id ( input_id , expected ) : \"\"\" Test CiteKey.standard_id property for common prefixes. \"\"\" citekey = CiteKey ( input_id ) assert citekey . standard_id == expected","title":"test_citekey_standard_id"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_arxiv","text":"def test_citekey_to_csl_item_arxiv ( ) View Source def test_citekey_to_csl_item_arxiv (): citekey = \"arxiv:cond-mat/0703470v2\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"ES92tcdg\" assert csl_item [ \"URL\" ] == \"https://arxiv.org/abs/cond-mat/0703470v2\" assert csl_item [ \"number\" ] == \"cond-mat/0703470v2\" assert csl_item [ \"version\" ] == \"v2\" assert csl_item [ \"type\" ] == \"report\" assert csl_item [ \"container-title\" ] == \"arXiv\" assert csl_item [ \"title\" ] == \"Portraits of Complex Networks\" authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"literal\" ] == \"J. P. Bagrow\" assert csl_item [ \"DOI\" ] == \"10.1209/0295-5075/81/68004\"","title":"test_citekey_to_csl_item_arxiv"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_clinical_trial","text":"def test_citekey_to_csl_item_clinical_trial ( ) Test clinicaltrials.gov citation support using CURIEs. https://github.com/manubot/manubot/issues/216 View Source def test_citekey_to_csl_item_clinical_trial (): \"\"\" Test clinicaltrials.gov citation support using CURIEs. https://github.com/manubot/manubot/issues/216 \"\"\" csl_item = citekey_to_csl_item ( \"clinicaltrials:NCT04292899\" ) assert csl_item [ \"title\" ]. startswith ( \"A Phase 3 Randomized Study\" ) assert csl_item [ \"source\" ]. startswith ( \"clinicaltrials.gov\" ) assert csl_item [ \"URL\" ] == \"https://clinicaltrials.gov/ct2/show/NCT04292899\"","title":"test_citekey_to_csl_item_clinical_trial"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_doi_datacite","text":"def test_citekey_to_csl_item_doi_datacite ( ) View Source @pytest . mark . xfail ( reason = \"https://twitter.com/dhimmel/status/950443969313419264\" ) def test_citekey_to_csl_item_doi_datacite () : citekey = \"doi:10.7287/peerj.preprints.3100v1\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"11cb5HXoY\" assert csl_item [ \"URL\" ] == \"https://doi.org/10.7287/peerj.preprints.3100v1\" assert csl_item [ \"DOI\" ] == \"10.7287/peerj.preprints.3100v1\" assert csl_item [ \"type\" ] == \"report\" assert ( csl_item [ \"title\" ] == \"Sci-Hub provides access to nearly all scholarly literature\" ) authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"family\" ] == \"Himmelstein\" assert authors [ -1 ][ \"family\" ] == \"Greene\"","title":"test_citekey_to_csl_item_doi_datacite"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_isbn","text":"def test_citekey_to_csl_item_isbn ( ) View Source def test_citekey_to_csl_item_isbn (): csl_item = citekey_to_csl_item ( \"isbn:9780387950693\" ) assert csl_item [ \"type\" ] == \"book\" assert csl_item [ \"title\" ] == \"Complex analysis\"","title":"test_citekey_to_csl_item_isbn"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_pmc","text":"def test_citekey_to_csl_item_pmc ( ) https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/pmc/?format=csl&id=3041534 View Source def test_citekey_to_csl_item_pmc (): \"\"\" https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/pmc/?format=csl&id=3041534 \"\"\" citekey = f \"pmc:PMC3041534\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"1CGP1eifE\" assert csl_item [ \"URL\" ] == \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041534/\" assert csl_item [ \"container-title-short\" ] == \"Summit Transl Bioinform\" assert ( csl_item [ \"title\" ] == \"Secondary Use of EHR: Data Quality Issues and Informatics Opportunities\" ) authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"family\" ] == \"Botsis\" assert csl_item [ \"PMID\" ] == \"21347133\" assert csl_item [ \"PMCID\" ] == \"PMC3041534\" assert \"generated by Manubot\" in csl_item [ \"note\" ] assert \"standard_id: pmc:PMC3041534\" in csl_item [ \"note\" ]","title":"test_citekey_to_csl_item_pmc"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_pubmed_1","text":"def test_citekey_to_csl_item_pubmed_1 ( ) Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=21347133&rettype=full View Source def test_citekey_to_csl_item_pubmed_1 (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=21347133&rettype=full \"\"\" citekey = \"pubmed:21347133\" csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ \"id\" ] == \"o7hs4FTC\" assert csl_item [ \"type\" ] == \"article-journal\" assert csl_item [ \"URL\" ] == \"https://www.ncbi.nlm.nih.gov/pubmed/21347133\" assert csl_item [ \"container-title\" ] == \"Summit on translational bioinformatics\" assert ( csl_item [ \"title\" ] == \"Secondary Use of EHR: Data Quality Issues and Informatics Opportunities.\" ) assert csl_item [ \"issued\" ][ \"date-parts\" ] == [[ 2010 , 3 , 1 ]] authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"given\" ] == \"Taxiarchis\" assert authors [ 0 ][ \"family\" ] == \"Botsis\" assert csl_item [ \"PMID\" ] == \"21347133\" assert csl_item [ \"PMCID\" ] == \"PMC3041534\"","title":"test_citekey_to_csl_item_pubmed_1"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_pubmed_2","text":"def test_citekey_to_csl_item_pubmed_2 ( ) Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=27094199&rettype=full View Source def test_citekey_to_csl_item_pubmed_2 (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=27094199&rettype=full \"\"\" citekey = \"pubmed:27094199\" csl_item = citekey_to_csl_item ( citekey ) print ( csl_item ) assert csl_item [ \"id\" ] == \"5v0vabZu\" assert csl_item [ \"type\" ] == \"article-journal\" assert csl_item [ \"URL\" ] == \"https://www.ncbi.nlm.nih.gov/pubmed/27094199\" assert csl_item [ \"container-title\" ] == \"Circulation. Cardiovascular genetics\" assert csl_item [ \"container-title-short\" ] == \"Circ Cardiovasc Genet\" assert csl_item [ \"page\" ] == \"179-84\" assert ( csl_item [ \"title\" ] == \"Genetic Association-Guided Analysis of Gene Networks for the Study of Complex Traits.\" ) assert csl_item [ \"issued\" ][ \"date-parts\" ] == [[ 2016 , 4 ]] authors = csl_item [ \"author\" ] assert authors [ 0 ][ \"given\" ] == \"Casey S\" assert authors [ 0 ][ \"family\" ] == \"Greene\" assert csl_item [ \"PMID\" ] == \"27094199\" assert csl_item [ \"DOI\" ] == \"10.1161/circgenetics.115.001181\"","title":"test_citekey_to_csl_item_pubmed_2"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_pubmed_book","text":"def test_citekey_to_csl_item_pubmed_book ( caplog ) Extracting CSL metadata from books in PubMed is not supported. Logic not implemented to parse XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29227604&rettype=full View Source def test_citekey_to_csl_item_pubmed_book ( caplog ): \"\"\" Extracting CSL metadata from books in PubMed is not supported. Logic not implemented to parse XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29227604&rettype=full \"\"\" csl_item = citekey_to_csl_item ( \"pmid:29227604\" , log_level = \"ERROR\" ) assert csl_item is None assert \"Unsupported PubMed record: no <Article> element\" in caplog . text","title":"test_citekey_to_csl_item_pubmed_book"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_pubmed_with_numeric_month","text":"def test_citekey_to_csl_item_pubmed_with_numeric_month ( ) Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29028984&rettype=full See https://github.com/manubot/manubot/issues/69 View Source def test_citekey_to_csl_item_pubmed_with_numeric_month (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29028984&rettype=full See https://github.com/manubot/manubot/issues/69 \"\"\" citekey = \"pmid:29028984\" csl_item = citekey_to_csl_item ( citekey ) print ( csl_item ) assert csl_item [ \"issued\" ][ \"date-parts\" ] == [[ 2018 , 3 , 15 ]]","title":"test_citekey_to_csl_item_pubmed_with_numeric_month"},{"location":"reference/manubot/cite/tests/test_citeproc/","text":"Module manubot.cite.tests.test_citeproc View Source import json import pathlib import pytest from manubot.cite.citeproc import remove_jsonschema_errors directory = pathlib . Path ( __file__ ) . parent csl_instances = [ x . name for x in directory . glob ( \"csl-json/*-csl\" )] def load_json ( path ): return json . loads ( path . read_text ( encoding = \"utf-8-sig\" )) def test_json_is_readable_on_windows_in_different_oem_encoding (): name = \"crossref-deep-review-csl\" path = directory / \"csl-json\" / name / \"raw.json\" content = path . read_text ( encoding = \"utf-8-sig\" ) assert content json1 = load_json ( path ) assert json1 @pytest.mark.parametrize ( \"name\" , csl_instances ) def test_remove_jsonschema_errors ( name ): \"\"\" Tests whether CSL pruning of raw.json produces the expected pruned.json. The fidelity of the remove_jsonschema_errors implementation is difficult to assess theoretically, therefore its performance should be evaluated empirically with thorough test coverage. It is recommended that all conceivable conformations of invalid CSL that citeproc methods may generate be tested. To create a new test case, derive pruned.json from raw.json, by manually deleting any invalid fields. Do not use `manubot cite` to directly generate pruned.json as that also relies on remove_jsonschema_errors for pruning. \"\"\" data_dir = directory / \"csl-json\" / name raw = load_json ( data_dir / \"raw.json\" ) expected = load_json ( data_dir / \"pruned.json\" ) pruned = remove_jsonschema_errors ( raw ) assert pruned == expected Variables csl_instances directory Functions load_json def load_json ( path ) View Source def load_json ( path ): return json . loads ( path . read_text ( encoding = \"utf-8-sig\" )) test_json_is_readable_on_windows_in_different_oem_encoding def test_json_is_readable_on_windows_in_different_oem_encoding ( ) View Source def test_json_is_readable_on_windows_in_different_oem_encoding (): name = \"crossref-deep-review-csl\" path = directory / \"csl-json\" / name / \"raw.json\" content = path . read_text ( encoding = \"utf-8-sig\" ) assert content json1 = load_json ( path ) assert json1 test_remove_jsonschema_errors def test_remove_jsonschema_errors ( name ) Tests whether CSL pruning of raw.json produces the expected pruned.json. The fidelity of the remove_jsonschema_errors implementation is difficult to assess theoretically, therefore its performance should be evaluated empirically with thorough test coverage. It is recommended that all conceivable conformations of invalid CSL that citeproc methods may generate be tested. To create a new test case, derive pruned.json from raw.json, by manually deleting any invalid fields. Do not use manubot cite to directly generate pruned.json as that also relies on remove_jsonschema_errors for pruning. View Source @pytest . mark . parametrize ( \"name\" , csl_instances ) def test_remove_jsonschema_errors ( name ) : \"\"\" Tests whether CSL pruning of raw.json produces the expected pruned.json. The fidelity of the remove_jsonschema_errors implementation is difficult to assess theoretically, therefore its performance should be evaluated empirically with thorough test coverage. It is recommended that all conceivable conformations of invalid CSL that citeproc methods may generate be tested. To create a new test case, derive pruned.json from raw.json, by manually deleting any invalid fields. Do not use `manubot cite` to directly generate pruned.json as that also relies on remove_jsonschema_errors for pruning. \"\"\" data_dir = directory / \"csl-json\" / name raw = load_json ( data_dir / \"raw.json\" ) expected = load_json ( data_dir / \"pruned.json\" ) pruned = remove_jsonschema_errors ( raw ) assert pruned == expected","title":"Test Citeproc"},{"location":"reference/manubot/cite/tests/test_citeproc/#module-manubotciteteststest_citeproc","text":"View Source import json import pathlib import pytest from manubot.cite.citeproc import remove_jsonschema_errors directory = pathlib . Path ( __file__ ) . parent csl_instances = [ x . name for x in directory . glob ( \"csl-json/*-csl\" )] def load_json ( path ): return json . loads ( path . read_text ( encoding = \"utf-8-sig\" )) def test_json_is_readable_on_windows_in_different_oem_encoding (): name = \"crossref-deep-review-csl\" path = directory / \"csl-json\" / name / \"raw.json\" content = path . read_text ( encoding = \"utf-8-sig\" ) assert content json1 = load_json ( path ) assert json1 @pytest.mark.parametrize ( \"name\" , csl_instances ) def test_remove_jsonschema_errors ( name ): \"\"\" Tests whether CSL pruning of raw.json produces the expected pruned.json. The fidelity of the remove_jsonschema_errors implementation is difficult to assess theoretically, therefore its performance should be evaluated empirically with thorough test coverage. It is recommended that all conceivable conformations of invalid CSL that citeproc methods may generate be tested. To create a new test case, derive pruned.json from raw.json, by manually deleting any invalid fields. Do not use `manubot cite` to directly generate pruned.json as that also relies on remove_jsonschema_errors for pruning. \"\"\" data_dir = directory / \"csl-json\" / name raw = load_json ( data_dir / \"raw.json\" ) expected = load_json ( data_dir / \"pruned.json\" ) pruned = remove_jsonschema_errors ( raw ) assert pruned == expected","title":"Module manubot.cite.tests.test_citeproc"},{"location":"reference/manubot/cite/tests/test_citeproc/#variables","text":"csl_instances directory","title":"Variables"},{"location":"reference/manubot/cite/tests/test_citeproc/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_citeproc/#load_json","text":"def load_json ( path ) View Source def load_json ( path ): return json . loads ( path . read_text ( encoding = \"utf-8-sig\" ))","title":"load_json"},{"location":"reference/manubot/cite/tests/test_citeproc/#test_json_is_readable_on_windows_in_different_oem_encoding","text":"def test_json_is_readable_on_windows_in_different_oem_encoding ( ) View Source def test_json_is_readable_on_windows_in_different_oem_encoding (): name = \"crossref-deep-review-csl\" path = directory / \"csl-json\" / name / \"raw.json\" content = path . read_text ( encoding = \"utf-8-sig\" ) assert content json1 = load_json ( path ) assert json1","title":"test_json_is_readable_on_windows_in_different_oem_encoding"},{"location":"reference/manubot/cite/tests/test_citeproc/#test_remove_jsonschema_errors","text":"def test_remove_jsonschema_errors ( name ) Tests whether CSL pruning of raw.json produces the expected pruned.json. The fidelity of the remove_jsonschema_errors implementation is difficult to assess theoretically, therefore its performance should be evaluated empirically with thorough test coverage. It is recommended that all conceivable conformations of invalid CSL that citeproc methods may generate be tested. To create a new test case, derive pruned.json from raw.json, by manually deleting any invalid fields. Do not use manubot cite to directly generate pruned.json as that also relies on remove_jsonschema_errors for pruning. View Source @pytest . mark . parametrize ( \"name\" , csl_instances ) def test_remove_jsonschema_errors ( name ) : \"\"\" Tests whether CSL pruning of raw.json produces the expected pruned.json. The fidelity of the remove_jsonschema_errors implementation is difficult to assess theoretically, therefore its performance should be evaluated empirically with thorough test coverage. It is recommended that all conceivable conformations of invalid CSL that citeproc methods may generate be tested. To create a new test case, derive pruned.json from raw.json, by manually deleting any invalid fields. Do not use `manubot cite` to directly generate pruned.json as that also relies on remove_jsonschema_errors for pruning. \"\"\" data_dir = directory / \"csl-json\" / name raw = load_json ( data_dir / \"raw.json\" ) expected = load_json ( data_dir / \"pruned.json\" ) pruned = remove_jsonschema_errors ( raw ) assert pruned == expected","title":"test_remove_jsonschema_errors"},{"location":"reference/manubot/cite/tests/test_csl_item/","text":"Module manubot.cite.tests.test_csl_item View Source import copy import datetime import pytest from ..csl_item import ( CSL_Item , assert_csl_item_type , date_to_date_parts , date_parts_to_string , ) class Test_CSL_Item : def test_constuctor_empty ( self ): assert CSL_Item () == {} def test_constuctor_by_dict ( self ): d = { \"title\" : \"My book\" } assert CSL_Item ( d ) == d def test_constuctor_by_keyword ( self ): assert CSL_Item ( type = \"journal-article\" ) == { \"type\" : \"journal-article\" } def test_constuctor_by_dict_keyword_combination ( self ): assert CSL_Item ({ \"title\" : \"My journal article\" }, type = \"journal-article\" ) == { \"title\" : \"My journal article\" , \"type\" : \"journal-article\" , } def test_recursive_constructor ( self ): assert CSL_Item ( CSL_Item ()) == {} assert CSL_Item ( CSL_Item ( abc = 1 )) == { \"abc\" : 1 } def test_constructor_leaves_no_inplace_effects ( self ): dict1 = { \"a\" : 1 } ci = CSL_Item ( dict1 , b = 2 ) assert ci == { \"a\" : 1 , \"b\" : 2 } assert dict1 == { \"a\" : 1 } def test_correct_invalid_type ( self ): assert CSL_Item ( type = \"journal-article\" ) . correct_invalid_type () == { \"type\" : \"article-journal\" } def test_set_default_type ( self ): assert CSL_Item () . set_default_type () == { \"type\" : \"entry\" } def test_no_change_of_type ( self ): assert CSL_Item ( type = \"book\" ) . correct_invalid_type () == { \"type\" : \"book\" } assert CSL_Item ( type = \"book\" ) . set_default_type () == { \"type\" : \"book\" } def test_clean ( self ): csl_item = CSL_Item ( type = \"chapter\" , id = \"abc\" ) csl_item . clean ( prune = True ) assert csl_item == { \"type\" : \"chapter\" , \"id\" : \"abc\" } def test_clean_set_id ( self ): csl_item = CSL_Item ( type = \"chapter\" ) csl_item . set_id ( \"abc\" ) csl_item . clean ( prune = True ) assert csl_item == { \"type\" : \"chapter\" , \"id\" : \"abc\" } def test_assert_csl_item_type_passes (): assert_csl_item_type ( CSL_Item ()) def test_assert_csl_item_type_raises_error_on_dict (): with pytest . raises ( TypeError ): assert_csl_item_type ({}) @pytest.mark.parametrize ( [ \"csl_item\" , \"standard_citation\" ], [ pytest . param ( { \"id\" : \"my-id\" , \"standard_citation\" : \"doi:10.7554/elife.32822\" }, \"doi:10.7554/elife.32822\" , id = \"from_standard_citation\" , ), pytest . param ( { \"id\" : \"doi:10.7554/elife.32822\" }, \"doi:10.7554/elife.32822\" , id = \"from_doi_id\" , ), pytest . param ( { \"id\" : \"DOI:10.7554/ELIFE.32822\" }, \"doi:10.7554/elife.32822\" , id = \"from_doi_id_standardize\" , ), pytest . param ({ \"id\" : \"my-id\" }, \"my-id\" , id = \"from_raw_id\" ), ], ) def test_csl_item_standardize_id ( csl_item , standard_citation ): csl_item = CSL_Item ( csl_item ) output = csl_item . standardize_id () assert output is csl_item assert output [ \"id\" ] == standard_citation def test_csl_item_standardize_id_repeated (): csl_item = CSL_Item ( id = \"pmid:1\" , type = \"article-journal\" ) csl_item_1 = copy . deepcopy ( csl_item . standardize_id ()) assert \"standard_citation\" not in \"csl_item\" csl_item_2 = copy . deepcopy ( csl_item . standardize_id ()) assert csl_item_1 == csl_item_2 def test_csl_item_standardize_id_note (): \"\"\" Test extracting standard_id from a note and setting additional note fields. \"\"\" csl_item = CSL_Item ( { \"id\" : \"original-id\" , \"type\" : \"article-journal\" , \"note\" : \"standard_id: doi:10.1371/journal.PPAT.1006256\" , } ) csl_item . standardize_id () assert csl_item [ \"id\" ] == \"doi:10.1371/journal.ppat.1006256\" note_dict = csl_item . note_dict assert note_dict [ \"original_id\" ] == \"original-id\" assert note_dict [ \"original_standard_id\" ] == \"doi:10.1371/journal.PPAT.1006256\" @pytest.mark.parametrize ( [ \"input_note\" , \"text\" , \"dictionary\" , \"expected_note\" ], [ ( \"\" , \"\" , {}, \"\" ), ( None , \"\" , {}, \"\" ), ( \"preexisting note\" , \"\" , {}, \"preexisting note\" ), ( \"preexisting note\" , \"\" , { \"key\" : \"the value\" }, \"preexisting note \\n key: the value\" , ), ( \"\" , \"\" , { \"KEYOKAY\" : \"the value\" }, \"KEYOKAY: the value\" ), ( \"preexisting note\" , \"\" , { \"KEY-NOT-OKAY\" : \"the value\" }, \"preexisting note\" ), ( \"\" , \"\" , { \"standard_citation\" : \"doi:10.7554/elife.32822\" }, \"standard_citation: doi:10.7554/elife.32822\" , ), ( \"This CSL Item was produced using Manubot.\" , \"\" , { \"standard_citation\" : \"doi:10.7554/elife.32822\" }, \"This CSL Item was produced using Manubot. \\n standard_citation: doi:10.7554/elife.32822\" , ), ], ) def test_csl_item_note_append ( input_note , text , dictionary , expected_note ): csl_item = CSL_Item ({ \"id\" : \"test_csl_item\" , \"type\" : \"entry\" , \"note\" : input_note }) csl_item . note_append_text ( text ) csl_item . note_append_dict ( dictionary ) assert csl_item . note == expected_note @pytest.mark.parametrize ( [ \"note\" , \"dictionary\" ], [ ( \"This is a note \\n key_one: value \\n KEYTWO: value 2 \" , { \"key_one\" : \"value\" , \"KEYTWO\" : \"value 2\" }, ), ( \"BAD_KEY: good value \\n good-key: good value\" , { \"good-key\" : \"good value\" }), ( \"This is a note {:key_one: value} {:KEYTWO: value 2 } \" , { \"key_one\" : \"value\" , \"KEYTWO\" : \"value 2\" }, ), ( \"{:BAD_KEY: good value} \\n {:good-key: good value}\" , { \"good-key\" : \"good value\" }), ( \"Mixed line-entry and braced-entry syntax \\n GOODKEY: good value \\n {:good-key: good value}\" , { \"GOODKEY\" : \"good value\" , \"good-key\" : \"good value\" }, ), ( \"Note without any key-value pairs\" , {}), ( \"Other text \\n standard_citation: doi:10/ckcj \\n More other text\" , { \"standard_citation\" : \"doi:10/ckcj\" }, ), ], ) def test_csl_item_note_dict ( note , dictionary ): csl_item = CSL_Item ( note = note ) assert csl_item . note_dict == dictionary @pytest.mark.parametrize ( [ \"date\" , \"expected\" ], [ ( None , None ), ( \"\" , None ), ( \"2019\" , [ 2019 ]), ( \"2019-01\" , [ 2019 , 1 ]), ( \"2019-12\" , [ 2019 , 12 ]), ( \"2019-12-31\" , [ 2019 , 12 , 31 ]), ( \"2019-12-99\" , [ 2019 , 12 ]), ( \"2019-12-01\" , [ 2019 , 12 , 1 ]), ( \" 2019-12-01 \" , [ 2019 , 12 , 1 ]), ( \"2019-12-30T23:32:16Z\" , [ 2019 , 12 , 30 ]), ( datetime . date ( 2019 , 12 , 31 ), [ 2019 , 12 , 31 ]), ( datetime . datetime ( 2019 , 12 , 31 , 23 , 32 , 16 ), [ 2019 , 12 , 31 ]), ], ) def test_date_to_date_parts ( date , expected ): assert date_to_date_parts ( date ) == expected @pytest.mark.parametrize ( [ \"expected\" , \"date_parts\" , \"fill\" ], [ ( None , None , False ), ( None , [], True ), ( None , [], False ), ( None , None , True ), ( \"2019\" , [ 2019 ], False ), ( \"2019-01-01\" , [ 2019 ], True ), ( \"2019-01\" , [ 2019 , 1 ], False ), ( \"2019-12\" , [ 2019 , 12 ], False ), ( \"2019-12-01\" , [ 2019 , 12 ], True ), ( \"2019-12-31\" , [ 2019 , 12 , 31 ], False ), ( \"2019-12-31\" , [ 2019 , 12 , 31 ], True ), ( \"2019-12\" , [ 2019 , 12 , \"bad day\" ], False ), ( \"2019-12-01\" , [ 2019 , 12 , 1 ], False ), ( \"2019-12-01\" , [ \"2019\" , \"12\" , \"01\" ], False ), ( \"2019-02-01\" , [ \"2019\" , \"2\" , \"1\" ], False ), ( \"2019-12-31\" , [ 2019 , 12 , 31 , 23 , 32 , 16 ], False ), ( \"2019-12-31\" , [ 2019 , 12 , 31 , 23 , 32 , 16 ], True ), ( \"0080-07-14\" , [ 80 , 7 , 14 ], False ), ( \"0080-07-14\" , [ \"80\" , \"07\" , 14 ], False ), ], ) def test_date_parts_to_string ( expected , date_parts , fill ): assert expected == date_parts_to_string ( date_parts , fill = fill ) Functions test_assert_csl_item_type_passes def test_assert_csl_item_type_passes ( ) View Source def test_assert_csl_item_type_passes (): assert_csl_item_type ( CSL_Item ()) test_assert_csl_item_type_raises_error_on_dict def test_assert_csl_item_type_raises_error_on_dict ( ) View Source def test_assert_csl_item_type_raises_error_on_dict (): with pytest . raises ( TypeError ): assert_csl_item_type ( {} ) test_csl_item_note_append def test_csl_item_note_append ( input_note , text , dictionary , expected_note ) View Source @pytest . mark . parametrize ( [ \"input_note\", \"text\", \"dictionary\", \"expected_note\" ] , [ (\"\", \"\", {}, \"\"), (None, \"\", {}, \"\"), (\"preexisting note\", \"\", {}, \"preexisting note\"), ( \"preexisting note\", \"\", {\"key\": \"the value\"}, \"preexisting note\\nkey: the value\", ), (\"\", \"\", {\"KEYOKAY\": \"the value\"}, \"KEYOKAY: the value\"), (\"preexisting note\", \"\", {\"KEY-NOT-OKAY\": \"the value\"}, \"preexisting note\"), ( \"\", \"\", {\"standard_citation\": \"doi:10.7554/elife.32822\"}, \"standard_citation: doi:10.7554/elife.32822\", ), ( \"This CSL Item was produced using Manubot.\", \"\", {\"standard_citation\": \"doi:10.7554/elife.32822\"}, \"This CSL Item was produced using Manubot.\\nstandard_citation: doi:10.7554/elife.32822\", ), ] , ) def test_csl_item_note_append ( input_note , text , dictionary , expected_note ) : csl_item = CSL_Item ( { \"id\" : \"test_csl_item\" , \"type\" : \"entry\" , \"note\" : input_note } ) csl_item . note_append_text ( text ) csl_item . note_append_dict ( dictionary ) assert csl_item . note == expected_note test_csl_item_note_dict def test_csl_item_note_dict ( note , dictionary ) View Source @pytest . mark . parametrize ( [ \"note\", \"dictionary\" ] , [ ( \"This is a note\\nkey_one: value\\nKEYTWO: value 2 \", {\"key_one\": \"value\", \"KEYTWO\": \"value 2\"}, ), (\"BAD_KEY: good value\\ngood-key: good value\", {\"good-key\": \"good value\"}), ( \"This is a note {:key_one: value} {:KEYTWO: value 2 } \", {\"key_one\": \"value\", \"KEYTWO\": \"value 2\"}, ), (\"{:BAD_KEY: good value}\\n{:good-key: good value}\", {\"good-key\": \"good value\"}), ( \"Mixed line-entry and braced-entry syntax\\nGOODKEY: good value\\n{:good-key: good value}\", {\"GOODKEY\": \"good value\", \"good-key\": \"good value\"}, ), (\"Note without any key-value pairs\", {}), ( \"Other text\\nstandard_citation: doi:10/ckcj\\nMore other text\", {\"standard_citation\": \"doi:10/ckcj\"}, ), ] , ) def test_csl_item_note_dict ( note , dictionary ) : csl_item = CSL_Item ( note = note ) assert csl_item . note_dict == dictionary test_csl_item_standardize_id def test_csl_item_standardize_id ( csl_item , standard_citation ) View Source @pytest . mark . parametrize ( [ \"csl_item\", \"standard_citation\" ] , [ pytest.param( {\"id\": \"my-id\", \"standard_citation\": \"doi:10.7554/elife.32822\"}, \"doi:10.7554/elife.32822\", id=\"from_standard_citation\", ), pytest.param( {\"id\": \"doi:10.7554/elife.32822\"}, \"doi:10.7554/elife.32822\", id=\"from_doi_id\", ), pytest.param( {\"id\": \"DOI:10.7554/ELIFE.32822\"}, \"doi:10.7554/elife.32822\", id=\"from_doi_id_standardize\", ), pytest.param({\"id\": \"my-id\"}, \"my-id\", id=\"from_raw_id\"), ] , ) def test_csl_item_standardize_id ( csl_item , standard_citation ) : csl_item = CSL_Item ( csl_item ) output = csl_item . standardize_id () assert output is csl_item assert output [ \"id\" ] == standard_citation test_csl_item_standardize_id_note def test_csl_item_standardize_id_note ( ) Test extracting standard_id from a note and setting additional note fields. View Source def test_csl_item_standardize_id_note (): \"\"\" Test extracting standard_id from a note and setting additional note fields. \"\"\" csl_item = CSL_Item ( { \"id\" : \"original-id\" , \"type\" : \"article-journal\" , \"note\" : \"standard_id: doi:10.1371/journal.PPAT.1006256\" , } ) csl_item . standardize_id () assert csl_item [ \"id\" ] == \"doi:10.1371/journal.ppat.1006256\" note_dict = csl_item . note_dict assert note_dict [ \"original_id\" ] == \"original-id\" assert note_dict [ \"original_standard_id\" ] == \"doi:10.1371/journal.PPAT.1006256\" test_csl_item_standardize_id_repeated def test_csl_item_standardize_id_repeated ( ) View Source def test_csl_item_standardize_id_repeated (): csl_item = CSL_Item ( id = \"pmid:1\" , type = \"article-journal\" ) csl_item_1 = copy . deepcopy ( csl_item . standardize_id ()) assert \"standard_citation\" not in \"csl_item\" csl_item_2 = copy . deepcopy ( csl_item . standardize_id ()) assert csl_item_1 == csl_item_2 test_date_parts_to_string def test_date_parts_to_string ( expected , date_parts , fill ) View Source @pytest . mark . parametrize ( [ \"expected\", \"date_parts\", \"fill\" ] , [ (None, None, False), (None, [ ] , True ), ( None , [] , False ), ( None , None , True ), ( \"2019\" , [ 2019 ] , False ), ( \"2019-01-01\" , [ 2019 ] , True ), ( \"2019-01\" , [ 2019, 1 ] , False ), ( \"2019-12\" , [ 2019, 12 ] , False ), ( \"2019-12-01\" , [ 2019, 12 ] , True ), ( \"2019-12-31\" , [ 2019, 12, 31 ] , False ), ( \"2019-12-31\" , [ 2019, 12, 31 ] , True ), ( \"2019-12\" , [ 2019, 12, \"bad day\" ] , False ), ( \"2019-12-01\" , [ 2019, 12, 1 ] , False ), ( \"2019-12-01\" , [ \"2019\", \"12\", \"01\" ] , False ), ( \"2019-02-01\" , [ \"2019\", \"2\", \"1\" ] , False ), ( \"2019-12-31\" , [ 2019, 12, 31, 23, 32, 16 ] , False ), ( \"2019-12-31\" , [ 2019, 12, 31, 23, 32, 16 ] , True ), ( \"0080-07-14\" , [ 80, 7, 14 ] , False ), ( \"0080-07-14\" , [ \"80\", \"07\", 14 ] , False ), ] , ) def test_date_parts_to_string ( expected , date_parts , fill ) : assert expected == date_parts_to_string ( date_parts , fill = fill ) test_date_to_date_parts def test_date_to_date_parts ( date , expected ) View Source @pytest . mark . parametrize ( [ \"date\", \"expected\" ] , [ (None, None), (\"\", None), (\"2019\", [2019 ] ), ( \"2019-01\" , [ 2019, 1 ] ), ( \"2019-12\" , [ 2019, 12 ] ), ( \"2019-12-31\" , [ 2019, 12, 31 ] ), ( \"2019-12-99\" , [ 2019, 12 ] ), ( \"2019-12-01\" , [ 2019, 12, 1 ] ), ( \" 2019-12-01 \" , [ 2019, 12, 1 ] ), ( \"2019-12-30T23:32:16Z\" , [ 2019, 12, 30 ] ), ( datetime . date ( 2019 , 12 , 31 ), [ 2019, 12, 31 ] ), ( datetime . datetime ( 2019 , 12 , 31 , 23 , 32 , 16 ), [ 2019, 12, 31 ] ), ] , ) def test_date_to_date_parts ( date , expected ) : assert date_to_date_parts ( date ) == expected Classes Test_CSL_Item class Test_CSL_Item ( / , * args , ** kwargs ) View Source class Test_CSL_Item: def test_constuctor_empty ( self ): assert CSL_Item () == {} def test_constuctor_by_dict ( self ): d = { \"title\" : \"My book\" } assert CSL_Item ( d ) == d def test_constuctor_by_keyword ( self ): assert CSL_Item ( type = \"journal-article\" ) == { \"type\" : \"journal-article\" } def test_constuctor_by_dict_keyword_combination ( self ): assert CSL_Item ({ \"title\" : \"My journal article\" }, type = \"journal-article\" ) == { \"title\" : \"My journal article\" , \"type\" : \"journal-article\" , } def test_recursive_constructor ( self ): assert CSL_Item ( CSL_Item ()) == {} assert CSL_Item ( CSL_Item ( abc = 1 )) == { \"abc\" : 1 } def test_constructor_leaves_no_inplace_effects ( self ): dict1 = { \"a\" : 1 } ci = CSL_Item ( dict1 , b = 2 ) assert ci == { \"a\" : 1 , \"b\" : 2 } assert dict1 == { \"a\" : 1 } def test_correct_invalid_type ( self ): assert CSL_Item ( type = \"journal-article\" ). correct_invalid_type () == { \"type\" : \"article-journal\" } def test_set_default_type ( self ): assert CSL_Item (). set_default_type () == { \"type\" : \"entry\" } def test_no_change_of_type ( self ): assert CSL_Item ( type = \"book\" ). correct_invalid_type () == { \"type\" : \"book\" } assert CSL_Item ( type = \"book\" ). set_default_type () == { \"type\" : \"book\" } def test_clean ( self ): csl_item = CSL_Item ( type = \"chapter\" , id = \"abc\" ) csl_item . clean ( prune = True ) assert csl_item == { \"type\" : \"chapter\" , \"id\" : \"abc\" } def test_clean_set_id ( self ): csl_item = CSL_Item ( type = \"chapter\" ) csl_item . set_id ( \"abc\" ) csl_item . clean ( prune = True ) assert csl_item == { \"type\" : \"chapter\" , \"id\" : \"abc\" } Methods test_clean def test_clean ( self ) View Source def test_clean ( self ): csl_item = CSL_Item ( type = \"chapter\" , id = \"abc\" ) csl_item . clean ( prune = True ) assert csl_item == { \"type\" : \"chapter\" , \"id\" : \"abc\" } test_clean_set_id def test_clean_set_id ( self ) View Source def test_clean_set_id ( self ): csl_item = CSL_Item ( type = \"chapter\" ) csl_item . set_id ( \"abc\" ) csl_item . clean ( prune = True ) assert csl_item == { \"type\" : \"chapter\" , \"id\" : \"abc\" } test_constructor_leaves_no_inplace_effects def test_constructor_leaves_no_inplace_effects ( self ) View Source def test_constructor_leaves_no_inplace_effects ( self ): dict1 = { \"a\" : 1 } ci = CSL_Item ( dict1 , b = 2 ) assert ci == { \"a\" : 1 , \"b\" : 2 } assert dict1 == { \"a\" : 1 } test_constuctor_by_dict def test_constuctor_by_dict ( self ) View Source def test_constuctor_by_dict ( self ): d = { \"title\" : \"My book\" } assert CSL_Item ( d ) == d test_constuctor_by_dict_keyword_combination def test_constuctor_by_dict_keyword_combination ( self ) View Source def test_constuctor_by_dict_keyword_combination ( self ): assert CSL_Item ( { \"title\" : \"My journal article\" } , type = \"journal-article\" ) == { \"title\" : \"My journal article\" , \"type\" : \"journal-article\" , } test_constuctor_by_keyword def test_constuctor_by_keyword ( self ) View Source def test_constuctor_by_keyword ( self ): assert CSL_Item ( type = \"journal-article\" ) == { \"type\" : \"journal-article\" } test_constuctor_empty def test_constuctor_empty ( self ) View Source def test_constuctor_empty ( self ): assert CSL_Item () == {} test_correct_invalid_type def test_correct_invalid_type ( self ) View Source def test_correct_invalid_type ( self ): assert CSL_Item ( type = \"journal-article\" ). correct_invalid_type () == { \"type\" : \"article-journal\" } test_no_change_of_type def test_no_change_of_type ( self ) View Source def test_no_change_of_type ( self ): assert CSL_Item ( type = \"book\" ). correct_invalid_type () == { \"type\" : \"book\" } assert CSL_Item ( type = \"book\" ). set_default_type () == { \"type\" : \"book\" } test_recursive_constructor def test_recursive_constructor ( self ) View Source def test_recursive_constructor ( self ): assert CSL_Item ( CSL_Item ()) == {} assert CSL_Item ( CSL_Item ( abc = 1 )) == { \"abc\" : 1 } test_set_default_type def test_set_default_type ( self ) View Source def test_set_default_type ( self ): assert CSL_Item (). set_default_type () == { \"type\" : \"entry\" }","title":"Test Csl Item"},{"location":"reference/manubot/cite/tests/test_csl_item/#module-manubotciteteststest_csl_item","text":"View Source import copy import datetime import pytest from ..csl_item import ( CSL_Item , assert_csl_item_type , date_to_date_parts , date_parts_to_string , ) class Test_CSL_Item : def test_constuctor_empty ( self ): assert CSL_Item () == {} def test_constuctor_by_dict ( self ): d = { \"title\" : \"My book\" } assert CSL_Item ( d ) == d def test_constuctor_by_keyword ( self ): assert CSL_Item ( type = \"journal-article\" ) == { \"type\" : \"journal-article\" } def test_constuctor_by_dict_keyword_combination ( self ): assert CSL_Item ({ \"title\" : \"My journal article\" }, type = \"journal-article\" ) == { \"title\" : \"My journal article\" , \"type\" : \"journal-article\" , } def test_recursive_constructor ( self ): assert CSL_Item ( CSL_Item ()) == {} assert CSL_Item ( CSL_Item ( abc = 1 )) == { \"abc\" : 1 } def test_constructor_leaves_no_inplace_effects ( self ): dict1 = { \"a\" : 1 } ci = CSL_Item ( dict1 , b = 2 ) assert ci == { \"a\" : 1 , \"b\" : 2 } assert dict1 == { \"a\" : 1 } def test_correct_invalid_type ( self ): assert CSL_Item ( type = \"journal-article\" ) . correct_invalid_type () == { \"type\" : \"article-journal\" } def test_set_default_type ( self ): assert CSL_Item () . set_default_type () == { \"type\" : \"entry\" } def test_no_change_of_type ( self ): assert CSL_Item ( type = \"book\" ) . correct_invalid_type () == { \"type\" : \"book\" } assert CSL_Item ( type = \"book\" ) . set_default_type () == { \"type\" : \"book\" } def test_clean ( self ): csl_item = CSL_Item ( type = \"chapter\" , id = \"abc\" ) csl_item . clean ( prune = True ) assert csl_item == { \"type\" : \"chapter\" , \"id\" : \"abc\" } def test_clean_set_id ( self ): csl_item = CSL_Item ( type = \"chapter\" ) csl_item . set_id ( \"abc\" ) csl_item . clean ( prune = True ) assert csl_item == { \"type\" : \"chapter\" , \"id\" : \"abc\" } def test_assert_csl_item_type_passes (): assert_csl_item_type ( CSL_Item ()) def test_assert_csl_item_type_raises_error_on_dict (): with pytest . raises ( TypeError ): assert_csl_item_type ({}) @pytest.mark.parametrize ( [ \"csl_item\" , \"standard_citation\" ], [ pytest . param ( { \"id\" : \"my-id\" , \"standard_citation\" : \"doi:10.7554/elife.32822\" }, \"doi:10.7554/elife.32822\" , id = \"from_standard_citation\" , ), pytest . param ( { \"id\" : \"doi:10.7554/elife.32822\" }, \"doi:10.7554/elife.32822\" , id = \"from_doi_id\" , ), pytest . param ( { \"id\" : \"DOI:10.7554/ELIFE.32822\" }, \"doi:10.7554/elife.32822\" , id = \"from_doi_id_standardize\" , ), pytest . param ({ \"id\" : \"my-id\" }, \"my-id\" , id = \"from_raw_id\" ), ], ) def test_csl_item_standardize_id ( csl_item , standard_citation ): csl_item = CSL_Item ( csl_item ) output = csl_item . standardize_id () assert output is csl_item assert output [ \"id\" ] == standard_citation def test_csl_item_standardize_id_repeated (): csl_item = CSL_Item ( id = \"pmid:1\" , type = \"article-journal\" ) csl_item_1 = copy . deepcopy ( csl_item . standardize_id ()) assert \"standard_citation\" not in \"csl_item\" csl_item_2 = copy . deepcopy ( csl_item . standardize_id ()) assert csl_item_1 == csl_item_2 def test_csl_item_standardize_id_note (): \"\"\" Test extracting standard_id from a note and setting additional note fields. \"\"\" csl_item = CSL_Item ( { \"id\" : \"original-id\" , \"type\" : \"article-journal\" , \"note\" : \"standard_id: doi:10.1371/journal.PPAT.1006256\" , } ) csl_item . standardize_id () assert csl_item [ \"id\" ] == \"doi:10.1371/journal.ppat.1006256\" note_dict = csl_item . note_dict assert note_dict [ \"original_id\" ] == \"original-id\" assert note_dict [ \"original_standard_id\" ] == \"doi:10.1371/journal.PPAT.1006256\" @pytest.mark.parametrize ( [ \"input_note\" , \"text\" , \"dictionary\" , \"expected_note\" ], [ ( \"\" , \"\" , {}, \"\" ), ( None , \"\" , {}, \"\" ), ( \"preexisting note\" , \"\" , {}, \"preexisting note\" ), ( \"preexisting note\" , \"\" , { \"key\" : \"the value\" }, \"preexisting note \\n key: the value\" , ), ( \"\" , \"\" , { \"KEYOKAY\" : \"the value\" }, \"KEYOKAY: the value\" ), ( \"preexisting note\" , \"\" , { \"KEY-NOT-OKAY\" : \"the value\" }, \"preexisting note\" ), ( \"\" , \"\" , { \"standard_citation\" : \"doi:10.7554/elife.32822\" }, \"standard_citation: doi:10.7554/elife.32822\" , ), ( \"This CSL Item was produced using Manubot.\" , \"\" , { \"standard_citation\" : \"doi:10.7554/elife.32822\" }, \"This CSL Item was produced using Manubot. \\n standard_citation: doi:10.7554/elife.32822\" , ), ], ) def test_csl_item_note_append ( input_note , text , dictionary , expected_note ): csl_item = CSL_Item ({ \"id\" : \"test_csl_item\" , \"type\" : \"entry\" , \"note\" : input_note }) csl_item . note_append_text ( text ) csl_item . note_append_dict ( dictionary ) assert csl_item . note == expected_note @pytest.mark.parametrize ( [ \"note\" , \"dictionary\" ], [ ( \"This is a note \\n key_one: value \\n KEYTWO: value 2 \" , { \"key_one\" : \"value\" , \"KEYTWO\" : \"value 2\" }, ), ( \"BAD_KEY: good value \\n good-key: good value\" , { \"good-key\" : \"good value\" }), ( \"This is a note {:key_one: value} {:KEYTWO: value 2 } \" , { \"key_one\" : \"value\" , \"KEYTWO\" : \"value 2\" }, ), ( \"{:BAD_KEY: good value} \\n {:good-key: good value}\" , { \"good-key\" : \"good value\" }), ( \"Mixed line-entry and braced-entry syntax \\n GOODKEY: good value \\n {:good-key: good value}\" , { \"GOODKEY\" : \"good value\" , \"good-key\" : \"good value\" }, ), ( \"Note without any key-value pairs\" , {}), ( \"Other text \\n standard_citation: doi:10/ckcj \\n More other text\" , { \"standard_citation\" : \"doi:10/ckcj\" }, ), ], ) def test_csl_item_note_dict ( note , dictionary ): csl_item = CSL_Item ( note = note ) assert csl_item . note_dict == dictionary @pytest.mark.parametrize ( [ \"date\" , \"expected\" ], [ ( None , None ), ( \"\" , None ), ( \"2019\" , [ 2019 ]), ( \"2019-01\" , [ 2019 , 1 ]), ( \"2019-12\" , [ 2019 , 12 ]), ( \"2019-12-31\" , [ 2019 , 12 , 31 ]), ( \"2019-12-99\" , [ 2019 , 12 ]), ( \"2019-12-01\" , [ 2019 , 12 , 1 ]), ( \" 2019-12-01 \" , [ 2019 , 12 , 1 ]), ( \"2019-12-30T23:32:16Z\" , [ 2019 , 12 , 30 ]), ( datetime . date ( 2019 , 12 , 31 ), [ 2019 , 12 , 31 ]), ( datetime . datetime ( 2019 , 12 , 31 , 23 , 32 , 16 ), [ 2019 , 12 , 31 ]), ], ) def test_date_to_date_parts ( date , expected ): assert date_to_date_parts ( date ) == expected @pytest.mark.parametrize ( [ \"expected\" , \"date_parts\" , \"fill\" ], [ ( None , None , False ), ( None , [], True ), ( None , [], False ), ( None , None , True ), ( \"2019\" , [ 2019 ], False ), ( \"2019-01-01\" , [ 2019 ], True ), ( \"2019-01\" , [ 2019 , 1 ], False ), ( \"2019-12\" , [ 2019 , 12 ], False ), ( \"2019-12-01\" , [ 2019 , 12 ], True ), ( \"2019-12-31\" , [ 2019 , 12 , 31 ], False ), ( \"2019-12-31\" , [ 2019 , 12 , 31 ], True ), ( \"2019-12\" , [ 2019 , 12 , \"bad day\" ], False ), ( \"2019-12-01\" , [ 2019 , 12 , 1 ], False ), ( \"2019-12-01\" , [ \"2019\" , \"12\" , \"01\" ], False ), ( \"2019-02-01\" , [ \"2019\" , \"2\" , \"1\" ], False ), ( \"2019-12-31\" , [ 2019 , 12 , 31 , 23 , 32 , 16 ], False ), ( \"2019-12-31\" , [ 2019 , 12 , 31 , 23 , 32 , 16 ], True ), ( \"0080-07-14\" , [ 80 , 7 , 14 ], False ), ( \"0080-07-14\" , [ \"80\" , \"07\" , 14 ], False ), ], ) def test_date_parts_to_string ( expected , date_parts , fill ): assert expected == date_parts_to_string ( date_parts , fill = fill )","title":"Module manubot.cite.tests.test_csl_item"},{"location":"reference/manubot/cite/tests/test_csl_item/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_assert_csl_item_type_passes","text":"def test_assert_csl_item_type_passes ( ) View Source def test_assert_csl_item_type_passes (): assert_csl_item_type ( CSL_Item ())","title":"test_assert_csl_item_type_passes"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_assert_csl_item_type_raises_error_on_dict","text":"def test_assert_csl_item_type_raises_error_on_dict ( ) View Source def test_assert_csl_item_type_raises_error_on_dict (): with pytest . raises ( TypeError ): assert_csl_item_type ( {} )","title":"test_assert_csl_item_type_raises_error_on_dict"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_csl_item_note_append","text":"def test_csl_item_note_append ( input_note , text , dictionary , expected_note ) View Source @pytest . mark . parametrize ( [ \"input_note\", \"text\", \"dictionary\", \"expected_note\" ] , [ (\"\", \"\", {}, \"\"), (None, \"\", {}, \"\"), (\"preexisting note\", \"\", {}, \"preexisting note\"), ( \"preexisting note\", \"\", {\"key\": \"the value\"}, \"preexisting note\\nkey: the value\", ), (\"\", \"\", {\"KEYOKAY\": \"the value\"}, \"KEYOKAY: the value\"), (\"preexisting note\", \"\", {\"KEY-NOT-OKAY\": \"the value\"}, \"preexisting note\"), ( \"\", \"\", {\"standard_citation\": \"doi:10.7554/elife.32822\"}, \"standard_citation: doi:10.7554/elife.32822\", ), ( \"This CSL Item was produced using Manubot.\", \"\", {\"standard_citation\": \"doi:10.7554/elife.32822\"}, \"This CSL Item was produced using Manubot.\\nstandard_citation: doi:10.7554/elife.32822\", ), ] , ) def test_csl_item_note_append ( input_note , text , dictionary , expected_note ) : csl_item = CSL_Item ( { \"id\" : \"test_csl_item\" , \"type\" : \"entry\" , \"note\" : input_note } ) csl_item . note_append_text ( text ) csl_item . note_append_dict ( dictionary ) assert csl_item . note == expected_note","title":"test_csl_item_note_append"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_csl_item_note_dict","text":"def test_csl_item_note_dict ( note , dictionary ) View Source @pytest . mark . parametrize ( [ \"note\", \"dictionary\" ] , [ ( \"This is a note\\nkey_one: value\\nKEYTWO: value 2 \", {\"key_one\": \"value\", \"KEYTWO\": \"value 2\"}, ), (\"BAD_KEY: good value\\ngood-key: good value\", {\"good-key\": \"good value\"}), ( \"This is a note {:key_one: value} {:KEYTWO: value 2 } \", {\"key_one\": \"value\", \"KEYTWO\": \"value 2\"}, ), (\"{:BAD_KEY: good value}\\n{:good-key: good value}\", {\"good-key\": \"good value\"}), ( \"Mixed line-entry and braced-entry syntax\\nGOODKEY: good value\\n{:good-key: good value}\", {\"GOODKEY\": \"good value\", \"good-key\": \"good value\"}, ), (\"Note without any key-value pairs\", {}), ( \"Other text\\nstandard_citation: doi:10/ckcj\\nMore other text\", {\"standard_citation\": \"doi:10/ckcj\"}, ), ] , ) def test_csl_item_note_dict ( note , dictionary ) : csl_item = CSL_Item ( note = note ) assert csl_item . note_dict == dictionary","title":"test_csl_item_note_dict"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_csl_item_standardize_id","text":"def test_csl_item_standardize_id ( csl_item , standard_citation ) View Source @pytest . mark . parametrize ( [ \"csl_item\", \"standard_citation\" ] , [ pytest.param( {\"id\": \"my-id\", \"standard_citation\": \"doi:10.7554/elife.32822\"}, \"doi:10.7554/elife.32822\", id=\"from_standard_citation\", ), pytest.param( {\"id\": \"doi:10.7554/elife.32822\"}, \"doi:10.7554/elife.32822\", id=\"from_doi_id\", ), pytest.param( {\"id\": \"DOI:10.7554/ELIFE.32822\"}, \"doi:10.7554/elife.32822\", id=\"from_doi_id_standardize\", ), pytest.param({\"id\": \"my-id\"}, \"my-id\", id=\"from_raw_id\"), ] , ) def test_csl_item_standardize_id ( csl_item , standard_citation ) : csl_item = CSL_Item ( csl_item ) output = csl_item . standardize_id () assert output is csl_item assert output [ \"id\" ] == standard_citation","title":"test_csl_item_standardize_id"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_csl_item_standardize_id_note","text":"def test_csl_item_standardize_id_note ( ) Test extracting standard_id from a note and setting additional note fields. View Source def test_csl_item_standardize_id_note (): \"\"\" Test extracting standard_id from a note and setting additional note fields. \"\"\" csl_item = CSL_Item ( { \"id\" : \"original-id\" , \"type\" : \"article-journal\" , \"note\" : \"standard_id: doi:10.1371/journal.PPAT.1006256\" , } ) csl_item . standardize_id () assert csl_item [ \"id\" ] == \"doi:10.1371/journal.ppat.1006256\" note_dict = csl_item . note_dict assert note_dict [ \"original_id\" ] == \"original-id\" assert note_dict [ \"original_standard_id\" ] == \"doi:10.1371/journal.PPAT.1006256\"","title":"test_csl_item_standardize_id_note"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_csl_item_standardize_id_repeated","text":"def test_csl_item_standardize_id_repeated ( ) View Source def test_csl_item_standardize_id_repeated (): csl_item = CSL_Item ( id = \"pmid:1\" , type = \"article-journal\" ) csl_item_1 = copy . deepcopy ( csl_item . standardize_id ()) assert \"standard_citation\" not in \"csl_item\" csl_item_2 = copy . deepcopy ( csl_item . standardize_id ()) assert csl_item_1 == csl_item_2","title":"test_csl_item_standardize_id_repeated"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_date_parts_to_string","text":"def test_date_parts_to_string ( expected , date_parts , fill ) View Source @pytest . mark . parametrize ( [ \"expected\", \"date_parts\", \"fill\" ] , [ (None, None, False), (None, [ ] , True ), ( None , [] , False ), ( None , None , True ), ( \"2019\" , [ 2019 ] , False ), ( \"2019-01-01\" , [ 2019 ] , True ), ( \"2019-01\" , [ 2019, 1 ] , False ), ( \"2019-12\" , [ 2019, 12 ] , False ), ( \"2019-12-01\" , [ 2019, 12 ] , True ), ( \"2019-12-31\" , [ 2019, 12, 31 ] , False ), ( \"2019-12-31\" , [ 2019, 12, 31 ] , True ), ( \"2019-12\" , [ 2019, 12, \"bad day\" ] , False ), ( \"2019-12-01\" , [ 2019, 12, 1 ] , False ), ( \"2019-12-01\" , [ \"2019\", \"12\", \"01\" ] , False ), ( \"2019-02-01\" , [ \"2019\", \"2\", \"1\" ] , False ), ( \"2019-12-31\" , [ 2019, 12, 31, 23, 32, 16 ] , False ), ( \"2019-12-31\" , [ 2019, 12, 31, 23, 32, 16 ] , True ), ( \"0080-07-14\" , [ 80, 7, 14 ] , False ), ( \"0080-07-14\" , [ \"80\", \"07\", 14 ] , False ), ] , ) def test_date_parts_to_string ( expected , date_parts , fill ) : assert expected == date_parts_to_string ( date_parts , fill = fill )","title":"test_date_parts_to_string"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_date_to_date_parts","text":"def test_date_to_date_parts ( date , expected ) View Source @pytest . mark . parametrize ( [ \"date\", \"expected\" ] , [ (None, None), (\"\", None), (\"2019\", [2019 ] ), ( \"2019-01\" , [ 2019, 1 ] ), ( \"2019-12\" , [ 2019, 12 ] ), ( \"2019-12-31\" , [ 2019, 12, 31 ] ), ( \"2019-12-99\" , [ 2019, 12 ] ), ( \"2019-12-01\" , [ 2019, 12, 1 ] ), ( \" 2019-12-01 \" , [ 2019, 12, 1 ] ), ( \"2019-12-30T23:32:16Z\" , [ 2019, 12, 30 ] ), ( datetime . date ( 2019 , 12 , 31 ), [ 2019, 12, 31 ] ), ( datetime . datetime ( 2019 , 12 , 31 , 23 , 32 , 16 ), [ 2019, 12, 31 ] ), ] , ) def test_date_to_date_parts ( date , expected ) : assert date_to_date_parts ( date ) == expected","title":"test_date_to_date_parts"},{"location":"reference/manubot/cite/tests/test_csl_item/#classes","text":"","title":"Classes"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_csl_item","text":"class Test_CSL_Item ( / , * args , ** kwargs ) View Source class Test_CSL_Item: def test_constuctor_empty ( self ): assert CSL_Item () == {} def test_constuctor_by_dict ( self ): d = { \"title\" : \"My book\" } assert CSL_Item ( d ) == d def test_constuctor_by_keyword ( self ): assert CSL_Item ( type = \"journal-article\" ) == { \"type\" : \"journal-article\" } def test_constuctor_by_dict_keyword_combination ( self ): assert CSL_Item ({ \"title\" : \"My journal article\" }, type = \"journal-article\" ) == { \"title\" : \"My journal article\" , \"type\" : \"journal-article\" , } def test_recursive_constructor ( self ): assert CSL_Item ( CSL_Item ()) == {} assert CSL_Item ( CSL_Item ( abc = 1 )) == { \"abc\" : 1 } def test_constructor_leaves_no_inplace_effects ( self ): dict1 = { \"a\" : 1 } ci = CSL_Item ( dict1 , b = 2 ) assert ci == { \"a\" : 1 , \"b\" : 2 } assert dict1 == { \"a\" : 1 } def test_correct_invalid_type ( self ): assert CSL_Item ( type = \"journal-article\" ). correct_invalid_type () == { \"type\" : \"article-journal\" } def test_set_default_type ( self ): assert CSL_Item (). set_default_type () == { \"type\" : \"entry\" } def test_no_change_of_type ( self ): assert CSL_Item ( type = \"book\" ). correct_invalid_type () == { \"type\" : \"book\" } assert CSL_Item ( type = \"book\" ). set_default_type () == { \"type\" : \"book\" } def test_clean ( self ): csl_item = CSL_Item ( type = \"chapter\" , id = \"abc\" ) csl_item . clean ( prune = True ) assert csl_item == { \"type\" : \"chapter\" , \"id\" : \"abc\" } def test_clean_set_id ( self ): csl_item = CSL_Item ( type = \"chapter\" ) csl_item . set_id ( \"abc\" ) csl_item . clean ( prune = True ) assert csl_item == { \"type\" : \"chapter\" , \"id\" : \"abc\" }","title":"Test_CSL_Item"},{"location":"reference/manubot/cite/tests/test_csl_item/#methods","text":"","title":"Methods"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_clean","text":"def test_clean ( self ) View Source def test_clean ( self ): csl_item = CSL_Item ( type = \"chapter\" , id = \"abc\" ) csl_item . clean ( prune = True ) assert csl_item == { \"type\" : \"chapter\" , \"id\" : \"abc\" }","title":"test_clean"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_clean_set_id","text":"def test_clean_set_id ( self ) View Source def test_clean_set_id ( self ): csl_item = CSL_Item ( type = \"chapter\" ) csl_item . set_id ( \"abc\" ) csl_item . clean ( prune = True ) assert csl_item == { \"type\" : \"chapter\" , \"id\" : \"abc\" }","title":"test_clean_set_id"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_constructor_leaves_no_inplace_effects","text":"def test_constructor_leaves_no_inplace_effects ( self ) View Source def test_constructor_leaves_no_inplace_effects ( self ): dict1 = { \"a\" : 1 } ci = CSL_Item ( dict1 , b = 2 ) assert ci == { \"a\" : 1 , \"b\" : 2 } assert dict1 == { \"a\" : 1 }","title":"test_constructor_leaves_no_inplace_effects"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_constuctor_by_dict","text":"def test_constuctor_by_dict ( self ) View Source def test_constuctor_by_dict ( self ): d = { \"title\" : \"My book\" } assert CSL_Item ( d ) == d","title":"test_constuctor_by_dict"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_constuctor_by_dict_keyword_combination","text":"def test_constuctor_by_dict_keyword_combination ( self ) View Source def test_constuctor_by_dict_keyword_combination ( self ): assert CSL_Item ( { \"title\" : \"My journal article\" } , type = \"journal-article\" ) == { \"title\" : \"My journal article\" , \"type\" : \"journal-article\" , }","title":"test_constuctor_by_dict_keyword_combination"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_constuctor_by_keyword","text":"def test_constuctor_by_keyword ( self ) View Source def test_constuctor_by_keyword ( self ): assert CSL_Item ( type = \"journal-article\" ) == { \"type\" : \"journal-article\" }","title":"test_constuctor_by_keyword"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_constuctor_empty","text":"def test_constuctor_empty ( self ) View Source def test_constuctor_empty ( self ): assert CSL_Item () == {}","title":"test_constuctor_empty"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_correct_invalid_type","text":"def test_correct_invalid_type ( self ) View Source def test_correct_invalid_type ( self ): assert CSL_Item ( type = \"journal-article\" ). correct_invalid_type () == { \"type\" : \"article-journal\" }","title":"test_correct_invalid_type"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_no_change_of_type","text":"def test_no_change_of_type ( self ) View Source def test_no_change_of_type ( self ): assert CSL_Item ( type = \"book\" ). correct_invalid_type () == { \"type\" : \"book\" } assert CSL_Item ( type = \"book\" ). set_default_type () == { \"type\" : \"book\" }","title":"test_no_change_of_type"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_recursive_constructor","text":"def test_recursive_constructor ( self ) View Source def test_recursive_constructor ( self ): assert CSL_Item ( CSL_Item ()) == {} assert CSL_Item ( CSL_Item ( abc = 1 )) == { \"abc\" : 1 }","title":"test_recursive_constructor"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_set_default_type","text":"def test_set_default_type ( self ) View Source def test_set_default_type ( self ): assert CSL_Item (). set_default_type () == { \"type\" : \"entry\" }","title":"test_set_default_type"},{"location":"reference/manubot/cite/tests/test_curie/","text":"Module manubot.cite.tests.test_curie View Source import pytest from ..curie import curie_to_url , get_namespaces , get_prefix_to_namespace def test_get_namespaces_with_compile_patterns (): \"\"\" To see printed output when this test passes, run: ```shell # show regexes that do not match the sampleId pytest --capture=no --verbose manubot/cite/tests/test_curie.py ``` \"\"\" namespaces = get_namespaces ( compile_patterns = True ) assert isinstance ( namespaces , list ) for namespace in namespaces : # ensure prefix field exists assert namespace [ \"prefix\" ] # check whether compiled pattern matches example identifier # do not fail when no match, since this is an upstream issue # https://github.com/identifiers-org/identifiers-org.github.io/issues/99 compact_id = namespace [ \"sampleId\" ] if namespace [ \"namespaceEmbeddedInLui\" ]: compact_id = f \"{namespace['curiePrefix']}:{compact_id}\" match = namespace [ \"compiled_pattern\" ] . fullmatch ( compact_id ) if not match : print ( f \"{namespace['prefix']} regex \" f \"{namespace['compiled_pattern'].pattern} \" f \"does not match {compact_id}\" ) if namespace [ \"prefix\" ] != namespace [ \"curiePrefix\" ] . lower (): print ( f \"{namespace['prefix']} identifiers use \" f \"curiePrefix {namespace['curiePrefix']}\" ) def test_get_prefix_to_namespace (): prefix_to_namespace = get_prefix_to_namespace () assert isinstance ( prefix_to_namespace , dict ) assert \"doid\" in prefix_to_namespace namespace = prefix_to_namespace [ \"doid\" ] namespace [ \"curiePrefix\" ] = \"DOID\" @pytest.mark.parametrize ( \"curie, expected\" , [ ( \"doi:10.1038/nbt1156\" , \"https://identifiers.org/doi:10.1038/nbt1156\" ), ( \"DOI:10.1038/nbt1156\" , \"https://identifiers.org/doi:10.1038/nbt1156\" ), ( \"arXiv:0807.4956v1\" , \"https://identifiers.org/arxiv:0807.4956v1\" ), ( \"taxonomy:9606\" , \"https://identifiers.org/taxonomy:9606\" ), ( \"CHEBI:36927\" , \"https://identifiers.org/CHEBI:36927\" ), ( \"ChEBI:36927\" , \"https://identifiers.org/CHEBI:36927\" ), ( \"DOID:11337\" , \"https://identifiers.org/DOID:11337\" ), ( \"doid:11337\" , \"https://identifiers.org/DOID:11337\" ), ( \"clinicaltrials:NCT00222573\" , \"https://identifiers.org/clinicaltrials:NCT00222573\" , ), # https://github.com/identifiers-org/identifiers-org.github.io/issues/99#issuecomment-614690283 pytest . param ( \"GRO:0007133\" , \"https://identifiers.org/GRO:0007133\" , id = \"gramene.growthstage\" , ), ], ) def test_curie_to_url ( curie , expected ): url = curie_to_url ( curie ) assert url == expected def test_curie_to_url_bad_curie (): with pytest . raises ( ValueError ): curie_to_url ( \"this.is.not:a_curie\" ) Functions test_curie_to_url def test_curie_to_url ( curie , expected ) View Source @pytest . mark . parametrize ( \"curie, expected\" , [ (\"doi:10.1038/nbt1156\", \"https://identifiers.org/doi:10.1038/nbt1156\"), (\"DOI:10.1038/nbt1156\", \"https://identifiers.org/doi:10.1038/nbt1156\"), (\"arXiv:0807.4956v1\", \"https://identifiers.org/arxiv:0807.4956v1\"), (\"taxonomy:9606\", \"https://identifiers.org/taxonomy:9606\"), (\"CHEBI:36927\", \"https://identifiers.org/CHEBI:36927\"), (\"ChEBI:36927\", \"https://identifiers.org/CHEBI:36927\"), (\"DOID:11337\", \"https://identifiers.org/DOID:11337\"), (\"doid:11337\", \"https://identifiers.org/DOID:11337\"), ( \"clinicaltrials:NCT00222573\", \"https://identifiers.org/clinicaltrials:NCT00222573\", ), # https://github.com/identifiers-org/identifiers-org.github.io/issues/99#issuecomment-614690283 pytest.param( \"GRO:0007133\", \"https://identifiers.org/GRO:0007133\", id=\"gramene.growthstage\", ), ] , ) def test_curie_to_url ( curie , expected ) : url = curie_to_url ( curie ) assert url == expected test_curie_to_url_bad_curie def test_curie_to_url_bad_curie ( ) View Source def test_curie_to_url_bad_curie (): with pytest . raises ( ValueError ): curie_to_url ( \"this.is.not:a_curie\" ) test_get_namespaces_with_compile_patterns def test_get_namespaces_with_compile_patterns ( ) To see printed output when this test passes, run: # show regexes that do not match the sampleId pytest --capture = no --verbose manubot/cite/tests/test_curie.py View Source def test_get_namespaces_with_compile_patterns (): \"\"\" To see printed output when this test passes, run: ```shell # show regexes that do not match the sampleId pytest --capture=no --verbose manubot/cite/tests/test_curie.py ``` \"\"\" namespaces = get_namespaces ( compile_patterns = True ) assert isinstance ( namespaces , list ) for namespace in namespaces : # ensure prefix field exists assert namespace [ \"prefix\" ] # check whether compiled pattern matches example identifier # do not fail when no match , since this is an upstream issue # https : // github . com / identifiers - org / identifiers - org . github . io / issues / 99 compact_id = namespace [ \"sampleId\" ] if namespace [ \"namespaceEmbeddedInLui\" ]: compact_id = f \"{namespace['curiePrefix']}:{compact_id}\" match = namespace [ \"compiled_pattern\" ]. fullmatch ( compact_id ) if not match : print ( f \"{namespace['prefix']} regex \" f \"{namespace['compiled_pattern'].pattern} \" f \"does not match {compact_id}\" ) if namespace [ \"prefix\" ] != namespace [ \"curiePrefix\" ]. lower (): print ( f \"{namespace['prefix']} identifiers use \" f \"curiePrefix {namespace['curiePrefix']}\" ) test_get_prefix_to_namespace def test_get_prefix_to_namespace ( ) View Source def test_get_prefix_to_namespace (): prefix_to_namespace = get_prefix_to_namespace () assert isinstance ( prefix_to_namespace , dict ) assert \"doid\" in prefix_to_namespace namespace = prefix_to_namespace [ \"doid\" ] namespace [ \"curiePrefix\" ] = \"DOID\"","title":"Test Curie"},{"location":"reference/manubot/cite/tests/test_curie/#module-manubotciteteststest_curie","text":"View Source import pytest from ..curie import curie_to_url , get_namespaces , get_prefix_to_namespace def test_get_namespaces_with_compile_patterns (): \"\"\" To see printed output when this test passes, run: ```shell # show regexes that do not match the sampleId pytest --capture=no --verbose manubot/cite/tests/test_curie.py ``` \"\"\" namespaces = get_namespaces ( compile_patterns = True ) assert isinstance ( namespaces , list ) for namespace in namespaces : # ensure prefix field exists assert namespace [ \"prefix\" ] # check whether compiled pattern matches example identifier # do not fail when no match, since this is an upstream issue # https://github.com/identifiers-org/identifiers-org.github.io/issues/99 compact_id = namespace [ \"sampleId\" ] if namespace [ \"namespaceEmbeddedInLui\" ]: compact_id = f \"{namespace['curiePrefix']}:{compact_id}\" match = namespace [ \"compiled_pattern\" ] . fullmatch ( compact_id ) if not match : print ( f \"{namespace['prefix']} regex \" f \"{namespace['compiled_pattern'].pattern} \" f \"does not match {compact_id}\" ) if namespace [ \"prefix\" ] != namespace [ \"curiePrefix\" ] . lower (): print ( f \"{namespace['prefix']} identifiers use \" f \"curiePrefix {namespace['curiePrefix']}\" ) def test_get_prefix_to_namespace (): prefix_to_namespace = get_prefix_to_namespace () assert isinstance ( prefix_to_namespace , dict ) assert \"doid\" in prefix_to_namespace namespace = prefix_to_namespace [ \"doid\" ] namespace [ \"curiePrefix\" ] = \"DOID\" @pytest.mark.parametrize ( \"curie, expected\" , [ ( \"doi:10.1038/nbt1156\" , \"https://identifiers.org/doi:10.1038/nbt1156\" ), ( \"DOI:10.1038/nbt1156\" , \"https://identifiers.org/doi:10.1038/nbt1156\" ), ( \"arXiv:0807.4956v1\" , \"https://identifiers.org/arxiv:0807.4956v1\" ), ( \"taxonomy:9606\" , \"https://identifiers.org/taxonomy:9606\" ), ( \"CHEBI:36927\" , \"https://identifiers.org/CHEBI:36927\" ), ( \"ChEBI:36927\" , \"https://identifiers.org/CHEBI:36927\" ), ( \"DOID:11337\" , \"https://identifiers.org/DOID:11337\" ), ( \"doid:11337\" , \"https://identifiers.org/DOID:11337\" ), ( \"clinicaltrials:NCT00222573\" , \"https://identifiers.org/clinicaltrials:NCT00222573\" , ), # https://github.com/identifiers-org/identifiers-org.github.io/issues/99#issuecomment-614690283 pytest . param ( \"GRO:0007133\" , \"https://identifiers.org/GRO:0007133\" , id = \"gramene.growthstage\" , ), ], ) def test_curie_to_url ( curie , expected ): url = curie_to_url ( curie ) assert url == expected def test_curie_to_url_bad_curie (): with pytest . raises ( ValueError ): curie_to_url ( \"this.is.not:a_curie\" )","title":"Module manubot.cite.tests.test_curie"},{"location":"reference/manubot/cite/tests/test_curie/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_curie/#test_curie_to_url","text":"def test_curie_to_url ( curie , expected ) View Source @pytest . mark . parametrize ( \"curie, expected\" , [ (\"doi:10.1038/nbt1156\", \"https://identifiers.org/doi:10.1038/nbt1156\"), (\"DOI:10.1038/nbt1156\", \"https://identifiers.org/doi:10.1038/nbt1156\"), (\"arXiv:0807.4956v1\", \"https://identifiers.org/arxiv:0807.4956v1\"), (\"taxonomy:9606\", \"https://identifiers.org/taxonomy:9606\"), (\"CHEBI:36927\", \"https://identifiers.org/CHEBI:36927\"), (\"ChEBI:36927\", \"https://identifiers.org/CHEBI:36927\"), (\"DOID:11337\", \"https://identifiers.org/DOID:11337\"), (\"doid:11337\", \"https://identifiers.org/DOID:11337\"), ( \"clinicaltrials:NCT00222573\", \"https://identifiers.org/clinicaltrials:NCT00222573\", ), # https://github.com/identifiers-org/identifiers-org.github.io/issues/99#issuecomment-614690283 pytest.param( \"GRO:0007133\", \"https://identifiers.org/GRO:0007133\", id=\"gramene.growthstage\", ), ] , ) def test_curie_to_url ( curie , expected ) : url = curie_to_url ( curie ) assert url == expected","title":"test_curie_to_url"},{"location":"reference/manubot/cite/tests/test_curie/#test_curie_to_url_bad_curie","text":"def test_curie_to_url_bad_curie ( ) View Source def test_curie_to_url_bad_curie (): with pytest . raises ( ValueError ): curie_to_url ( \"this.is.not:a_curie\" )","title":"test_curie_to_url_bad_curie"},{"location":"reference/manubot/cite/tests/test_curie/#test_get_namespaces_with_compile_patterns","text":"def test_get_namespaces_with_compile_patterns ( ) To see printed output when this test passes, run: # show regexes that do not match the sampleId pytest --capture = no --verbose manubot/cite/tests/test_curie.py View Source def test_get_namespaces_with_compile_patterns (): \"\"\" To see printed output when this test passes, run: ```shell # show regexes that do not match the sampleId pytest --capture=no --verbose manubot/cite/tests/test_curie.py ``` \"\"\" namespaces = get_namespaces ( compile_patterns = True ) assert isinstance ( namespaces , list ) for namespace in namespaces : # ensure prefix field exists assert namespace [ \"prefix\" ] # check whether compiled pattern matches example identifier # do not fail when no match , since this is an upstream issue # https : // github . com / identifiers - org / identifiers - org . github . io / issues / 99 compact_id = namespace [ \"sampleId\" ] if namespace [ \"namespaceEmbeddedInLui\" ]: compact_id = f \"{namespace['curiePrefix']}:{compact_id}\" match = namespace [ \"compiled_pattern\" ]. fullmatch ( compact_id ) if not match : print ( f \"{namespace['prefix']} regex \" f \"{namespace['compiled_pattern'].pattern} \" f \"does not match {compact_id}\" ) if namespace [ \"prefix\" ] != namespace [ \"curiePrefix\" ]. lower (): print ( f \"{namespace['prefix']} identifiers use \" f \"curiePrefix {namespace['curiePrefix']}\" )","title":"test_get_namespaces_with_compile_patterns"},{"location":"reference/manubot/cite/tests/test_curie/#test_get_prefix_to_namespace","text":"def test_get_prefix_to_namespace ( ) View Source def test_get_prefix_to_namespace (): prefix_to_namespace = get_prefix_to_namespace () assert isinstance ( prefix_to_namespace , dict ) assert \"doid\" in prefix_to_namespace namespace = prefix_to_namespace [ \"doid\" ] namespace [ \"curiePrefix\" ] = \"DOID\"","title":"test_get_prefix_to_namespace"},{"location":"reference/manubot/cite/tests/test_doi/","text":"Module manubot.cite.tests.test_doi View Source import pytest from manubot.cite.doi import ( expand_short_doi , get_doi_csl_item , get_doi_csl_item_zotero , get_doi_csl_item_crosscite , ) def test_expand_short_doi (): doi = expand_short_doi ( \"10/b6vnmd\" ) assert doi == \"10.1016/s0933-3657(96)00367-3\" def test_expand_short_doi_invalid (): with pytest . raises ( ValueError , match = \"Handle not found. Double check short_doi\" ): expand_short_doi ( \"10/b6vnmdxxxxxx\" ) def test_expand_short_doi_not_short (): with pytest . raises ( ValueError , match = \"shortDOIs start with `10/`\" ): expand_short_doi ( \"10.1016/S0933-3657(96)00367-3\" ) def test_get_doi_csl_item_crosscite (): doi = \"10.1101/142760\" csl_item = get_doi_csl_item_crosscite ( doi ) assert isinstance ( csl_item , dict ) csl_item [ \"publisher\" ] == \"Cold Spring Harbor Laboratory\" def test_get_doi_csl_item_zotero (): \"\"\" As of 2019-10-25, DOI Content Negotiation (i.e. crosscite) encodes the consortium in author.name rather than author.literal. Zotero translation-server encodes the consortium in author.family, which is better than nothing since it's a valid CSL JSON field. https://github.com/manubot/manubot/issues/158 \"\"\" doi = \"10.1038/ng.3834\" csl_item = get_doi_csl_item_zotero ( doi ) assert isinstance ( csl_item , dict ) csl_item [ \"author\" ][ 9 ][ \"family\" ] == \"GTEx Consortium\" def test_get_doi_csl_item (): \"\"\" Test URL is set with shortDOI when calling get_doi_csl_item. \"\"\" doi = \"10.1101/142760\" csl_item = get_doi_csl_item ( doi ) assert isinstance ( csl_item , dict ) assert csl_item [ \"URL\" ] == \"https://doi.org/gbpvh5\" def test_get_doi_crosscite_with_consortium_author (): \"\"\" Make sure the author \"GTEx Consortium\" is properly encoded using the `author.literal` CSL JSON field. References: - <https://github.com/manubot/manubot/issues/158> - <https://github.com/crosscite/content-negotiation/issues/92> \"\"\" doi = \"10.1038/ng.3834\" csl_item = get_doi_csl_item_crosscite ( doi ) assert isinstance ( csl_item , dict ) assert any ( author . get ( \"literal\" ) == \"GTEx Consortium\" for author in csl_item [ \"author\" ] ) Functions test_expand_short_doi def test_expand_short_doi ( ) View Source def test_expand_short_doi (): doi = expand_short_doi ( \"10/b6vnmd\" ) assert doi == \"10.1016/s0933-3657(96)00367-3\" test_expand_short_doi_invalid def test_expand_short_doi_invalid ( ) View Source def test_expand_short_doi_invalid (): with pytest . raises ( ValueError , match = \"Handle not found. Double check short_doi\" ): expand_short_doi ( \"10/b6vnmdxxxxxx\" ) test_expand_short_doi_not_short def test_expand_short_doi_not_short ( ) View Source def test_expand_short_doi_not_short (): with pytest . raises ( ValueError , match = \"shortDOIs start with `10/`\" ): expand_short_doi ( \"10.1016/S0933-3657(96)00367-3\" ) test_get_doi_crosscite_with_consortium_author def test_get_doi_crosscite_with_consortium_author ( ) Make sure the author \"GTEx Consortium\" is properly encoded using the author.literal CSL JSON field. References: https://github.com/manubot/manubot/issues/158 https://github.com/crosscite/content-negotiation/issues/92 View Source def test_get_doi_crosscite_with_consortium_author (): \"\"\" Make sure the author \" GTEx Consortium \" is properly encoded using the `author.literal` CSL JSON field. References: - <https://github.com/manubot/manubot/issues/158> - <https://github.com/crosscite/content-negotiation/issues/92> \"\"\" doi = \"10.1038/ng.3834\" csl_item = get_doi_csl_item_crosscite ( doi ) assert isinstance ( csl_item , dict ) assert any ( author . get ( \"literal\" ) == \"GTEx Consortium\" for author in csl_item [ \"author\" ] ) test_get_doi_csl_item def test_get_doi_csl_item ( ) Test URL is set with shortDOI when calling get_doi_csl_item. View Source def test_get_doi_csl_item (): \"\"\" Test URL is set with shortDOI when calling get_doi_csl_item. \"\"\" doi = \"10.1101/142760\" csl_item = get_doi_csl_item ( doi ) assert isinstance ( csl_item , dict ) assert csl_item [ \"URL\" ] == \"https://doi.org/gbpvh5\" test_get_doi_csl_item_crosscite def test_get_doi_csl_item_crosscite ( ) View Source def test_get_doi_csl_item_crosscite (): doi = \"10.1101/142760\" csl_item = get_doi_csl_item_crosscite ( doi ) assert isinstance ( csl_item , dict ) csl_item [ \"publisher\" ] == \"Cold Spring Harbor Laboratory\" test_get_doi_csl_item_zotero def test_get_doi_csl_item_zotero ( ) As of 2019-10-25, DOI Content Negotiation (i.e. crosscite) encodes the consortium in author.name rather than author.literal. Zotero translation-server encodes the consortium in author.family, which is better than nothing since it's a valid CSL JSON field. https://github.com/manubot/manubot/issues/158 View Source def test_get_doi_csl_item_zotero (): \"\"\" As of 2019-10-25, DOI Content Negotiation (i.e. crosscite) encodes the consortium in author.name rather than author.literal. Zotero translation-server encodes the consortium in author.family, which is better than nothing since it's a valid CSL JSON field. https://github.com/manubot/manubot/issues/158 \"\"\" doi = \"10.1038/ng.3834\" csl_item = get_doi_csl_item_zotero ( doi ) assert isinstance ( csl_item , dict ) csl_item [ \"author\" ][ 9 ][ \"family\" ] == \"GTEx Consortium\"","title":"Test Doi"},{"location":"reference/manubot/cite/tests/test_doi/#module-manubotciteteststest_doi","text":"View Source import pytest from manubot.cite.doi import ( expand_short_doi , get_doi_csl_item , get_doi_csl_item_zotero , get_doi_csl_item_crosscite , ) def test_expand_short_doi (): doi = expand_short_doi ( \"10/b6vnmd\" ) assert doi == \"10.1016/s0933-3657(96)00367-3\" def test_expand_short_doi_invalid (): with pytest . raises ( ValueError , match = \"Handle not found. Double check short_doi\" ): expand_short_doi ( \"10/b6vnmdxxxxxx\" ) def test_expand_short_doi_not_short (): with pytest . raises ( ValueError , match = \"shortDOIs start with `10/`\" ): expand_short_doi ( \"10.1016/S0933-3657(96)00367-3\" ) def test_get_doi_csl_item_crosscite (): doi = \"10.1101/142760\" csl_item = get_doi_csl_item_crosscite ( doi ) assert isinstance ( csl_item , dict ) csl_item [ \"publisher\" ] == \"Cold Spring Harbor Laboratory\" def test_get_doi_csl_item_zotero (): \"\"\" As of 2019-10-25, DOI Content Negotiation (i.e. crosscite) encodes the consortium in author.name rather than author.literal. Zotero translation-server encodes the consortium in author.family, which is better than nothing since it's a valid CSL JSON field. https://github.com/manubot/manubot/issues/158 \"\"\" doi = \"10.1038/ng.3834\" csl_item = get_doi_csl_item_zotero ( doi ) assert isinstance ( csl_item , dict ) csl_item [ \"author\" ][ 9 ][ \"family\" ] == \"GTEx Consortium\" def test_get_doi_csl_item (): \"\"\" Test URL is set with shortDOI when calling get_doi_csl_item. \"\"\" doi = \"10.1101/142760\" csl_item = get_doi_csl_item ( doi ) assert isinstance ( csl_item , dict ) assert csl_item [ \"URL\" ] == \"https://doi.org/gbpvh5\" def test_get_doi_crosscite_with_consortium_author (): \"\"\" Make sure the author \"GTEx Consortium\" is properly encoded using the `author.literal` CSL JSON field. References: - <https://github.com/manubot/manubot/issues/158> - <https://github.com/crosscite/content-negotiation/issues/92> \"\"\" doi = \"10.1038/ng.3834\" csl_item = get_doi_csl_item_crosscite ( doi ) assert isinstance ( csl_item , dict ) assert any ( author . get ( \"literal\" ) == \"GTEx Consortium\" for author in csl_item [ \"author\" ] )","title":"Module manubot.cite.tests.test_doi"},{"location":"reference/manubot/cite/tests/test_doi/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_doi/#test_expand_short_doi","text":"def test_expand_short_doi ( ) View Source def test_expand_short_doi (): doi = expand_short_doi ( \"10/b6vnmd\" ) assert doi == \"10.1016/s0933-3657(96)00367-3\"","title":"test_expand_short_doi"},{"location":"reference/manubot/cite/tests/test_doi/#test_expand_short_doi_invalid","text":"def test_expand_short_doi_invalid ( ) View Source def test_expand_short_doi_invalid (): with pytest . raises ( ValueError , match = \"Handle not found. Double check short_doi\" ): expand_short_doi ( \"10/b6vnmdxxxxxx\" )","title":"test_expand_short_doi_invalid"},{"location":"reference/manubot/cite/tests/test_doi/#test_expand_short_doi_not_short","text":"def test_expand_short_doi_not_short ( ) View Source def test_expand_short_doi_not_short (): with pytest . raises ( ValueError , match = \"shortDOIs start with `10/`\" ): expand_short_doi ( \"10.1016/S0933-3657(96)00367-3\" )","title":"test_expand_short_doi_not_short"},{"location":"reference/manubot/cite/tests/test_doi/#test_get_doi_crosscite_with_consortium_author","text":"def test_get_doi_crosscite_with_consortium_author ( ) Make sure the author \"GTEx Consortium\" is properly encoded using the author.literal CSL JSON field. References: https://github.com/manubot/manubot/issues/158 https://github.com/crosscite/content-negotiation/issues/92 View Source def test_get_doi_crosscite_with_consortium_author (): \"\"\" Make sure the author \" GTEx Consortium \" is properly encoded using the `author.literal` CSL JSON field. References: - <https://github.com/manubot/manubot/issues/158> - <https://github.com/crosscite/content-negotiation/issues/92> \"\"\" doi = \"10.1038/ng.3834\" csl_item = get_doi_csl_item_crosscite ( doi ) assert isinstance ( csl_item , dict ) assert any ( author . get ( \"literal\" ) == \"GTEx Consortium\" for author in csl_item [ \"author\" ] )","title":"test_get_doi_crosscite_with_consortium_author"},{"location":"reference/manubot/cite/tests/test_doi/#test_get_doi_csl_item","text":"def test_get_doi_csl_item ( ) Test URL is set with shortDOI when calling get_doi_csl_item. View Source def test_get_doi_csl_item (): \"\"\" Test URL is set with shortDOI when calling get_doi_csl_item. \"\"\" doi = \"10.1101/142760\" csl_item = get_doi_csl_item ( doi ) assert isinstance ( csl_item , dict ) assert csl_item [ \"URL\" ] == \"https://doi.org/gbpvh5\"","title":"test_get_doi_csl_item"},{"location":"reference/manubot/cite/tests/test_doi/#test_get_doi_csl_item_crosscite","text":"def test_get_doi_csl_item_crosscite ( ) View Source def test_get_doi_csl_item_crosscite (): doi = \"10.1101/142760\" csl_item = get_doi_csl_item_crosscite ( doi ) assert isinstance ( csl_item , dict ) csl_item [ \"publisher\" ] == \"Cold Spring Harbor Laboratory\"","title":"test_get_doi_csl_item_crosscite"},{"location":"reference/manubot/cite/tests/test_doi/#test_get_doi_csl_item_zotero","text":"def test_get_doi_csl_item_zotero ( ) As of 2019-10-25, DOI Content Negotiation (i.e. crosscite) encodes the consortium in author.name rather than author.literal. Zotero translation-server encodes the consortium in author.family, which is better than nothing since it's a valid CSL JSON field. https://github.com/manubot/manubot/issues/158 View Source def test_get_doi_csl_item_zotero (): \"\"\" As of 2019-10-25, DOI Content Negotiation (i.e. crosscite) encodes the consortium in author.name rather than author.literal. Zotero translation-server encodes the consortium in author.family, which is better than nothing since it's a valid CSL JSON field. https://github.com/manubot/manubot/issues/158 \"\"\" doi = \"10.1038/ng.3834\" csl_item = get_doi_csl_item_zotero ( doi ) assert isinstance ( csl_item , dict ) csl_item [ \"author\" ][ 9 ][ \"family\" ] == \"GTEx Consortium\"","title":"test_get_doi_csl_item_zotero"},{"location":"reference/manubot/cite/tests/test_handlers/","text":"Module manubot.cite.tests.test_handlers View Source from ..handlers import _generate_prefix_to_handler , prefix_to_handler import pytest def test_prefix_to_handler (): \"\"\" If this test fails, copy the output from `print(expected)` to use as the value for `handlers.prefix_to_handler`. \"\"\" expected = _generate_prefix_to_handler () print ( expected ) assert prefix_to_handler == expected @pytest.mark.parametrize ( \"prefix\" , [ \"raw\" , \"tag\" ]) def test_legacy_prefixes_are_unhandled ( prefix ): \"\"\" For backwards compatability, these prefixes should be unhandled. In the past, these prefixes referred to citekeys that were not resolvable identifiers by themselves. \"\"\" assert prefix not in prefix_to_handler Variables prefix_to_handler Functions test_legacy_prefixes_are_unhandled def test_legacy_prefixes_are_unhandled ( prefix ) For backwards compatability, these prefixes should be unhandled. In the past, these prefixes referred to citekeys that were not resolvable identifiers by themselves. View Source @pytest . mark . parametrize ( \"prefix\" , [ \"raw\", \"tag\" ] ) def test_legacy_prefixes_are_unhandled ( prefix ) : \"\"\" For backwards compatability, these prefixes should be unhandled. In the past, these prefixes referred to citekeys that were not resolvable identifiers by themselves. \"\"\" assert prefix not in prefix_to_handler test_prefix_to_handler def test_prefix_to_handler ( ) If this test fails, copy the output from print(expected) to use as the value for handlers.prefix_to_handler . View Source def test_prefix_to_handler (): \"\"\" If this test fails, copy the output from `print(expected)` to use as the value for `handlers.prefix_to_handler`. \"\"\" expected = _generate_prefix_to_handler () print ( expected ) assert prefix_to_handler == expected","title":"Test Handlers"},{"location":"reference/manubot/cite/tests/test_handlers/#module-manubotciteteststest_handlers","text":"View Source from ..handlers import _generate_prefix_to_handler , prefix_to_handler import pytest def test_prefix_to_handler (): \"\"\" If this test fails, copy the output from `print(expected)` to use as the value for `handlers.prefix_to_handler`. \"\"\" expected = _generate_prefix_to_handler () print ( expected ) assert prefix_to_handler == expected @pytest.mark.parametrize ( \"prefix\" , [ \"raw\" , \"tag\" ]) def test_legacy_prefixes_are_unhandled ( prefix ): \"\"\" For backwards compatability, these prefixes should be unhandled. In the past, these prefixes referred to citekeys that were not resolvable identifiers by themselves. \"\"\" assert prefix not in prefix_to_handler","title":"Module manubot.cite.tests.test_handlers"},{"location":"reference/manubot/cite/tests/test_handlers/#variables","text":"prefix_to_handler","title":"Variables"},{"location":"reference/manubot/cite/tests/test_handlers/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_handlers/#test_legacy_prefixes_are_unhandled","text":"def test_legacy_prefixes_are_unhandled ( prefix ) For backwards compatability, these prefixes should be unhandled. In the past, these prefixes referred to citekeys that were not resolvable identifiers by themselves. View Source @pytest . mark . parametrize ( \"prefix\" , [ \"raw\", \"tag\" ] ) def test_legacy_prefixes_are_unhandled ( prefix ) : \"\"\" For backwards compatability, these prefixes should be unhandled. In the past, these prefixes referred to citekeys that were not resolvable identifiers by themselves. \"\"\" assert prefix not in prefix_to_handler","title":"test_legacy_prefixes_are_unhandled"},{"location":"reference/manubot/cite/tests/test_handlers/#test_prefix_to_handler","text":"def test_prefix_to_handler ( ) If this test fails, copy the output from print(expected) to use as the value for handlers.prefix_to_handler . View Source def test_prefix_to_handler (): \"\"\" If this test fails, copy the output from `print(expected)` to use as the value for `handlers.prefix_to_handler`. \"\"\" expected = _generate_prefix_to_handler () print ( expected ) assert prefix_to_handler == expected","title":"test_prefix_to_handler"},{"location":"reference/manubot/cite/tests/test_isbn/","text":"Module manubot.cite.tests.test_isbn View Source import pytest from manubot.cite.isbn import ( get_isbn_csl_item_citoid , get_isbn_csl_item_isbnlib , get_isbn_csl_item_zotero , ) @pytest.mark.xfail ( reason = \"Quotation in title removed at some upstream point\" ) def test_citekey_to_csl_item_isbnlib_title_with_quotation_mark (): csl_item = get_isbn_csl_item_isbnlib ( \"9780312353780\" ) assert csl_item [ \"type\" ] == \"book\" assert csl_item [ \"title\" ] . startswith ( '\"F\" is for Fugitive' ) def test_get_isbn_csl_item_citoid_weird_date (): \"\"\" isbn:9780719561023 has a date value of \"(2004 printing)\" https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9780719561023 \"\"\" csl_item = get_isbn_csl_item_citoid ( \"9780719561023\" ) assert csl_item [ \"issued\" ][ \"date-parts\" ] == [[ 2004 ]] assert csl_item [ \"ISBN\" ] == \"9780719561023\" def test_get_isbn_csl_item_citoid_not_found (): \"\"\" isbn:9781439566039 is not found by Citoid: https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9781439566039 \"\"\" with pytest . raises ( KeyError , match = r \"Metadata for ISBN [0-9]{10,13} not found\" ): get_isbn_csl_item_citoid ( \"9781439566039\" ) def test_get_isbn_csl_item_zotero_with_note_issue (): \"\"\" translation-server returns two metadata records for this ISBN. The second has itemType=note and previously caused CSL export to fail. https://github.com/zotero/translation-server/issues/67 \"\"\" isbn = \"9780262517638\" csl_item = get_isbn_csl_item_zotero ( isbn ) assert csl_item [ \"author\" ][ 0 ][ \"family\" ] == \"Suber\" Functions test_citekey_to_csl_item_isbnlib_title_with_quotation_mark def test_citekey_to_csl_item_isbnlib_title_with_quotation_mark ( ) View Source @pytest . mark . xfail ( reason = \"Quotation in title removed at some upstream point\" ) def test_citekey_to_csl_item_isbnlib_title_with_quotation_mark () : csl_item = get_isbn_csl_item_isbnlib ( \"9780312353780\" ) assert csl_item [ \"type\" ] == \"book\" assert csl_item [ \"title\" ] . startswith ( '\"F\" is for Fugitive' ) test_get_isbn_csl_item_citoid_not_found def test_get_isbn_csl_item_citoid_not_found ( ) isbn:9781439566039 is not found by Citoid: https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9781439566039 View Source def test_get_isbn_csl_item_citoid_not_found (): \"\"\" isbn:9781439566039 is not found by Citoid: https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9781439566039 \"\"\" with pytest . raises ( KeyError , match = r \"Metadata for ISBN [0-9]{10,13} not found\" ): get_isbn_csl_item_citoid ( \"9781439566039\" ) test_get_isbn_csl_item_citoid_weird_date def test_get_isbn_csl_item_citoid_weird_date ( ) isbn:9780719561023 has a date value of \"(2004 printing)\" https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9780719561023 View Source def test_get_isbn_csl_item_citoid_weird_date (): \"\"\" isbn:9780719561023 has a date value of \" ( 2004 printing ) \" https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9780719561023 \"\"\" csl_item = get_isbn_csl_item_citoid ( \"9780719561023\" ) assert csl_item [ \"issued\" ][ \"date-parts\" ] == [[ 2004 ]] assert csl_item [ \"ISBN\" ] == \"9780719561023\" test_get_isbn_csl_item_zotero_with_note_issue def test_get_isbn_csl_item_zotero_with_note_issue ( ) translation-server returns two metadata records for this ISBN. The second has itemType=note and previously caused CSL export to fail. https://github.com/zotero/translation-server/issues/67 View Source def test_get_isbn_csl_item_zotero_with_note_issue (): \"\"\" translation-server returns two metadata records for this ISBN. The second has itemType=note and previously caused CSL export to fail. https://github.com/zotero/translation-server/issues/67 \"\"\" isbn = \"9780262517638\" csl_item = get_isbn_csl_item_zotero ( isbn ) assert csl_item [ \"author\" ][ 0 ][ \"family\" ] == \"Suber\"","title":"Test Isbn"},{"location":"reference/manubot/cite/tests/test_isbn/#module-manubotciteteststest_isbn","text":"View Source import pytest from manubot.cite.isbn import ( get_isbn_csl_item_citoid , get_isbn_csl_item_isbnlib , get_isbn_csl_item_zotero , ) @pytest.mark.xfail ( reason = \"Quotation in title removed at some upstream point\" ) def test_citekey_to_csl_item_isbnlib_title_with_quotation_mark (): csl_item = get_isbn_csl_item_isbnlib ( \"9780312353780\" ) assert csl_item [ \"type\" ] == \"book\" assert csl_item [ \"title\" ] . startswith ( '\"F\" is for Fugitive' ) def test_get_isbn_csl_item_citoid_weird_date (): \"\"\" isbn:9780719561023 has a date value of \"(2004 printing)\" https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9780719561023 \"\"\" csl_item = get_isbn_csl_item_citoid ( \"9780719561023\" ) assert csl_item [ \"issued\" ][ \"date-parts\" ] == [[ 2004 ]] assert csl_item [ \"ISBN\" ] == \"9780719561023\" def test_get_isbn_csl_item_citoid_not_found (): \"\"\" isbn:9781439566039 is not found by Citoid: https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9781439566039 \"\"\" with pytest . raises ( KeyError , match = r \"Metadata for ISBN [0-9]{10,13} not found\" ): get_isbn_csl_item_citoid ( \"9781439566039\" ) def test_get_isbn_csl_item_zotero_with_note_issue (): \"\"\" translation-server returns two metadata records for this ISBN. The second has itemType=note and previously caused CSL export to fail. https://github.com/zotero/translation-server/issues/67 \"\"\" isbn = \"9780262517638\" csl_item = get_isbn_csl_item_zotero ( isbn ) assert csl_item [ \"author\" ][ 0 ][ \"family\" ] == \"Suber\"","title":"Module manubot.cite.tests.test_isbn"},{"location":"reference/manubot/cite/tests/test_isbn/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_isbn/#test_citekey_to_csl_item_isbnlib_title_with_quotation_mark","text":"def test_citekey_to_csl_item_isbnlib_title_with_quotation_mark ( ) View Source @pytest . mark . xfail ( reason = \"Quotation in title removed at some upstream point\" ) def test_citekey_to_csl_item_isbnlib_title_with_quotation_mark () : csl_item = get_isbn_csl_item_isbnlib ( \"9780312353780\" ) assert csl_item [ \"type\" ] == \"book\" assert csl_item [ \"title\" ] . startswith ( '\"F\" is for Fugitive' )","title":"test_citekey_to_csl_item_isbnlib_title_with_quotation_mark"},{"location":"reference/manubot/cite/tests/test_isbn/#test_get_isbn_csl_item_citoid_not_found","text":"def test_get_isbn_csl_item_citoid_not_found ( ) isbn:9781439566039 is not found by Citoid: https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9781439566039 View Source def test_get_isbn_csl_item_citoid_not_found (): \"\"\" isbn:9781439566039 is not found by Citoid: https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9781439566039 \"\"\" with pytest . raises ( KeyError , match = r \"Metadata for ISBN [0-9]{10,13} not found\" ): get_isbn_csl_item_citoid ( \"9781439566039\" )","title":"test_get_isbn_csl_item_citoid_not_found"},{"location":"reference/manubot/cite/tests/test_isbn/#test_get_isbn_csl_item_citoid_weird_date","text":"def test_get_isbn_csl_item_citoid_weird_date ( ) isbn:9780719561023 has a date value of \"(2004 printing)\" https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9780719561023 View Source def test_get_isbn_csl_item_citoid_weird_date (): \"\"\" isbn:9780719561023 has a date value of \" ( 2004 printing ) \" https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9780719561023 \"\"\" csl_item = get_isbn_csl_item_citoid ( \"9780719561023\" ) assert csl_item [ \"issued\" ][ \"date-parts\" ] == [[ 2004 ]] assert csl_item [ \"ISBN\" ] == \"9780719561023\"","title":"test_get_isbn_csl_item_citoid_weird_date"},{"location":"reference/manubot/cite/tests/test_isbn/#test_get_isbn_csl_item_zotero_with_note_issue","text":"def test_get_isbn_csl_item_zotero_with_note_issue ( ) translation-server returns two metadata records for this ISBN. The second has itemType=note and previously caused CSL export to fail. https://github.com/zotero/translation-server/issues/67 View Source def test_get_isbn_csl_item_zotero_with_note_issue (): \"\"\" translation-server returns two metadata records for this ISBN. The second has itemType=note and previously caused CSL export to fail. https://github.com/zotero/translation-server/issues/67 \"\"\" isbn = \"9780262517638\" csl_item = get_isbn_csl_item_zotero ( isbn ) assert csl_item [ \"author\" ][ 0 ][ \"family\" ] == \"Suber\"","title":"test_get_isbn_csl_item_zotero_with_note_issue"},{"location":"reference/manubot/cite/tests/test_pubmed/","text":"Module manubot.cite.tests.test_pubmed View Source import pytest from manubot.cite.pubmed import ( get_pmcid_and_pmid_for_doi , get_pmid_for_doi , get_pubmed_ids_for_doi , ) @pytest.mark.parametrize ( ( \"doi\" , \"pmid\" ), [ ( \"10.1098/rsif.2017.0387\" , \"29618526\" ), # in PubMed and PMC ( \"10.1161/CIRCGENETICS.115.001181\" , \"27094199\" ), # in PubMed but not PMC ( \"10.7717/peerj-cs.134\" , None ), # DOI in journal not indexed by PubMed ( \"10.1161/CIRC\" , None ), # invalid DOI ], ) def test_get_pmid_for_doi ( doi , pmid ): output = get_pmid_for_doi ( doi ) assert pmid == output @pytest.mark.parametrize ( ( \"doi\" , \"id_dict\" ), [ ( \"10.1098/rsif.2017.0387\" , { \"PMCID\" : \"PMC5938574\" , \"PMID\" : \"29618526\" }), ( \"10.7554/ELIFE.32822\" , { \"PMCID\" : \"PMC5832410\" , \"PMID\" : \"29424689\" }), ( \"10.1161/CIRCGENETICS.115.001181\" , {}), # only in PubMed, not in PMC ( \"10.7717/peerj.000\" , {}), # Non-existent DOI ( \"10.peerj.000\" , {}), # malformed DOI ], ) def test_get_pmcid_and_pmid_for_doi ( doi , id_dict ): output = get_pmcid_and_pmid_for_doi ( doi ) assert id_dict == output @pytest.mark.parametrize ( ( \"doi\" , \"id_dict\" ), [ ( \"10.1098/rsif.2017.0387\" , { \"PMCID\" : \"PMC5938574\" , \"PMID\" : \"29618526\" }), ( \"10.7554/ELIFE.32822\" , { \"PMCID\" : \"PMC5832410\" , \"PMID\" : \"29424689\" }), ( \"10.1161/CIRCGENETICS.115.001181\" , { \"PMID\" : \"27094199\" }, ), # only in PubMed, not in PMC ( \"10.7717/peerj.000\" , {}), # Non-existent DOI ], ) def test_get_pubmed_ids_for_doi ( doi , id_dict ): output = get_pubmed_ids_for_doi ( doi ) assert id_dict == output Functions test_get_pmcid_and_pmid_for_doi def test_get_pmcid_and_pmid_for_doi ( doi , id_dict ) View Source @pytest . mark . parametrize ( ( \"doi\" , \"id_dict\" ), [ (\"10.1098/rsif.2017.0387\", {\"PMCID\": \"PMC5938574\", \"PMID\": \"29618526\"}), (\"10.7554/ELIFE.32822\", {\"PMCID\": \"PMC5832410\", \"PMID\": \"29424689\"}), (\"10.1161/CIRCGENETICS.115.001181\", {}), # only in PubMed, not in PMC (\"10.7717/peerj.000\", {}), # Non-existent DOI (\"10.peerj.000\", {}), # malformed DOI ] , ) def test_get_pmcid_and_pmid_for_doi ( doi , id_dict ) : output = get_pmcid_and_pmid_for_doi ( doi ) assert id_dict == output test_get_pmid_for_doi def test_get_pmid_for_doi ( doi , pmid ) View Source @pytest . mark . parametrize ( ( \"doi\" , \"pmid\" ), [ (\"10.1098/rsif.2017.0387\", \"29618526\"), # in PubMed and PMC (\"10.1161/CIRCGENETICS.115.001181\", \"27094199\"), # in PubMed but not PMC (\"10.7717/peerj-cs.134\", None), # DOI in journal not indexed by PubMed (\"10.1161/CIRC\", None), # invalid DOI ] , ) def test_get_pmid_for_doi ( doi , pmid ) : output = get_pmid_for_doi ( doi ) assert pmid == output test_get_pubmed_ids_for_doi def test_get_pubmed_ids_for_doi ( doi , id_dict ) View Source @pytest . mark . parametrize ( ( \"doi\" , \"id_dict\" ), [ (\"10.1098/rsif.2017.0387\", {\"PMCID\": \"PMC5938574\", \"PMID\": \"29618526\"}), (\"10.7554/ELIFE.32822\", {\"PMCID\": \"PMC5832410\", \"PMID\": \"29424689\"}), ( \"10.1161/CIRCGENETICS.115.001181\", {\"PMID\": \"27094199\"}, ), # only in PubMed, not in PMC (\"10.7717/peerj.000\", {}), # Non-existent DOI ] , ) def test_get_pubmed_ids_for_doi ( doi , id_dict ) : output = get_pubmed_ids_for_doi ( doi ) assert id_dict == output","title":"Test Pubmed"},{"location":"reference/manubot/cite/tests/test_pubmed/#module-manubotciteteststest_pubmed","text":"View Source import pytest from manubot.cite.pubmed import ( get_pmcid_and_pmid_for_doi , get_pmid_for_doi , get_pubmed_ids_for_doi , ) @pytest.mark.parametrize ( ( \"doi\" , \"pmid\" ), [ ( \"10.1098/rsif.2017.0387\" , \"29618526\" ), # in PubMed and PMC ( \"10.1161/CIRCGENETICS.115.001181\" , \"27094199\" ), # in PubMed but not PMC ( \"10.7717/peerj-cs.134\" , None ), # DOI in journal not indexed by PubMed ( \"10.1161/CIRC\" , None ), # invalid DOI ], ) def test_get_pmid_for_doi ( doi , pmid ): output = get_pmid_for_doi ( doi ) assert pmid == output @pytest.mark.parametrize ( ( \"doi\" , \"id_dict\" ), [ ( \"10.1098/rsif.2017.0387\" , { \"PMCID\" : \"PMC5938574\" , \"PMID\" : \"29618526\" }), ( \"10.7554/ELIFE.32822\" , { \"PMCID\" : \"PMC5832410\" , \"PMID\" : \"29424689\" }), ( \"10.1161/CIRCGENETICS.115.001181\" , {}), # only in PubMed, not in PMC ( \"10.7717/peerj.000\" , {}), # Non-existent DOI ( \"10.peerj.000\" , {}), # malformed DOI ], ) def test_get_pmcid_and_pmid_for_doi ( doi , id_dict ): output = get_pmcid_and_pmid_for_doi ( doi ) assert id_dict == output @pytest.mark.parametrize ( ( \"doi\" , \"id_dict\" ), [ ( \"10.1098/rsif.2017.0387\" , { \"PMCID\" : \"PMC5938574\" , \"PMID\" : \"29618526\" }), ( \"10.7554/ELIFE.32822\" , { \"PMCID\" : \"PMC5832410\" , \"PMID\" : \"29424689\" }), ( \"10.1161/CIRCGENETICS.115.001181\" , { \"PMID\" : \"27094199\" }, ), # only in PubMed, not in PMC ( \"10.7717/peerj.000\" , {}), # Non-existent DOI ], ) def test_get_pubmed_ids_for_doi ( doi , id_dict ): output = get_pubmed_ids_for_doi ( doi ) assert id_dict == output","title":"Module manubot.cite.tests.test_pubmed"},{"location":"reference/manubot/cite/tests/test_pubmed/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_pubmed/#test_get_pmcid_and_pmid_for_doi","text":"def test_get_pmcid_and_pmid_for_doi ( doi , id_dict ) View Source @pytest . mark . parametrize ( ( \"doi\" , \"id_dict\" ), [ (\"10.1098/rsif.2017.0387\", {\"PMCID\": \"PMC5938574\", \"PMID\": \"29618526\"}), (\"10.7554/ELIFE.32822\", {\"PMCID\": \"PMC5832410\", \"PMID\": \"29424689\"}), (\"10.1161/CIRCGENETICS.115.001181\", {}), # only in PubMed, not in PMC (\"10.7717/peerj.000\", {}), # Non-existent DOI (\"10.peerj.000\", {}), # malformed DOI ] , ) def test_get_pmcid_and_pmid_for_doi ( doi , id_dict ) : output = get_pmcid_and_pmid_for_doi ( doi ) assert id_dict == output","title":"test_get_pmcid_and_pmid_for_doi"},{"location":"reference/manubot/cite/tests/test_pubmed/#test_get_pmid_for_doi","text":"def test_get_pmid_for_doi ( doi , pmid ) View Source @pytest . mark . parametrize ( ( \"doi\" , \"pmid\" ), [ (\"10.1098/rsif.2017.0387\", \"29618526\"), # in PubMed and PMC (\"10.1161/CIRCGENETICS.115.001181\", \"27094199\"), # in PubMed but not PMC (\"10.7717/peerj-cs.134\", None), # DOI in journal not indexed by PubMed (\"10.1161/CIRC\", None), # invalid DOI ] , ) def test_get_pmid_for_doi ( doi , pmid ) : output = get_pmid_for_doi ( doi ) assert pmid == output","title":"test_get_pmid_for_doi"},{"location":"reference/manubot/cite/tests/test_pubmed/#test_get_pubmed_ids_for_doi","text":"def test_get_pubmed_ids_for_doi ( doi , id_dict ) View Source @pytest . mark . parametrize ( ( \"doi\" , \"id_dict\" ), [ (\"10.1098/rsif.2017.0387\", {\"PMCID\": \"PMC5938574\", \"PMID\": \"29618526\"}), (\"10.7554/ELIFE.32822\", {\"PMCID\": \"PMC5832410\", \"PMID\": \"29424689\"}), ( \"10.1161/CIRCGENETICS.115.001181\", {\"PMID\": \"27094199\"}, ), # only in PubMed, not in PMC (\"10.7717/peerj.000\", {}), # Non-existent DOI ] , ) def test_get_pubmed_ids_for_doi ( doi , id_dict ) : output = get_pubmed_ids_for_doi ( doi ) assert id_dict == output","title":"test_get_pubmed_ids_for_doi"},{"location":"reference/manubot/cite/tests/test_unpaywall/","text":"Module manubot.cite.tests.test_unpaywall View Source from ..unpaywall import Unpaywall , Unpaywall_DOI , Unpaywall_arXiv def test_unpaywall_doi (): doi = \"10.1371/journal.pcbi.1007250\" unpaywall = Unpaywall_DOI ( doi ) assert isinstance ( unpaywall . oa_locations , list ) assert unpaywall . best_pdf . has_creative_commons_license def test_unpaywall_arxiv (): arxiv_id = \"1912.04616\" unpaywall = Unpaywall_arXiv ( arxiv_id , use_doi = False ) assert isinstance ( unpaywall . oa_locations , list ) best_pdf = unpaywall . best_pdf assert isinstance ( best_pdf , dict ) assert best_pdf [ \"url\" ] == \"https://arxiv.org/pdf/1912.04616.pdf\" assert best_pdf [ \"url_for_landing_page\" ] == \"https://arxiv.org/abs/1912.04616\" assert best_pdf [ \"license\" ] == \"cc-by-sa\" def test_unpaywall_from_citekey (): \"\"\" https://arxiv.org/abs/1906.11964 is now published in https://doi.org/10.1162/qss_a_00023. Therefore, locations are coming from Unpaywall_DOI since defaulting to use_doi=True. \"\"\" unpaywall = Unpaywall . from_citekey ( \"arxiv:1906.11964v3\" ) assert isinstance ( unpaywall , Unpaywall_arXiv ) best_pdf = unpaywall . best_pdf assert \"arxiv.org/abs/1906.11964\" in best_pdf [ \"url_for_landing_page\" ] def test_unpaywall_from_csl_item (): csl_item = { \"id\" : \"ijxfHyzg\" , \"URL\" : \"https://arxiv.org/abs/1908.11459\" , \"title\" : \"Introducing: The Game Jam License\" , \"note\" : \"license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/ \\n This CSL JSON Item was automatically generated by Manubot v0.3.0 using citation-by-identifier. \\n standard_id: arxiv:1908.11459\" , } unpaywall = Unpaywall . from_csl_item ( csl_item ) assert isinstance ( unpaywall , Unpaywall_arXiv ) best_pdf = unpaywall . best_pdf assert best_pdf [ \"url_for_landing_page\" ] == \"https://arxiv.org/abs/1908.11459\" def test_unpaywall_from_csl_item_with_doi (): csl_item = { \"id\" : \"ijxfHyzg\" , \"URL\" : \"https://arxiv.org/abs/1908.11459\" , \"title\" : \"Introducing: The Game Jam License\" , \"note\" : \"license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/ \\n This CSL JSON Item was automatically generated by Manubot v0.3.0 using citation-by-identifier. \\n standard_id: arxiv:1908.11459\" , \"DOI\" : \"10.1145/3337722.3341844\" , } unpaywall = Unpaywall . from_csl_item ( csl_item ) # Unpaywall.from_csl_item uses DOI lookup when available assert isinstance ( unpaywall , Unpaywall_DOI ) assert unpaywall . best_pdf [ \"url_for_pdf\" ] Functions test_unpaywall_arxiv def test_unpaywall_arxiv ( ) View Source def test_unpaywall_arxiv (): arxiv_id = \"1912.04616\" unpaywall = Unpaywall_arXiv ( arxiv_id , use_doi = False ) assert isinstance ( unpaywall . oa_locations , list ) best_pdf = unpaywall . best_pdf assert isinstance ( best_pdf , dict ) assert best_pdf [ \"url\" ] == \"https://arxiv.org/pdf/1912.04616.pdf\" assert best_pdf [ \"url_for_landing_page\" ] == \"https://arxiv.org/abs/1912.04616\" assert best_pdf [ \"license\" ] == \"cc-by-sa\" test_unpaywall_doi def test_unpaywall_doi ( ) View Source def test_unpaywall_doi (): doi = \"10.1371/journal.pcbi.1007250\" unpaywall = Unpaywall_DOI ( doi ) assert isinstance ( unpaywall . oa_locations , list ) assert unpaywall . best_pdf . has_creative_commons_license test_unpaywall_from_citekey def test_unpaywall_from_citekey ( ) https://arxiv.org/abs/1906.11964 is now published in https://doi.org/10.1162/qss_a_00023. Therefore, locations are coming from Unpaywall_DOI since defaulting to use_doi=True. View Source def test_unpaywall_from_citekey (): \"\"\" https://arxiv.org/abs/1906.11964 is now published in https://doi.org/10.1162/qss_a_00023. Therefore, locations are coming from Unpaywall_DOI since defaulting to use_doi=True. \"\"\" unpaywall = Unpaywall . from_citekey ( \"arxiv:1906.11964v3\" ) assert isinstance ( unpaywall , Unpaywall_arXiv ) best_pdf = unpaywall . best_pdf assert \"arxiv.org/abs/1906.11964\" in best_pdf [ \"url_for_landing_page\" ] test_unpaywall_from_csl_item def test_unpaywall_from_csl_item ( ) View Source def test_unpaywall_from_csl_item (): csl_item = { \"id\" : \"ijxfHyzg\" , \"URL\" : \"https://arxiv.org/abs/1908.11459\" , \"title\" : \"Introducing: The Game Jam License\" , \"note\" : \"license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\\nThis CSL JSON Item was automatically generated by Manubot v0.3.0 using citation-by-identifier.\\nstandard_id: arxiv:1908.11459\" , } unpaywall = Unpaywall . from_csl_item ( csl_item ) assert isinstance ( unpaywall , Unpaywall_arXiv ) best_pdf = unpaywall . best_pdf assert best_pdf [ \"url_for_landing_page\" ] == \"https://arxiv.org/abs/1908.11459\" test_unpaywall_from_csl_item_with_doi def test_unpaywall_from_csl_item_with_doi ( ) View Source def test_unpaywall_from_csl_item_with_doi (): csl_item = { \"id\" : \"ijxfHyzg\" , \"URL\" : \"https://arxiv.org/abs/1908.11459\" , \"title\" : \"Introducing: The Game Jam License\" , \"note\" : \"license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\\nThis CSL JSON Item was automatically generated by Manubot v0.3.0 using citation-by-identifier.\\nstandard_id: arxiv:1908.11459\" , \"DOI\" : \"10.1145/3337722.3341844\" , } unpaywall = Unpaywall . from_csl_item ( csl_item ) # Unpaywall . from_csl_item uses DOI lookup when available assert isinstance ( unpaywall , Unpaywall_DOI ) assert unpaywall . best_pdf [ \"url_for_pdf\" ]","title":"Test Unpaywall"},{"location":"reference/manubot/cite/tests/test_unpaywall/#module-manubotciteteststest_unpaywall","text":"View Source from ..unpaywall import Unpaywall , Unpaywall_DOI , Unpaywall_arXiv def test_unpaywall_doi (): doi = \"10.1371/journal.pcbi.1007250\" unpaywall = Unpaywall_DOI ( doi ) assert isinstance ( unpaywall . oa_locations , list ) assert unpaywall . best_pdf . has_creative_commons_license def test_unpaywall_arxiv (): arxiv_id = \"1912.04616\" unpaywall = Unpaywall_arXiv ( arxiv_id , use_doi = False ) assert isinstance ( unpaywall . oa_locations , list ) best_pdf = unpaywall . best_pdf assert isinstance ( best_pdf , dict ) assert best_pdf [ \"url\" ] == \"https://arxiv.org/pdf/1912.04616.pdf\" assert best_pdf [ \"url_for_landing_page\" ] == \"https://arxiv.org/abs/1912.04616\" assert best_pdf [ \"license\" ] == \"cc-by-sa\" def test_unpaywall_from_citekey (): \"\"\" https://arxiv.org/abs/1906.11964 is now published in https://doi.org/10.1162/qss_a_00023. Therefore, locations are coming from Unpaywall_DOI since defaulting to use_doi=True. \"\"\" unpaywall = Unpaywall . from_citekey ( \"arxiv:1906.11964v3\" ) assert isinstance ( unpaywall , Unpaywall_arXiv ) best_pdf = unpaywall . best_pdf assert \"arxiv.org/abs/1906.11964\" in best_pdf [ \"url_for_landing_page\" ] def test_unpaywall_from_csl_item (): csl_item = { \"id\" : \"ijxfHyzg\" , \"URL\" : \"https://arxiv.org/abs/1908.11459\" , \"title\" : \"Introducing: The Game Jam License\" , \"note\" : \"license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/ \\n This CSL JSON Item was automatically generated by Manubot v0.3.0 using citation-by-identifier. \\n standard_id: arxiv:1908.11459\" , } unpaywall = Unpaywall . from_csl_item ( csl_item ) assert isinstance ( unpaywall , Unpaywall_arXiv ) best_pdf = unpaywall . best_pdf assert best_pdf [ \"url_for_landing_page\" ] == \"https://arxiv.org/abs/1908.11459\" def test_unpaywall_from_csl_item_with_doi (): csl_item = { \"id\" : \"ijxfHyzg\" , \"URL\" : \"https://arxiv.org/abs/1908.11459\" , \"title\" : \"Introducing: The Game Jam License\" , \"note\" : \"license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/ \\n This CSL JSON Item was automatically generated by Manubot v0.3.0 using citation-by-identifier. \\n standard_id: arxiv:1908.11459\" , \"DOI\" : \"10.1145/3337722.3341844\" , } unpaywall = Unpaywall . from_csl_item ( csl_item ) # Unpaywall.from_csl_item uses DOI lookup when available assert isinstance ( unpaywall , Unpaywall_DOI ) assert unpaywall . best_pdf [ \"url_for_pdf\" ]","title":"Module manubot.cite.tests.test_unpaywall"},{"location":"reference/manubot/cite/tests/test_unpaywall/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_unpaywall/#test_unpaywall_arxiv","text":"def test_unpaywall_arxiv ( ) View Source def test_unpaywall_arxiv (): arxiv_id = \"1912.04616\" unpaywall = Unpaywall_arXiv ( arxiv_id , use_doi = False ) assert isinstance ( unpaywall . oa_locations , list ) best_pdf = unpaywall . best_pdf assert isinstance ( best_pdf , dict ) assert best_pdf [ \"url\" ] == \"https://arxiv.org/pdf/1912.04616.pdf\" assert best_pdf [ \"url_for_landing_page\" ] == \"https://arxiv.org/abs/1912.04616\" assert best_pdf [ \"license\" ] == \"cc-by-sa\"","title":"test_unpaywall_arxiv"},{"location":"reference/manubot/cite/tests/test_unpaywall/#test_unpaywall_doi","text":"def test_unpaywall_doi ( ) View Source def test_unpaywall_doi (): doi = \"10.1371/journal.pcbi.1007250\" unpaywall = Unpaywall_DOI ( doi ) assert isinstance ( unpaywall . oa_locations , list ) assert unpaywall . best_pdf . has_creative_commons_license","title":"test_unpaywall_doi"},{"location":"reference/manubot/cite/tests/test_unpaywall/#test_unpaywall_from_citekey","text":"def test_unpaywall_from_citekey ( ) https://arxiv.org/abs/1906.11964 is now published in https://doi.org/10.1162/qss_a_00023. Therefore, locations are coming from Unpaywall_DOI since defaulting to use_doi=True. View Source def test_unpaywall_from_citekey (): \"\"\" https://arxiv.org/abs/1906.11964 is now published in https://doi.org/10.1162/qss_a_00023. Therefore, locations are coming from Unpaywall_DOI since defaulting to use_doi=True. \"\"\" unpaywall = Unpaywall . from_citekey ( \"arxiv:1906.11964v3\" ) assert isinstance ( unpaywall , Unpaywall_arXiv ) best_pdf = unpaywall . best_pdf assert \"arxiv.org/abs/1906.11964\" in best_pdf [ \"url_for_landing_page\" ]","title":"test_unpaywall_from_citekey"},{"location":"reference/manubot/cite/tests/test_unpaywall/#test_unpaywall_from_csl_item","text":"def test_unpaywall_from_csl_item ( ) View Source def test_unpaywall_from_csl_item (): csl_item = { \"id\" : \"ijxfHyzg\" , \"URL\" : \"https://arxiv.org/abs/1908.11459\" , \"title\" : \"Introducing: The Game Jam License\" , \"note\" : \"license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\\nThis CSL JSON Item was automatically generated by Manubot v0.3.0 using citation-by-identifier.\\nstandard_id: arxiv:1908.11459\" , } unpaywall = Unpaywall . from_csl_item ( csl_item ) assert isinstance ( unpaywall , Unpaywall_arXiv ) best_pdf = unpaywall . best_pdf assert best_pdf [ \"url_for_landing_page\" ] == \"https://arxiv.org/abs/1908.11459\"","title":"test_unpaywall_from_csl_item"},{"location":"reference/manubot/cite/tests/test_unpaywall/#test_unpaywall_from_csl_item_with_doi","text":"def test_unpaywall_from_csl_item_with_doi ( ) View Source def test_unpaywall_from_csl_item_with_doi (): csl_item = { \"id\" : \"ijxfHyzg\" , \"URL\" : \"https://arxiv.org/abs/1908.11459\" , \"title\" : \"Introducing: The Game Jam License\" , \"note\" : \"license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\\nThis CSL JSON Item was automatically generated by Manubot v0.3.0 using citation-by-identifier.\\nstandard_id: arxiv:1908.11459\" , \"DOI\" : \"10.1145/3337722.3341844\" , } unpaywall = Unpaywall . from_csl_item ( csl_item ) # Unpaywall . from_csl_item uses DOI lookup when available assert isinstance ( unpaywall , Unpaywall_DOI ) assert unpaywall . best_pdf [ \"url_for_pdf\" ]","title":"test_unpaywall_from_csl_item_with_doi"},{"location":"reference/manubot/cite/tests/test_url/","text":"Module manubot.cite.tests.test_url View Source import pytest from manubot.cite.url import get_url_csl_item_zotero def test_get_url_csl_item_zotero_nyt (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'https://nyti.ms/1NuB0WJ' \\ 'https://translate.manubot.org/web' ``` \"\"\" url = \"https://nyti.ms/1NuB0WJ\" csl_item = get_url_csl_item_zotero ( url ) assert ( csl_item [ \"title\" ] == \"Unraveling the Ties of Altitude, Oxygen and Lung Cancer\" ) assert csl_item [ \"author\" ][ 0 ][ \"family\" ] == \"Johnson\" def test_get_url_csl_item_zotero_manubot (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/' \\ 'https://translate.manubot.org/web' ``` \"\"\" url = \"https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/\" csl_item = get_url_csl_item_zotero ( url ) assert csl_item [ \"title\" ] == \"Open collaborative writing with Manubot\" assert csl_item [ \"author\" ][ 1 ][ \"family\" ] == \"Slochower\" # Zotero CSL exporter returns mixed string/int date-parts # https://github.com/zotero/zotero/issues/1603 assert [ int ( x ) for x in csl_item [ \"issued\" ][ \"date-parts\" ][ 0 ]] == [ 2018 , 12 , 18 ] @pytest.mark.skip ( reason = \"test intermittently fails as metadata varies between two states\" ) def test_get_url_csl_item_zotero_github (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927' \\ 'https://translate.manubot.org/web' ``` Note: this test may have temporary failures, due to performance of translation-server. It seems that sometimes translation-server returns a different title for the same URL. A real mystery. See also: https://github.com/manubot/manubot/pull/139#discussion_r328703233 Proposed action: Probably should inquire upstream or change the test. \"\"\" url = \"https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927\" csl_item = get_url_csl_item_zotero ( url ) # FIXME: arbitraryly, csl_item['abstract'], and not csl_item['title'] contains the title. assert csl_item [ \"title\" ] . startswith ( \"Flexible and powerful data analysis\" ) assert csl_item [ \"source\" ] == \"GitHub\" Functions test_get_url_csl_item_zotero_github def test_get_url_csl_item_zotero_github ( ) This command creates two translation-server queries. The first query is equivalent to: curl --verbose --header \"Content-Type: text/plain\" --data 'https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927' 'https://translate.manubot.org/web' Note: this test may have temporary failures, due to performance of translation-server. It seems that sometimes translation-server returns a different title for the same URL. A real mystery. See also: https://github.com/manubot/manubot/pull/139#discussion_r328703233 Proposed action: Probably should inquire upstream or change the test. View Source @pytest . mark . skip ( reason = \"test intermittently fails as metadata varies between two states\" ) def test_get_url_csl_item_zotero_github () : \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \" Content - Type : text / plain \" \\ --data 'https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927' \\ 'https://translate.manubot.org/web' ``` Note: this test may have temporary failures, due to performance of translation-server. It seems that sometimes translation-server returns a different title for the same URL. A real mystery. See also: https://github.com/manubot/manubot/pull/139#discussion_r328703233 Proposed action: Probably should inquire upstream or change the test. \"\"\" url = \"https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927\" csl_item = get_url_csl_item_zotero ( url ) # FIXME : arbitraryly , csl_item [ 'abstract' ] , and not csl_item [ 'title' ] contains the title . assert csl_item [ \"title\" ] . startswith ( \"Flexible and powerful data analysis\" ) assert csl_item [ \"source\" ] == \"GitHub\" test_get_url_csl_item_zotero_manubot def test_get_url_csl_item_zotero_manubot ( ) This command creates two translation-server queries. The first query is equivalent to: curl --verbose --header \"Content-Type: text/plain\" --data 'https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/' 'https://translate.manubot.org/web' View Source def test_get_url_csl_item_zotero_manubot (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \" Content - Type : text / plain \" \\ --data 'https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/' \\ 'https://translate.manubot.org/web' ``` \"\"\" url = \"https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/\" csl_item = get_url_csl_item_zotero ( url ) assert csl_item [ \"title\" ] == \"Open collaborative writing with Manubot\" assert csl_item [ \"author\" ][ 1 ][ \"family\" ] == \"Slochower\" # Zotero CSL exporter returns mixed string / int date - parts # https : // github . com / zotero / zotero / issues / 1603 assert [ int ( x ) for x in csl_item [ \"issued\" ][ \"date-parts\" ][ 0 ]] == [ 2018 , 12 , 18 ] test_get_url_csl_item_zotero_nyt def test_get_url_csl_item_zotero_nyt ( ) This command creates two translation-server queries. The first query is equivalent to: curl --verbose --header \"Content-Type: text/plain\" --data 'https://nyti.ms/1NuB0WJ' 'https://translate.manubot.org/web' View Source def test_get_url_csl_item_zotero_nyt (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \" Content - Type : text / plain \" \\ --data 'https://nyti.ms/1NuB0WJ' \\ 'https://translate.manubot.org/web' ``` \"\"\" url = \"https://nyti.ms/1NuB0WJ\" csl_item = get_url_csl_item_zotero ( url ) assert ( csl_item [ \"title\" ] == \"Unraveling the Ties of Altitude, Oxygen and Lung Cancer\" ) assert csl_item [ \"author\" ][ 0 ][ \"family\" ] == \"Johnson\"","title":"Test Url"},{"location":"reference/manubot/cite/tests/test_url/#module-manubotciteteststest_url","text":"View Source import pytest from manubot.cite.url import get_url_csl_item_zotero def test_get_url_csl_item_zotero_nyt (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'https://nyti.ms/1NuB0WJ' \\ 'https://translate.manubot.org/web' ``` \"\"\" url = \"https://nyti.ms/1NuB0WJ\" csl_item = get_url_csl_item_zotero ( url ) assert ( csl_item [ \"title\" ] == \"Unraveling the Ties of Altitude, Oxygen and Lung Cancer\" ) assert csl_item [ \"author\" ][ 0 ][ \"family\" ] == \"Johnson\" def test_get_url_csl_item_zotero_manubot (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/' \\ 'https://translate.manubot.org/web' ``` \"\"\" url = \"https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/\" csl_item = get_url_csl_item_zotero ( url ) assert csl_item [ \"title\" ] == \"Open collaborative writing with Manubot\" assert csl_item [ \"author\" ][ 1 ][ \"family\" ] == \"Slochower\" # Zotero CSL exporter returns mixed string/int date-parts # https://github.com/zotero/zotero/issues/1603 assert [ int ( x ) for x in csl_item [ \"issued\" ][ \"date-parts\" ][ 0 ]] == [ 2018 , 12 , 18 ] @pytest.mark.skip ( reason = \"test intermittently fails as metadata varies between two states\" ) def test_get_url_csl_item_zotero_github (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927' \\ 'https://translate.manubot.org/web' ``` Note: this test may have temporary failures, due to performance of translation-server. It seems that sometimes translation-server returns a different title for the same URL. A real mystery. See also: https://github.com/manubot/manubot/pull/139#discussion_r328703233 Proposed action: Probably should inquire upstream or change the test. \"\"\" url = \"https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927\" csl_item = get_url_csl_item_zotero ( url ) # FIXME: arbitraryly, csl_item['abstract'], and not csl_item['title'] contains the title. assert csl_item [ \"title\" ] . startswith ( \"Flexible and powerful data analysis\" ) assert csl_item [ \"source\" ] == \"GitHub\"","title":"Module manubot.cite.tests.test_url"},{"location":"reference/manubot/cite/tests/test_url/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_url/#test_get_url_csl_item_zotero_github","text":"def test_get_url_csl_item_zotero_github ( ) This command creates two translation-server queries. The first query is equivalent to: curl --verbose --header \"Content-Type: text/plain\" --data 'https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927' 'https://translate.manubot.org/web' Note: this test may have temporary failures, due to performance of translation-server. It seems that sometimes translation-server returns a different title for the same URL. A real mystery. See also: https://github.com/manubot/manubot/pull/139#discussion_r328703233 Proposed action: Probably should inquire upstream or change the test. View Source @pytest . mark . skip ( reason = \"test intermittently fails as metadata varies between two states\" ) def test_get_url_csl_item_zotero_github () : \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \" Content - Type : text / plain \" \\ --data 'https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927' \\ 'https://translate.manubot.org/web' ``` Note: this test may have temporary failures, due to performance of translation-server. It seems that sometimes translation-server returns a different title for the same URL. A real mystery. See also: https://github.com/manubot/manubot/pull/139#discussion_r328703233 Proposed action: Probably should inquire upstream or change the test. \"\"\" url = \"https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927\" csl_item = get_url_csl_item_zotero ( url ) # FIXME : arbitraryly , csl_item [ 'abstract' ] , and not csl_item [ 'title' ] contains the title . assert csl_item [ \"title\" ] . startswith ( \"Flexible and powerful data analysis\" ) assert csl_item [ \"source\" ] == \"GitHub\"","title":"test_get_url_csl_item_zotero_github"},{"location":"reference/manubot/cite/tests/test_url/#test_get_url_csl_item_zotero_manubot","text":"def test_get_url_csl_item_zotero_manubot ( ) This command creates two translation-server queries. The first query is equivalent to: curl --verbose --header \"Content-Type: text/plain\" --data 'https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/' 'https://translate.manubot.org/web' View Source def test_get_url_csl_item_zotero_manubot (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \" Content - Type : text / plain \" \\ --data 'https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/' \\ 'https://translate.manubot.org/web' ``` \"\"\" url = \"https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/\" csl_item = get_url_csl_item_zotero ( url ) assert csl_item [ \"title\" ] == \"Open collaborative writing with Manubot\" assert csl_item [ \"author\" ][ 1 ][ \"family\" ] == \"Slochower\" # Zotero CSL exporter returns mixed string / int date - parts # https : // github . com / zotero / zotero / issues / 1603 assert [ int ( x ) for x in csl_item [ \"issued\" ][ \"date-parts\" ][ 0 ]] == [ 2018 , 12 , 18 ]","title":"test_get_url_csl_item_zotero_manubot"},{"location":"reference/manubot/cite/tests/test_url/#test_get_url_csl_item_zotero_nyt","text":"def test_get_url_csl_item_zotero_nyt ( ) This command creates two translation-server queries. The first query is equivalent to: curl --verbose --header \"Content-Type: text/plain\" --data 'https://nyti.ms/1NuB0WJ' 'https://translate.manubot.org/web' View Source def test_get_url_csl_item_zotero_nyt (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \" Content - Type : text / plain \" \\ --data 'https://nyti.ms/1NuB0WJ' \\ 'https://translate.manubot.org/web' ``` \"\"\" url = \"https://nyti.ms/1NuB0WJ\" csl_item = get_url_csl_item_zotero ( url ) assert ( csl_item [ \"title\" ] == \"Unraveling the Ties of Altitude, Oxygen and Lung Cancer\" ) assert csl_item [ \"author\" ][ 0 ][ \"family\" ] == \"Johnson\"","title":"test_get_url_csl_item_zotero_nyt"},{"location":"reference/manubot/cite/tests/test_wikidata/","text":"Module manubot.cite.tests.test_wikidata View Source from manubot.cite.wikidata import get_wikidata_csl_item def test_get_wikidata_csl_item (): \"\"\" Test metadata extraction from https://www.wikidata.org/wiki/Q50051684 \"\"\" wikidata_id = \"Q50051684\" csl_item = get_wikidata_csl_item ( wikidata_id ) assert ( \"Sci-Hub provides access to nearly all scholarly literature\" in csl_item [ \"title\" ] ) assert csl_item [ \"container-title\" ] == \"eLife\" assert csl_item [ \"DOI\" ] == \"10.7554/elife.32822\" def test_get_wikidata_csl_item_author_ordering (): \"\"\" Test extraction of author ordering from https://www.wikidata.org/wiki/Q50051684. Wikidata uses a \"series ordinal\" qualifier that must be considered or else author ordering may be wrong. Author ordering was previously not properly set by the Wikidata translator https://github.com/zotero/translators/issues/1790 \"\"\" wikidata_id = \"Q50051684\" csl_item = get_wikidata_csl_item ( wikidata_id ) family_names = [ author [ \"family\" ] for author in csl_item [ \"author\" ]] print ( family_names ) assert family_names == [ \"Himmelstein\" , \"Romero\" , \"Levernier\" , \"Munro\" , \"McLaughlin\" , \"Greshake\" , # actually should be Greshake Tzovaras \"Greene\" , ] Functions test_get_wikidata_csl_item def test_get_wikidata_csl_item ( ) Test metadata extraction from https://www.wikidata.org/wiki/Q50051684 View Source def test_get_wikidata_csl_item (): \"\"\" Test metadata extraction from https://www.wikidata.org/wiki/Q50051684 \"\"\" wikidata_id = \"Q50051684\" csl_item = get_wikidata_csl_item ( wikidata_id ) assert ( \"Sci-Hub provides access to nearly all scholarly literature\" in csl_item [ \"title\" ] ) assert csl_item [ \"container-title\" ] == \"eLife\" assert csl_item [ \"DOI\" ] == \"10.7554/elife.32822\" test_get_wikidata_csl_item_author_ordering def test_get_wikidata_csl_item_author_ordering ( ) Test extraction of author ordering from https://www.wikidata.org/wiki/Q50051684. Wikidata uses a \"series ordinal\" qualifier that must be considered or else author ordering may be wrong. Author ordering was previously not properly set by the Wikidata translator https://github.com/zotero/translators/issues/1790 View Source def test_get_wikidata_csl_item_author_ordering (): \"\"\" Test extraction of author ordering from https://www.wikidata.org/wiki/Q50051684. Wikidata uses a \" series ordinal \" qualifier that must be considered or else author ordering may be wrong. Author ordering was previously not properly set by the Wikidata translator https://github.com/zotero/translators/issues/1790 \"\"\" wikidata_id = \"Q50051684\" csl_item = get_wikidata_csl_item ( wikidata_id ) family_names = [ author [ \"family\" ] for author in csl_item [ \"author\" ]] print ( family_names ) assert family_names == [ \"Himmelstein\" , \"Romero\" , \"Levernier\" , \"Munro\" , \"McLaughlin\" , \"Greshake\" , # actually should be Greshake Tzovaras \"Greene\" , ]","title":"Test Wikidata"},{"location":"reference/manubot/cite/tests/test_wikidata/#module-manubotciteteststest_wikidata","text":"View Source from manubot.cite.wikidata import get_wikidata_csl_item def test_get_wikidata_csl_item (): \"\"\" Test metadata extraction from https://www.wikidata.org/wiki/Q50051684 \"\"\" wikidata_id = \"Q50051684\" csl_item = get_wikidata_csl_item ( wikidata_id ) assert ( \"Sci-Hub provides access to nearly all scholarly literature\" in csl_item [ \"title\" ] ) assert csl_item [ \"container-title\" ] == \"eLife\" assert csl_item [ \"DOI\" ] == \"10.7554/elife.32822\" def test_get_wikidata_csl_item_author_ordering (): \"\"\" Test extraction of author ordering from https://www.wikidata.org/wiki/Q50051684. Wikidata uses a \"series ordinal\" qualifier that must be considered or else author ordering may be wrong. Author ordering was previously not properly set by the Wikidata translator https://github.com/zotero/translators/issues/1790 \"\"\" wikidata_id = \"Q50051684\" csl_item = get_wikidata_csl_item ( wikidata_id ) family_names = [ author [ \"family\" ] for author in csl_item [ \"author\" ]] print ( family_names ) assert family_names == [ \"Himmelstein\" , \"Romero\" , \"Levernier\" , \"Munro\" , \"McLaughlin\" , \"Greshake\" , # actually should be Greshake Tzovaras \"Greene\" , ]","title":"Module manubot.cite.tests.test_wikidata"},{"location":"reference/manubot/cite/tests/test_wikidata/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_wikidata/#test_get_wikidata_csl_item","text":"def test_get_wikidata_csl_item ( ) Test metadata extraction from https://www.wikidata.org/wiki/Q50051684 View Source def test_get_wikidata_csl_item (): \"\"\" Test metadata extraction from https://www.wikidata.org/wiki/Q50051684 \"\"\" wikidata_id = \"Q50051684\" csl_item = get_wikidata_csl_item ( wikidata_id ) assert ( \"Sci-Hub provides access to nearly all scholarly literature\" in csl_item [ \"title\" ] ) assert csl_item [ \"container-title\" ] == \"eLife\" assert csl_item [ \"DOI\" ] == \"10.7554/elife.32822\"","title":"test_get_wikidata_csl_item"},{"location":"reference/manubot/cite/tests/test_wikidata/#test_get_wikidata_csl_item_author_ordering","text":"def test_get_wikidata_csl_item_author_ordering ( ) Test extraction of author ordering from https://www.wikidata.org/wiki/Q50051684. Wikidata uses a \"series ordinal\" qualifier that must be considered or else author ordering may be wrong. Author ordering was previously not properly set by the Wikidata translator https://github.com/zotero/translators/issues/1790 View Source def test_get_wikidata_csl_item_author_ordering (): \"\"\" Test extraction of author ordering from https://www.wikidata.org/wiki/Q50051684. Wikidata uses a \" series ordinal \" qualifier that must be considered or else author ordering may be wrong. Author ordering was previously not properly set by the Wikidata translator https://github.com/zotero/translators/issues/1790 \"\"\" wikidata_id = \"Q50051684\" csl_item = get_wikidata_csl_item ( wikidata_id ) family_names = [ author [ \"family\" ] for author in csl_item [ \"author\" ]] print ( family_names ) assert family_names == [ \"Himmelstein\" , \"Romero\" , \"Levernier\" , \"Munro\" , \"McLaughlin\" , \"Greshake\" , # actually should be Greshake Tzovaras \"Greene\" , ]","title":"test_get_wikidata_csl_item_author_ordering"},{"location":"reference/manubot/cite/tests/test_zotero/","text":"Module manubot.cite.tests.test_zotero View Source import pytest from manubot.cite.zotero import web_query , search_query , export_as_csl def test_web_query (): \"\"\" The translation-server web endpoint can be tested via curl: ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'https://bigthink.com/neurobonkers/a-pirate-bay-for-science' \\ 'https://translate.manubot.org/web' ``` An outdated installation of translation-server caused the web query for this URL to be extraordinarily slow but has now been fixed. See https://github.com/zotero/translation-server/issues/63 \"\"\" url = \"https://bigthink.com/neurobonkers/a-pirate-bay-for-science\" zotero_data = web_query ( url ) assert isinstance ( zotero_data , list ) assert len ( zotero_data ) == 1 assert zotero_data [ 0 ][ \"title\" ] . startswith ( \"Meet the Robin Hood of Science\" ) def test_export_as_csl (): \"\"\" CSL export can be tested via curl: ``` curl \\ --header \"Content-Type: application/json\" \\ --data '[{\"key\": \"IN22XN53\", \"itemType\": \"webpage\", \"date\": \"2016-02-09T20:12:00\"}]' \\ 'https://translate.manubot.org/export?format=csljson' ``` \"\"\" zotero_data = [ { \"key\" : \"IN22XN53\" , \"version\" : 0 , \"itemType\" : \"webpage\" , \"creators\" : [], \"tags\" : [], \"title\" : \"Meet the Robin Hood of Science\" , \"websiteTitle\" : \"Big Think\" , \"date\" : \"2016-02-09T20:12:00\" , \"url\" : \"https://bigthink.com/neurobonkers/a-pirate-bay-for-science\" , \"abstractNote\" : \"How one researcher created a pirate bay for science more powerful than even libraries at top universities.\" , \"language\" : \"en\" , \"accessDate\" : \"2018-12-06T20:10:14Z\" , } ] csl_item = export_as_csl ( zotero_data )[ 0 ] assert csl_item [ \"title\" ] == \"Meet the Robin Hood of Science\" assert csl_item [ \"container-title\" ] == \"Big Think\" def test_web_query_returns_single_result_legacy_manubot_url (): \"\"\" Check that single=1 is specified for web queries. Without this, Zotero can prefer translators that return multiple choices. This occurs with legacy Manubot mansucripts, which get assigned the DOI translator as top priority. https://github.com/zotero/translation-server/issues/65 ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/' \\ 'https://translate.manubot.org/web?single=1' ``` \"\"\" url = \"https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/\" zotero_metadata = web_query ( url ) assert isinstance ( zotero_metadata , list ) assert len ( zotero_metadata ) == 1 ( zotero_metadata ,) = zotero_metadata assert ( zotero_metadata [ \"title\" ] == \"Sci-Hub provides access to nearly all scholarly literature\" ) def test_web_query_returns_single_result_pubmed_url (): \"\"\" See test_web_query_returns_single_result_legacy_manubot_url docstring. ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D' \\ 'https://translate.manubot.org/web?single=1' ``` \"\"\" url = \"https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D\" zotero_metadata = web_query ( url ) assert isinstance ( zotero_metadata , list ) assert len ( zotero_metadata ) == 1 ( zotero_metadata ,) = zotero_metadata assert zotero_metadata [ \"title\" ] . startswith ( \"sci-hub[title]\" ) def test_search_query_isbn (): \"\"\" The translation-server search endpoint can be tested via curl: ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'isbn:9781339919881' \\ 'https://translate.manubot.org/search' ``` \"\"\" identifier = \"isbn:9781339919881\" zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ \"title\" ] . startswith ( \"The hetnet awakens\" ) def test_search_query_arxiv (): \"\"\" Test citing https://arxiv.org/abs/1604.05363v1 The translation-server search endpoint can be tested via curl: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'arxiv:1604.05363v1' \\ 'https://translate.manubot.org/search' ``` \"\"\" identifier = \"arxiv:1604.05363v1\" zotero_data = search_query ( identifier ) assert ( zotero_data [ 0 ][ \"title\" ] == \"Comparing Published Scientific Journal Articles to Their Pre-print Versions\" ) assert zotero_data [ 0 ][ \"creators\" ][ - 1 ][ \"firstName\" ] == \"Todd\" assert zotero_data [ 0 ][ \"date\" ] == \"2016-04-18\" @pytest.mark.parametrize ( \"identifier\" , [ \"30571677\" , # https://www.ncbi.nlm.nih.gov/pubmed/30571677 \"doi:10.1371/journal.pcbi.1006561\" , # https://doi.org/10.1371/journal.pcbi.1006561 ], ) def test_search_query ( identifier ): \"\"\" The translation-server search endpoint can be tested via curl: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data '30571677' \\ 'https://translate.manubot.org/search' ``` translation-server does not support PMIDs with a `pmid:` prefix. https://github.com/zotero/translation-server/issues/71 \"\"\" zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ \"title\" ] . startswith ( \"Ten simple rules for documenting scientific software\" ) assert zotero_data [ 0 ][ \"creators\" ][ 0 ][ \"lastName\" ] == \"Lee\" Functions test_export_as_csl def test_export_as_csl ( ) CSL export can be tested via curl: curl --header \"Content-Type: application/json\" --data '[{\"key\": \"IN22XN53\", \"itemType\": \"webpage\", \"date\": \"2016-02-09T20:12:00\"}]' 'https://translate.manubot.org/export?format=csljson' View Source def test_export_as_csl (): \"\"\" CSL export can be tested via curl: ``` curl \\ --header \" Content - Type : application / json \" \\ --data '[{\" key \": \" IN22XN53 \", \" itemType \": \" webpage \", \" date \": \" 2016 - 02 - 09 T20 : 12 : 00 \"}]' \\ 'https://translate.manubot.org/export?format=csljson' ``` \"\"\" zotero_data = [ { \"key\" : \"IN22XN53\" , \"version\" : 0 , \"itemType\" : \"webpage\" , \"creators\" : [], \"tags\" : [], \"title\" : \"Meet the Robin Hood of Science\" , \"websiteTitle\" : \"Big Think\" , \"date\" : \"2016-02-09T20:12:00\" , \"url\" : \"https://bigthink.com/neurobonkers/a-pirate-bay-for-science\" , \"abstractNote\" : \"How one researcher created a pirate bay for science more powerful than even libraries at top universities.\" , \"language\" : \"en\" , \"accessDate\" : \"2018-12-06T20:10:14Z\" , } ] csl_item = export_as_csl ( zotero_data )[ 0 ] assert csl_item [ \"title\" ] == \"Meet the Robin Hood of Science\" assert csl_item [ \"container-title\" ] == \"Big Think\" test_search_query def test_search_query ( identifier ) The translation-server search endpoint can be tested via curl: curl --verbose --header \"Content-Type: text/plain\" --data '30571677' 'https://translate.manubot.org/search' translation-server does not support PMIDs with a pmid: prefix. https://github.com/zotero/translation-server/issues/71 View Source @pytest . mark . parametrize ( \"identifier\" , [ \"30571677\", # https://www.ncbi.nlm.nih.gov/pubmed/30571677 \"doi:10.1371/journal.pcbi.1006561\", # https://doi.org/10.1371/journal.pcbi.1006561 ] , ) def test_search_query ( identifier ) : \"\"\" The translation-server search endpoint can be tested via curl: ``` curl --verbose \\ --header \" Content - Type : text / plain \" \\ --data '30571677' \\ 'https://translate.manubot.org/search' ``` translation-server does not support PMIDs with a `pmid:` prefix. https://github.com/zotero/translation-server/issues/71 \"\"\" zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ \"title\" ] . startswith ( \"Ten simple rules for documenting scientific software\" ) assert zotero_data [ 0 ][ \"creators\" ][ 0 ][ \"lastName\" ] == \"Lee\" test_search_query_arxiv def test_search_query_arxiv ( ) Test citing https://arxiv.org/abs/1604.05363v1 The translation-server search endpoint can be tested via curl: curl --verbose --header \"Content-Type: text/plain\" --data 'arxiv:1604.05363v1' 'https://translate.manubot.org/search' View Source def test_search_query_arxiv (): \"\"\" Test citing https://arxiv.org/abs/1604.05363v1 The translation-server search endpoint can be tested via curl: ``` curl --verbose \\ --header \" Content - Type : text / plain \" \\ --data 'arxiv:1604.05363v1' \\ 'https://translate.manubot.org/search' ``` \"\"\" identifier = \"arxiv:1604.05363v1\" zotero_data = search_query ( identifier ) assert ( zotero_data [ 0 ][ \"title\" ] == \"Comparing Published Scientific Journal Articles to Their Pre-print Versions\" ) assert zotero_data [ 0 ][ \"creators\" ][ - 1 ][ \"firstName\" ] == \"Todd\" assert zotero_data [ 0 ][ \"date\" ] == \"2016-04-18\" test_search_query_isbn def test_search_query_isbn ( ) The translation-server search endpoint can be tested via curl: curl --header \"Content-Type: text/plain\" --data 'isbn:9781339919881' 'https://translate.manubot.org/search' View Source def test_search_query_isbn (): \"\"\" The translation-server search endpoint can be tested via curl: ``` curl \\ --header \" Content - Type : text / plain \" \\ --data 'isbn:9781339919881' \\ 'https://translate.manubot.org/search' ``` \"\"\" identifier = \"isbn:9781339919881\" zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ \"title\" ]. startswith ( \"The hetnet awakens\" ) test_web_query def test_web_query ( ) The translation-server web endpoint can be tested via curl: curl --header \"Content-Type: text/plain\" --data 'https://bigthink.com/neurobonkers/a-pirate-bay-for-science' 'https://translate.manubot.org/web' An outdated installation of translation-server caused the web query for this URL to be extraordinarily slow but has now been fixed. See https://github.com/zotero/translation-server/issues/63 View Source def test_web_query (): \"\"\" The translation-server web endpoint can be tested via curl: ``` curl \\ --header \" Content - Type : text / plain \" \\ --data 'https://bigthink.com/neurobonkers/a-pirate-bay-for-science' \\ 'https://translate.manubot.org/web' ``` An outdated installation of translation-server caused the web query for this URL to be extraordinarily slow but has now been fixed. See https://github.com/zotero/translation-server/issues/63 \"\"\" url = \"https://bigthink.com/neurobonkers/a-pirate-bay-for-science\" zotero_data = web_query ( url ) assert isinstance ( zotero_data , list ) assert len ( zotero_data ) == 1 assert zotero_data [ 0 ][ \"title\" ]. startswith ( \"Meet the Robin Hood of Science\" ) test_web_query_returns_single_result_legacy_manubot_url def test_web_query_returns_single_result_legacy_manubot_url ( ) Check that single=1 is specified for web queries. Without this, Zotero can prefer translators that return multiple choices. This occurs with legacy Manubot mansucripts, which get assigned the DOI translator as top priority. https://github.com/zotero/translation-server/issues/65 curl --header \"Content-Type: text/plain\" --data 'https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/' 'https://translate.manubot.org/web?single=1' View Source def test_web_query_returns_single_result_legacy_manubot_url (): \"\"\" Check that single=1 is specified for web queries. Without this, Zotero can prefer translators that return multiple choices. This occurs with legacy Manubot mansucripts, which get assigned the DOI translator as top priority. https://github.com/zotero/translation-server/issues/65 ``` curl \\ --header \" Content - Type : text / plain \" \\ --data 'https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/' \\ 'https://translate.manubot.org/web?single=1' ``` \"\"\" url = \"https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/\" zotero_metadata = web_query ( url ) assert isinstance ( zotero_metadata , list ) assert len ( zotero_metadata ) == 1 ( zotero_metadata ,) = zotero_metadata assert ( zotero_metadata [ \"title\" ] == \"Sci-Hub provides access to nearly all scholarly literature\" ) test_web_query_returns_single_result_pubmed_url def test_web_query_returns_single_result_pubmed_url ( ) See test_web_query_returns_single_result_legacy_manubot_url docstring. curl --header \"Content-Type: text/plain\" --data 'https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D' 'https://translate.manubot.org/web?single=1' View Source def test_web_query_returns_single_result_pubmed_url () : \"\"\" See test_web_query_returns_single_result_legacy_manubot_url docstring. ``` curl \\ --header \" Content - Type : text / plain \" \\ --data 'https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D' \\ 'https://translate.manubot.org/web?single=1' ``` \"\"\" url = \"https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D\" zotero_metadata = web_query ( url ) assert isinstance ( zotero_metadata , list ) assert len ( zotero_metadata ) == 1 ( zotero_metadata ,) = zotero_metadata assert zotero_metadata [ \"title\" ] . startswith ( \"sci-hub[title]\" )","title":"Test Zotero"},{"location":"reference/manubot/cite/tests/test_zotero/#module-manubotciteteststest_zotero","text":"View Source import pytest from manubot.cite.zotero import web_query , search_query , export_as_csl def test_web_query (): \"\"\" The translation-server web endpoint can be tested via curl: ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'https://bigthink.com/neurobonkers/a-pirate-bay-for-science' \\ 'https://translate.manubot.org/web' ``` An outdated installation of translation-server caused the web query for this URL to be extraordinarily slow but has now been fixed. See https://github.com/zotero/translation-server/issues/63 \"\"\" url = \"https://bigthink.com/neurobonkers/a-pirate-bay-for-science\" zotero_data = web_query ( url ) assert isinstance ( zotero_data , list ) assert len ( zotero_data ) == 1 assert zotero_data [ 0 ][ \"title\" ] . startswith ( \"Meet the Robin Hood of Science\" ) def test_export_as_csl (): \"\"\" CSL export can be tested via curl: ``` curl \\ --header \"Content-Type: application/json\" \\ --data '[{\"key\": \"IN22XN53\", \"itemType\": \"webpage\", \"date\": \"2016-02-09T20:12:00\"}]' \\ 'https://translate.manubot.org/export?format=csljson' ``` \"\"\" zotero_data = [ { \"key\" : \"IN22XN53\" , \"version\" : 0 , \"itemType\" : \"webpage\" , \"creators\" : [], \"tags\" : [], \"title\" : \"Meet the Robin Hood of Science\" , \"websiteTitle\" : \"Big Think\" , \"date\" : \"2016-02-09T20:12:00\" , \"url\" : \"https://bigthink.com/neurobonkers/a-pirate-bay-for-science\" , \"abstractNote\" : \"How one researcher created a pirate bay for science more powerful than even libraries at top universities.\" , \"language\" : \"en\" , \"accessDate\" : \"2018-12-06T20:10:14Z\" , } ] csl_item = export_as_csl ( zotero_data )[ 0 ] assert csl_item [ \"title\" ] == \"Meet the Robin Hood of Science\" assert csl_item [ \"container-title\" ] == \"Big Think\" def test_web_query_returns_single_result_legacy_manubot_url (): \"\"\" Check that single=1 is specified for web queries. Without this, Zotero can prefer translators that return multiple choices. This occurs with legacy Manubot mansucripts, which get assigned the DOI translator as top priority. https://github.com/zotero/translation-server/issues/65 ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/' \\ 'https://translate.manubot.org/web?single=1' ``` \"\"\" url = \"https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/\" zotero_metadata = web_query ( url ) assert isinstance ( zotero_metadata , list ) assert len ( zotero_metadata ) == 1 ( zotero_metadata ,) = zotero_metadata assert ( zotero_metadata [ \"title\" ] == \"Sci-Hub provides access to nearly all scholarly literature\" ) def test_web_query_returns_single_result_pubmed_url (): \"\"\" See test_web_query_returns_single_result_legacy_manubot_url docstring. ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D' \\ 'https://translate.manubot.org/web?single=1' ``` \"\"\" url = \"https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D\" zotero_metadata = web_query ( url ) assert isinstance ( zotero_metadata , list ) assert len ( zotero_metadata ) == 1 ( zotero_metadata ,) = zotero_metadata assert zotero_metadata [ \"title\" ] . startswith ( \"sci-hub[title]\" ) def test_search_query_isbn (): \"\"\" The translation-server search endpoint can be tested via curl: ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'isbn:9781339919881' \\ 'https://translate.manubot.org/search' ``` \"\"\" identifier = \"isbn:9781339919881\" zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ \"title\" ] . startswith ( \"The hetnet awakens\" ) def test_search_query_arxiv (): \"\"\" Test citing https://arxiv.org/abs/1604.05363v1 The translation-server search endpoint can be tested via curl: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'arxiv:1604.05363v1' \\ 'https://translate.manubot.org/search' ``` \"\"\" identifier = \"arxiv:1604.05363v1\" zotero_data = search_query ( identifier ) assert ( zotero_data [ 0 ][ \"title\" ] == \"Comparing Published Scientific Journal Articles to Their Pre-print Versions\" ) assert zotero_data [ 0 ][ \"creators\" ][ - 1 ][ \"firstName\" ] == \"Todd\" assert zotero_data [ 0 ][ \"date\" ] == \"2016-04-18\" @pytest.mark.parametrize ( \"identifier\" , [ \"30571677\" , # https://www.ncbi.nlm.nih.gov/pubmed/30571677 \"doi:10.1371/journal.pcbi.1006561\" , # https://doi.org/10.1371/journal.pcbi.1006561 ], ) def test_search_query ( identifier ): \"\"\" The translation-server search endpoint can be tested via curl: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data '30571677' \\ 'https://translate.manubot.org/search' ``` translation-server does not support PMIDs with a `pmid:` prefix. https://github.com/zotero/translation-server/issues/71 \"\"\" zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ \"title\" ] . startswith ( \"Ten simple rules for documenting scientific software\" ) assert zotero_data [ 0 ][ \"creators\" ][ 0 ][ \"lastName\" ] == \"Lee\"","title":"Module manubot.cite.tests.test_zotero"},{"location":"reference/manubot/cite/tests/test_zotero/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_zotero/#test_export_as_csl","text":"def test_export_as_csl ( ) CSL export can be tested via curl: curl --header \"Content-Type: application/json\" --data '[{\"key\": \"IN22XN53\", \"itemType\": \"webpage\", \"date\": \"2016-02-09T20:12:00\"}]' 'https://translate.manubot.org/export?format=csljson' View Source def test_export_as_csl (): \"\"\" CSL export can be tested via curl: ``` curl \\ --header \" Content - Type : application / json \" \\ --data '[{\" key \": \" IN22XN53 \", \" itemType \": \" webpage \", \" date \": \" 2016 - 02 - 09 T20 : 12 : 00 \"}]' \\ 'https://translate.manubot.org/export?format=csljson' ``` \"\"\" zotero_data = [ { \"key\" : \"IN22XN53\" , \"version\" : 0 , \"itemType\" : \"webpage\" , \"creators\" : [], \"tags\" : [], \"title\" : \"Meet the Robin Hood of Science\" , \"websiteTitle\" : \"Big Think\" , \"date\" : \"2016-02-09T20:12:00\" , \"url\" : \"https://bigthink.com/neurobonkers/a-pirate-bay-for-science\" , \"abstractNote\" : \"How one researcher created a pirate bay for science more powerful than even libraries at top universities.\" , \"language\" : \"en\" , \"accessDate\" : \"2018-12-06T20:10:14Z\" , } ] csl_item = export_as_csl ( zotero_data )[ 0 ] assert csl_item [ \"title\" ] == \"Meet the Robin Hood of Science\" assert csl_item [ \"container-title\" ] == \"Big Think\"","title":"test_export_as_csl"},{"location":"reference/manubot/cite/tests/test_zotero/#test_search_query","text":"def test_search_query ( identifier ) The translation-server search endpoint can be tested via curl: curl --verbose --header \"Content-Type: text/plain\" --data '30571677' 'https://translate.manubot.org/search' translation-server does not support PMIDs with a pmid: prefix. https://github.com/zotero/translation-server/issues/71 View Source @pytest . mark . parametrize ( \"identifier\" , [ \"30571677\", # https://www.ncbi.nlm.nih.gov/pubmed/30571677 \"doi:10.1371/journal.pcbi.1006561\", # https://doi.org/10.1371/journal.pcbi.1006561 ] , ) def test_search_query ( identifier ) : \"\"\" The translation-server search endpoint can be tested via curl: ``` curl --verbose \\ --header \" Content - Type : text / plain \" \\ --data '30571677' \\ 'https://translate.manubot.org/search' ``` translation-server does not support PMIDs with a `pmid:` prefix. https://github.com/zotero/translation-server/issues/71 \"\"\" zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ \"title\" ] . startswith ( \"Ten simple rules for documenting scientific software\" ) assert zotero_data [ 0 ][ \"creators\" ][ 0 ][ \"lastName\" ] == \"Lee\"","title":"test_search_query"},{"location":"reference/manubot/cite/tests/test_zotero/#test_search_query_arxiv","text":"def test_search_query_arxiv ( ) Test citing https://arxiv.org/abs/1604.05363v1 The translation-server search endpoint can be tested via curl: curl --verbose --header \"Content-Type: text/plain\" --data 'arxiv:1604.05363v1' 'https://translate.manubot.org/search' View Source def test_search_query_arxiv (): \"\"\" Test citing https://arxiv.org/abs/1604.05363v1 The translation-server search endpoint can be tested via curl: ``` curl --verbose \\ --header \" Content - Type : text / plain \" \\ --data 'arxiv:1604.05363v1' \\ 'https://translate.manubot.org/search' ``` \"\"\" identifier = \"arxiv:1604.05363v1\" zotero_data = search_query ( identifier ) assert ( zotero_data [ 0 ][ \"title\" ] == \"Comparing Published Scientific Journal Articles to Their Pre-print Versions\" ) assert zotero_data [ 0 ][ \"creators\" ][ - 1 ][ \"firstName\" ] == \"Todd\" assert zotero_data [ 0 ][ \"date\" ] == \"2016-04-18\"","title":"test_search_query_arxiv"},{"location":"reference/manubot/cite/tests/test_zotero/#test_search_query_isbn","text":"def test_search_query_isbn ( ) The translation-server search endpoint can be tested via curl: curl --header \"Content-Type: text/plain\" --data 'isbn:9781339919881' 'https://translate.manubot.org/search' View Source def test_search_query_isbn (): \"\"\" The translation-server search endpoint can be tested via curl: ``` curl \\ --header \" Content - Type : text / plain \" \\ --data 'isbn:9781339919881' \\ 'https://translate.manubot.org/search' ``` \"\"\" identifier = \"isbn:9781339919881\" zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ \"title\" ]. startswith ( \"The hetnet awakens\" )","title":"test_search_query_isbn"},{"location":"reference/manubot/cite/tests/test_zotero/#test_web_query","text":"def test_web_query ( ) The translation-server web endpoint can be tested via curl: curl --header \"Content-Type: text/plain\" --data 'https://bigthink.com/neurobonkers/a-pirate-bay-for-science' 'https://translate.manubot.org/web' An outdated installation of translation-server caused the web query for this URL to be extraordinarily slow but has now been fixed. See https://github.com/zotero/translation-server/issues/63 View Source def test_web_query (): \"\"\" The translation-server web endpoint can be tested via curl: ``` curl \\ --header \" Content - Type : text / plain \" \\ --data 'https://bigthink.com/neurobonkers/a-pirate-bay-for-science' \\ 'https://translate.manubot.org/web' ``` An outdated installation of translation-server caused the web query for this URL to be extraordinarily slow but has now been fixed. See https://github.com/zotero/translation-server/issues/63 \"\"\" url = \"https://bigthink.com/neurobonkers/a-pirate-bay-for-science\" zotero_data = web_query ( url ) assert isinstance ( zotero_data , list ) assert len ( zotero_data ) == 1 assert zotero_data [ 0 ][ \"title\" ]. startswith ( \"Meet the Robin Hood of Science\" )","title":"test_web_query"},{"location":"reference/manubot/cite/tests/test_zotero/#test_web_query_returns_single_result_legacy_manubot_url","text":"def test_web_query_returns_single_result_legacy_manubot_url ( ) Check that single=1 is specified for web queries. Without this, Zotero can prefer translators that return multiple choices. This occurs with legacy Manubot mansucripts, which get assigned the DOI translator as top priority. https://github.com/zotero/translation-server/issues/65 curl --header \"Content-Type: text/plain\" --data 'https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/' 'https://translate.manubot.org/web?single=1' View Source def test_web_query_returns_single_result_legacy_manubot_url (): \"\"\" Check that single=1 is specified for web queries. Without this, Zotero can prefer translators that return multiple choices. This occurs with legacy Manubot mansucripts, which get assigned the DOI translator as top priority. https://github.com/zotero/translation-server/issues/65 ``` curl \\ --header \" Content - Type : text / plain \" \\ --data 'https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/' \\ 'https://translate.manubot.org/web?single=1' ``` \"\"\" url = \"https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/\" zotero_metadata = web_query ( url ) assert isinstance ( zotero_metadata , list ) assert len ( zotero_metadata ) == 1 ( zotero_metadata ,) = zotero_metadata assert ( zotero_metadata [ \"title\" ] == \"Sci-Hub provides access to nearly all scholarly literature\" )","title":"test_web_query_returns_single_result_legacy_manubot_url"},{"location":"reference/manubot/cite/tests/test_zotero/#test_web_query_returns_single_result_pubmed_url","text":"def test_web_query_returns_single_result_pubmed_url ( ) See test_web_query_returns_single_result_legacy_manubot_url docstring. curl --header \"Content-Type: text/plain\" --data 'https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D' 'https://translate.manubot.org/web?single=1' View Source def test_web_query_returns_single_result_pubmed_url () : \"\"\" See test_web_query_returns_single_result_legacy_manubot_url docstring. ``` curl \\ --header \" Content - Type : text / plain \" \\ --data 'https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D' \\ 'https://translate.manubot.org/web?single=1' ``` \"\"\" url = \"https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D\" zotero_metadata = web_query ( url ) assert isinstance ( zotero_metadata , list ) assert len ( zotero_metadata ) == 1 ( zotero_metadata ,) = zotero_metadata assert zotero_metadata [ \"title\" ] . startswith ( \"sci-hub[title]\" )","title":"test_web_query_returns_single_result_pubmed_url"},{"location":"reference/manubot/pandoc/","text":"Module manubot.pandoc Sub-modules manubot.pandoc.bibliography manubot.pandoc.cite_filter manubot.pandoc.util","title":"Index"},{"location":"reference/manubot/pandoc/#module-manubotpandoc","text":"","title":"Module manubot.pandoc"},{"location":"reference/manubot/pandoc/#sub-modules","text":"manubot.pandoc.bibliography manubot.pandoc.cite_filter manubot.pandoc.util","title":"Sub-modules"},{"location":"reference/manubot/pandoc/bibliography/","text":"Module manubot.pandoc.bibliography View Source import json import logging import subprocess from manubot.pandoc.util import get_pandoc_info from manubot.util import shlex_join def load_bibliography ( path = None , text = None , input_format = None ): \"\"\" Convert a bibliography to CSL JSON using `pandoc-citeproc --bib2json`. Accepts either a bibliography path or text (string). If supplying text, pandoc-citeproc will likely require input_format be specified. The CSL JSON is returned as Python objects. Parameters ---------- path : str, pathlike, or None Path to a bibliography file. Extension is used by pandoc-citeproc to infer the format of the input. text : str or None Text representation of the bibligriophy, such as a JSON-formatted string. `input_format` should be specified if providing text input. input_format : str or None Manually specified input formatted that is supported by pandoc-citeproc: https://github.com/jgm/pandoc-citeproc/blob/master/man/pandoc-citeproc.1.md#options Returns ------- csl_json : JSON-like object CSL JSON Data for the references encoded by the input bibliography. \"\"\" use_text = path is None use_path = text is None if not ( use_text ^ use_path ): raise ValueError ( \"load_bibliography: specify either path or text but not both.\" ) if not get_pandoc_info ()[ \"pandoc-citeproc\" ]: logging . error ( \"pandoc-citeproc not found on system: manubot.pandoc.bibliography.load_bibliography returning empty CSL JSON\" ) return [] args = [ \"pandoc-citeproc\" , \"--bib2json\" ] if input_format : args . extend ([ \"--format\" , input_format ]) run_kwargs = {} if use_path : args . append ( str ( path )) if use_text : run_kwargs [ \"input\" ] = text logging . info ( \"call_pandoc subprocess args: \\n >>> \" + shlex_join ( args )) process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ** run_kwargs , ) logging . info ( f \"captured stderr: \\n {process.stderr}\" ) process . check_returncode () try : csl_json = json . loads ( process . stdout ) except Exception : logging . exception ( f \"Error parsing bib2json output as JSON: \\n {process.stdout}\" ) csl_json = [] return csl_json Functions load_bibliography def load_bibliography ( path = None , text = None , input_format = None ) Convert a bibliography to CSL JSON using pandoc-citeproc --bib2json . Accepts either a bibliography path or text (string). If supplying text, pandoc-citeproc will likely require input_format be specified. The CSL JSON is returned as Python objects. Parameters path : str, pathlike, or None Path to a bibliography file. Extension is used by pandoc-citeproc to infer the format of the input. text : str or None Text representation of the bibligriophy, such as a JSON-formatted string. input_format should be specified if providing text input. input_format : str or None Manually specified input formatted that is supported by pandoc-citeproc: https://github.com/jgm/pandoc-citeproc/blob/master/man/pandoc-citeproc.1.md#options Returns csl_json : JSON-like object CSL JSON Data for the references encoded by the input bibliography. View Source def load_bibliography ( path = None , text = None , input_format = None ): \"\"\" Convert a bibliography to CSL JSON using `pandoc-citeproc --bib2json`. Accepts either a bibliography path or text (string). If supplying text, pandoc-citeproc will likely require input_format be specified. The CSL JSON is returned as Python objects. Parameters ---------- path : str, pathlike, or None Path to a bibliography file. Extension is used by pandoc-citeproc to infer the format of the input. text : str or None Text representation of the bibligriophy, such as a JSON-formatted string. `input_format` should be specified if providing text input. input_format : str or None Manually specified input formatted that is supported by pandoc-citeproc: https://github.com/jgm/pandoc-citeproc/blob/master/man/pandoc-citeproc.1.md#options Returns ------- csl_json : JSON-like object CSL JSON Data for the references encoded by the input bibliography. \"\"\" use_text = path is None use_path = text is None if not ( use_text ^ use_path ): raise ValueError ( \"load_bibliography: specify either path or text but not both.\" ) if not get_pandoc_info ()[ \"pandoc-citeproc\" ]: logging . error ( \"pandoc-citeproc not found on system: manubot.pandoc.bibliography.load_bibliography returning empty CSL JSON\" ) return [] args = [ \"pandoc-citeproc\" , \"--bib2json\" ] if input_format : args . extend ([ \"--format\" , input_format ]) run_kwargs = {} if use_path : args . append ( str ( path )) if use_text : run_kwargs [ \"input\" ] = text logging . info ( \"call_pandoc subprocess args:\\n>>> \" + shlex_join ( args )) process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ** run_kwargs , ) logging . info ( f \"captured stderr:\\n{process.stderr}\" ) process . check_returncode () try : csl_json = json . loads ( process . stdout ) except Exception : logging . exception ( f \"Error parsing bib2json output as JSON:\\n{process.stdout}\" ) csl_json = [] return csl_json","title":"Bibliography"},{"location":"reference/manubot/pandoc/bibliography/#module-manubotpandocbibliography","text":"View Source import json import logging import subprocess from manubot.pandoc.util import get_pandoc_info from manubot.util import shlex_join def load_bibliography ( path = None , text = None , input_format = None ): \"\"\" Convert a bibliography to CSL JSON using `pandoc-citeproc --bib2json`. Accepts either a bibliography path or text (string). If supplying text, pandoc-citeproc will likely require input_format be specified. The CSL JSON is returned as Python objects. Parameters ---------- path : str, pathlike, or None Path to a bibliography file. Extension is used by pandoc-citeproc to infer the format of the input. text : str or None Text representation of the bibligriophy, such as a JSON-formatted string. `input_format` should be specified if providing text input. input_format : str or None Manually specified input formatted that is supported by pandoc-citeproc: https://github.com/jgm/pandoc-citeproc/blob/master/man/pandoc-citeproc.1.md#options Returns ------- csl_json : JSON-like object CSL JSON Data for the references encoded by the input bibliography. \"\"\" use_text = path is None use_path = text is None if not ( use_text ^ use_path ): raise ValueError ( \"load_bibliography: specify either path or text but not both.\" ) if not get_pandoc_info ()[ \"pandoc-citeproc\" ]: logging . error ( \"pandoc-citeproc not found on system: manubot.pandoc.bibliography.load_bibliography returning empty CSL JSON\" ) return [] args = [ \"pandoc-citeproc\" , \"--bib2json\" ] if input_format : args . extend ([ \"--format\" , input_format ]) run_kwargs = {} if use_path : args . append ( str ( path )) if use_text : run_kwargs [ \"input\" ] = text logging . info ( \"call_pandoc subprocess args: \\n >>> \" + shlex_join ( args )) process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ** run_kwargs , ) logging . info ( f \"captured stderr: \\n {process.stderr}\" ) process . check_returncode () try : csl_json = json . loads ( process . stdout ) except Exception : logging . exception ( f \"Error parsing bib2json output as JSON: \\n {process.stdout}\" ) csl_json = [] return csl_json","title":"Module manubot.pandoc.bibliography"},{"location":"reference/manubot/pandoc/bibliography/#functions","text":"","title":"Functions"},{"location":"reference/manubot/pandoc/bibliography/#load_bibliography","text":"def load_bibliography ( path = None , text = None , input_format = None ) Convert a bibliography to CSL JSON using pandoc-citeproc --bib2json . Accepts either a bibliography path or text (string). If supplying text, pandoc-citeproc will likely require input_format be specified. The CSL JSON is returned as Python objects.","title":"load_bibliography"},{"location":"reference/manubot/pandoc/bibliography/#parameters","text":"path : str, pathlike, or None Path to a bibliography file. Extension is used by pandoc-citeproc to infer the format of the input. text : str or None Text representation of the bibligriophy, such as a JSON-formatted string. input_format should be specified if providing text input. input_format : str or None Manually specified input formatted that is supported by pandoc-citeproc: https://github.com/jgm/pandoc-citeproc/blob/master/man/pandoc-citeproc.1.md#options","title":"Parameters"},{"location":"reference/manubot/pandoc/bibliography/#returns","text":"csl_json : JSON-like object CSL JSON Data for the references encoded by the input bibliography. View Source def load_bibliography ( path = None , text = None , input_format = None ): \"\"\" Convert a bibliography to CSL JSON using `pandoc-citeproc --bib2json`. Accepts either a bibliography path or text (string). If supplying text, pandoc-citeproc will likely require input_format be specified. The CSL JSON is returned as Python objects. Parameters ---------- path : str, pathlike, or None Path to a bibliography file. Extension is used by pandoc-citeproc to infer the format of the input. text : str or None Text representation of the bibligriophy, such as a JSON-formatted string. `input_format` should be specified if providing text input. input_format : str or None Manually specified input formatted that is supported by pandoc-citeproc: https://github.com/jgm/pandoc-citeproc/blob/master/man/pandoc-citeproc.1.md#options Returns ------- csl_json : JSON-like object CSL JSON Data for the references encoded by the input bibliography. \"\"\" use_text = path is None use_path = text is None if not ( use_text ^ use_path ): raise ValueError ( \"load_bibliography: specify either path or text but not both.\" ) if not get_pandoc_info ()[ \"pandoc-citeproc\" ]: logging . error ( \"pandoc-citeproc not found on system: manubot.pandoc.bibliography.load_bibliography returning empty CSL JSON\" ) return [] args = [ \"pandoc-citeproc\" , \"--bib2json\" ] if input_format : args . extend ([ \"--format\" , input_format ]) run_kwargs = {} if use_path : args . append ( str ( path )) if use_text : run_kwargs [ \"input\" ] = text logging . info ( \"call_pandoc subprocess args:\\n>>> \" + shlex_join ( args )) process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ** run_kwargs , ) logging . info ( f \"captured stderr:\\n{process.stderr}\" ) process . check_returncode () try : csl_json = json . loads ( process . stdout ) except Exception : logging . exception ( f \"Error parsing bib2json output as JSON:\\n{process.stdout}\" ) csl_json = [] return csl_json","title":"Returns"},{"location":"reference/manubot/pandoc/cite_filter/","text":"Module manubot.pandoc.cite_filter This module defines a pandoc filter for manubot cite functionality. Related development commands: # export to plain text pandoc --to = plain --standalone --bibliography = manubot/pandoc/tests/test_cite_filter/bibliography.json --bibliography = manubot/pandoc/tests/test_cite_filter/bibliography.bib --filter = pandoc-manubot-cite --filter = pandoc-citeproc manubot/pandoc/tests/test_cite_filter/input.md # call the filter manually using pandoc JSON output pandoc --to = json manubot/pandoc/tests/test_cite_filter/input.md | python manubot/pandoc/test_cite.py markdown Related resources on pandoc filters: python pandocfilters package python panflute package panflute Citation class View Source \"\"\" This module defines a pandoc filter for manubot cite functionality. Related development commands: ```shell # export to plain text pandoc \\ --to=plain \\ --standalone \\ --bibliography=manubot/pandoc/tests/test_cite_filter/bibliography.json \\ --bibliography=manubot/pandoc/tests/test_cite_filter/bibliography.bib \\ --filter=pandoc-manubot-cite \\ --filter=pandoc-citeproc \\ manubot/pandoc/tests/test_cite_filter/input.md # call the filter manually using pandoc JSON output pandoc \\ --to=json \\ manubot/pandoc/tests/test_cite_filter/input.md \\ | python manubot/pandoc/test_cite.py markdown ``` Related resources on pandoc filters: - [python pandocfilters package](https://github.com/jgm/pandocfilters) - [python panflute package](https://github.com/sergiocorreia/panflute) - [panflute Citation class](http://scorreia.com/software/panflute/code.html#panflute.elements.Citation) \"\"\" import argparse import logging import panflute as pf from manubot.cite.citations import Citations global_variables = { \"manuscript_citekeys\" : list (), } def parse_args (): \"\"\" Read command line arguments \"\"\" parser = argparse . ArgumentParser ( description = \"Pandoc filter for citation by persistent identifier. \" \"Filters are command-line programs that read and write a JSON-encoded abstract syntax tree for Pandoc. \" \"Unless you are debugging, run this filter as part of a pandoc command by specifying --filter=pandoc-manubot-cite.\" ) parser . add_argument ( \"target_format\" , help = \"output format of the pandoc command, as per Pandoc's --to option\" , ) parser . add_argument ( \"--input\" , nargs = \"?\" , type = argparse . FileType ( \"r\" , encoding = \"utf-8\" ), help = \"path read JSON input (defaults to stdin)\" , ) parser . add_argument ( \"--output\" , nargs = \"?\" , type = argparse . FileType ( \"w\" , encoding = \"utf-8\" ), help = \"path to write JSON output (defaults to stdout)\" , ) args = parser . parse_args () return args def _get_citekeys_action ( elem , doc ): \"\"\" Panflute action to extract citationId from all Citations in the AST. \"\"\" if not isinstance ( elem , pf . Citation ): return None manuscript_citekeys = global_variables [ \"manuscript_citekeys\" ] manuscript_citekeys . append ( elem . id ) return None def _citation_to_id_action ( elem , doc ): \"\"\" Panflute action to update the citationId of Citations in the AST with their manubot-created keys. \"\"\" if not isinstance ( elem , pf . Citation ): return None mapper = global_variables [ \"citekey_shortener\" ] if elem . id in mapper : elem . id = mapper [ elem . id ] return None def _get_reference_link_citekey_aliases ( elem , doc ): \"\"\" Extract citekey aliases from the document that were defined using markdown's link reference syntax. https://spec.commonmark.org/0.29/#link-reference-definitions Based on pandoc-url2cite implementation by phiresky at https://github.com/phiresky/pandoc-url2cite/blob/b28374a9a037a5ce1747b8567160d8dffd64177e/index.ts#L118-L152 \"\"\" if type ( elem ) != pf . Para : # require link reference definitions to be in their own paragraph return while ( len ( elem . content ) >= 3 and type ( elem . content [ 0 ]) == pf . Cite and len ( elem . content [ 0 ] . citations ) == 1 and type ( elem . content [ 1 ]) == pf . Str and elem . content [ 1 ] . text == \":\" ): # paragraph consists of at least a Cite (with one Citaiton), # a Str (equal to \":\"), and additional elements, such as a # link destination and possibly more link-reference definitions. space_index = 3 if type ( elem . content [ 2 ]) == pf . Space else 2 destination = elem . content [ space_index ] if type ( destination ) == pf . Str : # paragraph starts with `[@something]: something` # save info to citekeys and remove from paragraph citekey = elem . content [ 0 ] . citations [ 0 ] . id citekey_aliases = global_variables [ \"citekey_aliases\" ] if ( citekey in citekey_aliases and citekey_aliases [ citekey ] != destination . text ): logging . warning ( f \"multiple aliases defined for @{citekey}\" ) citekey_aliases [ citekey ] = destination . text # found citation, add it to citekeys and remove it from document elem . content = elem . content [ space_index + 1 :] # remove leading SoftBreak, before continuing if len ( elem . content ) > 0 and type ( elem . content [ 0 ]) == pf . SoftBreak : elem . content . pop ( 0 ) def _get_load_manual_references_kwargs ( doc ) -> dict : \"\"\" Return keyword arguments for Citations.load_manual_references. \"\"\" manual_refs = doc . get_metadata ( \"references\" , default = []) bibliography_paths = doc . get_metadata ( \"bibliography\" , default = []) if not isinstance ( bibliography_paths , list ): bibliography_paths = [ bibliography_paths ] return dict ( paths = bibliography_paths , extra_csl_items = manual_refs ,) def process_citations ( doc ): \"\"\" Apply citation-by-identifier to a Python object representation of Pandoc's Abstract Syntax Tree. The following Pandoc metadata fields are considered: - bibliography (use to define reference metadata manually) - citekey-aliases (use to define tags for cite-by-id citations) - manubot-requests-cache-path - manubot-clear-requests-cache - manubot-output-citekeys: path to write TSV table of citekeys - manubot-output-bibliography: path to write generated CSL JSON bibliography \"\"\" citekey_aliases = doc . get_metadata ( \"citekey-aliases\" , default = {}) if not isinstance ( citekey_aliases , dict ): logging . warning ( f \"Expected metadata.citekey-aliases to be a dict, \" f \"but received a {citekey_aliases.__class__.__name__}. Disregarding.\" ) citekey_aliases = dict () global_variables [ \"citekey_aliases\" ] = citekey_aliases doc . walk ( _get_reference_link_citekey_aliases ) doc . walk ( _get_citekeys_action ) manuscript_citekeys = global_variables [ \"manuscript_citekeys\" ] citations = Citations ( input_ids = manuscript_citekeys , aliases = citekey_aliases ) citations . csl_item_failure_log_level = \"ERROR\" requests_cache_path = doc . get_metadata ( \"manubot-requests-cache-path\" ) if requests_cache_path : from manubot.process.requests_cache import RequestsCache req_cache = RequestsCache ( requests_cache_path ) req_cache . mkdir () req_cache . install () if doc . get_metadata ( \"manubot-clear-requests-cache\" , default = False ): req_cache . clear () citations . filter_pandoc_xnos () citations . load_manual_references ( ** _get_load_manual_references_kwargs ( doc )) citations . inspect ( log_level = \"WARNING\" ) citations . get_csl_items () global_variables [ \"citekey_shortener\" ] = citations . input_to_csl_id doc . walk ( _citation_to_id_action ) if requests_cache_path : req_cache . close () citations . write_citekeys_tsv ( path = doc . get_metadata ( \"manubot-output-citekeys\" )) citations . write_csl_json ( path = doc . get_metadata ( \"manubot-output-bibliography\" )) # Update pandoc metadata with fields that this filter # has either consumed, created, or modified. doc . metadata [ \"bibliography\" ] = [] doc . metadata [ \"references\" ] = citations . csl_items doc . metadata [ \"citekey_aliases\" ] = citekey_aliases def main (): from manubot.command import setup_logging_and_errors , exit_if_error_handler_fired diagnostics = setup_logging_and_errors () args = parse_args () # Let panflute handle io to sys.stdout / sys.stdin to set utf-8 encoding. # args.input=None for stdin, args.output=None for stdout doc = pf . load ( input_stream = args . input ) log_level = doc . get_metadata ( \"manubot-log-level\" , \"WARNING\" ) diagnostics [ \"logger\" ] . setLevel ( getattr ( logging , log_level )) process_citations ( doc ) pf . dump ( doc , output_stream = args . output ) if doc . get_metadata ( \"manubot-fail-on-errors\" , False ): exit_if_error_handler_fired ( diagnostics [ \"error_handler\" ]) if __name__ == \"__main__\" : main () Variables global_variables Functions main def main ( ) View Source def main (): from manubot.command import setup_logging_and_errors , exit_if_error_handler_fired diagnostics = setup_logging_and_errors () args = parse_args () # Let panflute handle io to sys.stdout / sys.stdin to set utf-8 encoding. # args.input=None for stdin, args.output=None for stdout doc = pf . load ( input_stream = args . input ) log_level = doc . get_metadata ( \"manubot-log-level\" , \"WARNING\" ) diagnostics [ \"logger\" ] . setLevel ( getattr ( logging , log_level )) process_citations ( doc ) pf . dump ( doc , output_stream = args . output ) if doc . get_metadata ( \"manubot-fail-on-errors\" , False ): exit_if_error_handler_fired ( diagnostics [ \"error_handler\" ]) parse_args def parse_args ( ) Read command line arguments View Source def parse_args (): \"\"\" Read command line arguments \"\"\" parser = argparse . ArgumentParser ( description = \"Pandoc filter for citation by persistent identifier. \" \"Filters are command-line programs that read and write a JSON-encoded abstract syntax tree for Pandoc. \" \"Unless you are debugging, run this filter as part of a pandoc command by specifying --filter=pandoc-manubot-cite.\" ) parser . add_argument ( \"target_format\" , help = \"output format of the pandoc command, as per Pandoc's --to option\" , ) parser . add_argument ( \"--input\" , nargs = \"?\" , type = argparse . FileType ( \"r\" , encoding = \"utf-8\" ), help = \"path read JSON input (defaults to stdin)\" , ) parser . add_argument ( \"--output\" , nargs = \"?\" , type = argparse . FileType ( \"w\" , encoding = \"utf-8\" ), help = \"path to write JSON output (defaults to stdout)\" , ) args = parser . parse_args () return args process_citations def process_citations ( doc ) Apply citation-by-identifier to a Python object representation of Pandoc's Abstract Syntax Tree. The following Pandoc metadata fields are considered: bibliography (use to define reference metadata manually) citekey-aliases (use to define tags for cite-by-id citations) manubot-requests-cache-path manubot-clear-requests-cache manubot-output-citekeys: path to write TSV table of citekeys manubot-output-bibliography: path to write generated CSL JSON bibliography View Source def process_citations ( doc ): \"\"\" Apply citation-by-identifier to a Python object representation of Pandoc's Abstract Syntax Tree. The following Pandoc metadata fields are considered: - bibliography (use to define reference metadata manually) - citekey-aliases (use to define tags for cite-by-id citations) - manubot-requests-cache-path - manubot-clear-requests-cache - manubot-output-citekeys: path to write TSV table of citekeys - manubot-output-bibliography: path to write generated CSL JSON bibliography \"\"\" citekey_aliases = doc . get_metadata ( \"citekey-aliases\" , default = {} ) if not isinstance ( citekey_aliases , dict ): logging . warning ( f \"Expected metadata.citekey-aliases to be a dict, \" f \"but received a {citekey_aliases.__class__.__name__}. Disregarding.\" ) citekey_aliases = dict () global_variables [ \"citekey_aliases\" ] = citekey_aliases doc . walk ( _get_reference_link_citekey_aliases ) doc . walk ( _get_citekeys_action ) manuscript_citekeys = global_variables [ \"manuscript_citekeys\" ] citations = Citations ( input_ids = manuscript_citekeys , aliases = citekey_aliases ) citations . csl_item_failure_log_level = \"ERROR\" requests_cache_path = doc . get_metadata ( \"manubot-requests-cache-path\" ) if requests_cache_path : from manubot . process . requests_cache import RequestsCache req_cache = RequestsCache ( requests_cache_path ) req_cache . mkdir () req_cache . install () if doc . get_metadata ( \"manubot-clear-requests-cache\" , default = False ): req_cache . clear () citations . filter_pandoc_xnos () citations . load_manual_references ( ** _get_load_manual_references_kwargs ( doc )) citations . inspect ( log_level = \"WARNING\" ) citations . get_csl_items () global_variables [ \"citekey_shortener\" ] = citations . input_to_csl_id doc . walk ( _citation_to_id_action ) if requests_cache_path : req_cache . close () citations . write_citekeys_tsv ( path = doc . get_metadata ( \"manubot-output-citekeys\" )) citations . write_csl_json ( path = doc . get_metadata ( \"manubot-output-bibliography\" )) # Update pandoc metadata with fields that this filter # has either consumed , created , or modified . doc . metadata [ \"bibliography\" ] = [] doc . metadata [ \"references\" ] = citations . csl_items doc . metadata [ \"citekey_aliases\" ] = citekey_aliases","title":"Cite Filter"},{"location":"reference/manubot/pandoc/cite_filter/#module-manubotpandoccite_filter","text":"This module defines a pandoc filter for manubot cite functionality. Related development commands: # export to plain text pandoc --to = plain --standalone --bibliography = manubot/pandoc/tests/test_cite_filter/bibliography.json --bibliography = manubot/pandoc/tests/test_cite_filter/bibliography.bib --filter = pandoc-manubot-cite --filter = pandoc-citeproc manubot/pandoc/tests/test_cite_filter/input.md # call the filter manually using pandoc JSON output pandoc --to = json manubot/pandoc/tests/test_cite_filter/input.md | python manubot/pandoc/test_cite.py markdown Related resources on pandoc filters: python pandocfilters package python panflute package panflute Citation class View Source \"\"\" This module defines a pandoc filter for manubot cite functionality. Related development commands: ```shell # export to plain text pandoc \\ --to=plain \\ --standalone \\ --bibliography=manubot/pandoc/tests/test_cite_filter/bibliography.json \\ --bibliography=manubot/pandoc/tests/test_cite_filter/bibliography.bib \\ --filter=pandoc-manubot-cite \\ --filter=pandoc-citeproc \\ manubot/pandoc/tests/test_cite_filter/input.md # call the filter manually using pandoc JSON output pandoc \\ --to=json \\ manubot/pandoc/tests/test_cite_filter/input.md \\ | python manubot/pandoc/test_cite.py markdown ``` Related resources on pandoc filters: - [python pandocfilters package](https://github.com/jgm/pandocfilters) - [python panflute package](https://github.com/sergiocorreia/panflute) - [panflute Citation class](http://scorreia.com/software/panflute/code.html#panflute.elements.Citation) \"\"\" import argparse import logging import panflute as pf from manubot.cite.citations import Citations global_variables = { \"manuscript_citekeys\" : list (), } def parse_args (): \"\"\" Read command line arguments \"\"\" parser = argparse . ArgumentParser ( description = \"Pandoc filter for citation by persistent identifier. \" \"Filters are command-line programs that read and write a JSON-encoded abstract syntax tree for Pandoc. \" \"Unless you are debugging, run this filter as part of a pandoc command by specifying --filter=pandoc-manubot-cite.\" ) parser . add_argument ( \"target_format\" , help = \"output format of the pandoc command, as per Pandoc's --to option\" , ) parser . add_argument ( \"--input\" , nargs = \"?\" , type = argparse . FileType ( \"r\" , encoding = \"utf-8\" ), help = \"path read JSON input (defaults to stdin)\" , ) parser . add_argument ( \"--output\" , nargs = \"?\" , type = argparse . FileType ( \"w\" , encoding = \"utf-8\" ), help = \"path to write JSON output (defaults to stdout)\" , ) args = parser . parse_args () return args def _get_citekeys_action ( elem , doc ): \"\"\" Panflute action to extract citationId from all Citations in the AST. \"\"\" if not isinstance ( elem , pf . Citation ): return None manuscript_citekeys = global_variables [ \"manuscript_citekeys\" ] manuscript_citekeys . append ( elem . id ) return None def _citation_to_id_action ( elem , doc ): \"\"\" Panflute action to update the citationId of Citations in the AST with their manubot-created keys. \"\"\" if not isinstance ( elem , pf . Citation ): return None mapper = global_variables [ \"citekey_shortener\" ] if elem . id in mapper : elem . id = mapper [ elem . id ] return None def _get_reference_link_citekey_aliases ( elem , doc ): \"\"\" Extract citekey aliases from the document that were defined using markdown's link reference syntax. https://spec.commonmark.org/0.29/#link-reference-definitions Based on pandoc-url2cite implementation by phiresky at https://github.com/phiresky/pandoc-url2cite/blob/b28374a9a037a5ce1747b8567160d8dffd64177e/index.ts#L118-L152 \"\"\" if type ( elem ) != pf . Para : # require link reference definitions to be in their own paragraph return while ( len ( elem . content ) >= 3 and type ( elem . content [ 0 ]) == pf . Cite and len ( elem . content [ 0 ] . citations ) == 1 and type ( elem . content [ 1 ]) == pf . Str and elem . content [ 1 ] . text == \":\" ): # paragraph consists of at least a Cite (with one Citaiton), # a Str (equal to \":\"), and additional elements, such as a # link destination and possibly more link-reference definitions. space_index = 3 if type ( elem . content [ 2 ]) == pf . Space else 2 destination = elem . content [ space_index ] if type ( destination ) == pf . Str : # paragraph starts with `[@something]: something` # save info to citekeys and remove from paragraph citekey = elem . content [ 0 ] . citations [ 0 ] . id citekey_aliases = global_variables [ \"citekey_aliases\" ] if ( citekey in citekey_aliases and citekey_aliases [ citekey ] != destination . text ): logging . warning ( f \"multiple aliases defined for @{citekey}\" ) citekey_aliases [ citekey ] = destination . text # found citation, add it to citekeys and remove it from document elem . content = elem . content [ space_index + 1 :] # remove leading SoftBreak, before continuing if len ( elem . content ) > 0 and type ( elem . content [ 0 ]) == pf . SoftBreak : elem . content . pop ( 0 ) def _get_load_manual_references_kwargs ( doc ) -> dict : \"\"\" Return keyword arguments for Citations.load_manual_references. \"\"\" manual_refs = doc . get_metadata ( \"references\" , default = []) bibliography_paths = doc . get_metadata ( \"bibliography\" , default = []) if not isinstance ( bibliography_paths , list ): bibliography_paths = [ bibliography_paths ] return dict ( paths = bibliography_paths , extra_csl_items = manual_refs ,) def process_citations ( doc ): \"\"\" Apply citation-by-identifier to a Python object representation of Pandoc's Abstract Syntax Tree. The following Pandoc metadata fields are considered: - bibliography (use to define reference metadata manually) - citekey-aliases (use to define tags for cite-by-id citations) - manubot-requests-cache-path - manubot-clear-requests-cache - manubot-output-citekeys: path to write TSV table of citekeys - manubot-output-bibliography: path to write generated CSL JSON bibliography \"\"\" citekey_aliases = doc . get_metadata ( \"citekey-aliases\" , default = {}) if not isinstance ( citekey_aliases , dict ): logging . warning ( f \"Expected metadata.citekey-aliases to be a dict, \" f \"but received a {citekey_aliases.__class__.__name__}. Disregarding.\" ) citekey_aliases = dict () global_variables [ \"citekey_aliases\" ] = citekey_aliases doc . walk ( _get_reference_link_citekey_aliases ) doc . walk ( _get_citekeys_action ) manuscript_citekeys = global_variables [ \"manuscript_citekeys\" ] citations = Citations ( input_ids = manuscript_citekeys , aliases = citekey_aliases ) citations . csl_item_failure_log_level = \"ERROR\" requests_cache_path = doc . get_metadata ( \"manubot-requests-cache-path\" ) if requests_cache_path : from manubot.process.requests_cache import RequestsCache req_cache = RequestsCache ( requests_cache_path ) req_cache . mkdir () req_cache . install () if doc . get_metadata ( \"manubot-clear-requests-cache\" , default = False ): req_cache . clear () citations . filter_pandoc_xnos () citations . load_manual_references ( ** _get_load_manual_references_kwargs ( doc )) citations . inspect ( log_level = \"WARNING\" ) citations . get_csl_items () global_variables [ \"citekey_shortener\" ] = citations . input_to_csl_id doc . walk ( _citation_to_id_action ) if requests_cache_path : req_cache . close () citations . write_citekeys_tsv ( path = doc . get_metadata ( \"manubot-output-citekeys\" )) citations . write_csl_json ( path = doc . get_metadata ( \"manubot-output-bibliography\" )) # Update pandoc metadata with fields that this filter # has either consumed, created, or modified. doc . metadata [ \"bibliography\" ] = [] doc . metadata [ \"references\" ] = citations . csl_items doc . metadata [ \"citekey_aliases\" ] = citekey_aliases def main (): from manubot.command import setup_logging_and_errors , exit_if_error_handler_fired diagnostics = setup_logging_and_errors () args = parse_args () # Let panflute handle io to sys.stdout / sys.stdin to set utf-8 encoding. # args.input=None for stdin, args.output=None for stdout doc = pf . load ( input_stream = args . input ) log_level = doc . get_metadata ( \"manubot-log-level\" , \"WARNING\" ) diagnostics [ \"logger\" ] . setLevel ( getattr ( logging , log_level )) process_citations ( doc ) pf . dump ( doc , output_stream = args . output ) if doc . get_metadata ( \"manubot-fail-on-errors\" , False ): exit_if_error_handler_fired ( diagnostics [ \"error_handler\" ]) if __name__ == \"__main__\" : main ()","title":"Module manubot.pandoc.cite_filter"},{"location":"reference/manubot/pandoc/cite_filter/#variables","text":"global_variables","title":"Variables"},{"location":"reference/manubot/pandoc/cite_filter/#functions","text":"","title":"Functions"},{"location":"reference/manubot/pandoc/cite_filter/#main","text":"def main ( ) View Source def main (): from manubot.command import setup_logging_and_errors , exit_if_error_handler_fired diagnostics = setup_logging_and_errors () args = parse_args () # Let panflute handle io to sys.stdout / sys.stdin to set utf-8 encoding. # args.input=None for stdin, args.output=None for stdout doc = pf . load ( input_stream = args . input ) log_level = doc . get_metadata ( \"manubot-log-level\" , \"WARNING\" ) diagnostics [ \"logger\" ] . setLevel ( getattr ( logging , log_level )) process_citations ( doc ) pf . dump ( doc , output_stream = args . output ) if doc . get_metadata ( \"manubot-fail-on-errors\" , False ): exit_if_error_handler_fired ( diagnostics [ \"error_handler\" ])","title":"main"},{"location":"reference/manubot/pandoc/cite_filter/#parse_args","text":"def parse_args ( ) Read command line arguments View Source def parse_args (): \"\"\" Read command line arguments \"\"\" parser = argparse . ArgumentParser ( description = \"Pandoc filter for citation by persistent identifier. \" \"Filters are command-line programs that read and write a JSON-encoded abstract syntax tree for Pandoc. \" \"Unless you are debugging, run this filter as part of a pandoc command by specifying --filter=pandoc-manubot-cite.\" ) parser . add_argument ( \"target_format\" , help = \"output format of the pandoc command, as per Pandoc's --to option\" , ) parser . add_argument ( \"--input\" , nargs = \"?\" , type = argparse . FileType ( \"r\" , encoding = \"utf-8\" ), help = \"path read JSON input (defaults to stdin)\" , ) parser . add_argument ( \"--output\" , nargs = \"?\" , type = argparse . FileType ( \"w\" , encoding = \"utf-8\" ), help = \"path to write JSON output (defaults to stdout)\" , ) args = parser . parse_args () return args","title":"parse_args"},{"location":"reference/manubot/pandoc/cite_filter/#process_citations","text":"def process_citations ( doc ) Apply citation-by-identifier to a Python object representation of Pandoc's Abstract Syntax Tree. The following Pandoc metadata fields are considered: bibliography (use to define reference metadata manually) citekey-aliases (use to define tags for cite-by-id citations) manubot-requests-cache-path manubot-clear-requests-cache manubot-output-citekeys: path to write TSV table of citekeys manubot-output-bibliography: path to write generated CSL JSON bibliography View Source def process_citations ( doc ): \"\"\" Apply citation-by-identifier to a Python object representation of Pandoc's Abstract Syntax Tree. The following Pandoc metadata fields are considered: - bibliography (use to define reference metadata manually) - citekey-aliases (use to define tags for cite-by-id citations) - manubot-requests-cache-path - manubot-clear-requests-cache - manubot-output-citekeys: path to write TSV table of citekeys - manubot-output-bibliography: path to write generated CSL JSON bibliography \"\"\" citekey_aliases = doc . get_metadata ( \"citekey-aliases\" , default = {} ) if not isinstance ( citekey_aliases , dict ): logging . warning ( f \"Expected metadata.citekey-aliases to be a dict, \" f \"but received a {citekey_aliases.__class__.__name__}. Disregarding.\" ) citekey_aliases = dict () global_variables [ \"citekey_aliases\" ] = citekey_aliases doc . walk ( _get_reference_link_citekey_aliases ) doc . walk ( _get_citekeys_action ) manuscript_citekeys = global_variables [ \"manuscript_citekeys\" ] citations = Citations ( input_ids = manuscript_citekeys , aliases = citekey_aliases ) citations . csl_item_failure_log_level = \"ERROR\" requests_cache_path = doc . get_metadata ( \"manubot-requests-cache-path\" ) if requests_cache_path : from manubot . process . requests_cache import RequestsCache req_cache = RequestsCache ( requests_cache_path ) req_cache . mkdir () req_cache . install () if doc . get_metadata ( \"manubot-clear-requests-cache\" , default = False ): req_cache . clear () citations . filter_pandoc_xnos () citations . load_manual_references ( ** _get_load_manual_references_kwargs ( doc )) citations . inspect ( log_level = \"WARNING\" ) citations . get_csl_items () global_variables [ \"citekey_shortener\" ] = citations . input_to_csl_id doc . walk ( _citation_to_id_action ) if requests_cache_path : req_cache . close () citations . write_citekeys_tsv ( path = doc . get_metadata ( \"manubot-output-citekeys\" )) citations . write_csl_json ( path = doc . get_metadata ( \"manubot-output-bibliography\" )) # Update pandoc metadata with fields that this filter # has either consumed , created , or modified . doc . metadata [ \"bibliography\" ] = [] doc . metadata [ \"references\" ] = citations . csl_items doc . metadata [ \"citekey_aliases\" ] = citekey_aliases","title":"process_citations"},{"location":"reference/manubot/pandoc/util/","text":"Module manubot.pandoc.util View Source import logging import shutil import subprocess import functools @functools.lru_cache () def get_pandoc_info (): \"\"\" Return path and version information for the system's pandoc and pandoc-citeproc commands. When Pandoc is installed, the output will look like: ```python { 'pandoc': True, 'pandoc path': '/PATH_TO_EXECUTABLES/pandoc', 'pandoc version': (2, 5), 'pandoc-citeproc': True, 'pandoc-citeproc path': '/PATH_TO_EXECUTABLES/pandoc-citeproc', 'pandoc-citeproc version': (0, 15), } ``` If the executables are missing, the output will be like: ```python { 'pandoc': False, 'pandoc-citeproc': False, } ``` \"\"\" stats = dict () for command in \"pandoc\" , \"pandoc-citeproc\" : path = shutil . which ( command ) stats [ command ] = bool ( path ) if not path : continue version = subprocess . check_output ( args = [ command , \"--version\" ], encoding = \"utf-8\" ) logging . debug ( version ) version , * _discard = version . splitlines () _discard , version = version . strip () . split () from packaging.version import parse as parse_version version = parse_version ( version ) . release stats [ f \"{command} version\" ] = version stats [ f \"{command} path\" ] = path logging . info ( \" \\n \" . join ( f \"{k}: {v}\" for k , v in stats . items ())) return stats def get_pandoc_version () -> ( int , int , int ): \"\"\" Return pandoc version as tuple of major and minor version numbers, for example: (2, 7, 2) \"\"\" return get_pandoc_info ()[ \"pandoc version\" ] Functions get_pandoc_info def get_pandoc_info ( ) Return path and version information for the system's pandoc and pandoc-citeproc commands. When Pandoc is installed, the output will look like: { 'pandoc' : True , 'pandoc path' : '/PATH_TO_EXECUTABLES/pandoc' , 'pandoc version' : ( 2 , 5 ), 'pandoc-citeproc' : True , 'pandoc-citeproc path' : '/PATH_TO_EXECUTABLES/pandoc-citeproc' , 'pandoc-citeproc version' : ( 0 , 15 ), } If the executables are missing, the output will be like: { 'pandoc' : False , 'pandoc-citeproc' : False , } View Source @functools . lru_cache () def get_pandoc_info () : \"\"\" Return path and version information for the system's pandoc and pandoc-citeproc commands. When Pandoc is installed, the output will look like: ```python { 'pandoc': True, 'pandoc path': '/PATH_TO_EXECUTABLES/pandoc', 'pandoc version': (2, 5), 'pandoc-citeproc': True, 'pandoc-citeproc path': '/PATH_TO_EXECUTABLES/pandoc-citeproc', 'pandoc-citeproc version': (0, 15), } ``` If the executables are missing, the output will be like: ```python { 'pandoc': False, 'pandoc-citeproc': False, } ``` \"\"\" stats = dict () for command in \"pandoc\" , \"pandoc-citeproc\" : path = shutil . which ( command ) stats [ command ] = bool ( path ) if not path : continue version = subprocess . check_output ( args =[ command, \"--version\" ] , encoding = \"utf-8\" ) logging . debug ( version ) version , * _discard = version . splitlines () _discard , version = version . strip (). split () from packaging . version import parse as parse_version version = parse_version ( version ). release stats [ f\"{command} version\" ] = version stats [ f\"{command} path\" ] = path logging . info ( \"\\n\" . join ( f \"{k}: {v}\" for k , v in stats . items ())) return stats get_pandoc_version def get_pandoc_version ( ) -> ( < class ' int '>, <class ' int '>, <class ' int '>) Return pandoc version as tuple of major and minor version numbers, for example: (2, 7, 2) View Source def get_pandoc_version () -> ( int , int , int ): \"\"\" Return pandoc version as tuple of major and minor version numbers, for example: (2, 7, 2) \"\"\" return get_pandoc_info ()[ \"pandoc version\" ]","title":"Util"},{"location":"reference/manubot/pandoc/util/#module-manubotpandocutil","text":"View Source import logging import shutil import subprocess import functools @functools.lru_cache () def get_pandoc_info (): \"\"\" Return path and version information for the system's pandoc and pandoc-citeproc commands. When Pandoc is installed, the output will look like: ```python { 'pandoc': True, 'pandoc path': '/PATH_TO_EXECUTABLES/pandoc', 'pandoc version': (2, 5), 'pandoc-citeproc': True, 'pandoc-citeproc path': '/PATH_TO_EXECUTABLES/pandoc-citeproc', 'pandoc-citeproc version': (0, 15), } ``` If the executables are missing, the output will be like: ```python { 'pandoc': False, 'pandoc-citeproc': False, } ``` \"\"\" stats = dict () for command in \"pandoc\" , \"pandoc-citeproc\" : path = shutil . which ( command ) stats [ command ] = bool ( path ) if not path : continue version = subprocess . check_output ( args = [ command , \"--version\" ], encoding = \"utf-8\" ) logging . debug ( version ) version , * _discard = version . splitlines () _discard , version = version . strip () . split () from packaging.version import parse as parse_version version = parse_version ( version ) . release stats [ f \"{command} version\" ] = version stats [ f \"{command} path\" ] = path logging . info ( \" \\n \" . join ( f \"{k}: {v}\" for k , v in stats . items ())) return stats def get_pandoc_version () -> ( int , int , int ): \"\"\" Return pandoc version as tuple of major and minor version numbers, for example: (2, 7, 2) \"\"\" return get_pandoc_info ()[ \"pandoc version\" ]","title":"Module manubot.pandoc.util"},{"location":"reference/manubot/pandoc/util/#functions","text":"","title":"Functions"},{"location":"reference/manubot/pandoc/util/#get_pandoc_info","text":"def get_pandoc_info ( ) Return path and version information for the system's pandoc and pandoc-citeproc commands. When Pandoc is installed, the output will look like: { 'pandoc' : True , 'pandoc path' : '/PATH_TO_EXECUTABLES/pandoc' , 'pandoc version' : ( 2 , 5 ), 'pandoc-citeproc' : True , 'pandoc-citeproc path' : '/PATH_TO_EXECUTABLES/pandoc-citeproc' , 'pandoc-citeproc version' : ( 0 , 15 ), } If the executables are missing, the output will be like: { 'pandoc' : False , 'pandoc-citeproc' : False , } View Source @functools . lru_cache () def get_pandoc_info () : \"\"\" Return path and version information for the system's pandoc and pandoc-citeproc commands. When Pandoc is installed, the output will look like: ```python { 'pandoc': True, 'pandoc path': '/PATH_TO_EXECUTABLES/pandoc', 'pandoc version': (2, 5), 'pandoc-citeproc': True, 'pandoc-citeproc path': '/PATH_TO_EXECUTABLES/pandoc-citeproc', 'pandoc-citeproc version': (0, 15), } ``` If the executables are missing, the output will be like: ```python { 'pandoc': False, 'pandoc-citeproc': False, } ``` \"\"\" stats = dict () for command in \"pandoc\" , \"pandoc-citeproc\" : path = shutil . which ( command ) stats [ command ] = bool ( path ) if not path : continue version = subprocess . check_output ( args =[ command, \"--version\" ] , encoding = \"utf-8\" ) logging . debug ( version ) version , * _discard = version . splitlines () _discard , version = version . strip (). split () from packaging . version import parse as parse_version version = parse_version ( version ). release stats [ f\"{command} version\" ] = version stats [ f\"{command} path\" ] = path logging . info ( \"\\n\" . join ( f \"{k}: {v}\" for k , v in stats . items ())) return stats","title":"get_pandoc_info"},{"location":"reference/manubot/pandoc/util/#get_pandoc_version","text":"def get_pandoc_version ( ) -> ( < class ' int '>, <class ' int '>, <class ' int '>) Return pandoc version as tuple of major and minor version numbers, for example: (2, 7, 2) View Source def get_pandoc_version () -> ( int , int , int ): \"\"\" Return pandoc version as tuple of major and minor version numbers, for example: (2, 7, 2) \"\"\" return get_pandoc_info ()[ \"pandoc version\" ]","title":"get_pandoc_version"},{"location":"reference/manubot/process/","text":"Module manubot.process Sub-modules manubot.process.bibliography manubot.process.ci manubot.process.manuscript manubot.process.metadata manubot.process.process_command manubot.process.requests_cache manubot.process.tests manubot.process.util","title":"Index"},{"location":"reference/manubot/process/#module-manubotprocess","text":"","title":"Module manubot.process"},{"location":"reference/manubot/process/#sub-modules","text":"manubot.process.bibliography manubot.process.ci manubot.process.manuscript manubot.process.metadata manubot.process.process_command manubot.process.requests_cache manubot.process.tests manubot.process.util","title":"Sub-modules"},{"location":"reference/manubot/process/bibliography/","text":"Module manubot.process.bibliography View Source import json import logging import pathlib from manubot import __version__ as manubot_version from manubot.cite.citekey import shorten_citekey from manubot.util import read_serialized_data def load_bibliography ( path ) -> list : \"\"\" Load a bibliography as CSL Items (a CSL JSON Python object). For paths that already contain CSL Items (inferred from a .json or .yaml extension), parse these files directly. Otherwise, delegate conversion to CSL Items to pandoc-citeproc. \"\"\" path = pathlib . Path ( path ) if path . suffix in { \".json\" , \".yaml\" }: try : csl_items = read_serialized_data ( path ) except Exception : logging . exception ( f \"process.load_bibliography: error parsing {path}. \\n \" ) csl_items = [] else : from manubot.pandoc.bibliography import ( load_bibliography as load_bibliography_pandoc , ) csl_items = load_bibliography_pandoc ( path ) if not isinstance ( csl_items , list ): logging . error ( f \"process.load_bibliography: csl_items read from {path} are of type {type(csl_items)}. \" \"Setting csl_items to an empty list.\" ) csl_items = [] from manubot.cite.csl_item import CSL_Item csl_items = [ CSL_Item ( csl_item ) for csl_item in csl_items ] return csl_items def load_manual_references ( paths = [], extra_csl_items = []) -> dict : \"\"\" Read manual references (overrides) from files specified by a list of paths. Returns a standard_citation to CSL Item dictionary. extra_csl_items specifies JSON CSL stored as a Python object, to be used in addition to the CSL JSON stored as text in the file specified by path. Set paths=[] to only use extra_csl_items. \"\"\" from manubot.cite.csl_item import CSL_Item csl_items = [] paths = list ( dict . fromkeys ( paths )) # remove duplicates for path in paths : path = pathlib . Path ( path ) if not path . is_file (): logging . warning ( f \"process.load_bibliographies is skipping a non-existent path: {path}\" ) continue for csl_item in load_bibliography ( path ): csl_item . note_append_text ( f \"This CSL JSON Item was loaded by Manubot v{manubot_version} from a manual reference file.\" ) csl_item . note_append_dict ({ \"manual_reference_filename\" : path . name }) csl_items . append ( csl_item ) csl_items . extend ( map ( CSL_Item , extra_csl_items )) manual_refs = dict () for csl_item in csl_items : try : csl_item . standardize_id () except Exception : csl_item_str = json . dumps ( csl_item , indent = 2 ) logging . info ( f \"Skipping csl_item where setting standard_id failed: \\n {csl_item_str}\" , exc_info = True , ) continue standard_id = csl_item [ \"id\" ] csl_item . set_id ( shorten_citekey ( standard_id )) csl_item . clean () manual_refs [ standard_id ] = csl_item return manual_refs Variables manubot_version Functions load_bibliography def load_bibliography ( path ) -> list Load a bibliography as CSL Items (a CSL JSON Python object). For paths that already contain CSL Items (inferred from a .json or .yaml extension), parse these files directly. Otherwise, delegate conversion to CSL Items to pandoc-citeproc. View Source def load_bibliography ( path ) -> list : \"\"\" Load a bibliography as CSL Items (a CSL JSON Python object). For paths that already contain CSL Items (inferred from a .json or .yaml extension), parse these files directly. Otherwise, delegate conversion to CSL Items to pandoc-citeproc. \"\"\" path = pathlib . Path ( path ) if path . suffix in { \".json\" , \".yaml\" }: try : csl_items = read_serialized_data ( path ) except Exception : logging . exception ( f \"process.load_bibliography: error parsing {path}. \\n \" ) csl_items = [] else : from manubot.pandoc.bibliography import ( load_bibliography as load_bibliography_pandoc , ) csl_items = load_bibliography_pandoc ( path ) if not isinstance ( csl_items , list ): logging . error ( f \"process.load_bibliography: csl_items read from {path} are of type {type(csl_items)}. \" \"Setting csl_items to an empty list.\" ) csl_items = [] from manubot.cite.csl_item import CSL_Item csl_items = [ CSL_Item ( csl_item ) for csl_item in csl_items ] return csl_items load_manual_references def load_manual_references ( paths = [], extra_csl_items = [] ) -> dict Read manual references (overrides) from files specified by a list of paths. Returns a standard_citation to CSL Item dictionary. extra_csl_items specifies JSON CSL stored as a Python object, to be used in addition to the CSL JSON stored as text in the file specified by path. Set paths=[] to only use extra_csl_items. View Source def load_manual_references ( paths = [], extra_csl_items = []) -> dict : \"\"\" Read manual references (overrides) from files specified by a list of paths. Returns a standard_citation to CSL Item dictionary. extra_csl_items specifies JSON CSL stored as a Python object, to be used in addition to the CSL JSON stored as text in the file specified by path. Set paths=[] to only use extra_csl_items. \"\"\" from manubot.cite.csl_item import CSL_Item csl_items = [] paths = list ( dict . fromkeys ( paths )) # remove duplicates for path in paths : path = pathlib . Path ( path ) if not path . is_file (): logging . warning ( f \"process.load_bibliographies is skipping a non-existent path: {path}\" ) continue for csl_item in load_bibliography ( path ): csl_item . note_append_text ( f \"This CSL JSON Item was loaded by Manubot v{manubot_version} from a manual reference file.\" ) csl_item . note_append_dict ({ \"manual_reference_filename\" : path . name }) csl_items . append ( csl_item ) csl_items . extend ( map ( CSL_Item , extra_csl_items )) manual_refs = dict () for csl_item in csl_items : try : csl_item . standardize_id () except Exception : csl_item_str = json . dumps ( csl_item , indent = 2 ) logging . info ( f \"Skipping csl_item where setting standard_id failed: \\n {csl_item_str}\" , exc_info = True , ) continue standard_id = csl_item [ \"id\" ] csl_item . set_id ( shorten_citekey ( standard_id )) csl_item . clean () manual_refs [ standard_id ] = csl_item return manual_refs","title":"Bibliography"},{"location":"reference/manubot/process/bibliography/#module-manubotprocessbibliography","text":"View Source import json import logging import pathlib from manubot import __version__ as manubot_version from manubot.cite.citekey import shorten_citekey from manubot.util import read_serialized_data def load_bibliography ( path ) -> list : \"\"\" Load a bibliography as CSL Items (a CSL JSON Python object). For paths that already contain CSL Items (inferred from a .json or .yaml extension), parse these files directly. Otherwise, delegate conversion to CSL Items to pandoc-citeproc. \"\"\" path = pathlib . Path ( path ) if path . suffix in { \".json\" , \".yaml\" }: try : csl_items = read_serialized_data ( path ) except Exception : logging . exception ( f \"process.load_bibliography: error parsing {path}. \\n \" ) csl_items = [] else : from manubot.pandoc.bibliography import ( load_bibliography as load_bibliography_pandoc , ) csl_items = load_bibliography_pandoc ( path ) if not isinstance ( csl_items , list ): logging . error ( f \"process.load_bibliography: csl_items read from {path} are of type {type(csl_items)}. \" \"Setting csl_items to an empty list.\" ) csl_items = [] from manubot.cite.csl_item import CSL_Item csl_items = [ CSL_Item ( csl_item ) for csl_item in csl_items ] return csl_items def load_manual_references ( paths = [], extra_csl_items = []) -> dict : \"\"\" Read manual references (overrides) from files specified by a list of paths. Returns a standard_citation to CSL Item dictionary. extra_csl_items specifies JSON CSL stored as a Python object, to be used in addition to the CSL JSON stored as text in the file specified by path. Set paths=[] to only use extra_csl_items. \"\"\" from manubot.cite.csl_item import CSL_Item csl_items = [] paths = list ( dict . fromkeys ( paths )) # remove duplicates for path in paths : path = pathlib . Path ( path ) if not path . is_file (): logging . warning ( f \"process.load_bibliographies is skipping a non-existent path: {path}\" ) continue for csl_item in load_bibliography ( path ): csl_item . note_append_text ( f \"This CSL JSON Item was loaded by Manubot v{manubot_version} from a manual reference file.\" ) csl_item . note_append_dict ({ \"manual_reference_filename\" : path . name }) csl_items . append ( csl_item ) csl_items . extend ( map ( CSL_Item , extra_csl_items )) manual_refs = dict () for csl_item in csl_items : try : csl_item . standardize_id () except Exception : csl_item_str = json . dumps ( csl_item , indent = 2 ) logging . info ( f \"Skipping csl_item where setting standard_id failed: \\n {csl_item_str}\" , exc_info = True , ) continue standard_id = csl_item [ \"id\" ] csl_item . set_id ( shorten_citekey ( standard_id )) csl_item . clean () manual_refs [ standard_id ] = csl_item return manual_refs","title":"Module manubot.process.bibliography"},{"location":"reference/manubot/process/bibliography/#variables","text":"manubot_version","title":"Variables"},{"location":"reference/manubot/process/bibliography/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/bibliography/#load_bibliography","text":"def load_bibliography ( path ) -> list Load a bibliography as CSL Items (a CSL JSON Python object). For paths that already contain CSL Items (inferred from a .json or .yaml extension), parse these files directly. Otherwise, delegate conversion to CSL Items to pandoc-citeproc. View Source def load_bibliography ( path ) -> list : \"\"\" Load a bibliography as CSL Items (a CSL JSON Python object). For paths that already contain CSL Items (inferred from a .json or .yaml extension), parse these files directly. Otherwise, delegate conversion to CSL Items to pandoc-citeproc. \"\"\" path = pathlib . Path ( path ) if path . suffix in { \".json\" , \".yaml\" }: try : csl_items = read_serialized_data ( path ) except Exception : logging . exception ( f \"process.load_bibliography: error parsing {path}. \\n \" ) csl_items = [] else : from manubot.pandoc.bibliography import ( load_bibliography as load_bibliography_pandoc , ) csl_items = load_bibliography_pandoc ( path ) if not isinstance ( csl_items , list ): logging . error ( f \"process.load_bibliography: csl_items read from {path} are of type {type(csl_items)}. \" \"Setting csl_items to an empty list.\" ) csl_items = [] from manubot.cite.csl_item import CSL_Item csl_items = [ CSL_Item ( csl_item ) for csl_item in csl_items ] return csl_items","title":"load_bibliography"},{"location":"reference/manubot/process/bibliography/#load_manual_references","text":"def load_manual_references ( paths = [], extra_csl_items = [] ) -> dict Read manual references (overrides) from files specified by a list of paths. Returns a standard_citation to CSL Item dictionary. extra_csl_items specifies JSON CSL stored as a Python object, to be used in addition to the CSL JSON stored as text in the file specified by path. Set paths=[] to only use extra_csl_items. View Source def load_manual_references ( paths = [], extra_csl_items = []) -> dict : \"\"\" Read manual references (overrides) from files specified by a list of paths. Returns a standard_citation to CSL Item dictionary. extra_csl_items specifies JSON CSL stored as a Python object, to be used in addition to the CSL JSON stored as text in the file specified by path. Set paths=[] to only use extra_csl_items. \"\"\" from manubot.cite.csl_item import CSL_Item csl_items = [] paths = list ( dict . fromkeys ( paths )) # remove duplicates for path in paths : path = pathlib . Path ( path ) if not path . is_file (): logging . warning ( f \"process.load_bibliographies is skipping a non-existent path: {path}\" ) continue for csl_item in load_bibliography ( path ): csl_item . note_append_text ( f \"This CSL JSON Item was loaded by Manubot v{manubot_version} from a manual reference file.\" ) csl_item . note_append_dict ({ \"manual_reference_filename\" : path . name }) csl_items . append ( csl_item ) csl_items . extend ( map ( CSL_Item , extra_csl_items )) manual_refs = dict () for csl_item in csl_items : try : csl_item . standardize_id () except Exception : csl_item_str = json . dumps ( csl_item , indent = 2 ) logging . info ( f \"Skipping csl_item where setting standard_id failed: \\n {csl_item_str}\" , exc_info = True , ) continue standard_id = csl_item [ \"id\" ] csl_item . set_id ( shorten_citekey ( standard_id )) csl_item . clean () manual_refs [ standard_id ] = csl_item return manual_refs","title":"load_manual_references"},{"location":"reference/manubot/process/ci/","text":"Module manubot.process.ci View Source import os import logging supported_providers = [ \"github\" , \"travis\" , \"appveyor\" ] def get_continuous_integration_parameters (): \"\"\" Return a dictionary with information on a continuous integration build inferred from environment variables. The following dictionary keys are set (all values are also strings): - provider: name of CI provider, such as \"travis\" or \"appveyor\". - repo_slug: owner/name of the source code repository, i.e. \"manubot/rootstock\" - repo_owner: owner from repo_slug, i.e. \"manubot\" - repo_name: name from repo_slug, i.e. \"rootstock\" - commit: git commit being evaluated by the CI build For pull requests, CI services often build a merge commit with the default branch rather than the commit that was added to the pull request. In these cases, commit refers to this merge commit. - triggering_commit: git commit that triggered the CI build. - build_url: URL for the webpage with build details - job_url: URL for the webpage with job details GitHub Actions does not set an environment variable with triggering_commit for pull requests. Therefore, set the following environment variable in your workflow: ```yaml env: GITHUB_PULL_REQUEST_SHA: ${{ github.event.pull_request.head.sha }} ``` \"\"\" if os . getenv ( \"GITHUB_ACTIONS\" , \"false\" ) == \"true\" : # https://git.io/JvUf7 repo_slug = os . environ [ \"GITHUB_REPOSITORY\" ] repo_owner , repo_name = repo_slug . split ( \"/\" ) action_id = os . environ [ \"GITHUB_ACTION\" ] # GITHUB_SHA for pull_request event: Last merge commit on the GITHUB_REF branch # GITHUB_SHA for push event: Commit pushed, unless deleting a branch (when it's the default branch) # https://git.io/JvUfd github_sha = os . environ [ \"GITHUB_SHA\" ] ci_params = { \"provider\" : \"github\" , \"repo_slug\" : repo_slug , \"repo_owner\" : repo_owner , \"repo_name\" : repo_name , \"commit\" : github_sha , \"triggering_commit\" : os . getenv ( \"GITHUB_PULL_REQUEST_SHA\" ) or github_sha , \"build_url\" : f \"https://github.com/{repo_slug}/commit/{github_sha}/checks\" , \"job_url\" : f \"https://github.com/{repo_slug}/runs/{action_id}\" , } return ci_params if os . getenv ( \"TRAVIS\" , \"false\" ) == \"true\" : # https://docs.travis-ci.com/user/environment-variables/ repo_slug = os . environ [ \"TRAVIS_REPO_SLUG\" ] repo_owner , repo_name = repo_slug . split ( \"/\" ) return { \"provider\" : \"travis\" , \"repo_slug\" : repo_slug , \"repo_owner\" : repo_owner , \"repo_name\" : repo_name , \"commit\" : os . environ [ \"TRAVIS_COMMIT\" ], \"triggering_commit\" : os . getenv ( \"TRAVIS_PULL_REQUEST_SHA\" ) or os . environ [ \"TRAVIS_COMMIT\" ], \"build_url\" : os . environ [ \"TRAVIS_BUILD_WEB_URL\" ], \"job_url\" : os . environ [ \"TRAVIS_JOB_WEB_URL\" ], } if os . getenv ( \"APPVEYOR\" , \"false\" ) . lower () == \"true\" : # https://www.appveyor.com/docs/environment-variables/ repo_slug = os . environ [ \"APPVEYOR_REPO_NAME\" ] repo_owner , repo_name = repo_slug . split ( \"/\" ) provider_url = \"{APPVEYOR_URL}/project/{APPVEYOR_ACCOUNT_NAME}/{APPVEYOR_PROJECT_SLUG}\" . format ( ** os . environ ) build_url = f \"{provider_url}/builds/{os.environ['APPVEYOR_BUILD_ID']}\" return { \"provider\" : \"appveyor\" , \"provider_account\" : os . environ [ \"APPVEYOR_ACCOUNT_NAME\" ], \"repo_slug\" : repo_slug , \"repo_owner\" : repo_owner , \"repo_name\" : repo_name , \"commit\" : os . environ [ \"APPVEYOR_REPO_COMMIT\" ], \"triggering_commit\" : os . getenv ( \"APPVEYOR_PULL_REQUEST_HEAD_COMMIT\" ) or os . environ [ \"APPVEYOR_REPO_COMMIT\" ], \"build_url\" : build_url , \"job_url\" : f \"{provider_url}/build/job/{os.environ['APPVEYOR_JOB_ID']}\" , \"artifact_url\" : f \"{build_url}/artifacts\" , } if os . getenv ( \"CI\" , \"false\" ) . lower () == \"true\" : logging . warning ( \"Detected CI environment variable, but get_continuous_integration_parameters \" \"did not detect environment variables for a supported CI provider. \" \"Supported providers are: {}\" . format ( \", \" . join ( supported_providers )) ) return None Variables supported_providers Functions get_continuous_integration_parameters def get_continuous_integration_parameters ( ) Return a dictionary with information on a continuous integration build inferred from environment variables. The following dictionary keys are set (all values are also strings): - provider: name of CI provider, such as \"travis\" or \"appveyor\". - repo_slug: owner/name of the source code repository, i.e. \"manubot/rootstock\" - repo_owner: owner from repo_slug, i.e. \"manubot\" - repo_name: name from repo_slug, i.e. \"rootstock\" - commit: git commit being evaluated by the CI build For pull requests, CI services often build a merge commit with the default branch rather than the commit that was added to the pull request. In these cases, commit refers to this merge commit. - triggering_commit: git commit that triggered the CI build. - build_url: URL for the webpage with build details - job_url: URL for the webpage with job details GitHub Actions does not set an environment variable with triggering_commit for pull requests. Therefore, set the following environment variable in your workflow: env : GITHUB_PULL_REQUEST_SHA : ${{ github.event.pull_request.head.sha }} View Source def get_continuous_integration_parameters(): \"\"\" Return a dictionary with information on a continuous integration build inferred from environment variables. The following dictionary keys are set (all values are also strings): - provider: name of CI provider, such as \"travis\" or \"appveyor\". - repo_slug: owner/name of the source code repository, i.e. \"manubot/rootstock\" - repo_owner: owner from repo_slug, i.e. \"manubot\" - repo_name: name from repo_slug, i.e. \"rootstock\" - commit: git commit being evaluated by the CI build For pull requests, CI services often build a merge commit with the default branch rather than the commit that was added to the pull request. In these cases, commit refers to this merge commit. - triggering_commit: git commit that triggered the CI build. - build_url: URL for the webpage with build details - job_url: URL for the webpage with job details GitHub Actions does not set an environment variable with triggering_commit for pull requests. Therefore, set the following environment variable in your workflow: ```yaml env: GITHUB_PULL_REQUEST_SHA: ${ { github . event . pull_request . head . sha } } ``` \"\"\" if os.getenv(\"GITHUB_ACTIONS\", \"false\") == \"true\": # https://git.io/JvUf7 repo_slug = os.environ[\"GITHUB_REPOSITORY\"] repo_owner, repo_name = repo_slug.split(\"/\") action_id = os.environ[\"GITHUB_ACTION\"] # GITHUB_SHA for pull_request event: Last merge commit on the GITHUB_REF branch # GITHUB_SHA for push event: Commit pushed, unless deleting a branch (when it's the default branch) # https://git.io/JvUfd github_sha = os.environ[\"GITHUB_SHA\"] ci_params = { \"provider\": \"github\", \"repo_slug\": repo_slug, \"repo_owner\": repo_owner, \"repo_name\": repo_name, \"commit\": github_sha, \"triggering_commit\": os.getenv(\"GITHUB_PULL_REQUEST_SHA\") or github_sha, \"build_url\": f\"https://github.com/{repo_slug}/commit/{github_sha}/checks\", \"job_url\": f\"https://github.com/{repo_slug}/runs/{action_id}\", } return ci_params if os.getenv(\"TRAVIS\", \"false\") == \"true\": # https://docs.travis-ci.com/user/environment-variables/ repo_slug = os.environ[\"TRAVIS_REPO_SLUG\"] repo_owner, repo_name = repo_slug.split(\"/\") return { \"provider\": \"travis\", \"repo_slug\": repo_slug, \"repo_owner\": repo_owner, \"repo_name\": repo_name, \"commit\": os.environ[\"TRAVIS_COMMIT\"], \"triggering_commit\": os.getenv(\"TRAVIS_PULL_REQUEST_SHA\") or os.environ[\"TRAVIS_COMMIT\"], \"build_url\": os.environ[\"TRAVIS_BUILD_WEB_URL\"], \"job_url\": os.environ[\"TRAVIS_JOB_WEB_URL\"], } if os.getenv(\"APPVEYOR\", \"false\").lower() == \"true\": # https://www.appveyor.com/docs/environment-variables/ repo_slug = os.environ[\"APPVEYOR_REPO_NAME\"] repo_owner, repo_name = repo_slug.split(\"/\") provider_url = \"{APPVEYOR_URL}/project/{APPVEYOR_ACCOUNT_NAME}/{APPVEYOR_PROJECT_SLUG}\".format( **os.environ ) build_url = f\"{provider_url}/builds/{os.environ['APPVEYOR_BUILD_ID']}\" return { \"provider\": \"appveyor\", \"provider_account\": os.environ[\"APPVEYOR_ACCOUNT_NAME\"], \"repo_slug\": repo_slug, \"repo_owner\": repo_owner, \"repo_name\": repo_name, \"commit\": os.environ[\"APPVEYOR_REPO_COMMIT\"], \"triggering_commit\": os.getenv(\"APPVEYOR_PULL_REQUEST_HEAD_COMMIT\") or os.environ[\"APPVEYOR_REPO_COMMIT\"], \"build_url\": build_url, \"job_url\": f\"{provider_url}/build/job/{os.environ['APPVEYOR_JOB_ID']}\", \"artifact_url\": f\"{build_url}/artifacts\", } if os.getenv(\"CI\", \"false\").lower() == \"true\": logging.warning( \"Detected CI environment variable, but get_continuous_integration_parameters \" \"did not detect environment variables for a supported CI provider. \" \"Supported providers are: {}\".format(\", \".join(supported_providers)) ) return None","title":"Ci"},{"location":"reference/manubot/process/ci/#module-manubotprocessci","text":"View Source import os import logging supported_providers = [ \"github\" , \"travis\" , \"appveyor\" ] def get_continuous_integration_parameters (): \"\"\" Return a dictionary with information on a continuous integration build inferred from environment variables. The following dictionary keys are set (all values are also strings): - provider: name of CI provider, such as \"travis\" or \"appveyor\". - repo_slug: owner/name of the source code repository, i.e. \"manubot/rootstock\" - repo_owner: owner from repo_slug, i.e. \"manubot\" - repo_name: name from repo_slug, i.e. \"rootstock\" - commit: git commit being evaluated by the CI build For pull requests, CI services often build a merge commit with the default branch rather than the commit that was added to the pull request. In these cases, commit refers to this merge commit. - triggering_commit: git commit that triggered the CI build. - build_url: URL for the webpage with build details - job_url: URL for the webpage with job details GitHub Actions does not set an environment variable with triggering_commit for pull requests. Therefore, set the following environment variable in your workflow: ```yaml env: GITHUB_PULL_REQUEST_SHA: ${{ github.event.pull_request.head.sha }} ``` \"\"\" if os . getenv ( \"GITHUB_ACTIONS\" , \"false\" ) == \"true\" : # https://git.io/JvUf7 repo_slug = os . environ [ \"GITHUB_REPOSITORY\" ] repo_owner , repo_name = repo_slug . split ( \"/\" ) action_id = os . environ [ \"GITHUB_ACTION\" ] # GITHUB_SHA for pull_request event: Last merge commit on the GITHUB_REF branch # GITHUB_SHA for push event: Commit pushed, unless deleting a branch (when it's the default branch) # https://git.io/JvUfd github_sha = os . environ [ \"GITHUB_SHA\" ] ci_params = { \"provider\" : \"github\" , \"repo_slug\" : repo_slug , \"repo_owner\" : repo_owner , \"repo_name\" : repo_name , \"commit\" : github_sha , \"triggering_commit\" : os . getenv ( \"GITHUB_PULL_REQUEST_SHA\" ) or github_sha , \"build_url\" : f \"https://github.com/{repo_slug}/commit/{github_sha}/checks\" , \"job_url\" : f \"https://github.com/{repo_slug}/runs/{action_id}\" , } return ci_params if os . getenv ( \"TRAVIS\" , \"false\" ) == \"true\" : # https://docs.travis-ci.com/user/environment-variables/ repo_slug = os . environ [ \"TRAVIS_REPO_SLUG\" ] repo_owner , repo_name = repo_slug . split ( \"/\" ) return { \"provider\" : \"travis\" , \"repo_slug\" : repo_slug , \"repo_owner\" : repo_owner , \"repo_name\" : repo_name , \"commit\" : os . environ [ \"TRAVIS_COMMIT\" ], \"triggering_commit\" : os . getenv ( \"TRAVIS_PULL_REQUEST_SHA\" ) or os . environ [ \"TRAVIS_COMMIT\" ], \"build_url\" : os . environ [ \"TRAVIS_BUILD_WEB_URL\" ], \"job_url\" : os . environ [ \"TRAVIS_JOB_WEB_URL\" ], } if os . getenv ( \"APPVEYOR\" , \"false\" ) . lower () == \"true\" : # https://www.appveyor.com/docs/environment-variables/ repo_slug = os . environ [ \"APPVEYOR_REPO_NAME\" ] repo_owner , repo_name = repo_slug . split ( \"/\" ) provider_url = \"{APPVEYOR_URL}/project/{APPVEYOR_ACCOUNT_NAME}/{APPVEYOR_PROJECT_SLUG}\" . format ( ** os . environ ) build_url = f \"{provider_url}/builds/{os.environ['APPVEYOR_BUILD_ID']}\" return { \"provider\" : \"appveyor\" , \"provider_account\" : os . environ [ \"APPVEYOR_ACCOUNT_NAME\" ], \"repo_slug\" : repo_slug , \"repo_owner\" : repo_owner , \"repo_name\" : repo_name , \"commit\" : os . environ [ \"APPVEYOR_REPO_COMMIT\" ], \"triggering_commit\" : os . getenv ( \"APPVEYOR_PULL_REQUEST_HEAD_COMMIT\" ) or os . environ [ \"APPVEYOR_REPO_COMMIT\" ], \"build_url\" : build_url , \"job_url\" : f \"{provider_url}/build/job/{os.environ['APPVEYOR_JOB_ID']}\" , \"artifact_url\" : f \"{build_url}/artifacts\" , } if os . getenv ( \"CI\" , \"false\" ) . lower () == \"true\" : logging . warning ( \"Detected CI environment variable, but get_continuous_integration_parameters \" \"did not detect environment variables for a supported CI provider. \" \"Supported providers are: {}\" . format ( \", \" . join ( supported_providers )) ) return None","title":"Module manubot.process.ci"},{"location":"reference/manubot/process/ci/#variables","text":"supported_providers","title":"Variables"},{"location":"reference/manubot/process/ci/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/ci/#get_continuous_integration_parameters","text":"def get_continuous_integration_parameters ( ) Return a dictionary with information on a continuous integration build inferred from environment variables. The following dictionary keys are set (all values are also strings): - provider: name of CI provider, such as \"travis\" or \"appveyor\". - repo_slug: owner/name of the source code repository, i.e. \"manubot/rootstock\" - repo_owner: owner from repo_slug, i.e. \"manubot\" - repo_name: name from repo_slug, i.e. \"rootstock\" - commit: git commit being evaluated by the CI build For pull requests, CI services often build a merge commit with the default branch rather than the commit that was added to the pull request. In these cases, commit refers to this merge commit. - triggering_commit: git commit that triggered the CI build. - build_url: URL for the webpage with build details - job_url: URL for the webpage with job details GitHub Actions does not set an environment variable with triggering_commit for pull requests. Therefore, set the following environment variable in your workflow: env : GITHUB_PULL_REQUEST_SHA : ${{ github.event.pull_request.head.sha }} View Source def get_continuous_integration_parameters(): \"\"\" Return a dictionary with information on a continuous integration build inferred from environment variables. The following dictionary keys are set (all values are also strings): - provider: name of CI provider, such as \"travis\" or \"appveyor\". - repo_slug: owner/name of the source code repository, i.e. \"manubot/rootstock\" - repo_owner: owner from repo_slug, i.e. \"manubot\" - repo_name: name from repo_slug, i.e. \"rootstock\" - commit: git commit being evaluated by the CI build For pull requests, CI services often build a merge commit with the default branch rather than the commit that was added to the pull request. In these cases, commit refers to this merge commit. - triggering_commit: git commit that triggered the CI build. - build_url: URL for the webpage with build details - job_url: URL for the webpage with job details GitHub Actions does not set an environment variable with triggering_commit for pull requests. Therefore, set the following environment variable in your workflow: ```yaml env: GITHUB_PULL_REQUEST_SHA: ${ { github . event . pull_request . head . sha } } ``` \"\"\" if os.getenv(\"GITHUB_ACTIONS\", \"false\") == \"true\": # https://git.io/JvUf7 repo_slug = os.environ[\"GITHUB_REPOSITORY\"] repo_owner, repo_name = repo_slug.split(\"/\") action_id = os.environ[\"GITHUB_ACTION\"] # GITHUB_SHA for pull_request event: Last merge commit on the GITHUB_REF branch # GITHUB_SHA for push event: Commit pushed, unless deleting a branch (when it's the default branch) # https://git.io/JvUfd github_sha = os.environ[\"GITHUB_SHA\"] ci_params = { \"provider\": \"github\", \"repo_slug\": repo_slug, \"repo_owner\": repo_owner, \"repo_name\": repo_name, \"commit\": github_sha, \"triggering_commit\": os.getenv(\"GITHUB_PULL_REQUEST_SHA\") or github_sha, \"build_url\": f\"https://github.com/{repo_slug}/commit/{github_sha}/checks\", \"job_url\": f\"https://github.com/{repo_slug}/runs/{action_id}\", } return ci_params if os.getenv(\"TRAVIS\", \"false\") == \"true\": # https://docs.travis-ci.com/user/environment-variables/ repo_slug = os.environ[\"TRAVIS_REPO_SLUG\"] repo_owner, repo_name = repo_slug.split(\"/\") return { \"provider\": \"travis\", \"repo_slug\": repo_slug, \"repo_owner\": repo_owner, \"repo_name\": repo_name, \"commit\": os.environ[\"TRAVIS_COMMIT\"], \"triggering_commit\": os.getenv(\"TRAVIS_PULL_REQUEST_SHA\") or os.environ[\"TRAVIS_COMMIT\"], \"build_url\": os.environ[\"TRAVIS_BUILD_WEB_URL\"], \"job_url\": os.environ[\"TRAVIS_JOB_WEB_URL\"], } if os.getenv(\"APPVEYOR\", \"false\").lower() == \"true\": # https://www.appveyor.com/docs/environment-variables/ repo_slug = os.environ[\"APPVEYOR_REPO_NAME\"] repo_owner, repo_name = repo_slug.split(\"/\") provider_url = \"{APPVEYOR_URL}/project/{APPVEYOR_ACCOUNT_NAME}/{APPVEYOR_PROJECT_SLUG}\".format( **os.environ ) build_url = f\"{provider_url}/builds/{os.environ['APPVEYOR_BUILD_ID']}\" return { \"provider\": \"appveyor\", \"provider_account\": os.environ[\"APPVEYOR_ACCOUNT_NAME\"], \"repo_slug\": repo_slug, \"repo_owner\": repo_owner, \"repo_name\": repo_name, \"commit\": os.environ[\"APPVEYOR_REPO_COMMIT\"], \"triggering_commit\": os.getenv(\"APPVEYOR_PULL_REQUEST_HEAD_COMMIT\") or os.environ[\"APPVEYOR_REPO_COMMIT\"], \"build_url\": build_url, \"job_url\": f\"{provider_url}/build/job/{os.environ['APPVEYOR_JOB_ID']}\", \"artifact_url\": f\"{build_url}/artifacts\", } if os.getenv(\"CI\", \"false\").lower() == \"true\": logging.warning( \"Detected CI environment variable, but get_continuous_integration_parameters \" \"did not detect environment variables for a supported CI provider. \" \"Supported providers are: {}\".format(\", \".join(supported_providers)) ) return None","title":"get_continuous_integration_parameters"},{"location":"reference/manubot/process/manuscript/","text":"Module manubot.process.manuscript View Source import datetime import json import logging import pathlib def get_text ( directory ): \"\"\" Return a concatenated string of section texts from the specified directory. Text files should be UTF-8 encoded. \"\"\" section_dir = pathlib . Path ( directory ) paths = sorted ( section_dir . glob ( \"[0-9]*.md\" )) name_to_text = dict () for path in paths : name_to_text [ path . stem ] = path . read_text ( encoding = \"utf-8-sig\" ) logging . info ( \"Manuscript content parts: \\n \" + \" \\n \" . join ( name_to_text )) return \" \\n\\n \" . join ( name_to_text . values ()) + \" \\n \" def get_manuscript_stats ( text ): \"\"\" Compute manuscript statistics. \"\"\" stats = dict () stats [ \"word_count\" ] = len ( text . split ()) logging . info ( f \"Generated manscript stats: \\n {json.dumps(stats, indent=2)}\" ) return stats def datetime_now (): \"\"\" Return the current datetime, with timezone awareness https://stackoverflow.com/a/39079819/4651668 \"\"\" tzinfo = datetime . datetime . now ( datetime . timezone . utc ) . astimezone () . tzinfo return datetime . datetime . now ( tzinfo ) Functions datetime_now def datetime_now ( ) Return the current datetime, with timezone awareness https://stackoverflow.com/a/39079819/4651668 View Source def datetime_now (): \"\"\" Return the current datetime, with timezone awareness https://stackoverflow.com/a/39079819/4651668 \"\"\" tzinfo = datetime . datetime . now ( datetime . timezone . utc ). astimezone (). tzinfo return datetime . datetime . now ( tzinfo ) get_manuscript_stats def get_manuscript_stats ( text ) Compute manuscript statistics. View Source def get_manuscript_stats ( text ): \"\"\" Compute manuscript statistics. \"\"\" stats = dict () stats [ \"word_count\" ] = len ( text . split ()) logging . info ( f \"Generated manscript stats:\\n{json.dumps(stats, indent=2)}\" ) return stats get_text def get_text ( directory ) Return a concatenated string of section texts from the specified directory. Text files should be UTF-8 encoded. View Source def get_text ( directory ): \"\"\" Return a concatenated string of section texts from the specified directory. Text files should be UTF-8 encoded. \"\"\" section_dir = pathlib . Path ( directory ) paths = sorted ( section_dir . glob ( \"[0-9]*.md\" )) name_to_text = dict () for path in paths : name_to_text [ path . stem ] = path . read_text ( encoding = \"utf-8-sig\" ) logging . info ( \"Manuscript content parts:\\n\" + \"\\n\" . join ( name_to_text )) return \"\\n\\n\" . join ( name_to_text . values ()) + \"\\n\"","title":"Manuscript"},{"location":"reference/manubot/process/manuscript/#module-manubotprocessmanuscript","text":"View Source import datetime import json import logging import pathlib def get_text ( directory ): \"\"\" Return a concatenated string of section texts from the specified directory. Text files should be UTF-8 encoded. \"\"\" section_dir = pathlib . Path ( directory ) paths = sorted ( section_dir . glob ( \"[0-9]*.md\" )) name_to_text = dict () for path in paths : name_to_text [ path . stem ] = path . read_text ( encoding = \"utf-8-sig\" ) logging . info ( \"Manuscript content parts: \\n \" + \" \\n \" . join ( name_to_text )) return \" \\n\\n \" . join ( name_to_text . values ()) + \" \\n \" def get_manuscript_stats ( text ): \"\"\" Compute manuscript statistics. \"\"\" stats = dict () stats [ \"word_count\" ] = len ( text . split ()) logging . info ( f \"Generated manscript stats: \\n {json.dumps(stats, indent=2)}\" ) return stats def datetime_now (): \"\"\" Return the current datetime, with timezone awareness https://stackoverflow.com/a/39079819/4651668 \"\"\" tzinfo = datetime . datetime . now ( datetime . timezone . utc ) . astimezone () . tzinfo return datetime . datetime . now ( tzinfo )","title":"Module manubot.process.manuscript"},{"location":"reference/manubot/process/manuscript/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/manuscript/#datetime_now","text":"def datetime_now ( ) Return the current datetime, with timezone awareness https://stackoverflow.com/a/39079819/4651668 View Source def datetime_now (): \"\"\" Return the current datetime, with timezone awareness https://stackoverflow.com/a/39079819/4651668 \"\"\" tzinfo = datetime . datetime . now ( datetime . timezone . utc ). astimezone (). tzinfo return datetime . datetime . now ( tzinfo )","title":"datetime_now"},{"location":"reference/manubot/process/manuscript/#get_manuscript_stats","text":"def get_manuscript_stats ( text ) Compute manuscript statistics. View Source def get_manuscript_stats ( text ): \"\"\" Compute manuscript statistics. \"\"\" stats = dict () stats [ \"word_count\" ] = len ( text . split ()) logging . info ( f \"Generated manscript stats:\\n{json.dumps(stats, indent=2)}\" ) return stats","title":"get_manuscript_stats"},{"location":"reference/manubot/process/manuscript/#get_text","text":"def get_text ( directory ) Return a concatenated string of section texts from the specified directory. Text files should be UTF-8 encoded. View Source def get_text ( directory ): \"\"\" Return a concatenated string of section texts from the specified directory. Text files should be UTF-8 encoded. \"\"\" section_dir = pathlib . Path ( directory ) paths = sorted ( section_dir . glob ( \"[0-9]*.md\" )) name_to_text = dict () for path in paths : name_to_text [ path . stem ] = path . read_text ( encoding = \"utf-8-sig\" ) logging . info ( \"Manuscript content parts:\\n\" + \"\\n\" . join ( name_to_text )) return \"\\n\\n\" . join ( name_to_text . values ()) + \"\\n\"","title":"get_text"},{"location":"reference/manubot/process/metadata/","text":"Module manubot.process.metadata Tools for manuscript metadata processing including thumbnail detection and processing. View Source \"\"\" Tools for manuscript metadata processing including thumbnail detection and processing. \"\"\" import functools import logging import pathlib import subprocess from typing import Optional from urllib.parse import urljoin def get_header_includes ( variables : dict ) -> str : \"\"\" Render `header-includes-template.html` using information from `variables`. \"\"\" from .util import template_with_jinja2 path = pathlib . Path ( __file__ ) . parent . joinpath ( \"header-includes-template.html\" ) try : template = path . read_text ( encoding = \"utf-8-sig\" ) return template_with_jinja2 ( template , variables ) except Exception : logging . exception ( f \"Error generating header-includes.\" ) return \"\" def get_thumbnail_url ( thumbnail = None ): \"\"\" Starting with a user-specified `thumbnail` as either a path, URL, or None, return an absolute URL pointing to the thumbnail image. If the provided `thumbnail` is a URL, return this URL unmodified. If `thumbnail` is None, search for `thumbnail.png` within the git repository from which this function is executed. If `thumbnail` is a local path, the path should be relative to root directory of the git repository it is located in. If a local path is provided or detected, it is converted to a GitHub raw URL. \"\"\" from manubot.util import is_http_url if not thumbnail : message = \"get_thumbnail_url: thumbnail location not explicitly provided. \" thumbnail = _find_thumbnail_path () message += ( f \"Thumbnail detected at {thumbnail!r}\" if thumbnail else \"No local thumbnail detected\" ) logging . debug ( message ) elif is_http_url ( thumbnail ): logging . debug ( \"provided thumbnail is a URL. Pass it through.\" ) return thumbnail return _thumbnail_path_to_url ( thumbnail ) def _find_thumbnail_path (): \"\"\" If this this function is executed with a working directory that is inside a git repository, return the path to a `thumbnail.png` file located anywhere in that repository. Otherwise, return `None`. \"\"\" directory = git_repository_root () if not directory : return None paths = directory . glob ( \"**/thumbnail.png\" ) paths = [ path . relative_to ( directory ) for path in paths ] paths = sorted ( paths , key = lambda x : ( len ( x . parents ), x )) if not paths : return None return paths [ 0 ] . as_posix () def _thumbnail_path_to_url ( path ): \"\"\" Convert a local thumbnail path (string) to an absolute URL using the GitHub repository location detected using `get_continuous_integration_parameters`. \"\"\" if not path : return None from .ci import get_continuous_integration_parameters info = get_continuous_integration_parameters () try : url = f \"https://github.com/{info['repo_slug']}/raw/{info['triggering_commit']}/{path}\" except ( TypeError , KeyError ): return None return url @functools.lru_cache () def git_repository_root (): \"\"\" Return the path to repository root directory or `None` if indeterminate. \"\"\" for cmd in ( [ \"git\" , \"rev-parse\" , \"--show-superproject-working-tree\" ], [ \"git\" , \"rev-parse\" , \"--show-toplevel\" ], ): try : path = subprocess . check_output ( cmd , universal_newlines = True ) . rstrip ( \" \\r\\n \" ) if path : return pathlib . Path ( path ) except ( subprocess . CalledProcessError , OSError ): pass return None def get_manuscript_urls ( html_url : Optional [ str ] = None ) -> dict : \"\"\" Return a dictionary with URLs for a manuscript. An example for a manuscript where all URLs get set, inferred from continuous integration environment variables, is: ```python { \"html_url\": \"https://manubot.github.io/rootstock/\", \"pdf_url\": \"https://manubot.github.io/rootstock/manuscript.pdf\", \"html_url_versioned\": \"https://manubot.github.io/rootstock/v/7cf9071212ce33116ad09cf2237a370b180a3c35/\", \"pdf_url_versioned\": \"https://manubot.github.io/rootstock/v/7cf9071212ce33116ad09cf2237a370b180a3c35/manuscript.pdf\", } ``` Provide `html_url` to set a custom domain. If `html_url=\"https://git.dhimmel.com/bitcoin-whitepaper/\"`, the return dictionary will be like: ```python { \"html_url\": \"https://git.dhimmel.com/bitcoin-whitepaper/\", \"pdf_url\": \"https://git.dhimmel.com/bitcoin-whitepaper/manuscript.pdf\", \"html_url_versioned\": \"https://git.dhimmel.com/bitcoin-whitepaper/v/cb1f2c12eec8b56db9ef5f641ec805e2d449d319/\", \"pdf_url_versioned\": \"https://git.dhimmel.com/bitcoin-whitepaper/v/cb1f2c12eec8b56db9ef5f641ec805e2d449d319/manuscript.pdf\", } ``` Note the trailing `/` in `html_url`, which is required for proper functioning. \"\"\" import requests from .ci import get_continuous_integration_parameters urls = dict () ci_params = get_continuous_integration_parameters () if html_url is None : if not ci_params : return urls html_url = \"https://{repo_owner}.github.io/{repo_name}/\" . format ( ** ci_params ) urls [ \"html_url\" ] = html_url urls [ \"pdf_url\" ] = urljoin ( html_url , \"manuscript.pdf\" ) if not ci_params : return urls urls [ \"html_url_versioned\" ] = urljoin ( html_url , \"v/{commit}/\" . format ( ** ci_params )) urls [ \"pdf_url_versioned\" ] = urljoin ( urls [ \"html_url_versioned\" ], \"manuscript.pdf\" ) response = requests . head ( html_url , allow_redirects = True ) if not response . ok : logging . warning ( \"html_url is not web accessible. \" f \"{html_url} returned status code {response.status_code}. \" \"Ignore this warning if the manuscript has not yet been deployed for the first time. \" ) if response . history : logging . info ( \"html_url includes redirects. In order of oldest to most recent: \\n \" + \" \\n \" . join ( x . url for x in response . history + [ response ]) ) return urls def get_software_versions () -> dict : \"\"\" Return a dictionary of software versions for softwares components: - manubot_version: the semantic version number of the manubot python package. - rootstock_commit: the version of the rootstock repository, as a commit hash, included in the manuscript repository. Values whose detection fails are set to None. \"\"\" from manubot import __version__ as manubot_version return { \"manubot_version\" : manubot_version , \"rootstock_commit\" : get_rootstock_commit (), } def get_rootstock_commit () -> Optional [ str ]: \"\"\" Return the most recent commit in common between the git repository this function is run within (usually a Manubot manuscript repository) and the `master` branch of the `rootstock` remote. WARNING: This function may modify the git repository its executed within: - if the repository has not set the `roostock` remote, it is set to point to the default Rootstock repository of <https://github.com/manubot/rootstock>. - fetches the latest commits in the `master` branch of the `rootstock` remote \"\"\" from manubot.util import shlex_join # add rootstock remote if remote is not already set rootstock_remote = \"https://github.com/manubot/rootstock.git\" args = [ \"git\" , \"remote\" , \"add\" , \"rootstock\" , rootstock_remote ] process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) if process . returncode == 0 : logging . info ( \"get_rootstock_commit added a `rootstock` remote to the git repository.\" ) # find most recent common ancestor commit try : args = [ \"git\" , \"fetch\" , \"rootstock\" , \"master\" ] subprocess . check_output ( args , stderr = subprocess . PIPE , universal_newlines = True ) args = [ \"git\" , \"merge-base\" , \"HEAD\" , \"rootstock/master\" ] output = subprocess . check_output ( args , stderr = subprocess . PIPE , universal_newlines = True ) except subprocess . CalledProcessError as error : logging . warning ( f \"get_rootstock_commit: {shlex_join(error.cmd)!r} returned exit code {error.returncode} \" f \"with the following stdout: \\n {error.stdout} \\n \" f \"And the following stderr: \\n {error.stderr}\" ) return None rootstock_commit = output . strip () return rootstock_commit Functions get_header_includes def get_header_includes ( variables : dict ) -> str Render header-includes-template.html using information from variables . View Source def get_header_includes ( variables : dict ) -> str : \"\"\" Render `header-includes-template.html` using information from `variables`. \"\"\" from .util import template_with_jinja2 path = pathlib . Path ( __file__ ) . parent . joinpath ( \"header-includes-template.html\" ) try : template = path . read_text ( encoding = \"utf-8-sig\" ) return template_with_jinja2 ( template , variables ) except Exception : logging . exception ( f \"Error generating header-includes.\" ) return \"\" get_manuscript_urls def get_manuscript_urls ( html_url : Union [ str , NoneType ] = None ) -> dict Return a dictionary with URLs for a manuscript. An example for a manuscript where all URLs get set, inferred from continuous integration environment variables, is: { \"html_url\" : \"https://manubot.github.io/rootstock/\" , \"pdf_url\" : \"https://manubot.github.io/rootstock/manuscript.pdf\" , \"html_url_versioned\" : \"https://manubot.github.io/rootstock/v/7cf9071212ce33116ad09cf2237a370b180a3c35/\" , \"pdf_url_versioned\" : \"https://manubot.github.io/rootstock/v/7cf9071212ce33116ad09cf2237a370b180a3c35/manuscript.pdf\" , } Provide html_url to set a custom domain. If html_url=\"https://git.dhimmel.com/bitcoin-whitepaper/\" , the return dictionary will be like: { \"html_url\" : \"https://git.dhimmel.com/bitcoin-whitepaper/\" , \"pdf_url\" : \"https://git.dhimmel.com/bitcoin-whitepaper/manuscript.pdf\" , \"html_url_versioned\" : \"https://git.dhimmel.com/bitcoin-whitepaper/v/cb1f2c12eec8b56db9ef5f641ec805e2d449d319/\" , \"pdf_url_versioned\" : \"https://git.dhimmel.com/bitcoin-whitepaper/v/cb1f2c12eec8b56db9ef5f641ec805e2d449d319/manuscript.pdf\" , } Note the trailing / in html_url , which is required for proper functioning. View Source def get_manuscript_urls ( html_url : Optional [ str ] = None ) -> dict : \"\"\" Return a dictionary with URLs for a manuscript. An example for a manuscript where all URLs get set, inferred from continuous integration environment variables, is: ```python { \" html_url \": \" https : // manubot . github . io / rootstock / \", \" pdf_url \": \" https : // manubot . github . io / rootstock / manuscript . pdf \", \" html_url_versioned \": \" https : // manubot . github . io / rootstock / v / 7 cf9071212ce33116ad09cf2237a370b180a3c35 / \", \" pdf_url_versioned \": \" https : // manubot . github . io / rootstock / v / 7 cf9071212ce33116ad09cf2237a370b180a3c35 / manuscript . pdf \", } ``` Provide `html_url` to set a custom domain. If `html_url=\" https : // git . dhimmel . com / bitcoin - whitepaper / \"`, the return dictionary will be like: ```python { \" html_url \": \" https : // git . dhimmel . com / bitcoin - whitepaper / \", \" pdf_url \": \" https : // git . dhimmel . com / bitcoin - whitepaper / manuscript . pdf \", \" html_url_versioned \": \" https : // git . dhimmel . com / bitcoin - whitepaper / v / cb1f2c12eec8b56db9ef5f641ec805e2d449d319 / \", \" pdf_url_versioned \": \" https : // git . dhimmel . com / bitcoin - whitepaper / v / cb1f2c12eec8b56db9ef5f641ec805e2d449d319 / manuscript . pdf \", } ``` Note the trailing `/` in `html_url`, which is required for proper functioning. \"\"\" import requests from . ci import get_continuous_integration_parameters urls = dict () ci_params = get_continuous_integration_parameters () if html_url is None : if not ci_params : return urls html_url = \"https://{repo_owner}.github.io/{repo_name}/\" . format ( ** ci_params ) urls [ \"html_url\" ] = html_url urls [ \"pdf_url\" ] = urljoin ( html_url , \"manuscript.pdf\" ) if not ci_params : return urls urls [ \"html_url_versioned\" ] = urljoin ( html_url , \"v/{commit}/\" . format ( ** ci_params )) urls [ \"pdf_url_versioned\" ] = urljoin ( urls [ \"html_url_versioned\" ], \"manuscript.pdf\" ) response = requests . head ( html_url , allow_redirects = True ) if not response . ok : logging . warning ( \"html_url is not web accessible. \" f \"{html_url} returned status code {response.status_code}. \" \"Ignore this warning if the manuscript has not yet been deployed for the first time. \" ) if response . history : logging . info ( \"html_url includes redirects. In order of oldest to most recent:\\n\" + \"\\n\" . join ( x . url for x in response . history + [ response ]) ) return urls get_rootstock_commit def get_rootstock_commit ( ) -> Union [ str , NoneType ] Return the most recent commit in common between the git repository this function is run within (usually a Manubot manuscript repository) and the master branch of the rootstock remote. WARNING: This function may modify the git repository its executed within: if the repository has not set the roostock remote, it is set to point to the default Rootstock repository of https://github.com/manubot/rootstock . fetches the latest commits in the master branch of the rootstock remote View Source def get_rootstock_commit () -> Optional [ str ]: \"\"\" Return the most recent commit in common between the git repository this function is run within (usually a Manubot manuscript repository) and the `master` branch of the `rootstock` remote. WARNING: This function may modify the git repository its executed within: - if the repository has not set the `roostock` remote, it is set to point to the default Rootstock repository of <https://github.com/manubot/rootstock>. - fetches the latest commits in the `master` branch of the `rootstock` remote \"\"\" from manubot.util import shlex_join # add rootstock remote if remote is not already set rootstock_remote = \"https://github.com/manubot/rootstock.git\" args = [ \"git\" , \"remote\" , \"add\" , \"rootstock\" , rootstock_remote ] process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) if process . returncode == 0 : logging . info ( \"get_rootstock_commit added a `rootstock` remote to the git repository.\" ) # find most recent common ancestor commit try : args = [ \"git\" , \"fetch\" , \"rootstock\" , \"master\" ] subprocess . check_output ( args , stderr = subprocess . PIPE , universal_newlines = True ) args = [ \"git\" , \"merge-base\" , \"HEAD\" , \"rootstock/master\" ] output = subprocess . check_output ( args , stderr = subprocess . PIPE , universal_newlines = True ) except subprocess . CalledProcessError as error : logging . warning ( f \"get_rootstock_commit: {shlex_join(error.cmd)!r} returned exit code {error.returncode} \" f \"with the following stdout: \\n {error.stdout} \\n \" f \"And the following stderr: \\n {error.stderr}\" ) return None rootstock_commit = output . strip () return rootstock_commit get_software_versions def get_software_versions ( ) -> dict Return a dictionary of software versions for softwares components: manubot_version: the semantic version number of the manubot python package. rootstock_commit: the version of the rootstock repository, as a commit hash, included in the manuscript repository. Values whose detection fails are set to None. View Source def get_software_versions () -> dict : \"\"\" Return a dictionary of software versions for softwares components: - manubot_version: the semantic version number of the manubot python package. - rootstock_commit: the version of the rootstock repository, as a commit hash, included in the manuscript repository. Values whose detection fails are set to None. \"\"\" from manubot import __version__ as manubot_version return { \"manubot_version\" : manubot_version , \"rootstock_commit\" : get_rootstock_commit (), } get_thumbnail_url def get_thumbnail_url ( thumbnail = None ) Starting with a user-specified thumbnail as either a path, URL, or None, return an absolute URL pointing to the thumbnail image. If the provided thumbnail is a URL, return this URL unmodified. If thumbnail is None, search for thumbnail.png within the git repository from which this function is executed. If thumbnail is a local path, the path should be relative to root directory of the git repository it is located in. If a local path is provided or detected, it is converted to a GitHub raw URL. View Source def get_thumbnail_url ( thumbnail = None ): \"\"\" Starting with a user-specified `thumbnail` as either a path, URL, or None, return an absolute URL pointing to the thumbnail image. If the provided `thumbnail` is a URL, return this URL unmodified. If `thumbnail` is None, search for `thumbnail.png` within the git repository from which this function is executed. If `thumbnail` is a local path, the path should be relative to root directory of the git repository it is located in. If a local path is provided or detected, it is converted to a GitHub raw URL. \"\"\" from manubot.util import is_http_url if not thumbnail : message = \"get_thumbnail_url: thumbnail location not explicitly provided. \" thumbnail = _find_thumbnail_path () message += ( f \"Thumbnail detected at {thumbnail!r}\" if thumbnail else \"No local thumbnail detected\" ) logging . debug ( message ) elif is_http_url ( thumbnail ): logging . debug ( \"provided thumbnail is a URL. Pass it through.\" ) return thumbnail return _thumbnail_path_to_url ( thumbnail ) git_repository_root def git_repository_root ( ) Return the path to repository root directory or None if indeterminate. View Source @ functools . lru_cache () def git_repository_root (): \"\"\" Return the path to repository root directory or `None` if indeterminate. \"\"\" for cmd in ( [ \"git\" , \"rev-parse\" , \"--show-superproject-working-tree\" ], [ \"git\" , \"rev-parse\" , \"--show-toplevel\" ], ): try : path = subprocess . check_output ( cmd , universal_newlines = True ). rstrip ( \"\\r\\n\" ) if path : return pathlib . Path ( path ) except ( subprocess . CalledProcessError , OSError ): pass return None","title":"Metadata"},{"location":"reference/manubot/process/metadata/#module-manubotprocessmetadata","text":"Tools for manuscript metadata processing including thumbnail detection and processing. View Source \"\"\" Tools for manuscript metadata processing including thumbnail detection and processing. \"\"\" import functools import logging import pathlib import subprocess from typing import Optional from urllib.parse import urljoin def get_header_includes ( variables : dict ) -> str : \"\"\" Render `header-includes-template.html` using information from `variables`. \"\"\" from .util import template_with_jinja2 path = pathlib . Path ( __file__ ) . parent . joinpath ( \"header-includes-template.html\" ) try : template = path . read_text ( encoding = \"utf-8-sig\" ) return template_with_jinja2 ( template , variables ) except Exception : logging . exception ( f \"Error generating header-includes.\" ) return \"\" def get_thumbnail_url ( thumbnail = None ): \"\"\" Starting with a user-specified `thumbnail` as either a path, URL, or None, return an absolute URL pointing to the thumbnail image. If the provided `thumbnail` is a URL, return this URL unmodified. If `thumbnail` is None, search for `thumbnail.png` within the git repository from which this function is executed. If `thumbnail` is a local path, the path should be relative to root directory of the git repository it is located in. If a local path is provided or detected, it is converted to a GitHub raw URL. \"\"\" from manubot.util import is_http_url if not thumbnail : message = \"get_thumbnail_url: thumbnail location not explicitly provided. \" thumbnail = _find_thumbnail_path () message += ( f \"Thumbnail detected at {thumbnail!r}\" if thumbnail else \"No local thumbnail detected\" ) logging . debug ( message ) elif is_http_url ( thumbnail ): logging . debug ( \"provided thumbnail is a URL. Pass it through.\" ) return thumbnail return _thumbnail_path_to_url ( thumbnail ) def _find_thumbnail_path (): \"\"\" If this this function is executed with a working directory that is inside a git repository, return the path to a `thumbnail.png` file located anywhere in that repository. Otherwise, return `None`. \"\"\" directory = git_repository_root () if not directory : return None paths = directory . glob ( \"**/thumbnail.png\" ) paths = [ path . relative_to ( directory ) for path in paths ] paths = sorted ( paths , key = lambda x : ( len ( x . parents ), x )) if not paths : return None return paths [ 0 ] . as_posix () def _thumbnail_path_to_url ( path ): \"\"\" Convert a local thumbnail path (string) to an absolute URL using the GitHub repository location detected using `get_continuous_integration_parameters`. \"\"\" if not path : return None from .ci import get_continuous_integration_parameters info = get_continuous_integration_parameters () try : url = f \"https://github.com/{info['repo_slug']}/raw/{info['triggering_commit']}/{path}\" except ( TypeError , KeyError ): return None return url @functools.lru_cache () def git_repository_root (): \"\"\" Return the path to repository root directory or `None` if indeterminate. \"\"\" for cmd in ( [ \"git\" , \"rev-parse\" , \"--show-superproject-working-tree\" ], [ \"git\" , \"rev-parse\" , \"--show-toplevel\" ], ): try : path = subprocess . check_output ( cmd , universal_newlines = True ) . rstrip ( \" \\r\\n \" ) if path : return pathlib . Path ( path ) except ( subprocess . CalledProcessError , OSError ): pass return None def get_manuscript_urls ( html_url : Optional [ str ] = None ) -> dict : \"\"\" Return a dictionary with URLs for a manuscript. An example for a manuscript where all URLs get set, inferred from continuous integration environment variables, is: ```python { \"html_url\": \"https://manubot.github.io/rootstock/\", \"pdf_url\": \"https://manubot.github.io/rootstock/manuscript.pdf\", \"html_url_versioned\": \"https://manubot.github.io/rootstock/v/7cf9071212ce33116ad09cf2237a370b180a3c35/\", \"pdf_url_versioned\": \"https://manubot.github.io/rootstock/v/7cf9071212ce33116ad09cf2237a370b180a3c35/manuscript.pdf\", } ``` Provide `html_url` to set a custom domain. If `html_url=\"https://git.dhimmel.com/bitcoin-whitepaper/\"`, the return dictionary will be like: ```python { \"html_url\": \"https://git.dhimmel.com/bitcoin-whitepaper/\", \"pdf_url\": \"https://git.dhimmel.com/bitcoin-whitepaper/manuscript.pdf\", \"html_url_versioned\": \"https://git.dhimmel.com/bitcoin-whitepaper/v/cb1f2c12eec8b56db9ef5f641ec805e2d449d319/\", \"pdf_url_versioned\": \"https://git.dhimmel.com/bitcoin-whitepaper/v/cb1f2c12eec8b56db9ef5f641ec805e2d449d319/manuscript.pdf\", } ``` Note the trailing `/` in `html_url`, which is required for proper functioning. \"\"\" import requests from .ci import get_continuous_integration_parameters urls = dict () ci_params = get_continuous_integration_parameters () if html_url is None : if not ci_params : return urls html_url = \"https://{repo_owner}.github.io/{repo_name}/\" . format ( ** ci_params ) urls [ \"html_url\" ] = html_url urls [ \"pdf_url\" ] = urljoin ( html_url , \"manuscript.pdf\" ) if not ci_params : return urls urls [ \"html_url_versioned\" ] = urljoin ( html_url , \"v/{commit}/\" . format ( ** ci_params )) urls [ \"pdf_url_versioned\" ] = urljoin ( urls [ \"html_url_versioned\" ], \"manuscript.pdf\" ) response = requests . head ( html_url , allow_redirects = True ) if not response . ok : logging . warning ( \"html_url is not web accessible. \" f \"{html_url} returned status code {response.status_code}. \" \"Ignore this warning if the manuscript has not yet been deployed for the first time. \" ) if response . history : logging . info ( \"html_url includes redirects. In order of oldest to most recent: \\n \" + \" \\n \" . join ( x . url for x in response . history + [ response ]) ) return urls def get_software_versions () -> dict : \"\"\" Return a dictionary of software versions for softwares components: - manubot_version: the semantic version number of the manubot python package. - rootstock_commit: the version of the rootstock repository, as a commit hash, included in the manuscript repository. Values whose detection fails are set to None. \"\"\" from manubot import __version__ as manubot_version return { \"manubot_version\" : manubot_version , \"rootstock_commit\" : get_rootstock_commit (), } def get_rootstock_commit () -> Optional [ str ]: \"\"\" Return the most recent commit in common between the git repository this function is run within (usually a Manubot manuscript repository) and the `master` branch of the `rootstock` remote. WARNING: This function may modify the git repository its executed within: - if the repository has not set the `roostock` remote, it is set to point to the default Rootstock repository of <https://github.com/manubot/rootstock>. - fetches the latest commits in the `master` branch of the `rootstock` remote \"\"\" from manubot.util import shlex_join # add rootstock remote if remote is not already set rootstock_remote = \"https://github.com/manubot/rootstock.git\" args = [ \"git\" , \"remote\" , \"add\" , \"rootstock\" , rootstock_remote ] process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) if process . returncode == 0 : logging . info ( \"get_rootstock_commit added a `rootstock` remote to the git repository.\" ) # find most recent common ancestor commit try : args = [ \"git\" , \"fetch\" , \"rootstock\" , \"master\" ] subprocess . check_output ( args , stderr = subprocess . PIPE , universal_newlines = True ) args = [ \"git\" , \"merge-base\" , \"HEAD\" , \"rootstock/master\" ] output = subprocess . check_output ( args , stderr = subprocess . PIPE , universal_newlines = True ) except subprocess . CalledProcessError as error : logging . warning ( f \"get_rootstock_commit: {shlex_join(error.cmd)!r} returned exit code {error.returncode} \" f \"with the following stdout: \\n {error.stdout} \\n \" f \"And the following stderr: \\n {error.stderr}\" ) return None rootstock_commit = output . strip () return rootstock_commit","title":"Module manubot.process.metadata"},{"location":"reference/manubot/process/metadata/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/metadata/#get_header_includes","text":"def get_header_includes ( variables : dict ) -> str Render header-includes-template.html using information from variables . View Source def get_header_includes ( variables : dict ) -> str : \"\"\" Render `header-includes-template.html` using information from `variables`. \"\"\" from .util import template_with_jinja2 path = pathlib . Path ( __file__ ) . parent . joinpath ( \"header-includes-template.html\" ) try : template = path . read_text ( encoding = \"utf-8-sig\" ) return template_with_jinja2 ( template , variables ) except Exception : logging . exception ( f \"Error generating header-includes.\" ) return \"\"","title":"get_header_includes"},{"location":"reference/manubot/process/metadata/#get_manuscript_urls","text":"def get_manuscript_urls ( html_url : Union [ str , NoneType ] = None ) -> dict Return a dictionary with URLs for a manuscript. An example for a manuscript where all URLs get set, inferred from continuous integration environment variables, is: { \"html_url\" : \"https://manubot.github.io/rootstock/\" , \"pdf_url\" : \"https://manubot.github.io/rootstock/manuscript.pdf\" , \"html_url_versioned\" : \"https://manubot.github.io/rootstock/v/7cf9071212ce33116ad09cf2237a370b180a3c35/\" , \"pdf_url_versioned\" : \"https://manubot.github.io/rootstock/v/7cf9071212ce33116ad09cf2237a370b180a3c35/manuscript.pdf\" , } Provide html_url to set a custom domain. If html_url=\"https://git.dhimmel.com/bitcoin-whitepaper/\" , the return dictionary will be like: { \"html_url\" : \"https://git.dhimmel.com/bitcoin-whitepaper/\" , \"pdf_url\" : \"https://git.dhimmel.com/bitcoin-whitepaper/manuscript.pdf\" , \"html_url_versioned\" : \"https://git.dhimmel.com/bitcoin-whitepaper/v/cb1f2c12eec8b56db9ef5f641ec805e2d449d319/\" , \"pdf_url_versioned\" : \"https://git.dhimmel.com/bitcoin-whitepaper/v/cb1f2c12eec8b56db9ef5f641ec805e2d449d319/manuscript.pdf\" , } Note the trailing / in html_url , which is required for proper functioning. View Source def get_manuscript_urls ( html_url : Optional [ str ] = None ) -> dict : \"\"\" Return a dictionary with URLs for a manuscript. An example for a manuscript where all URLs get set, inferred from continuous integration environment variables, is: ```python { \" html_url \": \" https : // manubot . github . io / rootstock / \", \" pdf_url \": \" https : // manubot . github . io / rootstock / manuscript . pdf \", \" html_url_versioned \": \" https : // manubot . github . io / rootstock / v / 7 cf9071212ce33116ad09cf2237a370b180a3c35 / \", \" pdf_url_versioned \": \" https : // manubot . github . io / rootstock / v / 7 cf9071212ce33116ad09cf2237a370b180a3c35 / manuscript . pdf \", } ``` Provide `html_url` to set a custom domain. If `html_url=\" https : // git . dhimmel . com / bitcoin - whitepaper / \"`, the return dictionary will be like: ```python { \" html_url \": \" https : // git . dhimmel . com / bitcoin - whitepaper / \", \" pdf_url \": \" https : // git . dhimmel . com / bitcoin - whitepaper / manuscript . pdf \", \" html_url_versioned \": \" https : // git . dhimmel . com / bitcoin - whitepaper / v / cb1f2c12eec8b56db9ef5f641ec805e2d449d319 / \", \" pdf_url_versioned \": \" https : // git . dhimmel . com / bitcoin - whitepaper / v / cb1f2c12eec8b56db9ef5f641ec805e2d449d319 / manuscript . pdf \", } ``` Note the trailing `/` in `html_url`, which is required for proper functioning. \"\"\" import requests from . ci import get_continuous_integration_parameters urls = dict () ci_params = get_continuous_integration_parameters () if html_url is None : if not ci_params : return urls html_url = \"https://{repo_owner}.github.io/{repo_name}/\" . format ( ** ci_params ) urls [ \"html_url\" ] = html_url urls [ \"pdf_url\" ] = urljoin ( html_url , \"manuscript.pdf\" ) if not ci_params : return urls urls [ \"html_url_versioned\" ] = urljoin ( html_url , \"v/{commit}/\" . format ( ** ci_params )) urls [ \"pdf_url_versioned\" ] = urljoin ( urls [ \"html_url_versioned\" ], \"manuscript.pdf\" ) response = requests . head ( html_url , allow_redirects = True ) if not response . ok : logging . warning ( \"html_url is not web accessible. \" f \"{html_url} returned status code {response.status_code}. \" \"Ignore this warning if the manuscript has not yet been deployed for the first time. \" ) if response . history : logging . info ( \"html_url includes redirects. In order of oldest to most recent:\\n\" + \"\\n\" . join ( x . url for x in response . history + [ response ]) ) return urls","title":"get_manuscript_urls"},{"location":"reference/manubot/process/metadata/#get_rootstock_commit","text":"def get_rootstock_commit ( ) -> Union [ str , NoneType ] Return the most recent commit in common between the git repository this function is run within (usually a Manubot manuscript repository) and the master branch of the rootstock remote. WARNING: This function may modify the git repository its executed within: if the repository has not set the roostock remote, it is set to point to the default Rootstock repository of https://github.com/manubot/rootstock . fetches the latest commits in the master branch of the rootstock remote View Source def get_rootstock_commit () -> Optional [ str ]: \"\"\" Return the most recent commit in common between the git repository this function is run within (usually a Manubot manuscript repository) and the `master` branch of the `rootstock` remote. WARNING: This function may modify the git repository its executed within: - if the repository has not set the `roostock` remote, it is set to point to the default Rootstock repository of <https://github.com/manubot/rootstock>. - fetches the latest commits in the `master` branch of the `rootstock` remote \"\"\" from manubot.util import shlex_join # add rootstock remote if remote is not already set rootstock_remote = \"https://github.com/manubot/rootstock.git\" args = [ \"git\" , \"remote\" , \"add\" , \"rootstock\" , rootstock_remote ] process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) if process . returncode == 0 : logging . info ( \"get_rootstock_commit added a `rootstock` remote to the git repository.\" ) # find most recent common ancestor commit try : args = [ \"git\" , \"fetch\" , \"rootstock\" , \"master\" ] subprocess . check_output ( args , stderr = subprocess . PIPE , universal_newlines = True ) args = [ \"git\" , \"merge-base\" , \"HEAD\" , \"rootstock/master\" ] output = subprocess . check_output ( args , stderr = subprocess . PIPE , universal_newlines = True ) except subprocess . CalledProcessError as error : logging . warning ( f \"get_rootstock_commit: {shlex_join(error.cmd)!r} returned exit code {error.returncode} \" f \"with the following stdout: \\n {error.stdout} \\n \" f \"And the following stderr: \\n {error.stderr}\" ) return None rootstock_commit = output . strip () return rootstock_commit","title":"get_rootstock_commit"},{"location":"reference/manubot/process/metadata/#get_software_versions","text":"def get_software_versions ( ) -> dict Return a dictionary of software versions for softwares components: manubot_version: the semantic version number of the manubot python package. rootstock_commit: the version of the rootstock repository, as a commit hash, included in the manuscript repository. Values whose detection fails are set to None. View Source def get_software_versions () -> dict : \"\"\" Return a dictionary of software versions for softwares components: - manubot_version: the semantic version number of the manubot python package. - rootstock_commit: the version of the rootstock repository, as a commit hash, included in the manuscript repository. Values whose detection fails are set to None. \"\"\" from manubot import __version__ as manubot_version return { \"manubot_version\" : manubot_version , \"rootstock_commit\" : get_rootstock_commit (), }","title":"get_software_versions"},{"location":"reference/manubot/process/metadata/#get_thumbnail_url","text":"def get_thumbnail_url ( thumbnail = None ) Starting with a user-specified thumbnail as either a path, URL, or None, return an absolute URL pointing to the thumbnail image. If the provided thumbnail is a URL, return this URL unmodified. If thumbnail is None, search for thumbnail.png within the git repository from which this function is executed. If thumbnail is a local path, the path should be relative to root directory of the git repository it is located in. If a local path is provided or detected, it is converted to a GitHub raw URL. View Source def get_thumbnail_url ( thumbnail = None ): \"\"\" Starting with a user-specified `thumbnail` as either a path, URL, or None, return an absolute URL pointing to the thumbnail image. If the provided `thumbnail` is a URL, return this URL unmodified. If `thumbnail` is None, search for `thumbnail.png` within the git repository from which this function is executed. If `thumbnail` is a local path, the path should be relative to root directory of the git repository it is located in. If a local path is provided or detected, it is converted to a GitHub raw URL. \"\"\" from manubot.util import is_http_url if not thumbnail : message = \"get_thumbnail_url: thumbnail location not explicitly provided. \" thumbnail = _find_thumbnail_path () message += ( f \"Thumbnail detected at {thumbnail!r}\" if thumbnail else \"No local thumbnail detected\" ) logging . debug ( message ) elif is_http_url ( thumbnail ): logging . debug ( \"provided thumbnail is a URL. Pass it through.\" ) return thumbnail return _thumbnail_path_to_url ( thumbnail )","title":"get_thumbnail_url"},{"location":"reference/manubot/process/metadata/#git_repository_root","text":"def git_repository_root ( ) Return the path to repository root directory or None if indeterminate. View Source @ functools . lru_cache () def git_repository_root (): \"\"\" Return the path to repository root directory or `None` if indeterminate. \"\"\" for cmd in ( [ \"git\" , \"rev-parse\" , \"--show-superproject-working-tree\" ], [ \"git\" , \"rev-parse\" , \"--show-toplevel\" ], ): try : path = subprocess . check_output ( cmd , universal_newlines = True ). rstrip ( \"\\r\\n\" ) if path : return pathlib . Path ( path ) except ( subprocess . CalledProcessError , OSError ): pass return None","title":"git_repository_root"},{"location":"reference/manubot/process/process_command/","text":"Module manubot.process.process_command View Source import logging def cli_process ( args ): args_dict = vars ( args ) # Set paths for content content_dir = args . content_directory if not content_dir . is_dir (): logging . warning ( f \"content directory does not exist: {content_dir}\" ) args_dict [ \"citation_tags_path\" ] = content_dir . joinpath ( \"citation-tags.tsv\" ) args_dict [ \"meta_yaml_path\" ] = content_dir . joinpath ( \"metadata.yaml\" ) args_dict [ \"manual_references_paths\" ] = sorted ( content_dir . rglob ( \"manual-references*.*\" ) ) # Set paths for output output_dir = args . output_directory output_dir . mkdir ( parents = True , exist_ok = True ) args_dict [ \"manuscript_path\" ] = output_dir . joinpath ( \"manuscript.md\" ) args_dict [ \"citations_path\" ] = output_dir . joinpath ( \"citations.tsv\" ) args_dict [ \"references_path\" ] = output_dir . joinpath ( \"references.json\" ) args_dict [ \"variables_path\" ] = output_dir . joinpath ( \"variables.json\" ) # Set paths for caching args_dict [ \"cache_directory\" ] = args . cache_directory or output_dir args . cache_directory . mkdir ( parents = True , exist_ok = True ) args_dict [ \"requests_cache_path\" ] = str ( args . cache_directory . joinpath ( \"requests-cache\" ) ) from manubot.process.util import prepare_manuscript prepare_manuscript ( args ) Functions cli_process def cli_process ( args ) View Source def cli_process ( args ): args_dict = vars ( args ) # Set paths for content content_dir = args . content_directory if not content_dir . is_dir (): logging . warning ( f \"content directory does not exist: {content_dir}\" ) args_dict [ \"citation_tags_path\" ] = content_dir . joinpath ( \"citation-tags.tsv\" ) args_dict [ \"meta_yaml_path\" ] = content_dir . joinpath ( \"metadata.yaml\" ) args_dict [ \"manual_references_paths\" ] = sorted ( content_dir . rglob ( \"manual-references*.*\" ) ) # Set paths for output output_dir = args . output_directory output_dir . mkdir ( parents = True , exist_ok = True ) args_dict [ \"manuscript_path\" ] = output_dir . joinpath ( \"manuscript.md\" ) args_dict [ \"citations_path\" ] = output_dir . joinpath ( \"citations.tsv\" ) args_dict [ \"references_path\" ] = output_dir . joinpath ( \"references.json\" ) args_dict [ \"variables_path\" ] = output_dir . joinpath ( \"variables.json\" ) # Set paths for caching args_dict [ \"cache_directory\" ] = args . cache_directory or output_dir args . cache_directory . mkdir ( parents = True , exist_ok = True ) args_dict [ \"requests_cache_path\" ] = str ( args . cache_directory . joinpath ( \"requests-cache\" ) ) from manubot . process . util import prepare_manuscript prepare_manuscript ( args )","title":"Process Command"},{"location":"reference/manubot/process/process_command/#module-manubotprocessprocess_command","text":"View Source import logging def cli_process ( args ): args_dict = vars ( args ) # Set paths for content content_dir = args . content_directory if not content_dir . is_dir (): logging . warning ( f \"content directory does not exist: {content_dir}\" ) args_dict [ \"citation_tags_path\" ] = content_dir . joinpath ( \"citation-tags.tsv\" ) args_dict [ \"meta_yaml_path\" ] = content_dir . joinpath ( \"metadata.yaml\" ) args_dict [ \"manual_references_paths\" ] = sorted ( content_dir . rglob ( \"manual-references*.*\" ) ) # Set paths for output output_dir = args . output_directory output_dir . mkdir ( parents = True , exist_ok = True ) args_dict [ \"manuscript_path\" ] = output_dir . joinpath ( \"manuscript.md\" ) args_dict [ \"citations_path\" ] = output_dir . joinpath ( \"citations.tsv\" ) args_dict [ \"references_path\" ] = output_dir . joinpath ( \"references.json\" ) args_dict [ \"variables_path\" ] = output_dir . joinpath ( \"variables.json\" ) # Set paths for caching args_dict [ \"cache_directory\" ] = args . cache_directory or output_dir args . cache_directory . mkdir ( parents = True , exist_ok = True ) args_dict [ \"requests_cache_path\" ] = str ( args . cache_directory . joinpath ( \"requests-cache\" ) ) from manubot.process.util import prepare_manuscript prepare_manuscript ( args )","title":"Module manubot.process.process_command"},{"location":"reference/manubot/process/process_command/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/process_command/#cli_process","text":"def cli_process ( args ) View Source def cli_process ( args ): args_dict = vars ( args ) # Set paths for content content_dir = args . content_directory if not content_dir . is_dir (): logging . warning ( f \"content directory does not exist: {content_dir}\" ) args_dict [ \"citation_tags_path\" ] = content_dir . joinpath ( \"citation-tags.tsv\" ) args_dict [ \"meta_yaml_path\" ] = content_dir . joinpath ( \"metadata.yaml\" ) args_dict [ \"manual_references_paths\" ] = sorted ( content_dir . rglob ( \"manual-references*.*\" ) ) # Set paths for output output_dir = args . output_directory output_dir . mkdir ( parents = True , exist_ok = True ) args_dict [ \"manuscript_path\" ] = output_dir . joinpath ( \"manuscript.md\" ) args_dict [ \"citations_path\" ] = output_dir . joinpath ( \"citations.tsv\" ) args_dict [ \"references_path\" ] = output_dir . joinpath ( \"references.json\" ) args_dict [ \"variables_path\" ] = output_dir . joinpath ( \"variables.json\" ) # Set paths for caching args_dict [ \"cache_directory\" ] = args . cache_directory or output_dir args . cache_directory . mkdir ( parents = True , exist_ok = True ) args_dict [ \"requests_cache_path\" ] = str ( args . cache_directory . joinpath ( \"requests-cache\" ) ) from manubot . process . util import prepare_manuscript prepare_manuscript ( args )","title":"cli_process"},{"location":"reference/manubot/process/requests_cache/","text":"Module manubot.process.requests_cache View Source import logging import os import pathlib import requests import requests_cache class RequestsCache : def __init__ ( self , path ): self . path = os . fspath ( path ) def mkdir ( self ): \"\"\"make directory containing cache file if it doesn't exist\"\"\" directory = pathlib . Path ( self . path ) . parent directory . mkdir ( parents = True , exist_ok = True ) def install ( self ): \"\"\"install cache\"\"\" requests # require `import requests` in case this is essential for monkey patching by requests_cache. requests_cache . install_cache ( self . path , include_get_headers = True ) self . cache = requests_cache . get_cache () logging . info ( f \"requests-cache starting with {len(self.cache.responses)} cached responses\" ) def clear ( self ): \"\"\"clear cache\"\"\" logging . info ( \"Clearing requests-cache\" ) requests_cache . clear () def close ( self ): \"\"\"uninstall cache\"\"\" logging . info ( f \"requests-cache finished with {len(self.cache.responses)} cached responses\" ) requests_cache . uninstall_cache () Classes RequestsCache class RequestsCache ( path ) View Source class RequestsCache : def __init__ ( self , path ): self . path = os . fspath ( path ) def mkdir ( self ): \"\"\"make directory containing cache file if it doesn't exist\"\"\" directory = pathlib . Path ( self . path ) . parent directory . mkdir ( parents = True , exist_ok = True ) def install ( self ): \"\"\"install cache\"\"\" requests # require `import requests` in case this is essential for monkey patching by requests_cache. requests_cache . install_cache ( self . path , include_get_headers = True ) self . cache = requests_cache . get_cache () logging . info ( f \"requests-cache starting with {len(self.cache.responses)} cached responses\" ) def clear ( self ): \"\"\"clear cache\"\"\" logging . info ( \"Clearing requests-cache\" ) requests_cache . clear () def close ( self ): \"\"\"uninstall cache\"\"\" logging . info ( f \"requests-cache finished with {len(self.cache.responses)} cached responses\" ) requests_cache . uninstall_cache () Methods clear def clear ( self ) clear cache View Source def clear ( self ): \"\"\"clear cache\"\"\" logging . info ( \"Clearing requests-cache\" ) requests_cache . clear () close def close ( self ) uninstall cache View Source def close ( self ): \"\"\"uninstall cache\"\"\" logging . info ( f \"requests-cache finished with {len(self.cache.responses)} cached responses\" ) requests_cache . uninstall_cache () install def install ( self ) install cache View Source def install ( self ): \"\"\"install cache\"\"\" requests # require `import requests` in case this is essential for monkey patching by requests_cache. requests_cache . install_cache ( self . path , include_get_headers = True ) self . cache = requests_cache . get_cache () logging . info ( f \"requests-cache starting with {len(self.cache.responses)} cached responses\" ) mkdir def mkdir ( self ) make directory containing cache file if it doesn't exist View Source def mkdir ( self ): \"\"\"make directory containing cache file if it doesn't exist\"\"\" directory = pathlib . Path ( self . path ). parent directory . mkdir ( parents = True , exist_ok = True )","title":"Requests Cache"},{"location":"reference/manubot/process/requests_cache/#module-manubotprocessrequests_cache","text":"View Source import logging import os import pathlib import requests import requests_cache class RequestsCache : def __init__ ( self , path ): self . path = os . fspath ( path ) def mkdir ( self ): \"\"\"make directory containing cache file if it doesn't exist\"\"\" directory = pathlib . Path ( self . path ) . parent directory . mkdir ( parents = True , exist_ok = True ) def install ( self ): \"\"\"install cache\"\"\" requests # require `import requests` in case this is essential for monkey patching by requests_cache. requests_cache . install_cache ( self . path , include_get_headers = True ) self . cache = requests_cache . get_cache () logging . info ( f \"requests-cache starting with {len(self.cache.responses)} cached responses\" ) def clear ( self ): \"\"\"clear cache\"\"\" logging . info ( \"Clearing requests-cache\" ) requests_cache . clear () def close ( self ): \"\"\"uninstall cache\"\"\" logging . info ( f \"requests-cache finished with {len(self.cache.responses)} cached responses\" ) requests_cache . uninstall_cache ()","title":"Module manubot.process.requests_cache"},{"location":"reference/manubot/process/requests_cache/#classes","text":"","title":"Classes"},{"location":"reference/manubot/process/requests_cache/#requestscache","text":"class RequestsCache ( path ) View Source class RequestsCache : def __init__ ( self , path ): self . path = os . fspath ( path ) def mkdir ( self ): \"\"\"make directory containing cache file if it doesn't exist\"\"\" directory = pathlib . Path ( self . path ) . parent directory . mkdir ( parents = True , exist_ok = True ) def install ( self ): \"\"\"install cache\"\"\" requests # require `import requests` in case this is essential for monkey patching by requests_cache. requests_cache . install_cache ( self . path , include_get_headers = True ) self . cache = requests_cache . get_cache () logging . info ( f \"requests-cache starting with {len(self.cache.responses)} cached responses\" ) def clear ( self ): \"\"\"clear cache\"\"\" logging . info ( \"Clearing requests-cache\" ) requests_cache . clear () def close ( self ): \"\"\"uninstall cache\"\"\" logging . info ( f \"requests-cache finished with {len(self.cache.responses)} cached responses\" ) requests_cache . uninstall_cache ()","title":"RequestsCache"},{"location":"reference/manubot/process/requests_cache/#methods","text":"","title":"Methods"},{"location":"reference/manubot/process/requests_cache/#clear","text":"def clear ( self ) clear cache View Source def clear ( self ): \"\"\"clear cache\"\"\" logging . info ( \"Clearing requests-cache\" ) requests_cache . clear ()","title":"clear"},{"location":"reference/manubot/process/requests_cache/#close","text":"def close ( self ) uninstall cache View Source def close ( self ): \"\"\"uninstall cache\"\"\" logging . info ( f \"requests-cache finished with {len(self.cache.responses)} cached responses\" ) requests_cache . uninstall_cache ()","title":"close"},{"location":"reference/manubot/process/requests_cache/#install","text":"def install ( self ) install cache View Source def install ( self ): \"\"\"install cache\"\"\" requests # require `import requests` in case this is essential for monkey patching by requests_cache. requests_cache . install_cache ( self . path , include_get_headers = True ) self . cache = requests_cache . get_cache () logging . info ( f \"requests-cache starting with {len(self.cache.responses)} cached responses\" )","title":"install"},{"location":"reference/manubot/process/requests_cache/#mkdir","text":"def mkdir ( self ) make directory containing cache file if it doesn't exist View Source def mkdir ( self ): \"\"\"make directory containing cache file if it doesn't exist\"\"\" directory = pathlib . Path ( self . path ). parent directory . mkdir ( parents = True , exist_ok = True )","title":"mkdir"},{"location":"reference/manubot/process/util/","text":"Module manubot.process.util View Source import json import logging import os import re import warnings from typing import List , Optional import jinja2 import yaml from manubot.util import read_serialized_data , read_serialized_dict from manubot.process.ci import get_continuous_integration_parameters from manubot.process.metadata import ( get_header_includes , get_thumbnail_url , get_manuscript_urls , get_software_versions , ) from manubot.process.manuscript import ( datetime_now , get_manuscript_stats , get_text , ) def read_variable_files ( paths : List [ str ], variables : Optional [ dict ] = None ) -> dict : \"\"\" Read multiple serialized data files into a user_variables dictionary. Provide `paths` (a list of URLs or local file paths). Paths can optionally have a namespace prepended. For example: ```python paths = [ 'https://git.io/vbkqm', # update the dictionary's top-level 'namespace_1=https://git.io/vbkqm', # store under 'namespace_1' key 'namespace_2=some_local_path.json', # store under 'namespace_2' key ] ``` If a namespace is not provided, the JSON must contain a dictionary as its top level. Namespaces should consist only of ASCII alphanumeric characters (includes underscores, first character cannot be numeric). Pass a dictionary to `variables` to update an existing dictionary rather than create a new dictionary. \"\"\" if variables is None : variables = {} for path in paths : logging . info ( f \"Reading user-provided templating variables at {path!r}\" ) # Match only namespaces that are valid jinja2 variable names # http://jinja.pocoo.org/docs/2.10/api/#identifier-naming match = re . match ( r \"([a-zA-Z_][a-zA-Z0-9_]*)=(.+)\" , path ) if match : namespace , path = match . groups () logging . info ( f \"Using the {namespace!r} namespace for template variables from {path!r}\" ) try : if match : obj = { namespace : read_serialized_data ( path )} else : obj = read_serialized_dict ( path ) except Exception : logging . exception ( f \"Error reading template variables from {path!r}\" ) continue assert isinstance ( obj , dict ) conflicts = variables . keys () & obj . keys () if conflicts : logging . warning ( f \"Template variables in {path!r} overwrite existing \" \"values for the following keys: \\n \" + \" \\n \" . join ( conflicts ) ) variables . update ( obj ) logging . debug ( f \"Reading user-provided templating variables complete: \\n \" f \"{json.dumps(variables, indent=2, ensure_ascii=False)}\" ) return variables def _convert_field_to_list ( dictionary , field , separator = False , deprecation_warning_key = None ): \"\"\" Convert `dictionary[field]` to a list. If value is a string and `separator` is specified, split by `separator`. If `deprecation_warning_key` is provided, warn when `dictionary[field]` is a string. \"\"\" if field not in dictionary : return dictionary value = dictionary [ field ] if isinstance ( value , list ): return dictionary if isinstance ( value , str ): if separator is False : dictionary [ field ] = [ value ] else : dictionary [ field ] = value . split ( separator ) if deprecation_warning_key : warnings . warn ( f \"Expected list for {dictionary.get(deprecation_warning_key)}'s {field}. \" + ( f \"Assuming multiple {field} are `{separator}` separated. \" if separator else \"\" ) + f \"Please switch {field} to a list.\" , category = DeprecationWarning , ) return dictionary raise ValueError ( \"Unsupported value type {value.__class__.__name__}\" ) def add_author_affiliations ( variables : dict ) -> dict : \"\"\" Edit variables to contain numbered author affiliations. Specifically, add a list of affiliation_numbers for each author and add a list of affiliations to the top-level of variables. If no authors have any affiliations, variables is left unmodified. \"\"\" affiliations = list () for author in variables [ \"authors\" ]: _convert_field_to_list ( dictionary = author , field = \"affiliations\" , separator = \"; \" , deprecation_warning_key = \"name\" , ) _convert_field_to_list ( dictionary = author , field = \"funders\" , ) affiliations . extend ( author . get ( \"affiliations\" , [])) if not affiliations : return variables affiliations = list ( dict . fromkeys ( affiliations )) # deduplicate affil_to_number = { affil : i for i , affil in enumerate ( affiliations , start = 1 )} for author in variables [ \"authors\" ]: numbers = [ affil_to_number [ affil ] for affil in author . get ( \"affiliations\" , [])] author [ \"affiliation_numbers\" ] = sorted ( numbers ) variables [ \"affiliations\" ] = [ dict ( affiliation = affil , affiliation_number = i ) for affil , i in affil_to_number . items () ] return variables def load_variables ( args ) -> dict : \"\"\" Read `metadata.yaml` and files specified by `--template-variables-path` to generate manuscript variables available for jinja2 templating. Returns a dictionary, refered to as `variables`, with the following keys: - `pandoc`: a dictionary for passing options to Pandoc via the `yaml_metadata_block`. Fields in `pandoc` are either generated by Manubot or hard-coded by the user if `metadata.yaml` includes a `pandoc` dictionary. - `manubot`: a dictionary for manubot-related information and metadata. Fields in `manubot` are either generated by Manubot or hard-coded by the user if `metadata.yaml` includes a `manubot` dictionary. - All fields from a manuscript's `metadata.yaml` that are not interpreted by Manubot are copied to `variables`. Interpreted fields include `pandoc`, `manubot`, `title`, `keywords`, `authors` (formerly `author_info`, now deprecated), `lang`, and `thumbnail`. - User-specified fields inserted according to the `--template-variables-path` option. User-specified variables take highest precedence and can overwrite values for existing keys like `pandoc` or `manubot` (dangerous). \"\"\" # Generated manuscript variables variables = { \"pandoc\" : {}, \"manubot\" : {}} # Read metadata which contains pandoc_yaml_metadata # as well as authors information. if args . meta_yaml_path . is_file (): metadata = read_serialized_dict ( args . meta_yaml_path ) else : metadata = {} logging . warning ( f \"missing {args.meta_yaml_path} file with yaml_metadata_block for pandoc\" ) # Interpreted keys that are intended for pandoc move_to_pandoc = \"title\" , \"keywords\" , \"lang\" for key in move_to_pandoc : if key in metadata : variables [ \"pandoc\" ][ key ] = metadata . pop ( key ) # Add date to metadata now = datetime_now () logging . info ( f \"Using {now:%Z} timezone. \\n \" f \"Dating manuscript with the current datetime: {now.isoformat()}\" ) variables [ \"pandoc\" ][ \"date-meta\" ] = now . date () . isoformat () variables [ \"manubot\" ][ \"date\" ] = f \"{now:%B} {now.day}, {now.year}\" # Process authors metadata if \"author_info\" in metadata : authors = metadata . pop ( \"author_info\" , []) warnings . warn ( \"metadata.yaml: 'author_info' is deprecated. Use 'authors' instead.\" , category = DeprecationWarning , ) else : authors = metadata . pop ( \"authors\" , []) if authors is None : authors = [] variables [ \"pandoc\" ][ \"author-meta\" ] = [ author [ \"name\" ] for author in authors ] variables [ \"manubot\" ][ \"authors\" ] = authors add_author_affiliations ( variables [ \"manubot\" ]) # Set repository version metadata for CI builds ci_params = get_continuous_integration_parameters () if ci_params : variables [ \"manubot\" ][ \"ci_source\" ] = ci_params # Add manuscript URLs variables [ \"manubot\" ] . update ( get_manuscript_urls ( metadata . pop ( \"html_url\" , None ))) # Add software versions variables [ \"manubot\" ] . update ( get_software_versions ()) # Add thumbnail URL if present thumbnail_url = get_thumbnail_url ( metadata . pop ( \"thumbnail\" , None )) if thumbnail_url : variables [ \"manubot\" ][ \"thumbnail_url\" ] = thumbnail_url # Update variables with metadata.yaml pandoc/manubot dicts for key in \"pandoc\" , \"manubot\" : dict_ = metadata . pop ( key , {}) if not isinstance ( dict_ , dict ): logging . warning ( f \"load_variables expected metadata.yaml field {key!r} to be a dict.\" f \"Received a {dict_.__class__.__name__!r} instead.\" ) continue variables [ key ] . update ( dict_ ) # Update variables with uninterpreted metadata.yaml fields variables . update ( metadata ) # Update variables with user-provided variables here variables = read_variable_files ( args . template_variables_path , variables ) # Add header-includes metadata with <meta> information for the HTML output's <head> variables [ \"pandoc\" ][ \"header-includes\" ] = get_header_includes ( variables ) assert args . skip_citations # Extend Pandoc's metadata.bibliography field with manual references paths bibliographies = variables [ \"pandoc\" ] . get ( \"bibliography\" , []) if isinstance ( bibliographies , str ): bibliographies = [ bibliographies ] assert isinstance ( bibliographies , list ) bibliographies . extend ( args . manual_references_paths ) bibliographies = list ( map ( os . fspath , bibliographies )) variables [ \"pandoc\" ][ \"bibliography\" ] = bibliographies # enable pandoc-manubot-cite option to write bibliography to a file variables [ \"pandoc\" ][ \"manubot-output-bibliography\" ] = os . fspath ( args . references_path ) variables [ \"pandoc\" ][ \"manubot-output-citekeys\" ] = os . fspath ( args . citations_path ) variables [ \"pandoc\" ][ \"manubot-requests-cache-path\" ] = os . fspath ( args . requests_cache_path ) variables [ \"pandoc\" ][ \"manubot-clear-requests-cache\" ] = args . clear_requests_cache return variables def template_with_jinja2 ( text , variables ): \"\"\" Template using jinja2 with the variables dictionary unpacked as keyword arguments. \"\"\" jinja_environment = jinja2 . Environment ( loader = jinja2 . BaseLoader (), undefined = jinja2 . make_logging_undefined ( logging . getLogger ()), autoescape = False , comment_start_string = \"{##\" , comment_end_string = \"##}\" , extensions = [ \"jinja2.ext.do\" , \"jinja2.ext.loopcontrols\" ], ) template = jinja_environment . from_string ( text ) return template . render ( ** variables ) def prepare_manuscript ( args ): \"\"\" Compile manuscript, creating manuscript.md and references.json as inputs for pandoc. \"\"\" text = get_text ( args . content_directory ) assert args . skip_citations variables = load_variables ( args ) variables [ \"manubot\" ][ \"manuscript_stats\" ] = get_manuscript_stats ( text ) with args . variables_path . open ( \"w\" , encoding = \"utf-8\" ) as write_file : json . dump ( variables , write_file , ensure_ascii = False , indent = 2 ) write_file . write ( \" \\n \" ) text = template_with_jinja2 ( text , variables ) # Write manuscript for pandoc with args . manuscript_path . open ( \"w\" , encoding = \"utf-8\" ) as write_file : yaml . dump ( variables [ \"pandoc\" ], write_file , default_flow_style = False , explicit_start = True , explicit_end = True , width = float ( \"inf\" ), ) write_file . write ( \" \\n \" ) write_file . write ( text ) Functions add_author_affiliations def add_author_affiliations ( variables : dict ) -> dict Edit variables to contain numbered author affiliations. Specifically, add a list of affiliation_numbers for each author and add a list of affiliations to the top-level of variables. If no authors have any affiliations, variables is left unmodified. View Source def add_author_affiliations ( variables : dict ) -> dict : \"\"\" Edit variables to contain numbered author affiliations. Specifically, add a list of affiliation_numbers for each author and add a list of affiliations to the top-level of variables. If no authors have any affiliations, variables is left unmodified. \"\"\" affiliations = list () for author in variables [ \"authors\" ] : _convert_field_to_list ( dictionary = author , field = \"affiliations\" , separator = \"; \" , deprecation_warning_key = \"name\" , ) _convert_field_to_list ( dictionary = author , field = \"funders\" , ) affiliations . extend ( author . get ( \"affiliations\" , [] )) if not affiliations : return variables affiliations = list ( dict . fromkeys ( affiliations )) # deduplicate affil_to_number = { affil : i for i , affil in enumerate ( affiliations , start = 1 ) } for author in variables [ \"authors\" ] : numbers = [ affil_to_number[affil ] for affil in author . get ( \"affiliations\" , [] ) ] author [ \"affiliation_numbers\" ] = sorted ( numbers ) variables [ \"affiliations\" ] = [ dict(affiliation=affil, affiliation_number=i) for affil, i in affil_to_number.items() ] return variables load_variables def load_variables ( args ) -> dict Read metadata.yaml and files specified by --template-variables-path to generate manuscript variables available for jinja2 templating. Returns a dictionary, refered to as variables , with the following keys: pandoc : a dictionary for passing options to Pandoc via the yaml_metadata_block . Fields in pandoc are either generated by Manubot or hard-coded by the user if metadata.yaml includes a pandoc dictionary. manubot : a dictionary for manubot-related information and metadata. Fields in manubot are either generated by Manubot or hard-coded by the user if metadata.yaml includes a manubot dictionary. All fields from a manuscript's metadata.yaml that are not interpreted by Manubot are copied to variables . Interpreted fields include pandoc , manubot , title , keywords , authors (formerly author_info , now deprecated), lang , and thumbnail . User-specified fields inserted according to the --template-variables-path option. User-specified variables take highest precedence and can overwrite values for existing keys like pandoc or manubot (dangerous). View Source def load_variables ( args ) -> dict : \"\"\" Read `metadata.yaml` and files specified by `--template-variables-path` to generate manuscript variables available for jinja2 templating. Returns a dictionary, refered to as `variables`, with the following keys: - `pandoc`: a dictionary for passing options to Pandoc via the `yaml_metadata_block`. Fields in `pandoc` are either generated by Manubot or hard-coded by the user if `metadata.yaml` includes a `pandoc` dictionary. - `manubot`: a dictionary for manubot-related information and metadata. Fields in `manubot` are either generated by Manubot or hard-coded by the user if `metadata.yaml` includes a `manubot` dictionary. - All fields from a manuscript's `metadata.yaml` that are not interpreted by Manubot are copied to `variables`. Interpreted fields include `pandoc`, `manubot`, `title`, `keywords`, `authors` (formerly `author_info`, now deprecated), `lang`, and `thumbnail`. - User-specified fields inserted according to the `--template-variables-path` option. User-specified variables take highest precedence and can overwrite values for existing keys like `pandoc` or `manubot` (dangerous). \"\"\" # Generated manuscript variables variables = { \"pandoc\" : {} , \"manubot\" : {}} # Read metadata which contains pandoc_yaml_metadata # as well as authors information. if args . meta_yaml_path . is_file (): metadata = read_serialized_dict ( args . meta_yaml_path ) else : metadata = {} logging . warning ( f \"missing {args.meta_yaml_path} file with yaml_metadata_block for pandoc\" ) # Interpreted keys that are intended for pandoc move_to_pandoc = \"title\" , \"keywords\" , \"lang\" for key in move_to_pandoc : if key in metadata : variables [ \"pandoc\" ][ key ] = metadata . pop ( key ) # Add date to metadata now = datetime_now () logging . info ( f \"Using {now:%Z} timezone.\\n\" f \"Dating manuscript with the current datetime: {now.isoformat()}\" ) variables [ \"pandoc\" ][ \"date-meta\" ] = now . date (). isoformat () variables [ \"manubot\" ][ \"date\" ] = f \"{now:%B} {now.day}, {now.year}\" # Process authors metadata if \"author_info\" in metadata : authors = metadata . pop ( \"author_info\" , []) warnings . warn ( \"metadata.yaml: 'author_info' is deprecated. Use 'authors' instead.\" , category = DeprecationWarning , ) else : authors = metadata . pop ( \"authors\" , []) if authors is None : authors = [] variables [ \"pandoc\" ][ \"author-meta\" ] = [ author [ \"name\" ] for author in authors ] variables [ \"manubot\" ][ \"authors\" ] = authors add_author_affiliations ( variables [ \"manubot\" ]) # Set repository version metadata for CI builds ci_params = get_continuous_integration_parameters () if ci_params : variables [ \"manubot\" ][ \"ci_source\" ] = ci_params # Add manuscript URLs variables [ \"manubot\" ]. update ( get_manuscript_urls ( metadata . pop ( \"html_url\" , None ))) # Add software versions variables [ \"manubot\" ]. update ( get_software_versions ()) # Add thumbnail URL if present thumbnail_url = get_thumbnail_url ( metadata . pop ( \"thumbnail\" , None )) if thumbnail_url : variables [ \"manubot\" ][ \"thumbnail_url\" ] = thumbnail_url # Update variables with metadata.yaml pandoc/manubot dicts for key in \"pandoc\" , \"manubot\" : dict_ = metadata . pop ( key , {} ) if not isinstance ( dict_ , dict ): logging . warning ( f \"load_variables expected metadata.yaml field {key!r} to be a dict.\" f \"Received a {dict_.__class__.__name__!r} instead.\" ) continue variables [ key ]. update ( dict_ ) # Update variables with uninterpreted metadata.yaml fields variables . update ( metadata ) # Update variables with user-provided variables here variables = read_variable_files ( args . template_variables_path , variables ) # Add header-includes metadata with <meta> information for the HTML output's <head> variables [ \"pandoc\" ][ \"header-includes\" ] = get_header_includes ( variables ) assert args . skip_citations # Extend Pandoc's metadata.bibliography field with manual references paths bibliographies = variables [ \"pandoc\" ]. get ( \"bibliography\" , []) if isinstance ( bibliographies , str ): bibliographies = [ bibliographies ] assert isinstance ( bibliographies , list ) bibliographies . extend ( args . manual_references_paths ) bibliographies = list ( map ( os . fspath , bibliographies )) variables [ \"pandoc\" ][ \"bibliography\" ] = bibliographies # enable pandoc-manubot-cite option to write bibliography to a file variables [ \"pandoc\" ][ \"manubot-output-bibliography\" ] = os . fspath ( args . references_path ) variables [ \"pandoc\" ][ \"manubot-output-citekeys\" ] = os . fspath ( args . citations_path ) variables [ \"pandoc\" ][ \"manubot-requests-cache-path\" ] = os . fspath ( args . requests_cache_path ) variables [ \"pandoc\" ][ \"manubot-clear-requests-cache\" ] = args . clear_requests_cache return variables prepare_manuscript def prepare_manuscript ( args ) Compile manuscript, creating manuscript.md and references.json as inputs for pandoc. View Source def prepare_manuscript ( args ): \"\"\" Compile manuscript, creating manuscript.md and references.json as inputs for pandoc. \"\"\" text = get_text ( args . content_directory ) assert args . skip_citations variables = load_variables ( args ) variables [ \"manubot\" ][ \"manuscript_stats\" ] = get_manuscript_stats ( text ) with args . variables_path . open ( \"w\" , encoding = \"utf-8\" ) as write_file : json . dump ( variables , write_file , ensure_ascii = False , indent = 2 ) write_file . write ( \"\\n\" ) text = template_with_jinja2 ( text , variables ) # Write manuscript for pandoc with args . manuscript_path . open ( \"w\" , encoding = \"utf-8\" ) as write_file : yaml . dump ( variables [ \"pandoc\" ], write_file , default_flow_style = False , explicit_start = True , explicit_end = True , width = float ( \"inf\" ), ) write_file . write ( \"\\n\" ) write_file . write ( text ) read_variable_files def read_variable_files ( paths : List [ str ], variables : Union [ dict , NoneType ] = None ) -> dict Read multiple serialized data files into a user_variables dictionary. Provide paths (a list of URLs or local file paths). Paths can optionally have a namespace prepended. For example: paths = [ 'https://git.io/vbkqm' , # update the dictionary's top-level 'namespace_1=https://git.io/vbkqm' , # store under 'namespace_1' key 'namespace_2=some_local_path.json' , # store under 'namespace_2' key ] If a namespace is not provided, the JSON must contain a dictionary as its top level. Namespaces should consist only of ASCII alphanumeric characters (includes underscores, first character cannot be numeric). Pass a dictionary to variables to update an existing dictionary rather than create a new dictionary. View Source def read_variable_files ( paths : List [ str ], variables : Optional [ dict ] = None ) -> dict : \"\"\" Read multiple serialized data files into a user_variables dictionary. Provide `paths` (a list of URLs or local file paths). Paths can optionally have a namespace prepended. For example: ```python paths = [ 'https://git.io/vbkqm', # update the dictionary's top-level 'namespace_1=https://git.io/vbkqm', # store under 'namespace_1' key 'namespace_2=some_local_path.json', # store under 'namespace_2' key ] ``` If a namespace is not provided, the JSON must contain a dictionary as its top level. Namespaces should consist only of ASCII alphanumeric characters (includes underscores, first character cannot be numeric). Pass a dictionary to `variables` to update an existing dictionary rather than create a new dictionary. \"\"\" if variables is None : variables = {} for path in paths : logging . info ( f \"Reading user-provided templating variables at {path!r}\" ) # Match only namespaces that are valid jinja2 variable names # http://jinja.pocoo.org/docs/2.10/api/#identifier-naming match = re . match ( r \"([a-zA-Z_][a-zA-Z0-9_]*)=(.+)\" , path ) if match : namespace , path = match . groups () logging . info ( f \"Using the {namespace!r} namespace for template variables from {path!r}\" ) try : if match : obj = { namespace : read_serialized_data ( path ) } else : obj = read_serialized_dict ( path ) except Exception : logging . exception ( f \"Error reading template variables from {path!r}\" ) continue assert isinstance ( obj , dict ) conflicts = variables . keys () & obj . keys () if conflicts : logging . warning ( f \"Template variables in {path!r} overwrite existing \" \"values for the following keys:\\n\" + \"\\n\" . join ( conflicts ) ) variables . update ( obj ) logging . debug ( f \"Reading user-provided templating variables complete:\\n\" f \"{json.dumps(variables, indent=2, ensure_ascii=False)}\" ) return variables template_with_jinja2 def template_with_jinja2 ( text , variables ) Template using jinja2 with the variables dictionary unpacked as keyword arguments. View Source def template_with_jinja2 ( text , variables ): \"\"\" Template using jinja2 with the variables dictionary unpacked as keyword arguments. \"\"\" jinja_environment = jinja2 . Environment ( loader = jinja2 . BaseLoader (), undefined = jinja2 . make_logging_undefined ( logging . getLogger ()), autoescape = False , comment_start_string = \"{##\" , comment_end_string = \"##}\" , extensions = [ \"jinja2.ext.do\" , \"jinja2.ext.loopcontrols\" ], ) template = jinja_environment . from_string ( text ) return template . render ( ** variables )","title":"Util"},{"location":"reference/manubot/process/util/#module-manubotprocessutil","text":"View Source import json import logging import os import re import warnings from typing import List , Optional import jinja2 import yaml from manubot.util import read_serialized_data , read_serialized_dict from manubot.process.ci import get_continuous_integration_parameters from manubot.process.metadata import ( get_header_includes , get_thumbnail_url , get_manuscript_urls , get_software_versions , ) from manubot.process.manuscript import ( datetime_now , get_manuscript_stats , get_text , ) def read_variable_files ( paths : List [ str ], variables : Optional [ dict ] = None ) -> dict : \"\"\" Read multiple serialized data files into a user_variables dictionary. Provide `paths` (a list of URLs or local file paths). Paths can optionally have a namespace prepended. For example: ```python paths = [ 'https://git.io/vbkqm', # update the dictionary's top-level 'namespace_1=https://git.io/vbkqm', # store under 'namespace_1' key 'namespace_2=some_local_path.json', # store under 'namespace_2' key ] ``` If a namespace is not provided, the JSON must contain a dictionary as its top level. Namespaces should consist only of ASCII alphanumeric characters (includes underscores, first character cannot be numeric). Pass a dictionary to `variables` to update an existing dictionary rather than create a new dictionary. \"\"\" if variables is None : variables = {} for path in paths : logging . info ( f \"Reading user-provided templating variables at {path!r}\" ) # Match only namespaces that are valid jinja2 variable names # http://jinja.pocoo.org/docs/2.10/api/#identifier-naming match = re . match ( r \"([a-zA-Z_][a-zA-Z0-9_]*)=(.+)\" , path ) if match : namespace , path = match . groups () logging . info ( f \"Using the {namespace!r} namespace for template variables from {path!r}\" ) try : if match : obj = { namespace : read_serialized_data ( path )} else : obj = read_serialized_dict ( path ) except Exception : logging . exception ( f \"Error reading template variables from {path!r}\" ) continue assert isinstance ( obj , dict ) conflicts = variables . keys () & obj . keys () if conflicts : logging . warning ( f \"Template variables in {path!r} overwrite existing \" \"values for the following keys: \\n \" + \" \\n \" . join ( conflicts ) ) variables . update ( obj ) logging . debug ( f \"Reading user-provided templating variables complete: \\n \" f \"{json.dumps(variables, indent=2, ensure_ascii=False)}\" ) return variables def _convert_field_to_list ( dictionary , field , separator = False , deprecation_warning_key = None ): \"\"\" Convert `dictionary[field]` to a list. If value is a string and `separator` is specified, split by `separator`. If `deprecation_warning_key` is provided, warn when `dictionary[field]` is a string. \"\"\" if field not in dictionary : return dictionary value = dictionary [ field ] if isinstance ( value , list ): return dictionary if isinstance ( value , str ): if separator is False : dictionary [ field ] = [ value ] else : dictionary [ field ] = value . split ( separator ) if deprecation_warning_key : warnings . warn ( f \"Expected list for {dictionary.get(deprecation_warning_key)}'s {field}. \" + ( f \"Assuming multiple {field} are `{separator}` separated. \" if separator else \"\" ) + f \"Please switch {field} to a list.\" , category = DeprecationWarning , ) return dictionary raise ValueError ( \"Unsupported value type {value.__class__.__name__}\" ) def add_author_affiliations ( variables : dict ) -> dict : \"\"\" Edit variables to contain numbered author affiliations. Specifically, add a list of affiliation_numbers for each author and add a list of affiliations to the top-level of variables. If no authors have any affiliations, variables is left unmodified. \"\"\" affiliations = list () for author in variables [ \"authors\" ]: _convert_field_to_list ( dictionary = author , field = \"affiliations\" , separator = \"; \" , deprecation_warning_key = \"name\" , ) _convert_field_to_list ( dictionary = author , field = \"funders\" , ) affiliations . extend ( author . get ( \"affiliations\" , [])) if not affiliations : return variables affiliations = list ( dict . fromkeys ( affiliations )) # deduplicate affil_to_number = { affil : i for i , affil in enumerate ( affiliations , start = 1 )} for author in variables [ \"authors\" ]: numbers = [ affil_to_number [ affil ] for affil in author . get ( \"affiliations\" , [])] author [ \"affiliation_numbers\" ] = sorted ( numbers ) variables [ \"affiliations\" ] = [ dict ( affiliation = affil , affiliation_number = i ) for affil , i in affil_to_number . items () ] return variables def load_variables ( args ) -> dict : \"\"\" Read `metadata.yaml` and files specified by `--template-variables-path` to generate manuscript variables available for jinja2 templating. Returns a dictionary, refered to as `variables`, with the following keys: - `pandoc`: a dictionary for passing options to Pandoc via the `yaml_metadata_block`. Fields in `pandoc` are either generated by Manubot or hard-coded by the user if `metadata.yaml` includes a `pandoc` dictionary. - `manubot`: a dictionary for manubot-related information and metadata. Fields in `manubot` are either generated by Manubot or hard-coded by the user if `metadata.yaml` includes a `manubot` dictionary. - All fields from a manuscript's `metadata.yaml` that are not interpreted by Manubot are copied to `variables`. Interpreted fields include `pandoc`, `manubot`, `title`, `keywords`, `authors` (formerly `author_info`, now deprecated), `lang`, and `thumbnail`. - User-specified fields inserted according to the `--template-variables-path` option. User-specified variables take highest precedence and can overwrite values for existing keys like `pandoc` or `manubot` (dangerous). \"\"\" # Generated manuscript variables variables = { \"pandoc\" : {}, \"manubot\" : {}} # Read metadata which contains pandoc_yaml_metadata # as well as authors information. if args . meta_yaml_path . is_file (): metadata = read_serialized_dict ( args . meta_yaml_path ) else : metadata = {} logging . warning ( f \"missing {args.meta_yaml_path} file with yaml_metadata_block for pandoc\" ) # Interpreted keys that are intended for pandoc move_to_pandoc = \"title\" , \"keywords\" , \"lang\" for key in move_to_pandoc : if key in metadata : variables [ \"pandoc\" ][ key ] = metadata . pop ( key ) # Add date to metadata now = datetime_now () logging . info ( f \"Using {now:%Z} timezone. \\n \" f \"Dating manuscript with the current datetime: {now.isoformat()}\" ) variables [ \"pandoc\" ][ \"date-meta\" ] = now . date () . isoformat () variables [ \"manubot\" ][ \"date\" ] = f \"{now:%B} {now.day}, {now.year}\" # Process authors metadata if \"author_info\" in metadata : authors = metadata . pop ( \"author_info\" , []) warnings . warn ( \"metadata.yaml: 'author_info' is deprecated. Use 'authors' instead.\" , category = DeprecationWarning , ) else : authors = metadata . pop ( \"authors\" , []) if authors is None : authors = [] variables [ \"pandoc\" ][ \"author-meta\" ] = [ author [ \"name\" ] for author in authors ] variables [ \"manubot\" ][ \"authors\" ] = authors add_author_affiliations ( variables [ \"manubot\" ]) # Set repository version metadata for CI builds ci_params = get_continuous_integration_parameters () if ci_params : variables [ \"manubot\" ][ \"ci_source\" ] = ci_params # Add manuscript URLs variables [ \"manubot\" ] . update ( get_manuscript_urls ( metadata . pop ( \"html_url\" , None ))) # Add software versions variables [ \"manubot\" ] . update ( get_software_versions ()) # Add thumbnail URL if present thumbnail_url = get_thumbnail_url ( metadata . pop ( \"thumbnail\" , None )) if thumbnail_url : variables [ \"manubot\" ][ \"thumbnail_url\" ] = thumbnail_url # Update variables with metadata.yaml pandoc/manubot dicts for key in \"pandoc\" , \"manubot\" : dict_ = metadata . pop ( key , {}) if not isinstance ( dict_ , dict ): logging . warning ( f \"load_variables expected metadata.yaml field {key!r} to be a dict.\" f \"Received a {dict_.__class__.__name__!r} instead.\" ) continue variables [ key ] . update ( dict_ ) # Update variables with uninterpreted metadata.yaml fields variables . update ( metadata ) # Update variables with user-provided variables here variables = read_variable_files ( args . template_variables_path , variables ) # Add header-includes metadata with <meta> information for the HTML output's <head> variables [ \"pandoc\" ][ \"header-includes\" ] = get_header_includes ( variables ) assert args . skip_citations # Extend Pandoc's metadata.bibliography field with manual references paths bibliographies = variables [ \"pandoc\" ] . get ( \"bibliography\" , []) if isinstance ( bibliographies , str ): bibliographies = [ bibliographies ] assert isinstance ( bibliographies , list ) bibliographies . extend ( args . manual_references_paths ) bibliographies = list ( map ( os . fspath , bibliographies )) variables [ \"pandoc\" ][ \"bibliography\" ] = bibliographies # enable pandoc-manubot-cite option to write bibliography to a file variables [ \"pandoc\" ][ \"manubot-output-bibliography\" ] = os . fspath ( args . references_path ) variables [ \"pandoc\" ][ \"manubot-output-citekeys\" ] = os . fspath ( args . citations_path ) variables [ \"pandoc\" ][ \"manubot-requests-cache-path\" ] = os . fspath ( args . requests_cache_path ) variables [ \"pandoc\" ][ \"manubot-clear-requests-cache\" ] = args . clear_requests_cache return variables def template_with_jinja2 ( text , variables ): \"\"\" Template using jinja2 with the variables dictionary unpacked as keyword arguments. \"\"\" jinja_environment = jinja2 . Environment ( loader = jinja2 . BaseLoader (), undefined = jinja2 . make_logging_undefined ( logging . getLogger ()), autoescape = False , comment_start_string = \"{##\" , comment_end_string = \"##}\" , extensions = [ \"jinja2.ext.do\" , \"jinja2.ext.loopcontrols\" ], ) template = jinja_environment . from_string ( text ) return template . render ( ** variables ) def prepare_manuscript ( args ): \"\"\" Compile manuscript, creating manuscript.md and references.json as inputs for pandoc. \"\"\" text = get_text ( args . content_directory ) assert args . skip_citations variables = load_variables ( args ) variables [ \"manubot\" ][ \"manuscript_stats\" ] = get_manuscript_stats ( text ) with args . variables_path . open ( \"w\" , encoding = \"utf-8\" ) as write_file : json . dump ( variables , write_file , ensure_ascii = False , indent = 2 ) write_file . write ( \" \\n \" ) text = template_with_jinja2 ( text , variables ) # Write manuscript for pandoc with args . manuscript_path . open ( \"w\" , encoding = \"utf-8\" ) as write_file : yaml . dump ( variables [ \"pandoc\" ], write_file , default_flow_style = False , explicit_start = True , explicit_end = True , width = float ( \"inf\" ), ) write_file . write ( \" \\n \" ) write_file . write ( text )","title":"Module manubot.process.util"},{"location":"reference/manubot/process/util/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/util/#add_author_affiliations","text":"def add_author_affiliations ( variables : dict ) -> dict Edit variables to contain numbered author affiliations. Specifically, add a list of affiliation_numbers for each author and add a list of affiliations to the top-level of variables. If no authors have any affiliations, variables is left unmodified. View Source def add_author_affiliations ( variables : dict ) -> dict : \"\"\" Edit variables to contain numbered author affiliations. Specifically, add a list of affiliation_numbers for each author and add a list of affiliations to the top-level of variables. If no authors have any affiliations, variables is left unmodified. \"\"\" affiliations = list () for author in variables [ \"authors\" ] : _convert_field_to_list ( dictionary = author , field = \"affiliations\" , separator = \"; \" , deprecation_warning_key = \"name\" , ) _convert_field_to_list ( dictionary = author , field = \"funders\" , ) affiliations . extend ( author . get ( \"affiliations\" , [] )) if not affiliations : return variables affiliations = list ( dict . fromkeys ( affiliations )) # deduplicate affil_to_number = { affil : i for i , affil in enumerate ( affiliations , start = 1 ) } for author in variables [ \"authors\" ] : numbers = [ affil_to_number[affil ] for affil in author . get ( \"affiliations\" , [] ) ] author [ \"affiliation_numbers\" ] = sorted ( numbers ) variables [ \"affiliations\" ] = [ dict(affiliation=affil, affiliation_number=i) for affil, i in affil_to_number.items() ] return variables","title":"add_author_affiliations"},{"location":"reference/manubot/process/util/#load_variables","text":"def load_variables ( args ) -> dict Read metadata.yaml and files specified by --template-variables-path to generate manuscript variables available for jinja2 templating. Returns a dictionary, refered to as variables , with the following keys: pandoc : a dictionary for passing options to Pandoc via the yaml_metadata_block . Fields in pandoc are either generated by Manubot or hard-coded by the user if metadata.yaml includes a pandoc dictionary. manubot : a dictionary for manubot-related information and metadata. Fields in manubot are either generated by Manubot or hard-coded by the user if metadata.yaml includes a manubot dictionary. All fields from a manuscript's metadata.yaml that are not interpreted by Manubot are copied to variables . Interpreted fields include pandoc , manubot , title , keywords , authors (formerly author_info , now deprecated), lang , and thumbnail . User-specified fields inserted according to the --template-variables-path option. User-specified variables take highest precedence and can overwrite values for existing keys like pandoc or manubot (dangerous). View Source def load_variables ( args ) -> dict : \"\"\" Read `metadata.yaml` and files specified by `--template-variables-path` to generate manuscript variables available for jinja2 templating. Returns a dictionary, refered to as `variables`, with the following keys: - `pandoc`: a dictionary for passing options to Pandoc via the `yaml_metadata_block`. Fields in `pandoc` are either generated by Manubot or hard-coded by the user if `metadata.yaml` includes a `pandoc` dictionary. - `manubot`: a dictionary for manubot-related information and metadata. Fields in `manubot` are either generated by Manubot or hard-coded by the user if `metadata.yaml` includes a `manubot` dictionary. - All fields from a manuscript's `metadata.yaml` that are not interpreted by Manubot are copied to `variables`. Interpreted fields include `pandoc`, `manubot`, `title`, `keywords`, `authors` (formerly `author_info`, now deprecated), `lang`, and `thumbnail`. - User-specified fields inserted according to the `--template-variables-path` option. User-specified variables take highest precedence and can overwrite values for existing keys like `pandoc` or `manubot` (dangerous). \"\"\" # Generated manuscript variables variables = { \"pandoc\" : {} , \"manubot\" : {}} # Read metadata which contains pandoc_yaml_metadata # as well as authors information. if args . meta_yaml_path . is_file (): metadata = read_serialized_dict ( args . meta_yaml_path ) else : metadata = {} logging . warning ( f \"missing {args.meta_yaml_path} file with yaml_metadata_block for pandoc\" ) # Interpreted keys that are intended for pandoc move_to_pandoc = \"title\" , \"keywords\" , \"lang\" for key in move_to_pandoc : if key in metadata : variables [ \"pandoc\" ][ key ] = metadata . pop ( key ) # Add date to metadata now = datetime_now () logging . info ( f \"Using {now:%Z} timezone.\\n\" f \"Dating manuscript with the current datetime: {now.isoformat()}\" ) variables [ \"pandoc\" ][ \"date-meta\" ] = now . date (). isoformat () variables [ \"manubot\" ][ \"date\" ] = f \"{now:%B} {now.day}, {now.year}\" # Process authors metadata if \"author_info\" in metadata : authors = metadata . pop ( \"author_info\" , []) warnings . warn ( \"metadata.yaml: 'author_info' is deprecated. Use 'authors' instead.\" , category = DeprecationWarning , ) else : authors = metadata . pop ( \"authors\" , []) if authors is None : authors = [] variables [ \"pandoc\" ][ \"author-meta\" ] = [ author [ \"name\" ] for author in authors ] variables [ \"manubot\" ][ \"authors\" ] = authors add_author_affiliations ( variables [ \"manubot\" ]) # Set repository version metadata for CI builds ci_params = get_continuous_integration_parameters () if ci_params : variables [ \"manubot\" ][ \"ci_source\" ] = ci_params # Add manuscript URLs variables [ \"manubot\" ]. update ( get_manuscript_urls ( metadata . pop ( \"html_url\" , None ))) # Add software versions variables [ \"manubot\" ]. update ( get_software_versions ()) # Add thumbnail URL if present thumbnail_url = get_thumbnail_url ( metadata . pop ( \"thumbnail\" , None )) if thumbnail_url : variables [ \"manubot\" ][ \"thumbnail_url\" ] = thumbnail_url # Update variables with metadata.yaml pandoc/manubot dicts for key in \"pandoc\" , \"manubot\" : dict_ = metadata . pop ( key , {} ) if not isinstance ( dict_ , dict ): logging . warning ( f \"load_variables expected metadata.yaml field {key!r} to be a dict.\" f \"Received a {dict_.__class__.__name__!r} instead.\" ) continue variables [ key ]. update ( dict_ ) # Update variables with uninterpreted metadata.yaml fields variables . update ( metadata ) # Update variables with user-provided variables here variables = read_variable_files ( args . template_variables_path , variables ) # Add header-includes metadata with <meta> information for the HTML output's <head> variables [ \"pandoc\" ][ \"header-includes\" ] = get_header_includes ( variables ) assert args . skip_citations # Extend Pandoc's metadata.bibliography field with manual references paths bibliographies = variables [ \"pandoc\" ]. get ( \"bibliography\" , []) if isinstance ( bibliographies , str ): bibliographies = [ bibliographies ] assert isinstance ( bibliographies , list ) bibliographies . extend ( args . manual_references_paths ) bibliographies = list ( map ( os . fspath , bibliographies )) variables [ \"pandoc\" ][ \"bibliography\" ] = bibliographies # enable pandoc-manubot-cite option to write bibliography to a file variables [ \"pandoc\" ][ \"manubot-output-bibliography\" ] = os . fspath ( args . references_path ) variables [ \"pandoc\" ][ \"manubot-output-citekeys\" ] = os . fspath ( args . citations_path ) variables [ \"pandoc\" ][ \"manubot-requests-cache-path\" ] = os . fspath ( args . requests_cache_path ) variables [ \"pandoc\" ][ \"manubot-clear-requests-cache\" ] = args . clear_requests_cache return variables","title":"load_variables"},{"location":"reference/manubot/process/util/#prepare_manuscript","text":"def prepare_manuscript ( args ) Compile manuscript, creating manuscript.md and references.json as inputs for pandoc. View Source def prepare_manuscript ( args ): \"\"\" Compile manuscript, creating manuscript.md and references.json as inputs for pandoc. \"\"\" text = get_text ( args . content_directory ) assert args . skip_citations variables = load_variables ( args ) variables [ \"manubot\" ][ \"manuscript_stats\" ] = get_manuscript_stats ( text ) with args . variables_path . open ( \"w\" , encoding = \"utf-8\" ) as write_file : json . dump ( variables , write_file , ensure_ascii = False , indent = 2 ) write_file . write ( \"\\n\" ) text = template_with_jinja2 ( text , variables ) # Write manuscript for pandoc with args . manuscript_path . open ( \"w\" , encoding = \"utf-8\" ) as write_file : yaml . dump ( variables [ \"pandoc\" ], write_file , default_flow_style = False , explicit_start = True , explicit_end = True , width = float ( \"inf\" ), ) write_file . write ( \"\\n\" ) write_file . write ( text )","title":"prepare_manuscript"},{"location":"reference/manubot/process/util/#read_variable_files","text":"def read_variable_files ( paths : List [ str ], variables : Union [ dict , NoneType ] = None ) -> dict Read multiple serialized data files into a user_variables dictionary. Provide paths (a list of URLs or local file paths). Paths can optionally have a namespace prepended. For example: paths = [ 'https://git.io/vbkqm' , # update the dictionary's top-level 'namespace_1=https://git.io/vbkqm' , # store under 'namespace_1' key 'namespace_2=some_local_path.json' , # store under 'namespace_2' key ] If a namespace is not provided, the JSON must contain a dictionary as its top level. Namespaces should consist only of ASCII alphanumeric characters (includes underscores, first character cannot be numeric). Pass a dictionary to variables to update an existing dictionary rather than create a new dictionary. View Source def read_variable_files ( paths : List [ str ], variables : Optional [ dict ] = None ) -> dict : \"\"\" Read multiple serialized data files into a user_variables dictionary. Provide `paths` (a list of URLs or local file paths). Paths can optionally have a namespace prepended. For example: ```python paths = [ 'https://git.io/vbkqm', # update the dictionary's top-level 'namespace_1=https://git.io/vbkqm', # store under 'namespace_1' key 'namespace_2=some_local_path.json', # store under 'namespace_2' key ] ``` If a namespace is not provided, the JSON must contain a dictionary as its top level. Namespaces should consist only of ASCII alphanumeric characters (includes underscores, first character cannot be numeric). Pass a dictionary to `variables` to update an existing dictionary rather than create a new dictionary. \"\"\" if variables is None : variables = {} for path in paths : logging . info ( f \"Reading user-provided templating variables at {path!r}\" ) # Match only namespaces that are valid jinja2 variable names # http://jinja.pocoo.org/docs/2.10/api/#identifier-naming match = re . match ( r \"([a-zA-Z_][a-zA-Z0-9_]*)=(.+)\" , path ) if match : namespace , path = match . groups () logging . info ( f \"Using the {namespace!r} namespace for template variables from {path!r}\" ) try : if match : obj = { namespace : read_serialized_data ( path ) } else : obj = read_serialized_dict ( path ) except Exception : logging . exception ( f \"Error reading template variables from {path!r}\" ) continue assert isinstance ( obj , dict ) conflicts = variables . keys () & obj . keys () if conflicts : logging . warning ( f \"Template variables in {path!r} overwrite existing \" \"values for the following keys:\\n\" + \"\\n\" . join ( conflicts ) ) variables . update ( obj ) logging . debug ( f \"Reading user-provided templating variables complete:\\n\" f \"{json.dumps(variables, indent=2, ensure_ascii=False)}\" ) return variables","title":"read_variable_files"},{"location":"reference/manubot/process/util/#template_with_jinja2","text":"def template_with_jinja2 ( text , variables ) Template using jinja2 with the variables dictionary unpacked as keyword arguments. View Source def template_with_jinja2 ( text , variables ): \"\"\" Template using jinja2 with the variables dictionary unpacked as keyword arguments. \"\"\" jinja_environment = jinja2 . Environment ( loader = jinja2 . BaseLoader (), undefined = jinja2 . make_logging_undefined ( logging . getLogger ()), autoescape = False , comment_start_string = \"{##\" , comment_end_string = \"##}\" , extensions = [ \"jinja2.ext.do\" , \"jinja2.ext.loopcontrols\" ], ) template = jinja_environment . from_string ( text ) return template . render ( ** variables )","title":"template_with_jinja2"},{"location":"reference/manubot/process/tests/","text":"Module manubot.process.tests Sub-modules manubot.process.tests.test_bibliography manubot.process.tests.test_ci manubot.process.tests.test_metadata manubot.process.tests.test_process_command manubot.process.tests.test_util","title":"Index"},{"location":"reference/manubot/process/tests/#module-manubotprocesstests","text":"","title":"Module manubot.process.tests"},{"location":"reference/manubot/process/tests/#sub-modules","text":"manubot.process.tests.test_bibliography manubot.process.tests.test_ci manubot.process.tests.test_metadata manubot.process.tests.test_process_command manubot.process.tests.test_util","title":"Sub-modules"},{"location":"reference/manubot/process/tests/test_bibliography/","text":"Module manubot.process.tests.test_bibliography View Source import shutil import pytest from manubot.pandoc.tests.test_bibliography import bibliography_paths from manubot.process.bibliography import load_manual_references @pytest.mark.skipif ( not shutil . which ( \"pandoc-citeproc\" ), reason = \"pandoc-citeproc installation not found on system\" , ) class Test_load_manual_references : \"\"\" Tests loading multiple bibliography paths \"\"\" def setup_method ( self ): self . citation_to_csl_item = load_manual_references ( bibliography_paths ) print ( list ( self . citation_to_csl_item )) def test_csl_item_1 ( self ): assert \"doi:10.7554/elife.32822\" in self . citation_to_csl_item csl_item_1 = self . citation_to_csl_item [ \"doi:10.7554/elife.32822\" ] assert csl_item_1 [ \"title\" ] . startswith ( \"Sci-Hub\" ) assert \"CSL JSON Item was loaded by Manubot\" in csl_item_1 [ \"note\" ] assert \"manual_reference_filename: bibliography.json\" in csl_item_1 [ \"note\" ] assert \"standard_id: doi:10.7554/elife.32822\" in csl_item_1 [ \"note\" ] def test_csl_item_2 ( self ): # raw id corresponding to bibliography.bib assert \"noauthor_techblog:_nodate\" in self . citation_to_csl_item csl_item_2 = self . citation_to_csl_item [ \"noauthor_techblog:_nodate\" ] assert csl_item_2 [ \"title\" ] . startswith ( \"TechBlog\" ) assert \"manual_reference_filename: bibliography.bib\" in csl_item_2 [ \"note\" ] assert \"standard_id: noauthor_techblog:_nodate\" in csl_item_2 [ \"note\" ] def test_csl_item_3 ( self ): assert \"Beaulieu-Jones2017\" in self . citation_to_csl_item csl_item_3 = self . citation_to_csl_item [ \"Beaulieu-Jones2017\" ] assert csl_item_3 [ \"author\" ][ 0 ][ \"family\" ] == \"Beaulieu-Jones\" assert \"manual_reference_filename: bibliography.nbib\" in csl_item_3 [ \"note\" ] assert \"standard_id: Beaulieu-Jones2017\" in csl_item_3 [ \"note\" ] Variables bibliography_paths Classes Test_load_manual_references class Test_load_manual_references ( / , * args , ** kwargs ) Tests loading multiple bibliography paths View Source class Test_load_manual_references: \"\"\" Tests loading multiple bibliography paths \"\"\" def setup_method ( self ): self . citation_to_csl_item = load_manual_references ( bibliography_paths ) print ( list ( self . citation_to_csl_item )) def test_csl_item_1 ( self ): assert \"doi:10.7554/elife.32822\" in self . citation_to_csl_item csl_item_1 = self . citation_to_csl_item [ \"doi:10.7554/elife.32822\" ] assert csl_item_1 [ \"title\" ]. startswith ( \"Sci-Hub\" ) assert \"CSL JSON Item was loaded by Manubot\" in csl_item_1 [ \"note\" ] assert \"manual_reference_filename: bibliography.json\" in csl_item_1 [ \"note\" ] assert \"standard_id: doi:10.7554/elife.32822\" in csl_item_1 [ \"note\" ] def test_csl_item_2 ( self ): # raw id corresponding to bibliography.bib assert \"noauthor_techblog:_nodate\" in self . citation_to_csl_item csl_item_2 = self . citation_to_csl_item [ \"noauthor_techblog:_nodate\" ] assert csl_item_2 [ \"title\" ]. startswith ( \"TechBlog\" ) assert \"manual_reference_filename: bibliography.bib\" in csl_item_2 [ \"note\" ] assert \"standard_id: noauthor_techblog:_nodate\" in csl_item_2 [ \"note\" ] def test_csl_item_3 ( self ): assert \"Beaulieu-Jones2017\" in self . citation_to_csl_item csl_item_3 = self . citation_to_csl_item [ \"Beaulieu-Jones2017\" ] assert csl_item_3 [ \"author\" ][ 0 ][ \"family\" ] == \"Beaulieu-Jones\" assert \"manual_reference_filename: bibliography.nbib\" in csl_item_3 [ \"note\" ] assert \"standard_id: Beaulieu-Jones2017\" in csl_item_3 [ \"note\" ] Class variables pytestmark Methods setup_method def setup_method ( self ) View Source def setup_method ( self ): self . citation_to_csl_item = load_manual_references ( bibliography_paths ) print ( list ( self . citation_to_csl_item )) test_csl_item_1 def test_csl_item_1 ( self ) View Source def test_csl_item_1 ( self ): assert \"doi:10.7554/elife.32822\" in self . citation_to_csl_item csl_item_1 = self . citation_to_csl_item [ \"doi:10.7554/elife.32822\" ] assert csl_item_1 [ \"title\" ]. startswith ( \"Sci-Hub\" ) assert \"CSL JSON Item was loaded by Manubot\" in csl_item_1 [ \"note\" ] assert \"manual_reference_filename: bibliography.json\" in csl_item_1 [ \"note\" ] assert \"standard_id: doi:10.7554/elife.32822\" in csl_item_1 [ \"note\" ] test_csl_item_2 def test_csl_item_2 ( self ) View Source def test_csl_item_2 ( self ): # raw id corresponding to bibliography . bib assert \"noauthor_techblog:_nodate\" in self . citation_to_csl_item csl_item_2 = self . citation_to_csl_item [ \"noauthor_techblog:_nodate\" ] assert csl_item_2 [ \"title\" ]. startswith ( \"TechBlog\" ) assert \"manual_reference_filename: bibliography.bib\" in csl_item_2 [ \"note\" ] assert \"standard_id: noauthor_techblog:_nodate\" in csl_item_2 [ \"note\" ] test_csl_item_3 def test_csl_item_3 ( self ) View Source def test_csl_item_3 ( self ): assert \"Beaulieu-Jones2017\" in self . citation_to_csl_item csl_item_3 = self . citation_to_csl_item [ \"Beaulieu-Jones2017\" ] assert csl_item_3 [ \"author\" ][ 0 ][ \"family\" ] == \"Beaulieu-Jones\" assert \"manual_reference_filename: bibliography.nbib\" in csl_item_3 [ \"note\" ] assert \"standard_id: Beaulieu-Jones2017\" in csl_item_3 [ \"note\" ]","title":"Test Bibliography"},{"location":"reference/manubot/process/tests/test_bibliography/#module-manubotprocessteststest_bibliography","text":"View Source import shutil import pytest from manubot.pandoc.tests.test_bibliography import bibliography_paths from manubot.process.bibliography import load_manual_references @pytest.mark.skipif ( not shutil . which ( \"pandoc-citeproc\" ), reason = \"pandoc-citeproc installation not found on system\" , ) class Test_load_manual_references : \"\"\" Tests loading multiple bibliography paths \"\"\" def setup_method ( self ): self . citation_to_csl_item = load_manual_references ( bibliography_paths ) print ( list ( self . citation_to_csl_item )) def test_csl_item_1 ( self ): assert \"doi:10.7554/elife.32822\" in self . citation_to_csl_item csl_item_1 = self . citation_to_csl_item [ \"doi:10.7554/elife.32822\" ] assert csl_item_1 [ \"title\" ] . startswith ( \"Sci-Hub\" ) assert \"CSL JSON Item was loaded by Manubot\" in csl_item_1 [ \"note\" ] assert \"manual_reference_filename: bibliography.json\" in csl_item_1 [ \"note\" ] assert \"standard_id: doi:10.7554/elife.32822\" in csl_item_1 [ \"note\" ] def test_csl_item_2 ( self ): # raw id corresponding to bibliography.bib assert \"noauthor_techblog:_nodate\" in self . citation_to_csl_item csl_item_2 = self . citation_to_csl_item [ \"noauthor_techblog:_nodate\" ] assert csl_item_2 [ \"title\" ] . startswith ( \"TechBlog\" ) assert \"manual_reference_filename: bibliography.bib\" in csl_item_2 [ \"note\" ] assert \"standard_id: noauthor_techblog:_nodate\" in csl_item_2 [ \"note\" ] def test_csl_item_3 ( self ): assert \"Beaulieu-Jones2017\" in self . citation_to_csl_item csl_item_3 = self . citation_to_csl_item [ \"Beaulieu-Jones2017\" ] assert csl_item_3 [ \"author\" ][ 0 ][ \"family\" ] == \"Beaulieu-Jones\" assert \"manual_reference_filename: bibliography.nbib\" in csl_item_3 [ \"note\" ] assert \"standard_id: Beaulieu-Jones2017\" in csl_item_3 [ \"note\" ]","title":"Module manubot.process.tests.test_bibliography"},{"location":"reference/manubot/process/tests/test_bibliography/#variables","text":"bibliography_paths","title":"Variables"},{"location":"reference/manubot/process/tests/test_bibliography/#classes","text":"","title":"Classes"},{"location":"reference/manubot/process/tests/test_bibliography/#test_load_manual_references","text":"class Test_load_manual_references ( / , * args , ** kwargs ) Tests loading multiple bibliography paths View Source class Test_load_manual_references: \"\"\" Tests loading multiple bibliography paths \"\"\" def setup_method ( self ): self . citation_to_csl_item = load_manual_references ( bibliography_paths ) print ( list ( self . citation_to_csl_item )) def test_csl_item_1 ( self ): assert \"doi:10.7554/elife.32822\" in self . citation_to_csl_item csl_item_1 = self . citation_to_csl_item [ \"doi:10.7554/elife.32822\" ] assert csl_item_1 [ \"title\" ]. startswith ( \"Sci-Hub\" ) assert \"CSL JSON Item was loaded by Manubot\" in csl_item_1 [ \"note\" ] assert \"manual_reference_filename: bibliography.json\" in csl_item_1 [ \"note\" ] assert \"standard_id: doi:10.7554/elife.32822\" in csl_item_1 [ \"note\" ] def test_csl_item_2 ( self ): # raw id corresponding to bibliography.bib assert \"noauthor_techblog:_nodate\" in self . citation_to_csl_item csl_item_2 = self . citation_to_csl_item [ \"noauthor_techblog:_nodate\" ] assert csl_item_2 [ \"title\" ]. startswith ( \"TechBlog\" ) assert \"manual_reference_filename: bibliography.bib\" in csl_item_2 [ \"note\" ] assert \"standard_id: noauthor_techblog:_nodate\" in csl_item_2 [ \"note\" ] def test_csl_item_3 ( self ): assert \"Beaulieu-Jones2017\" in self . citation_to_csl_item csl_item_3 = self . citation_to_csl_item [ \"Beaulieu-Jones2017\" ] assert csl_item_3 [ \"author\" ][ 0 ][ \"family\" ] == \"Beaulieu-Jones\" assert \"manual_reference_filename: bibliography.nbib\" in csl_item_3 [ \"note\" ] assert \"standard_id: Beaulieu-Jones2017\" in csl_item_3 [ \"note\" ]","title":"Test_load_manual_references"},{"location":"reference/manubot/process/tests/test_bibliography/#class-variables","text":"pytestmark","title":"Class variables"},{"location":"reference/manubot/process/tests/test_bibliography/#methods","text":"","title":"Methods"},{"location":"reference/manubot/process/tests/test_bibliography/#setup_method","text":"def setup_method ( self ) View Source def setup_method ( self ): self . citation_to_csl_item = load_manual_references ( bibliography_paths ) print ( list ( self . citation_to_csl_item ))","title":"setup_method"},{"location":"reference/manubot/process/tests/test_bibliography/#test_csl_item_1","text":"def test_csl_item_1 ( self ) View Source def test_csl_item_1 ( self ): assert \"doi:10.7554/elife.32822\" in self . citation_to_csl_item csl_item_1 = self . citation_to_csl_item [ \"doi:10.7554/elife.32822\" ] assert csl_item_1 [ \"title\" ]. startswith ( \"Sci-Hub\" ) assert \"CSL JSON Item was loaded by Manubot\" in csl_item_1 [ \"note\" ] assert \"manual_reference_filename: bibliography.json\" in csl_item_1 [ \"note\" ] assert \"standard_id: doi:10.7554/elife.32822\" in csl_item_1 [ \"note\" ]","title":"test_csl_item_1"},{"location":"reference/manubot/process/tests/test_bibliography/#test_csl_item_2","text":"def test_csl_item_2 ( self ) View Source def test_csl_item_2 ( self ): # raw id corresponding to bibliography . bib assert \"noauthor_techblog:_nodate\" in self . citation_to_csl_item csl_item_2 = self . citation_to_csl_item [ \"noauthor_techblog:_nodate\" ] assert csl_item_2 [ \"title\" ]. startswith ( \"TechBlog\" ) assert \"manual_reference_filename: bibliography.bib\" in csl_item_2 [ \"note\" ] assert \"standard_id: noauthor_techblog:_nodate\" in csl_item_2 [ \"note\" ]","title":"test_csl_item_2"},{"location":"reference/manubot/process/tests/test_bibliography/#test_csl_item_3","text":"def test_csl_item_3 ( self ) View Source def test_csl_item_3 ( self ): assert \"Beaulieu-Jones2017\" in self . citation_to_csl_item csl_item_3 = self . citation_to_csl_item [ \"Beaulieu-Jones2017\" ] assert csl_item_3 [ \"author\" ][ 0 ][ \"family\" ] == \"Beaulieu-Jones\" assert \"manual_reference_filename: bibliography.nbib\" in csl_item_3 [ \"note\" ] assert \"standard_id: Beaulieu-Jones2017\" in csl_item_3 [ \"note\" ]","title":"test_csl_item_3"},{"location":"reference/manubot/process/tests/test_ci/","text":"Module manubot.process.tests.test_ci View Source import os import re import pytest from ..ci import get_continuous_integration_parameters @pytest.mark.skipif ( \"GITHUB_ACTION\" not in os . environ , reason = \"tests environment variables set by GitHub Actions only\" , ) @pytest.mark.skipif ( os . getenv ( \"GITHUB_REPOSITORY\" ) != \"manubot/manubot\" , reason = \"test fails on forks\" ) def test_get_continuous_integration_parameters_github (): info = get_continuous_integration_parameters () assert info is not None assert info [ \"provider\" ] == \"github\" assert info [ \"repo_slug\" ] == \"manubot/manubot\" assert info [ \"repo_owner\" ] == \"manubot\" assert info [ \"repo_name\" ] == \"manubot\" assert info [ \"commit\" ] assert info [ \"triggering_commit\" ] assert info [ \"build_url\" ] . startswith ( \"https://github.com/manubot/manubot/commit/\" ) assert info [ \"job_url\" ] . startswith ( \"https://github.com/manubot/manubot/runs/\" ) @pytest.mark.skipif ( \"TRAVIS\" not in os . environ , reason = \"tests environment variables set by Travis builds only\" , ) @pytest.mark.skipif ( os . getenv ( \"TRAVIS_REPO_SLUG\" ) != \"manubot/manubot\" , reason = \"test fails on forks\" ) def test_get_continuous_integration_parameters_travis (): info = get_continuous_integration_parameters () assert info is not None assert info [ \"provider\" ] == \"travis\" assert info [ \"repo_slug\" ] == \"manubot/manubot\" assert info [ \"repo_owner\" ] == \"manubot\" assert info [ \"repo_name\" ] == \"manubot\" assert info [ \"commit\" ] assert info [ \"triggering_commit\" ] assert info [ \"build_url\" ] . startswith ( \"https://travis-ci.com/manubot/manubot/builds/\" ) assert info [ \"job_url\" ] . startswith ( \"https://travis-ci.com/manubot/manubot/jobs/\" ) @pytest.mark.skipif ( \"APPVEYOR\" not in os . environ , reason = \"tests environment variables set by AppVeyor builds only\" , ) @pytest.mark.skipif ( os . getenv ( \"APPVEYOR_REPO_NAME\" ) != \"manubot/manubot\" , reason = \"test fails on forks\" ) def test_get_continuous_integration_parameters_appveyor (): info = get_continuous_integration_parameters () assert info is not None assert info [ \"provider\" ] == \"appveyor\" assert info [ \"provider_account\" ] == \"manubot\" assert info [ \"repo_slug\" ] == \"manubot/manubot\" assert info [ \"repo_owner\" ] == \"manubot\" assert info [ \"repo_name\" ] == \"manubot\" assert info [ \"commit\" ] assert info [ \"triggering_commit\" ] assert info [ \"build_url\" ] . startswith ( \"https://ci.appveyor.com/project/manubot/manubot/builds/\" ) assert info [ \"job_url\" ] . startswith ( \"https://ci.appveyor.com/project/manubot/manubot/build/job/\" ) assert re . fullmatch ( pattern = r \"https://ci\\.appveyor\\.com/project/manubot/manubot/builds/[0-9]+/artifacts\" , string = info [ \"artifact_url\" ], ) Functions test_get_continuous_integration_parameters_appveyor def test_get_continuous_integration_parameters_appveyor ( ) View Source @pytest . mark . skipif ( \"APPVEYOR\" not in os . environ , reason = \"tests environment variables set by AppVeyor builds only\" , ) @pytest . mark . skipif ( os . getenv ( \"APPVEYOR_REPO_NAME\" ) != \"manubot/manubot\" , reason = \"test fails on forks\" ) def test_get_continuous_integration_parameters_appveyor () : info = get_continuous_integration_parameters () assert info is not None assert info [ \"provider\" ] == \"appveyor\" assert info [ \"provider_account\" ] == \"manubot\" assert info [ \"repo_slug\" ] == \"manubot/manubot\" assert info [ \"repo_owner\" ] == \"manubot\" assert info [ \"repo_name\" ] == \"manubot\" assert info [ \"commit\" ] assert info [ \"triggering_commit\" ] assert info [ \"build_url\" ] . startswith ( \"https://ci.appveyor.com/project/manubot/manubot/builds/\" ) assert info [ \"job_url\" ] . startswith ( \"https://ci.appveyor.com/project/manubot/manubot/build/job/\" ) assert re . fullmatch ( pattern = r \"https://ci\\.appveyor\\.com/project/manubot/manubot/builds/[0-9]+/artifacts\" , string = info [ \"artifact_url\" ] , ) test_get_continuous_integration_parameters_github def test_get_continuous_integration_parameters_github ( ) View Source @pytest . mark . skipif ( \"GITHUB_ACTION\" not in os . environ , reason = \"tests environment variables set by GitHub Actions only\" , ) @pytest . mark . skipif ( os . getenv ( \"GITHUB_REPOSITORY\" ) != \"manubot/manubot\" , reason = \"test fails on forks\" ) def test_get_continuous_integration_parameters_github () : info = get_continuous_integration_parameters () assert info is not None assert info [ \"provider\" ] == \"github\" assert info [ \"repo_slug\" ] == \"manubot/manubot\" assert info [ \"repo_owner\" ] == \"manubot\" assert info [ \"repo_name\" ] == \"manubot\" assert info [ \"commit\" ] assert info [ \"triggering_commit\" ] assert info [ \"build_url\" ] . startswith ( \"https://github.com/manubot/manubot/commit/\" ) assert info [ \"job_url\" ] . startswith ( \"https://github.com/manubot/manubot/runs/\" ) test_get_continuous_integration_parameters_travis def test_get_continuous_integration_parameters_travis ( ) View Source @pytest . mark . skipif ( \"TRAVIS\" not in os . environ , reason = \"tests environment variables set by Travis builds only\" , ) @pytest . mark . skipif ( os . getenv ( \"TRAVIS_REPO_SLUG\" ) != \"manubot/manubot\" , reason = \"test fails on forks\" ) def test_get_continuous_integration_parameters_travis () : info = get_continuous_integration_parameters () assert info is not None assert info [ \"provider\" ] == \"travis\" assert info [ \"repo_slug\" ] == \"manubot/manubot\" assert info [ \"repo_owner\" ] == \"manubot\" assert info [ \"repo_name\" ] == \"manubot\" assert info [ \"commit\" ] assert info [ \"triggering_commit\" ] assert info [ \"build_url\" ] . startswith ( \"https://travis-ci.com/manubot/manubot/builds/\" ) assert info [ \"job_url\" ] . startswith ( \"https://travis-ci.com/manubot/manubot/jobs/\" )","title":"Test Ci"},{"location":"reference/manubot/process/tests/test_ci/#module-manubotprocessteststest_ci","text":"View Source import os import re import pytest from ..ci import get_continuous_integration_parameters @pytest.mark.skipif ( \"GITHUB_ACTION\" not in os . environ , reason = \"tests environment variables set by GitHub Actions only\" , ) @pytest.mark.skipif ( os . getenv ( \"GITHUB_REPOSITORY\" ) != \"manubot/manubot\" , reason = \"test fails on forks\" ) def test_get_continuous_integration_parameters_github (): info = get_continuous_integration_parameters () assert info is not None assert info [ \"provider\" ] == \"github\" assert info [ \"repo_slug\" ] == \"manubot/manubot\" assert info [ \"repo_owner\" ] == \"manubot\" assert info [ \"repo_name\" ] == \"manubot\" assert info [ \"commit\" ] assert info [ \"triggering_commit\" ] assert info [ \"build_url\" ] . startswith ( \"https://github.com/manubot/manubot/commit/\" ) assert info [ \"job_url\" ] . startswith ( \"https://github.com/manubot/manubot/runs/\" ) @pytest.mark.skipif ( \"TRAVIS\" not in os . environ , reason = \"tests environment variables set by Travis builds only\" , ) @pytest.mark.skipif ( os . getenv ( \"TRAVIS_REPO_SLUG\" ) != \"manubot/manubot\" , reason = \"test fails on forks\" ) def test_get_continuous_integration_parameters_travis (): info = get_continuous_integration_parameters () assert info is not None assert info [ \"provider\" ] == \"travis\" assert info [ \"repo_slug\" ] == \"manubot/manubot\" assert info [ \"repo_owner\" ] == \"manubot\" assert info [ \"repo_name\" ] == \"manubot\" assert info [ \"commit\" ] assert info [ \"triggering_commit\" ] assert info [ \"build_url\" ] . startswith ( \"https://travis-ci.com/manubot/manubot/builds/\" ) assert info [ \"job_url\" ] . startswith ( \"https://travis-ci.com/manubot/manubot/jobs/\" ) @pytest.mark.skipif ( \"APPVEYOR\" not in os . environ , reason = \"tests environment variables set by AppVeyor builds only\" , ) @pytest.mark.skipif ( os . getenv ( \"APPVEYOR_REPO_NAME\" ) != \"manubot/manubot\" , reason = \"test fails on forks\" ) def test_get_continuous_integration_parameters_appveyor (): info = get_continuous_integration_parameters () assert info is not None assert info [ \"provider\" ] == \"appveyor\" assert info [ \"provider_account\" ] == \"manubot\" assert info [ \"repo_slug\" ] == \"manubot/manubot\" assert info [ \"repo_owner\" ] == \"manubot\" assert info [ \"repo_name\" ] == \"manubot\" assert info [ \"commit\" ] assert info [ \"triggering_commit\" ] assert info [ \"build_url\" ] . startswith ( \"https://ci.appveyor.com/project/manubot/manubot/builds/\" ) assert info [ \"job_url\" ] . startswith ( \"https://ci.appveyor.com/project/manubot/manubot/build/job/\" ) assert re . fullmatch ( pattern = r \"https://ci\\.appveyor\\.com/project/manubot/manubot/builds/[0-9]+/artifacts\" , string = info [ \"artifact_url\" ], )","title":"Module manubot.process.tests.test_ci"},{"location":"reference/manubot/process/tests/test_ci/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/tests/test_ci/#test_get_continuous_integration_parameters_appveyor","text":"def test_get_continuous_integration_parameters_appveyor ( ) View Source @pytest . mark . skipif ( \"APPVEYOR\" not in os . environ , reason = \"tests environment variables set by AppVeyor builds only\" , ) @pytest . mark . skipif ( os . getenv ( \"APPVEYOR_REPO_NAME\" ) != \"manubot/manubot\" , reason = \"test fails on forks\" ) def test_get_continuous_integration_parameters_appveyor () : info = get_continuous_integration_parameters () assert info is not None assert info [ \"provider\" ] == \"appveyor\" assert info [ \"provider_account\" ] == \"manubot\" assert info [ \"repo_slug\" ] == \"manubot/manubot\" assert info [ \"repo_owner\" ] == \"manubot\" assert info [ \"repo_name\" ] == \"manubot\" assert info [ \"commit\" ] assert info [ \"triggering_commit\" ] assert info [ \"build_url\" ] . startswith ( \"https://ci.appveyor.com/project/manubot/manubot/builds/\" ) assert info [ \"job_url\" ] . startswith ( \"https://ci.appveyor.com/project/manubot/manubot/build/job/\" ) assert re . fullmatch ( pattern = r \"https://ci\\.appveyor\\.com/project/manubot/manubot/builds/[0-9]+/artifacts\" , string = info [ \"artifact_url\" ] , )","title":"test_get_continuous_integration_parameters_appveyor"},{"location":"reference/manubot/process/tests/test_ci/#test_get_continuous_integration_parameters_github","text":"def test_get_continuous_integration_parameters_github ( ) View Source @pytest . mark . skipif ( \"GITHUB_ACTION\" not in os . environ , reason = \"tests environment variables set by GitHub Actions only\" , ) @pytest . mark . skipif ( os . getenv ( \"GITHUB_REPOSITORY\" ) != \"manubot/manubot\" , reason = \"test fails on forks\" ) def test_get_continuous_integration_parameters_github () : info = get_continuous_integration_parameters () assert info is not None assert info [ \"provider\" ] == \"github\" assert info [ \"repo_slug\" ] == \"manubot/manubot\" assert info [ \"repo_owner\" ] == \"manubot\" assert info [ \"repo_name\" ] == \"manubot\" assert info [ \"commit\" ] assert info [ \"triggering_commit\" ] assert info [ \"build_url\" ] . startswith ( \"https://github.com/manubot/manubot/commit/\" ) assert info [ \"job_url\" ] . startswith ( \"https://github.com/manubot/manubot/runs/\" )","title":"test_get_continuous_integration_parameters_github"},{"location":"reference/manubot/process/tests/test_ci/#test_get_continuous_integration_parameters_travis","text":"def test_get_continuous_integration_parameters_travis ( ) View Source @pytest . mark . skipif ( \"TRAVIS\" not in os . environ , reason = \"tests environment variables set by Travis builds only\" , ) @pytest . mark . skipif ( os . getenv ( \"TRAVIS_REPO_SLUG\" ) != \"manubot/manubot\" , reason = \"test fails on forks\" ) def test_get_continuous_integration_parameters_travis () : info = get_continuous_integration_parameters () assert info is not None assert info [ \"provider\" ] == \"travis\" assert info [ \"repo_slug\" ] == \"manubot/manubot\" assert info [ \"repo_owner\" ] == \"manubot\" assert info [ \"repo_name\" ] == \"manubot\" assert info [ \"commit\" ] assert info [ \"triggering_commit\" ] assert info [ \"build_url\" ] . startswith ( \"https://travis-ci.com/manubot/manubot/builds/\" ) assert info [ \"job_url\" ] . startswith ( \"https://travis-ci.com/manubot/manubot/jobs/\" )","title":"test_get_continuous_integration_parameters_travis"},{"location":"reference/manubot/process/tests/test_metadata/","text":"Module manubot.process.tests.test_metadata View Source import copy import pytest from ..metadata import get_header_includes , get_thumbnail_url , get_manuscript_urls from ..ci import get_continuous_integration_parameters def test_get_header_includes_description_abstract (): # test that abstract and description set different fields if both supplied variables = { \"pandoc\" : { \"abstract\" : \"value for abstract\" }, \"manubot\" : { \"description\" : \"value for description\" }, } header_includes = get_header_includes ( copy . deepcopy ( variables )) assert ( '<meta name=\"citation_abstract\" content=\"value for abstract\" />' in header_includes ) assert ( '<meta name=\"description\" content=\"value for description\" />' in header_includes ) # test that abstract is used for description if description is not defined del variables [ \"manubot\" ][ \"description\" ] header_includes = get_header_includes ( copy . deepcopy ( variables )) assert ( '<meta name=\"citation_abstract\" content=\"value for abstract\" />' in header_includes ) assert '<meta name=\"description\" content=\"value for abstract\" />' in header_includes # test that if neither abstract nor description is defined, neither are inserted del variables [ \"pandoc\" ][ \"abstract\" ] header_includes = get_header_includes ( copy . deepcopy ( variables )) assert 'meta name=\"citation_abstract\"' not in header_includes assert 'meta name=\"description\"' not in header_includes ci_params = get_continuous_integration_parameters () or {} local_only = pytest . mark . skipif ( ci_params , reason = \"skipping on local build since test assumes supported CI behavior\" ) ci_only = pytest . mark . skipif ( not ci_params , reason = \"skipping on CI build since test assumes local behavior\" ) repo_raw_url_template = ( f \"https://github.com/{ci_params.get('repo_slug', '')}\" f \"/raw/{ci_params.get('triggering_commit', '')}/\" ) example_thumbnail_url = ( repo_raw_url_template + \"manubot/process/tests/manuscripts/example/thumbnail.png\" ) @pytest.mark.parametrize ( ( \"thumbnail\" , \"expected\" ), [ pytest . param ( None , None , id = \"None-on-local\" , marks = local_only ), pytest . param ( \"\" , None , id = \"empty-on-local\" , marks = local_only ), pytest . param ( None , example_thumbnail_url , id = \"None-on-ci\" , marks = ci_only ), pytest . param ( \"\" , example_thumbnail_url , id = \"empty-on-ci\" , marks = ci_only ), pytest . param ( \"path/to/thumbnail.png\" , None , id = \"local-path-on-local\" , marks = local_only ), pytest . param ( \"path/to/thumbnail.png\" , repo_raw_url_template + \"path/to/thumbnail.png\" , id = \"local-path-on-ci\" , marks = ci_only , ), pytest . param ( \"http://example.com/thumbnail.png\" , \"http://example.com/thumbnail.png\" , id = \"url-http\" , ), pytest . param ( \"https://example.com/thumbnail.png\" , \"https://example.com/thumbnail.png\" , id = \"url-https\" , ), ], ) def test_get_thumbnail_url ( thumbnail , expected ): thumbnail_url = get_thumbnail_url ( thumbnail ) assert expected == thumbnail_url @pytest.mark.parametrize ( ( \"html_url\" , \"expected\" ), [ pytest . param ( None , {}, id = \"html_url-none-local\" , marks = local_only ), pytest . param ( \"https://example.com/manuscript/\" , { \"html_url\" : \"https://example.com/manuscript/\" , \"pdf_url\" : \"https://example.com/manuscript/manuscript.pdf\" , }, id = \"html_url-set-local\" , marks = local_only , ), pytest . param ( None , { \"html_url\" : \"https://manubot.github.io/manubot/\" , \"pdf_url\" : \"https://manubot.github.io/manubot/manuscript.pdf\" , \"html_url_versioned\" : f \"https://manubot.github.io/manubot/v/{ci_params.get('commit')}/\" , \"pdf_url_versioned\" : f \"https://manubot.github.io/manubot/v/{ci_params.get('commit')}/manuscript.pdf\" , }, id = \"html_url-none-ci\" , marks = ci_only , ), pytest . param ( \"https://example.com/manuscript/\" , { \"html_url\" : \"https://example.com/manuscript/\" , \"pdf_url\" : \"https://example.com/manuscript/manuscript.pdf\" , \"html_url_versioned\" : f \"https://example.com/manuscript/v/{ci_params.get('commit')}/\" , \"pdf_url_versioned\" : f \"https://example.com/manuscript/v/{ci_params.get('commit')}/manuscript.pdf\" , }, id = \"html_url-set-ci\" , marks = ci_only , ), ], ) def test_get_manuscript_urls ( html_url , expected ): urls = get_manuscript_urls ( html_url ) assert urls == expected Variables ci_only ci_params example_thumbnail_url local_only repo_raw_url_template Functions test_get_header_includes_description_abstract def test_get_header_includes_description_abstract ( ) View Source def test_get_header_includes_description_abstract (): # test that abstract and description set different fields if both supplied variables = { \"pandoc\" : { \"abstract\" : \"value for abstract\" } , \"manubot\" : { \"description\" : \"value for description\" } , } header_includes = get_header_includes ( copy . deepcopy ( variables )) assert ( '<meta name=\"citation_abstract\" content=\"value for abstract\" />' in header_includes ) assert ( '<meta name=\"description\" content=\"value for description\" />' in header_includes ) # test that abstract is used for description if description is not defined del variables [ \"manubot\" ][ \"description\" ] header_includes = get_header_includes ( copy . deepcopy ( variables )) assert ( '<meta name=\"citation_abstract\" content=\"value for abstract\" />' in header_includes ) assert '<meta name=\"description\" content=\"value for abstract\" />' in header_includes # test that if neither abstract nor description is defined , neither are inserted del variables [ \"pandoc\" ][ \"abstract\" ] header_includes = get_header_includes ( copy . deepcopy ( variables )) assert 'meta name=\"citation_abstract\"' not in header_includes assert 'meta name=\"description\"' not in header_includes test_get_manuscript_urls def test_get_manuscript_urls ( html_url , expected ) View Source @pytest . mark . parametrize ( ( \"html_url\" , \"expected\" ), [ pytest.param(None, {}, id=\"html_url-none-local\", marks=local_only), pytest.param( \"https://example.com/manuscript/\", { \"html_url\": \"https://example.com/manuscript/\", \"pdf_url\": \"https://example.com/manuscript/manuscript.pdf\", }, id=\"html_url-set-local\", marks=local_only, ), pytest.param( None, { \"html_url\": \"https://manubot.github.io/manubot/\", \"pdf_url\": \"https://manubot.github.io/manubot/manuscript.pdf\", \"html_url_versioned\": f\"https://manubot.github.io/manubot/v/{ci_params.get('commit')}/\", \"pdf_url_versioned\": f\"https://manubot.github.io/manubot/v/{ci_params.get('commit')}/manuscript.pdf\", }, id=\"html_url-none-ci\", marks=ci_only, ), pytest.param( \"https://example.com/manuscript/\", { \"html_url\": \"https://example.com/manuscript/\", \"pdf_url\": \"https://example.com/manuscript/manuscript.pdf\", \"html_url_versioned\": f\"https://example.com/manuscript/v/{ci_params.get('commit')}/\", \"pdf_url_versioned\": f\"https://example.com/manuscript/v/{ci_params.get('commit')}/manuscript.pdf\", }, id=\"html_url-set-ci\", marks=ci_only, ), ] , ) def test_get_manuscript_urls ( html_url , expected ) : urls = get_manuscript_urls ( html_url ) assert urls == expected test_get_thumbnail_url def test_get_thumbnail_url ( thumbnail , expected ) View Source @pytest . mark . parametrize ( ( \"thumbnail\" , \"expected\" ), [ pytest.param(None, None, id=\"None-on-local\", marks=local_only), pytest.param(\"\", None, id=\"empty-on-local\", marks=local_only), pytest.param(None, example_thumbnail_url, id=\"None-on-ci\", marks=ci_only), pytest.param(\"\", example_thumbnail_url, id=\"empty-on-ci\", marks=ci_only), pytest.param( \"path/to/thumbnail.png\", None, id=\"local-path-on-local\", marks=local_only ), pytest.param( \"path/to/thumbnail.png\", repo_raw_url_template + \"path/to/thumbnail.png\", id=\"local-path-on-ci\", marks=ci_only, ), pytest.param( \"http://example.com/thumbnail.png\", \"http://example.com/thumbnail.png\", id=\"url-http\", ), pytest.param( \"https://example.com/thumbnail.png\", \"https://example.com/thumbnail.png\", id=\"url-https\", ), ] , ) def test_get_thumbnail_url ( thumbnail , expected ) : thumbnail_url = get_thumbnail_url ( thumbnail ) assert expected == thumbnail_url","title":"Test Metadata"},{"location":"reference/manubot/process/tests/test_metadata/#module-manubotprocessteststest_metadata","text":"View Source import copy import pytest from ..metadata import get_header_includes , get_thumbnail_url , get_manuscript_urls from ..ci import get_continuous_integration_parameters def test_get_header_includes_description_abstract (): # test that abstract and description set different fields if both supplied variables = { \"pandoc\" : { \"abstract\" : \"value for abstract\" }, \"manubot\" : { \"description\" : \"value for description\" }, } header_includes = get_header_includes ( copy . deepcopy ( variables )) assert ( '<meta name=\"citation_abstract\" content=\"value for abstract\" />' in header_includes ) assert ( '<meta name=\"description\" content=\"value for description\" />' in header_includes ) # test that abstract is used for description if description is not defined del variables [ \"manubot\" ][ \"description\" ] header_includes = get_header_includes ( copy . deepcopy ( variables )) assert ( '<meta name=\"citation_abstract\" content=\"value for abstract\" />' in header_includes ) assert '<meta name=\"description\" content=\"value for abstract\" />' in header_includes # test that if neither abstract nor description is defined, neither are inserted del variables [ \"pandoc\" ][ \"abstract\" ] header_includes = get_header_includes ( copy . deepcopy ( variables )) assert 'meta name=\"citation_abstract\"' not in header_includes assert 'meta name=\"description\"' not in header_includes ci_params = get_continuous_integration_parameters () or {} local_only = pytest . mark . skipif ( ci_params , reason = \"skipping on local build since test assumes supported CI behavior\" ) ci_only = pytest . mark . skipif ( not ci_params , reason = \"skipping on CI build since test assumes local behavior\" ) repo_raw_url_template = ( f \"https://github.com/{ci_params.get('repo_slug', '')}\" f \"/raw/{ci_params.get('triggering_commit', '')}/\" ) example_thumbnail_url = ( repo_raw_url_template + \"manubot/process/tests/manuscripts/example/thumbnail.png\" ) @pytest.mark.parametrize ( ( \"thumbnail\" , \"expected\" ), [ pytest . param ( None , None , id = \"None-on-local\" , marks = local_only ), pytest . param ( \"\" , None , id = \"empty-on-local\" , marks = local_only ), pytest . param ( None , example_thumbnail_url , id = \"None-on-ci\" , marks = ci_only ), pytest . param ( \"\" , example_thumbnail_url , id = \"empty-on-ci\" , marks = ci_only ), pytest . param ( \"path/to/thumbnail.png\" , None , id = \"local-path-on-local\" , marks = local_only ), pytest . param ( \"path/to/thumbnail.png\" , repo_raw_url_template + \"path/to/thumbnail.png\" , id = \"local-path-on-ci\" , marks = ci_only , ), pytest . param ( \"http://example.com/thumbnail.png\" , \"http://example.com/thumbnail.png\" , id = \"url-http\" , ), pytest . param ( \"https://example.com/thumbnail.png\" , \"https://example.com/thumbnail.png\" , id = \"url-https\" , ), ], ) def test_get_thumbnail_url ( thumbnail , expected ): thumbnail_url = get_thumbnail_url ( thumbnail ) assert expected == thumbnail_url @pytest.mark.parametrize ( ( \"html_url\" , \"expected\" ), [ pytest . param ( None , {}, id = \"html_url-none-local\" , marks = local_only ), pytest . param ( \"https://example.com/manuscript/\" , { \"html_url\" : \"https://example.com/manuscript/\" , \"pdf_url\" : \"https://example.com/manuscript/manuscript.pdf\" , }, id = \"html_url-set-local\" , marks = local_only , ), pytest . param ( None , { \"html_url\" : \"https://manubot.github.io/manubot/\" , \"pdf_url\" : \"https://manubot.github.io/manubot/manuscript.pdf\" , \"html_url_versioned\" : f \"https://manubot.github.io/manubot/v/{ci_params.get('commit')}/\" , \"pdf_url_versioned\" : f \"https://manubot.github.io/manubot/v/{ci_params.get('commit')}/manuscript.pdf\" , }, id = \"html_url-none-ci\" , marks = ci_only , ), pytest . param ( \"https://example.com/manuscript/\" , { \"html_url\" : \"https://example.com/manuscript/\" , \"pdf_url\" : \"https://example.com/manuscript/manuscript.pdf\" , \"html_url_versioned\" : f \"https://example.com/manuscript/v/{ci_params.get('commit')}/\" , \"pdf_url_versioned\" : f \"https://example.com/manuscript/v/{ci_params.get('commit')}/manuscript.pdf\" , }, id = \"html_url-set-ci\" , marks = ci_only , ), ], ) def test_get_manuscript_urls ( html_url , expected ): urls = get_manuscript_urls ( html_url ) assert urls == expected","title":"Module manubot.process.tests.test_metadata"},{"location":"reference/manubot/process/tests/test_metadata/#variables","text":"ci_only ci_params example_thumbnail_url local_only repo_raw_url_template","title":"Variables"},{"location":"reference/manubot/process/tests/test_metadata/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/tests/test_metadata/#test_get_header_includes_description_abstract","text":"def test_get_header_includes_description_abstract ( ) View Source def test_get_header_includes_description_abstract (): # test that abstract and description set different fields if both supplied variables = { \"pandoc\" : { \"abstract\" : \"value for abstract\" } , \"manubot\" : { \"description\" : \"value for description\" } , } header_includes = get_header_includes ( copy . deepcopy ( variables )) assert ( '<meta name=\"citation_abstract\" content=\"value for abstract\" />' in header_includes ) assert ( '<meta name=\"description\" content=\"value for description\" />' in header_includes ) # test that abstract is used for description if description is not defined del variables [ \"manubot\" ][ \"description\" ] header_includes = get_header_includes ( copy . deepcopy ( variables )) assert ( '<meta name=\"citation_abstract\" content=\"value for abstract\" />' in header_includes ) assert '<meta name=\"description\" content=\"value for abstract\" />' in header_includes # test that if neither abstract nor description is defined , neither are inserted del variables [ \"pandoc\" ][ \"abstract\" ] header_includes = get_header_includes ( copy . deepcopy ( variables )) assert 'meta name=\"citation_abstract\"' not in header_includes assert 'meta name=\"description\"' not in header_includes","title":"test_get_header_includes_description_abstract"},{"location":"reference/manubot/process/tests/test_metadata/#test_get_manuscript_urls","text":"def test_get_manuscript_urls ( html_url , expected ) View Source @pytest . mark . parametrize ( ( \"html_url\" , \"expected\" ), [ pytest.param(None, {}, id=\"html_url-none-local\", marks=local_only), pytest.param( \"https://example.com/manuscript/\", { \"html_url\": \"https://example.com/manuscript/\", \"pdf_url\": \"https://example.com/manuscript/manuscript.pdf\", }, id=\"html_url-set-local\", marks=local_only, ), pytest.param( None, { \"html_url\": \"https://manubot.github.io/manubot/\", \"pdf_url\": \"https://manubot.github.io/manubot/manuscript.pdf\", \"html_url_versioned\": f\"https://manubot.github.io/manubot/v/{ci_params.get('commit')}/\", \"pdf_url_versioned\": f\"https://manubot.github.io/manubot/v/{ci_params.get('commit')}/manuscript.pdf\", }, id=\"html_url-none-ci\", marks=ci_only, ), pytest.param( \"https://example.com/manuscript/\", { \"html_url\": \"https://example.com/manuscript/\", \"pdf_url\": \"https://example.com/manuscript/manuscript.pdf\", \"html_url_versioned\": f\"https://example.com/manuscript/v/{ci_params.get('commit')}/\", \"pdf_url_versioned\": f\"https://example.com/manuscript/v/{ci_params.get('commit')}/manuscript.pdf\", }, id=\"html_url-set-ci\", marks=ci_only, ), ] , ) def test_get_manuscript_urls ( html_url , expected ) : urls = get_manuscript_urls ( html_url ) assert urls == expected","title":"test_get_manuscript_urls"},{"location":"reference/manubot/process/tests/test_metadata/#test_get_thumbnail_url","text":"def test_get_thumbnail_url ( thumbnail , expected ) View Source @pytest . mark . parametrize ( ( \"thumbnail\" , \"expected\" ), [ pytest.param(None, None, id=\"None-on-local\", marks=local_only), pytest.param(\"\", None, id=\"empty-on-local\", marks=local_only), pytest.param(None, example_thumbnail_url, id=\"None-on-ci\", marks=ci_only), pytest.param(\"\", example_thumbnail_url, id=\"empty-on-ci\", marks=ci_only), pytest.param( \"path/to/thumbnail.png\", None, id=\"local-path-on-local\", marks=local_only ), pytest.param( \"path/to/thumbnail.png\", repo_raw_url_template + \"path/to/thumbnail.png\", id=\"local-path-on-ci\", marks=ci_only, ), pytest.param( \"http://example.com/thumbnail.png\", \"http://example.com/thumbnail.png\", id=\"url-http\", ), pytest.param( \"https://example.com/thumbnail.png\", \"https://example.com/thumbnail.png\", id=\"url-https\", ), ] , ) def test_get_thumbnail_url ( thumbnail , expected ) : thumbnail_url = get_thumbnail_url ( thumbnail ) assert expected == thumbnail_url","title":"test_get_thumbnail_url"},{"location":"reference/manubot/process/tests/test_process_command/","text":"Module manubot.process.tests.test_process_command View Source import pathlib import subprocess import pytest from manubot.util import shlex_join directory = pathlib . Path ( __file__ ) . parent . resolve () # List of manuscripts for testing. All subdirectories of ./manuscripts manuscripts = [ path . name for path in directory . joinpath ( \"manuscripts\" ) . iterdir () if path . is_dir () ] @pytest.mark.parametrize ( \"manuscript\" , manuscripts ) def test_example_manuscript ( manuscript ): \"\"\" Test command line execution of manubot to build an example manuscript. \"\"\" manuscript_dir = directory . joinpath ( \"manuscripts\" , manuscript ) args = [ \"manubot\" , \"process\" , \"--log-level\" , \"INFO\" , \"--skip-citations\" , \"--content-directory\" , str ( manuscript_dir . joinpath ( \"content\" )), \"--output-directory\" , str ( manuscript_dir . joinpath ( \"output\" )), ] if manuscript == \"variables\" : args . extend ( [ \"--template-variables-path\" , str ( manuscript_dir . joinpath ( \"content/template-variables.json\" )), ] ) process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" ) print ( shlex_join ( process . args )) print ( process . stderr ) assert process . returncode == 0 Variables directory manuscripts Functions test_example_manuscript def test_example_manuscript ( manuscript ) Test command line execution of manubot to build an example manuscript. View Source @pytest . mark . parametrize ( \"manuscript\" , manuscripts ) def test_example_manuscript ( manuscript ) : \"\"\" Test command line execution of manubot to build an example manuscript. \"\"\" manuscript_dir = directory . joinpath ( \"manuscripts\" , manuscript ) args = [ \"manubot\", \"process\", \"--log-level\", \"INFO\", \"--skip-citations\", \"--content-directory\", str(manuscript_dir.joinpath(\"content\")), \"--output-directory\", str(manuscript_dir.joinpath(\"output\")), ] if manuscript == \"variables\" : args . extend ( [ \"--template-variables-path\", str(manuscript_dir.joinpath(\"content/template-variables.json\")), ] ) process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" ) print ( shlex_join ( process . args )) print ( process . stderr ) assert process . returncode == 0","title":"Test Process Command"},{"location":"reference/manubot/process/tests/test_process_command/#module-manubotprocessteststest_process_command","text":"View Source import pathlib import subprocess import pytest from manubot.util import shlex_join directory = pathlib . Path ( __file__ ) . parent . resolve () # List of manuscripts for testing. All subdirectories of ./manuscripts manuscripts = [ path . name for path in directory . joinpath ( \"manuscripts\" ) . iterdir () if path . is_dir () ] @pytest.mark.parametrize ( \"manuscript\" , manuscripts ) def test_example_manuscript ( manuscript ): \"\"\" Test command line execution of manubot to build an example manuscript. \"\"\" manuscript_dir = directory . joinpath ( \"manuscripts\" , manuscript ) args = [ \"manubot\" , \"process\" , \"--log-level\" , \"INFO\" , \"--skip-citations\" , \"--content-directory\" , str ( manuscript_dir . joinpath ( \"content\" )), \"--output-directory\" , str ( manuscript_dir . joinpath ( \"output\" )), ] if manuscript == \"variables\" : args . extend ( [ \"--template-variables-path\" , str ( manuscript_dir . joinpath ( \"content/template-variables.json\" )), ] ) process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" ) print ( shlex_join ( process . args )) print ( process . stderr ) assert process . returncode == 0","title":"Module manubot.process.tests.test_process_command"},{"location":"reference/manubot/process/tests/test_process_command/#variables","text":"directory manuscripts","title":"Variables"},{"location":"reference/manubot/process/tests/test_process_command/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/tests/test_process_command/#test_example_manuscript","text":"def test_example_manuscript ( manuscript ) Test command line execution of manubot to build an example manuscript. View Source @pytest . mark . parametrize ( \"manuscript\" , manuscripts ) def test_example_manuscript ( manuscript ) : \"\"\" Test command line execution of manubot to build an example manuscript. \"\"\" manuscript_dir = directory . joinpath ( \"manuscripts\" , manuscript ) args = [ \"manubot\", \"process\", \"--log-level\", \"INFO\", \"--skip-citations\", \"--content-directory\", str(manuscript_dir.joinpath(\"content\")), \"--output-directory\", str(manuscript_dir.joinpath(\"output\")), ] if manuscript == \"variables\" : args . extend ( [ \"--template-variables-path\", str(manuscript_dir.joinpath(\"content/template-variables.json\")), ] ) process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" ) print ( shlex_join ( process . args )) print ( process . stderr ) assert process . returncode == 0","title":"test_example_manuscript"},{"location":"reference/manubot/process/tests/test_util/","text":"Module manubot.process.tests.test_util View Source import pathlib import pytest from ..util import add_author_affiliations , read_variable_files directory = pathlib . Path ( __file__ ) . parent . resolve () def test_read_variable_files_empty (): paths = [] user_variables = read_variable_files ( paths ) assert isinstance ( user_variables , dict ) and not user_variables def test_read_variable_files (): \"\"\" Test reading multiple JSON files, from both local paths and URLs. \"\"\" local_path = \"manuscripts/variables/content/template-variables.json\" local_path = directory . joinpath ( local_path ) paths = [ \"https://git.io/vbkqm\" , \"https://git.io/vbkqm\" , \"namespace_1=https://git.io/vbkqm\" , \"namespace_2=https://git.io/vbkqm\" , f \"namespace_2={local_path}\" , f \"namespace_3={local_path}\" , ] user_variables = read_variable_files ( paths ) assert \"namespace_1\" in user_variables assert \"namespace_2\" in user_variables assert \"namespace_3\" in user_variables assert user_variables [ \"generated_by\" ] == \"Manubot\" assert \"violet\" in user_variables [ \"namespace_1\" ][ \"rainbow\" ] assert \"yellow\" in user_variables [ \"namespace_2\" ][ \"rainbow\" ] assert \"orange\" in user_variables [ \"namespace_3\" ][ \"rainbow\" ] def test_add_author_affiliations_empty (): variables = {} variables [ \"authors\" ] = [{ \"name\" : \"Jane Roe\" }, { \"name\" : \"John Doe\" }] returned_variables = add_author_affiliations ( variables ) assert variables is returned_variables assert \"affiliations\" not in variables for author in variables [ \"authors\" ]: assert \"affiliation_numbers\" not in author def test_add_author_affiliations (): variables = {} variables [ \"authors\" ] = [ # Deprecated affiliations format (as a string that's `; ` separated) { \"name\" : \"Jane Roe\" , \"affiliations\" : \"Department of Doe, University of Roe; Peppertea University\" , }, # Prefered affiliations format as a list { \"name\" : \"John Doe\" , \"affiliations\" : [ \"Unique University\" , \"Peppertea University\" ], }, ] with pytest . warns ( DeprecationWarning ): returned_variables = add_author_affiliations ( variables ) assert variables is returned_variables assert variables [ \"affiliations\" ] == [ { \"affiliation\" : \"Department of Doe, University of Roe\" , \"affiliation_number\" : 1 , }, { \"affiliation\" : \"Peppertea University\" , \"affiliation_number\" : 2 }, { \"affiliation\" : \"Unique University\" , \"affiliation_number\" : 3 }, ] authors = variables [ \"authors\" ] assert authors [ 0 ][ \"affiliations\" ] == [ \"Department of Doe, University of Roe\" , \"Peppertea University\" , ] assert authors [ 0 ][ \"affiliation_numbers\" ] == [ 1 , 2 ] assert authors [ 1 ][ \"affiliation_numbers\" ] == [ 2 , 3 ] Variables directory Functions test_add_author_affiliations def test_add_author_affiliations ( ) View Source def test_add_author_affiliations (): variables = {} variables [ \"authors\" ] = [ # Deprecated affiliations format ( as a string that ' s ` ; ` separated ) { \"name\" : \"Jane Roe\" , \"affiliations\" : \"Department of Doe, University of Roe; Peppertea University\" , } , # Prefered affiliations format as a list { \"name\" : \"John Doe\" , \"affiliations\" : [ \"Unique University\" , \"Peppertea University\" ], } , ] with pytest . warns ( DeprecationWarning ): returned_variables = add_author_affiliations ( variables ) assert variables is returned_variables assert variables [ \"affiliations\" ] == [ { \"affiliation\" : \"Department of Doe, University of Roe\" , \"affiliation_number\" : 1 , } , { \"affiliation\" : \"Peppertea University\" , \"affiliation_number\" : 2 } , { \"affiliation\" : \"Unique University\" , \"affiliation_number\" : 3 } , ] authors = variables [ \"authors\" ] assert authors [ 0 ][ \"affiliations\" ] == [ \"Department of Doe, University of Roe\" , \"Peppertea University\" , ] assert authors [ 0 ][ \"affiliation_numbers\" ] == [ 1 , 2 ] assert authors [ 1 ][ \"affiliation_numbers\" ] == [ 2 , 3 ] test_add_author_affiliations_empty def test_add_author_affiliations_empty ( ) View Source def test_add_author_affiliations_empty (): variables = {} variables [ \"authors\" ] = [ { \"name\" : \"Jane Roe\" } , { \"name\" : \"John Doe\" } ] returned_variables = add_author_affiliations ( variables ) assert variables is returned_variables assert \"affiliations\" not in variables for author in variables [ \"authors\" ]: assert \"affiliation_numbers\" not in author test_read_variable_files def test_read_variable_files ( ) Test reading multiple JSON files, from both local paths and URLs. View Source def test_read_variable_files (): \"\"\" Test reading multiple JSON files, from both local paths and URLs. \"\"\" local_path = \"manuscripts/variables/content/template-variables.json\" local_path = directory . joinpath ( local_path ) paths = [ \"https://git.io/vbkqm\" , \"https://git.io/vbkqm\" , \"namespace_1=https://git.io/vbkqm\" , \"namespace_2=https://git.io/vbkqm\" , f \"namespace_2={local_path}\" , f \"namespace_3={local_path}\" , ] user_variables = read_variable_files ( paths ) assert \"namespace_1\" in user_variables assert \"namespace_2\" in user_variables assert \"namespace_3\" in user_variables assert user_variables [ \"generated_by\" ] == \"Manubot\" assert \"violet\" in user_variables [ \"namespace_1\" ][ \"rainbow\" ] assert \"yellow\" in user_variables [ \"namespace_2\" ][ \"rainbow\" ] assert \"orange\" in user_variables [ \"namespace_3\" ][ \"rainbow\" ] test_read_variable_files_empty def test_read_variable_files_empty ( ) View Source def test_read_variable_files_empty (): paths = [] user_variables = read_variable_files ( paths ) assert isinstance ( user_variables , dict ) and not user_variables","title":"Test Util"},{"location":"reference/manubot/process/tests/test_util/#module-manubotprocessteststest_util","text":"View Source import pathlib import pytest from ..util import add_author_affiliations , read_variable_files directory = pathlib . Path ( __file__ ) . parent . resolve () def test_read_variable_files_empty (): paths = [] user_variables = read_variable_files ( paths ) assert isinstance ( user_variables , dict ) and not user_variables def test_read_variable_files (): \"\"\" Test reading multiple JSON files, from both local paths and URLs. \"\"\" local_path = \"manuscripts/variables/content/template-variables.json\" local_path = directory . joinpath ( local_path ) paths = [ \"https://git.io/vbkqm\" , \"https://git.io/vbkqm\" , \"namespace_1=https://git.io/vbkqm\" , \"namespace_2=https://git.io/vbkqm\" , f \"namespace_2={local_path}\" , f \"namespace_3={local_path}\" , ] user_variables = read_variable_files ( paths ) assert \"namespace_1\" in user_variables assert \"namespace_2\" in user_variables assert \"namespace_3\" in user_variables assert user_variables [ \"generated_by\" ] == \"Manubot\" assert \"violet\" in user_variables [ \"namespace_1\" ][ \"rainbow\" ] assert \"yellow\" in user_variables [ \"namespace_2\" ][ \"rainbow\" ] assert \"orange\" in user_variables [ \"namespace_3\" ][ \"rainbow\" ] def test_add_author_affiliations_empty (): variables = {} variables [ \"authors\" ] = [{ \"name\" : \"Jane Roe\" }, { \"name\" : \"John Doe\" }] returned_variables = add_author_affiliations ( variables ) assert variables is returned_variables assert \"affiliations\" not in variables for author in variables [ \"authors\" ]: assert \"affiliation_numbers\" not in author def test_add_author_affiliations (): variables = {} variables [ \"authors\" ] = [ # Deprecated affiliations format (as a string that's `; ` separated) { \"name\" : \"Jane Roe\" , \"affiliations\" : \"Department of Doe, University of Roe; Peppertea University\" , }, # Prefered affiliations format as a list { \"name\" : \"John Doe\" , \"affiliations\" : [ \"Unique University\" , \"Peppertea University\" ], }, ] with pytest . warns ( DeprecationWarning ): returned_variables = add_author_affiliations ( variables ) assert variables is returned_variables assert variables [ \"affiliations\" ] == [ { \"affiliation\" : \"Department of Doe, University of Roe\" , \"affiliation_number\" : 1 , }, { \"affiliation\" : \"Peppertea University\" , \"affiliation_number\" : 2 }, { \"affiliation\" : \"Unique University\" , \"affiliation_number\" : 3 }, ] authors = variables [ \"authors\" ] assert authors [ 0 ][ \"affiliations\" ] == [ \"Department of Doe, University of Roe\" , \"Peppertea University\" , ] assert authors [ 0 ][ \"affiliation_numbers\" ] == [ 1 , 2 ] assert authors [ 1 ][ \"affiliation_numbers\" ] == [ 2 , 3 ]","title":"Module manubot.process.tests.test_util"},{"location":"reference/manubot/process/tests/test_util/#variables","text":"directory","title":"Variables"},{"location":"reference/manubot/process/tests/test_util/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/tests/test_util/#test_add_author_affiliations","text":"def test_add_author_affiliations ( ) View Source def test_add_author_affiliations (): variables = {} variables [ \"authors\" ] = [ # Deprecated affiliations format ( as a string that ' s ` ; ` separated ) { \"name\" : \"Jane Roe\" , \"affiliations\" : \"Department of Doe, University of Roe; Peppertea University\" , } , # Prefered affiliations format as a list { \"name\" : \"John Doe\" , \"affiliations\" : [ \"Unique University\" , \"Peppertea University\" ], } , ] with pytest . warns ( DeprecationWarning ): returned_variables = add_author_affiliations ( variables ) assert variables is returned_variables assert variables [ \"affiliations\" ] == [ { \"affiliation\" : \"Department of Doe, University of Roe\" , \"affiliation_number\" : 1 , } , { \"affiliation\" : \"Peppertea University\" , \"affiliation_number\" : 2 } , { \"affiliation\" : \"Unique University\" , \"affiliation_number\" : 3 } , ] authors = variables [ \"authors\" ] assert authors [ 0 ][ \"affiliations\" ] == [ \"Department of Doe, University of Roe\" , \"Peppertea University\" , ] assert authors [ 0 ][ \"affiliation_numbers\" ] == [ 1 , 2 ] assert authors [ 1 ][ \"affiliation_numbers\" ] == [ 2 , 3 ]","title":"test_add_author_affiliations"},{"location":"reference/manubot/process/tests/test_util/#test_add_author_affiliations_empty","text":"def test_add_author_affiliations_empty ( ) View Source def test_add_author_affiliations_empty (): variables = {} variables [ \"authors\" ] = [ { \"name\" : \"Jane Roe\" } , { \"name\" : \"John Doe\" } ] returned_variables = add_author_affiliations ( variables ) assert variables is returned_variables assert \"affiliations\" not in variables for author in variables [ \"authors\" ]: assert \"affiliation_numbers\" not in author","title":"test_add_author_affiliations_empty"},{"location":"reference/manubot/process/tests/test_util/#test_read_variable_files","text":"def test_read_variable_files ( ) Test reading multiple JSON files, from both local paths and URLs. View Source def test_read_variable_files (): \"\"\" Test reading multiple JSON files, from both local paths and URLs. \"\"\" local_path = \"manuscripts/variables/content/template-variables.json\" local_path = directory . joinpath ( local_path ) paths = [ \"https://git.io/vbkqm\" , \"https://git.io/vbkqm\" , \"namespace_1=https://git.io/vbkqm\" , \"namespace_2=https://git.io/vbkqm\" , f \"namespace_2={local_path}\" , f \"namespace_3={local_path}\" , ] user_variables = read_variable_files ( paths ) assert \"namespace_1\" in user_variables assert \"namespace_2\" in user_variables assert \"namespace_3\" in user_variables assert user_variables [ \"generated_by\" ] == \"Manubot\" assert \"violet\" in user_variables [ \"namespace_1\" ][ \"rainbow\" ] assert \"yellow\" in user_variables [ \"namespace_2\" ][ \"rainbow\" ] assert \"orange\" in user_variables [ \"namespace_3\" ][ \"rainbow\" ]","title":"test_read_variable_files"},{"location":"reference/manubot/process/tests/test_util/#test_read_variable_files_empty","text":"def test_read_variable_files_empty ( ) View Source def test_read_variable_files_empty (): paths = [] user_variables = read_variable_files ( paths ) assert isinstance ( user_variables , dict ) and not user_variables","title":"test_read_variable_files_empty"},{"location":"reference/manubot/tests/","text":"Module manubot.tests Sub-modules manubot.tests.test_command manubot.tests.test_imports manubot.tests.test_readme manubot.tests.test_util","title":"Index"},{"location":"reference/manubot/tests/#module-manubottests","text":"","title":"Module manubot.tests"},{"location":"reference/manubot/tests/#sub-modules","text":"manubot.tests.test_command manubot.tests.test_imports manubot.tests.test_readme manubot.tests.test_util","title":"Sub-modules"},{"location":"reference/manubot/tests/test_command/","text":"Module manubot.tests.test_command View Source import subprocess import manubot def test_version (): stdout = subprocess . check_output ([ \"manubot\" , \"--version\" ], universal_newlines = True ) version_str = f \"v{manubot.__version__}\" assert version_str == stdout . rstrip () def test_missing_subcommand (): process = subprocess . run ( [ \"manubot\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ) print ( process . stderr ) assert process . returncode == 2 assert \"error: the following arguments are required: subcommand\" in process . stderr Functions test_missing_subcommand def test_missing_subcommand ( ) View Source def test_missing_subcommand (): process = subprocess . run ( [ \"manubot\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ) print ( process . stderr ) assert process . returncode == 2 assert \"error: the following arguments are required: subcommand\" in process . stderr test_version def test_version ( ) View Source def test_version (): stdout = subprocess . check_output ([ \"manubot\" , \"--version\" ], universal_newlines = True ) version_str = f \"v{manubot.__version__}\" assert version_str == stdout . rstrip ()","title":"Test Command"},{"location":"reference/manubot/tests/test_command/#module-manubotteststest_command","text":"View Source import subprocess import manubot def test_version (): stdout = subprocess . check_output ([ \"manubot\" , \"--version\" ], universal_newlines = True ) version_str = f \"v{manubot.__version__}\" assert version_str == stdout . rstrip () def test_missing_subcommand (): process = subprocess . run ( [ \"manubot\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ) print ( process . stderr ) assert process . returncode == 2 assert \"error: the following arguments are required: subcommand\" in process . stderr","title":"Module manubot.tests.test_command"},{"location":"reference/manubot/tests/test_command/#functions","text":"","title":"Functions"},{"location":"reference/manubot/tests/test_command/#test_missing_subcommand","text":"def test_missing_subcommand ( ) View Source def test_missing_subcommand (): process = subprocess . run ( [ \"manubot\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = \"utf-8\" , ) print ( process . stderr ) assert process . returncode == 2 assert \"error: the following arguments are required: subcommand\" in process . stderr","title":"test_missing_subcommand"},{"location":"reference/manubot/tests/test_command/#test_version","text":"def test_version ( ) View Source def test_version (): stdout = subprocess . check_output ([ \"manubot\" , \"--version\" ], universal_newlines = True ) version_str = f \"v{manubot.__version__}\" assert version_str == stdout . rstrip ()","title":"test_version"},{"location":"reference/manubot/tests/test_imports/","text":"Module manubot.tests.test_imports View Source def test_imports (): import manubot.cite import manubot.cite.arxiv import manubot.cite.doi import manubot.cite.pubmed import manubot.cite.url import manubot.process.manuscript manubot Functions test_imports def test_imports ( ) View Source def test_imports (): import manubot.cite import manubot.cite.arxiv import manubot.cite.doi import manubot.cite.pubmed import manubot.cite.url import manubot.process.manuscript manubot","title":"Test Imports"},{"location":"reference/manubot/tests/test_imports/#module-manubotteststest_imports","text":"View Source def test_imports (): import manubot.cite import manubot.cite.arxiv import manubot.cite.doi import manubot.cite.pubmed import manubot.cite.url import manubot.process.manuscript manubot","title":"Module manubot.tests.test_imports"},{"location":"reference/manubot/tests/test_imports/#functions","text":"","title":"Functions"},{"location":"reference/manubot/tests/test_imports/#test_imports","text":"def test_imports ( ) View Source def test_imports (): import manubot.cite import manubot.cite.arxiv import manubot.cite.doi import manubot.cite.pubmed import manubot.cite.url import manubot.process.manuscript manubot","title":"test_imports"},{"location":"reference/manubot/tests/test_readme/","text":"Module manubot.tests.test_readme View Source import pathlib import re import shlex import subprocess import pytest readme_path = pathlib . Path ( __file__ ) . parent . parent . parent / \"README.md\" readme = readme_path . read_text ( encoding = \"utf-8-sig\" ) template = r \"\"\" <!-- test codeblock contains output of `{command}` --> ``` {output}``` \"\"\" pattern = template . format ( command = r \"(?P<command>.+?)\" , output = r \"(?P<output>.+?)\" ) pattern = re . compile ( pattern , re . DOTALL ) matches = list ( pattern . finditer ( readme )) @pytest.mark.parametrize ( argnames = [ \"command\" , \"expected\" ], argvalues = [ match . groups () for match in matches ], ids = [ match . group ( \"command\" ) for match in matches ], ) def test_readme_codeblock_contains_output_from ( command , expected ): \"\"\" If this test fails, ensure that codeblocks in README.md have the correct output. To enable this check for output in a codeblock, use the following construct: <!-- test codeblock contains output of `{command}` --> ``` {expected} ``` \"\"\" output = _get_output_from ( command ) assert output == expected def _get_output_from ( command ): return subprocess . check_output ( shlex . split ( command ), encoding = \"utf-8\" ) def _match_to_repl ( match ): template_dict = match . groupdict () template_dict [ \"output\" ] = _get_output_from ( template_dict [ \"command\" ]) return template . format ( ** template_dict ) if __name__ == \"__main__\" : \"\"\" Run `python manubot/tests/test_readme.py` to populate README codeblocks with output from the specified commands. \"\"\" repl_readme = pattern . sub ( repl = _match_to_repl , string = readme ) readme_path . write_text ( repl_readme , encoding = \"utf-8\" ) Variables matches pattern readme readme_path template Functions test_readme_codeblock_contains_output_from def test_readme_codeblock_contains_output_from ( command , expected ) If this test fails, ensure that codeblocks in README.md have the correct output. To enable this check for output in a codeblock, use the following construct: {expected} View Source @pytest . mark . parametrize ( argnames =[ \"command\", \"expected\" ] , argvalues =[ match.groups() for match in matches ] , ids =[ match.group(\"command\") for match in matches ] , ) def test_readme_codeblock_contains_output_from ( command , expected ) : \"\"\" If this test fails, ensure that codeblocks in README.md have the correct output. To enable this check for output in a codeblock, use the following construct: <!-- test codeblock contains output of `{command}` --> ``` {expected} ``` \"\"\" output = _get_output_from ( command ) assert output == expected","title":"Test Readme"},{"location":"reference/manubot/tests/test_readme/#module-manubotteststest_readme","text":"View Source import pathlib import re import shlex import subprocess import pytest readme_path = pathlib . Path ( __file__ ) . parent . parent . parent / \"README.md\" readme = readme_path . read_text ( encoding = \"utf-8-sig\" ) template = r \"\"\" <!-- test codeblock contains output of `{command}` --> ``` {output}``` \"\"\" pattern = template . format ( command = r \"(?P<command>.+?)\" , output = r \"(?P<output>.+?)\" ) pattern = re . compile ( pattern , re . DOTALL ) matches = list ( pattern . finditer ( readme )) @pytest.mark.parametrize ( argnames = [ \"command\" , \"expected\" ], argvalues = [ match . groups () for match in matches ], ids = [ match . group ( \"command\" ) for match in matches ], ) def test_readme_codeblock_contains_output_from ( command , expected ): \"\"\" If this test fails, ensure that codeblocks in README.md have the correct output. To enable this check for output in a codeblock, use the following construct: <!-- test codeblock contains output of `{command}` --> ``` {expected} ``` \"\"\" output = _get_output_from ( command ) assert output == expected def _get_output_from ( command ): return subprocess . check_output ( shlex . split ( command ), encoding = \"utf-8\" ) def _match_to_repl ( match ): template_dict = match . groupdict () template_dict [ \"output\" ] = _get_output_from ( template_dict [ \"command\" ]) return template . format ( ** template_dict ) if __name__ == \"__main__\" : \"\"\" Run `python manubot/tests/test_readme.py` to populate README codeblocks with output from the specified commands. \"\"\" repl_readme = pattern . sub ( repl = _match_to_repl , string = readme ) readme_path . write_text ( repl_readme , encoding = \"utf-8\" )","title":"Module manubot.tests.test_readme"},{"location":"reference/manubot/tests/test_readme/#variables","text":"matches pattern readme readme_path template","title":"Variables"},{"location":"reference/manubot/tests/test_readme/#functions","text":"","title":"Functions"},{"location":"reference/manubot/tests/test_readme/#test_readme_codeblock_contains_output_from","text":"def test_readme_codeblock_contains_output_from ( command , expected ) If this test fails, ensure that codeblocks in README.md have the correct output. To enable this check for output in a codeblock, use the following construct: {expected} View Source @pytest . mark . parametrize ( argnames =[ \"command\", \"expected\" ] , argvalues =[ match.groups() for match in matches ] , ids =[ match.group(\"command\") for match in matches ] , ) def test_readme_codeblock_contains_output_from ( command , expected ) : \"\"\" If this test fails, ensure that codeblocks in README.md have the correct output. To enable this check for output in a codeblock, use the following construct: <!-- test codeblock contains output of `{command}` --> ``` {expected} ``` \"\"\" output = _get_output_from ( command ) assert output == expected","title":"test_readme_codeblock_contains_output_from"},{"location":"reference/manubot/tests/test_util/","text":"Module manubot.tests.test_util View Source import manubot.util import pytest def test_shlex_join (): import pathlib args = [ \"command\" , \"positional arg\" , \"path_arg\" , pathlib . Path ( \"path\" )] output = manubot . util . shlex_join ( args ) assert output == \"command 'positional arg' path_arg path\" raw_repo_url = ( \"https://github.com/manubot/manubot/raw/ebac7abd754015a5ec24a6fff39c35a72d4dffb0/\" ) raw_manuscript_url = f \"{raw_repo_url}manubot/process/tests/manuscripts/example/\" def test_read_serialized_data_url_yaml (): url = raw_manuscript_url + \"content/metadata.yaml\" obj = manubot . util . read_serialized_data ( url ) assert obj [ \"title\" ] == \"Example manuscript for testing\" obj = manubot . util . read_serialized_dict ( url ) assert obj [ \"title\" ] == \"Example manuscript for testing\" def test_read_serialized_data_url_json (): url = raw_manuscript_url + \"content/manual-references.json\" obj = manubot . util . read_serialized_data ( url ) assert obj [ 0 ][ \"container-title\" ] == \"Engineuring\" with pytest . raises ( TypeError , match = \"Received 'list' instead\" ): manubot . util . read_serialized_dict ( url ) def test_read_serialized_dict_url_toml (): url = raw_repo_url + \"pyproject.toml\" obj = manubot . util . read_serialized_dict ( url ) assert \"black\" in obj [ \"tool\" ] Variables raw_manuscript_url raw_repo_url Functions test_read_serialized_data_url_json def test_read_serialized_data_url_json ( ) View Source def test_read_serialized_data_url_json (): url = raw_manuscript_url + \"content/manual-references.json\" obj = manubot . util . read_serialized_data ( url ) assert obj [ 0 ][ \"container-title\" ] == \"Engineuring\" with pytest . raises ( TypeError , match = \"Received 'list' instead\" ): manubot . util . read_serialized_dict ( url ) test_read_serialized_data_url_yaml def test_read_serialized_data_url_yaml ( ) View Source def test_read_serialized_data_url_yaml (): url = raw_manuscript_url + \"content/metadata.yaml\" obj = manubot . util . read_serialized_data ( url ) assert obj [ \"title\" ] == \"Example manuscript for testing\" obj = manubot . util . read_serialized_dict ( url ) assert obj [ \"title\" ] == \"Example manuscript for testing\" test_read_serialized_dict_url_toml def test_read_serialized_dict_url_toml ( ) View Source def test_read_serialized_dict_url_toml (): url = raw_repo_url + \"pyproject.toml\" obj = manubot . util . read_serialized_dict ( url ) assert \"black\" in obj [ \"tool\" ] test_shlex_join def test_shlex_join ( ) View Source def test_shlex_join (): import pathlib args = [ \"command\" , \"positional arg\" , \"path_arg\" , pathlib . Path ( \"path\" )] output = manubot . util . shlex_join ( args ) assert output == \"command 'positional arg' path_arg path\"","title":"Test Util"},{"location":"reference/manubot/tests/test_util/#module-manubotteststest_util","text":"View Source import manubot.util import pytest def test_shlex_join (): import pathlib args = [ \"command\" , \"positional arg\" , \"path_arg\" , pathlib . Path ( \"path\" )] output = manubot . util . shlex_join ( args ) assert output == \"command 'positional arg' path_arg path\" raw_repo_url = ( \"https://github.com/manubot/manubot/raw/ebac7abd754015a5ec24a6fff39c35a72d4dffb0/\" ) raw_manuscript_url = f \"{raw_repo_url}manubot/process/tests/manuscripts/example/\" def test_read_serialized_data_url_yaml (): url = raw_manuscript_url + \"content/metadata.yaml\" obj = manubot . util . read_serialized_data ( url ) assert obj [ \"title\" ] == \"Example manuscript for testing\" obj = manubot . util . read_serialized_dict ( url ) assert obj [ \"title\" ] == \"Example manuscript for testing\" def test_read_serialized_data_url_json (): url = raw_manuscript_url + \"content/manual-references.json\" obj = manubot . util . read_serialized_data ( url ) assert obj [ 0 ][ \"container-title\" ] == \"Engineuring\" with pytest . raises ( TypeError , match = \"Received 'list' instead\" ): manubot . util . read_serialized_dict ( url ) def test_read_serialized_dict_url_toml (): url = raw_repo_url + \"pyproject.toml\" obj = manubot . util . read_serialized_dict ( url ) assert \"black\" in obj [ \"tool\" ]","title":"Module manubot.tests.test_util"},{"location":"reference/manubot/tests/test_util/#variables","text":"raw_manuscript_url raw_repo_url","title":"Variables"},{"location":"reference/manubot/tests/test_util/#functions","text":"","title":"Functions"},{"location":"reference/manubot/tests/test_util/#test_read_serialized_data_url_json","text":"def test_read_serialized_data_url_json ( ) View Source def test_read_serialized_data_url_json (): url = raw_manuscript_url + \"content/manual-references.json\" obj = manubot . util . read_serialized_data ( url ) assert obj [ 0 ][ \"container-title\" ] == \"Engineuring\" with pytest . raises ( TypeError , match = \"Received 'list' instead\" ): manubot . util . read_serialized_dict ( url )","title":"test_read_serialized_data_url_json"},{"location":"reference/manubot/tests/test_util/#test_read_serialized_data_url_yaml","text":"def test_read_serialized_data_url_yaml ( ) View Source def test_read_serialized_data_url_yaml (): url = raw_manuscript_url + \"content/metadata.yaml\" obj = manubot . util . read_serialized_data ( url ) assert obj [ \"title\" ] == \"Example manuscript for testing\" obj = manubot . util . read_serialized_dict ( url ) assert obj [ \"title\" ] == \"Example manuscript for testing\"","title":"test_read_serialized_data_url_yaml"},{"location":"reference/manubot/tests/test_util/#test_read_serialized_dict_url_toml","text":"def test_read_serialized_dict_url_toml ( ) View Source def test_read_serialized_dict_url_toml (): url = raw_repo_url + \"pyproject.toml\" obj = manubot . util . read_serialized_dict ( url ) assert \"black\" in obj [ \"tool\" ]","title":"test_read_serialized_dict_url_toml"},{"location":"reference/manubot/tests/test_util/#test_shlex_join","text":"def test_shlex_join ( ) View Source def test_shlex_join (): import pathlib args = [ \"command\" , \"positional arg\" , \"path_arg\" , pathlib . Path ( \"path\" )] output = manubot . util . shlex_join ( args ) assert output == \"command 'positional arg' path_arg path\"","title":"test_shlex_join"},{"location":"reference/manubot/webpage/","text":"Module manubot.webpage Sub-modules manubot.webpage.webpage_command","title":"Index"},{"location":"reference/manubot/webpage/#module-manubotwebpage","text":"","title":"Module manubot.webpage"},{"location":"reference/manubot/webpage/#sub-modules","text":"manubot.webpage.webpage_command","title":"Sub-modules"},{"location":"reference/manubot/webpage/webpage_command/","text":"Module manubot.webpage.webpage_command View Source import logging import pathlib import shutil import subprocess from manubot.util import shlex_join def cli_webpage ( args ): \"\"\" Execute manubot webpage commands. args should be an argparse.Namespace object created by parser.parse_args. \"\"\" configure_args ( args ) logging . debug ( f \"Running `manubot webpage` with the following args: \\n {args}\" ) if args . timestamp : ots_upgrade ( args ) create_version ( args ) def configure_args ( args ): \"\"\" Perform additional processing of arguments that is not handled by argparse. Derive additional variables and add them to args. For example, add directories to args and create them if neccessary. Note that versions_directory is the parent of version_directory. \"\"\" args_dict = vars ( args ) # If --timestamp specified, check that opentimestamps-client is installed if args . timestamp : ots_executable_path = shutil . which ( \"ots\" ) if not ots_executable_path : logging . error ( \"manubot webpage --timestamp was specified but opentimestamps-client not found on system. \" \"Setting --timestamp=False. \" \"Fix this by installing https://pypi.org/project/opentimestamps-client/\" ) args_dict [ \"timestamp\" ] = False # Directory where Manubot outputs reside args_dict [ \"output_directory\" ] = pathlib . Path ( \"output\" ) # Set webpage directory args_dict [ \"webpage_directory\" ] = pathlib . Path ( \"webpage\" ) args . webpage_directory . mkdir ( exist_ok = True ) # Create webpage/v directory (if it doesn't already exist) args_dict [ \"versions_directory\" ] = args . webpage_directory . joinpath ( \"v\" ) args . versions_directory . mkdir ( exist_ok = True ) # Checkout existing version directories checkout_existing_versions ( args ) # Apply --version argument defaults if args . version is None : from manubot.process.ci import get_continuous_integration_parameters ci_params = get_continuous_integration_parameters () if ci_params : args_dict [ \"version\" ] = ci_params . get ( \"commit\" , \"local\" ) else : args_dict [ \"version\" ] = \"local\" # Create empty webpage/v/version directory version_directory = args . versions_directory . joinpath ( args . version ) if version_directory . is_dir (): logging . warning ( f \"{version_directory} exists: replacing it with an empty directory\" ) shutil . rmtree ( version_directory ) version_directory . mkdir () args_dict [ \"version_directory\" ] = version_directory # Symlink webpage/v/latest to point to webpage/v/commit latest_directory = args . versions_directory . joinpath ( \"latest\" ) if latest_directory . is_symlink () or latest_directory . is_file (): latest_directory . unlink () elif latest_directory . is_dir (): shutil . rmtree ( latest_directory ) latest_directory . symlink_to ( args . version , target_is_directory = True ) args_dict [ \"latest_directory\" ] = latest_directory # Create freeze directory freeze_directory = args . versions_directory . joinpath ( \"freeze\" ) freeze_directory . mkdir ( exist_ok = True ) args_dict [ \"freeze_directory\" ] = freeze_directory return args def checkout_existing_versions ( args ): \"\"\" Must populate webpage/v from the gh-pages branch to get history References: http://clubmate.fi/git-checkout-file-or-directories-from-another-branch/ https://stackoverflow.com/a/2668947/4651668 https://stackoverflow.com/a/16493707/4651668 Command modeled after: git --work-tree=webpage checkout upstream/gh-pages -- v \"\"\" if not args . checkout : return command = [ \"git\" , f \"--work-tree={args.webpage_directory}\" , \"checkout\" , args . checkout , \"--\" , \"v\" , ] logging . info ( f \"Attempting checkout with the following command: \\n {shlex_join(command)}\" ) process = subprocess . run ( command , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) if process . returncode == 0 : # Addresses an odd behavior where git checkout stages v/* files that don't actually exist subprocess . run ([ \"git\" , \"add\" , \"v\" ], stdout = subprocess . PIPE ) else : output = process . stdout . decode () message = ( f \"Checkout returned a nonzero exit status. See output: \\n {output.rstrip()}\" ) if \"pathspec\" in output : message += ( \" \\n Manubot note: if there are no preexisting webpage versions (like for a newly created manuscript), \" \"the pathspec error above is expected and can be safely ignored.\" ) # see https://github.com/manubot/rootstock/issues/183 logging . warning ( message ) def create_version ( args ): \"\"\" Populate the version directory for a new version. \"\"\" # Copy content/images to webpage/v/commit/images images_src = pathlib . Path ( \"content/images\" ) if images_src . exists (): shutil . copytree ( src = images_src , dst = args . version_directory . joinpath ( \"images\" )) # Copy output files to to webpage/v/version/ renamer = { \"manuscript.html\" : \"index.html\" , \"manuscript.pdf\" : \"manuscript.pdf\" } for src , dst in renamer . items (): src_path = args . output_directory . joinpath ( src ) if not src_path . exists (): continue dst_path = args . version_directory . joinpath ( dst ) shutil . copy2 ( src = src_path , dst = dst_path ) if args . timestamp : ots_stamp ( dst_path ) # Create v/freeze to redirect to v/commit path = pathlib . Path ( __file__ ) . with_name ( \"redirect-template.html\" ) redirect_html = path . read_text () redirect_html = redirect_html . format ( url = f \"../{args.version}/\" ) args . freeze_directory . joinpath ( \"index.html\" ) . write_text ( redirect_html ) def get_versions ( args ): \"\"\" Extract versions from the webpage/v directory, which should each contain a manuscript. \"\"\" versions = { x . name for x in args . versions_directory . iterdir () if x . is_dir ()} versions -= { \"freeze\" , \"latest\" } versions = sorted ( versions ) return versions def ots_upgrade ( args ): \"\"\" Upgrade OpenTimestamps .ots files in versioned commit directory trees. Upgrades each .ots file with a separate ots upgrade subprocess call due to https://github.com/opentimestamps/opentimestamps-client/issues/71 \"\"\" ots_paths = list () for version in get_versions ( args ): ots_paths . extend ( args . versions_directory . joinpath ( version ) . glob ( \"**/*.ots\" )) ots_paths . sort () for ots_path in ots_paths : process_args = [ \"ots\" ] if args . no_ots_cache : process_args . append ( \"--no-cache\" ) else : process_args . extend ([ \"--cache\" , str ( args . ots_cache )]) process_args . extend ([ \"upgrade\" , str ( ots_path )]) process = subprocess . run ( process_args , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , universal_newlines = True , ) message = f \">>> {shlex_join(process.args)} \\n {process.stdout}\" if process . returncode != 0 : logging . warning ( f \"OpenTimestamp upgrade failed with exit code {process.returncode}. \\n {message}\" ) elif not process . stdout . strip () == \"Success! Timestamp complete\" : logging . info ( message ) backup_path = ots_path . with_suffix ( \".ots.bak\" ) if backup_path . exists (): if process . returncode == 0 : backup_path . unlink () else : # Restore original timestamp if failure backup_path . rename ( ots_path ) def ots_stamp ( path ): \"\"\" Timestamp a file using OpenTimestamps. This function calls `ots stamp path`. If `path` does not exist, this function does nothing. \"\"\" process_args = [ \"ots\" , \"stamp\" , str ( path )] process = subprocess . run ( process_args , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , universal_newlines = True , ) if process . returncode != 0 : logging . warning ( f \"OpenTimestamp command returned nonzero code ({process.returncode}). \\n \" f \">>> {shlex_join(process.args)} \\n \" f \"{process.stdout}\" ) Functions checkout_existing_versions def checkout_existing_versions ( args ) Must populate webpage/v from the gh-pages branch to get history References: http://clubmate.fi/git-checkout-file-or-directories-from-another-branch/ https://stackoverflow.com/a/2668947/4651668 https://stackoverflow.com/a/16493707/4651668 Command modeled after: git --work-tree=webpage checkout upstream/gh-pages -- v View Source def checkout_existing_versions ( args ): \"\"\" Must populate webpage/v from the gh-pages branch to get history References: http://clubmate.fi/git-checkout-file-or-directories-from-another-branch/ https://stackoverflow.com/a/2668947/4651668 https://stackoverflow.com/a/16493707/4651668 Command modeled after: git --work-tree=webpage checkout upstream/gh-pages -- v \"\"\" if not args . checkout : return command = [ \"git\" , f \"--work-tree={args.webpage_directory}\" , \"checkout\" , args . checkout , \"--\" , \"v\" , ] logging . info ( f \"Attempting checkout with the following command:\\n{shlex_join(command)}\" ) process = subprocess . run ( command , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) if process . returncode == 0 : # Addresses an odd behavior where git checkout stages v /* files that don't actually exist subprocess.run([\"git\", \"add\", \"v\"], stdout=subprocess.PIPE) else: output = process.stdout.decode() message = ( f\"Checkout returned a nonzero exit status. See output:\\n{output.rstrip()}\" ) if \"pathspec\" in output: message += ( \"\\nManubot note: if there are no preexisting webpage versions (like for a newly created manuscript), \" \"the pathspec error above is expected and can be safely ignored.\" ) # see https://github.com/manubot/rootstock/issues/183 logging.warning(message) cli_webpage def cli_webpage ( args ) Execute manubot webpage commands. args should be an argparse.Namespace object created by parser.parse_args. View Source def cli_webpage ( args ): \"\"\" Execute manubot webpage commands. args should be an argparse.Namespace object created by parser.parse_args. \"\"\" configure_args ( args ) logging . debug ( f \"Running `manubot webpage` with the following args:\\n{args}\" ) if args . timestamp : ots_upgrade ( args ) create_version ( args ) configure_args def configure_args ( args ) Perform additional processing of arguments that is not handled by argparse. Derive additional variables and add them to args. For example, add directories to args and create them if neccessary. Note that versions_directory is the parent of version_directory. View Source def configure_args ( args ): \"\"\" Perform additional processing of arguments that is not handled by argparse. Derive additional variables and add them to args. For example, add directories to args and create them if neccessary. Note that versions_directory is the parent of version_directory. \"\"\" args_dict = vars ( args ) # If --timestamp specified, check that opentimestamps-client is installed if args . timestamp : ots_executable_path = shutil . which ( \"ots\" ) if not ots_executable_path : logging . error ( \"manubot webpage --timestamp was specified but opentimestamps-client not found on system. \" \"Setting --timestamp=False. \" \"Fix this by installing https://pypi.org/project/opentimestamps-client/\" ) args_dict [ \"timestamp\" ] = False # Directory where Manubot outputs reside args_dict [ \"output_directory\" ] = pathlib . Path ( \"output\" ) # Set webpage directory args_dict [ \"webpage_directory\" ] = pathlib . Path ( \"webpage\" ) args . webpage_directory . mkdir ( exist_ok = True ) # Create webpage / v directory ( if it doesn ' t already exist ) args_dict [ \"versions_directory\" ] = args . webpage_directory . joinpath ( \"v\" ) args . versions_directory . mkdir ( exist_ok = True ) # Checkout existing version directories checkout_existing_versions ( args ) # Apply --version argument defaults if args . version is None : from manubot . process . ci import get_continuous_integration_parameters ci_params = get_continuous_integration_parameters () if ci_params : args_dict [ \"version\" ] = ci_params . get ( \"commit\" , \"local\" ) else : args_dict [ \"version\" ] = \"local\" # Create empty webpage / v / version directory version_directory = args . versions_directory . joinpath ( args . version ) if version_directory . is_dir (): logging . warning ( f \"{version_directory} exists: replacing it with an empty directory\" ) shutil . rmtree ( version_directory ) version_directory . mkdir () args_dict [ \"version_directory\" ] = version_directory # Symlink webpage / v / latest to point to webpage / v / commit latest_directory = args . versions_directory . joinpath ( \"latest\" ) if latest_directory . is_symlink () or latest_directory . is_file (): latest_directory . unlink () elif latest_directory . is_dir (): shutil . rmtree ( latest_directory ) latest_directory . symlink_to ( args . version , target_is_directory = True ) args_dict [ \"latest_directory\" ] = latest_directory # Create freeze directory freeze_directory = args . versions_directory . joinpath ( \"freeze\" ) freeze_directory . mkdir ( exist_ok = True ) args_dict [ \"freeze_directory\" ] = freeze_directory return args create_version def create_version ( args ) Populate the version directory for a new version. View Source def create_version ( args ): \"\"\" Populate the version directory for a new version. \"\"\" # Copy content / images to webpage / v / commit / images images_src = pathlib . Path ( \"content/images\" ) if images_src . exists (): shutil . copytree ( src = images_src , dst = args . version_directory . joinpath ( \"images\" )) # Copy output files to to webpage / v / version / renamer = { \"manuscript.html\" : \"index.html\" , \"manuscript.pdf\" : \"manuscript.pdf\" } for src , dst in renamer . items (): src_path = args . output_directory . joinpath ( src ) if not src_path . exists (): continue dst_path = args . version_directory . joinpath ( dst ) shutil . copy2 ( src = src_path , dst = dst_path ) if args . timestamp : ots_stamp ( dst_path ) # Create v / freeze to redirect to v / commit path = pathlib . Path ( __file__ ). with_name ( \"redirect-template.html\" ) redirect_html = path . read_text () redirect_html = redirect_html . format ( url = f \"../{args.version}/\" ) args . freeze_directory . joinpath ( \"index.html\" ). write_text ( redirect_html ) get_versions def get_versions ( args ) Extract versions from the webpage/v directory, which should each contain a manuscript. View Source def get_versions ( args ): \"\"\" Extract versions from the webpage/v directory, which should each contain a manuscript. \"\"\" versions = { x . name for x in args . versions_directory . iterdir () if x . is_dir () } versions -= { \"freeze\" , \"latest\" } versions = sorted ( versions ) return versions ots_stamp def ots_stamp ( path ) Timestamp a file using OpenTimestamps. This function calls ots stamp path . If path does not exist, this function does nothing. View Source def ots_stamp ( path ): \"\"\" Timestamp a file using OpenTimestamps. This function calls `ots stamp path`. If `path` does not exist, this function does nothing. \"\"\" process_args = [ \"ots\" , \"stamp\" , str ( path )] process = subprocess . run ( process_args , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , universal_newlines = True , ) if process . returncode != 0 : logging . warning ( f \"OpenTimestamp command returned nonzero code ({process.returncode}).\\n\" f \">>> {shlex_join(process.args)}\\n\" f \"{process.stdout}\" ) ots_upgrade def ots_upgrade ( args ) Upgrade OpenTimestamps .ots files in versioned commit directory trees. Upgrades each .ots file with a separate ots upgrade subprocess call due to https://github.com/opentimestamps/opentimestamps-client/issues/71 View Source def ots_upgrade ( args ): \"\"\" Upgrade OpenTimestamps .ots files in versioned commit directory trees. Upgrades each .ots file with a separate ots upgrade subprocess call due to https://github.com/opentimestamps/opentimestamps-client/issues/71 \"\"\" ots_paths = list () for version in get_versions ( args ): ots_paths . extend ( args . versions_directory . joinpath ( version ). glob ( \"**/*.ots\" )) ots_paths . sort () for ots_path in ots_paths : process_args = [ \"ots\" ] if args . no_ots_cache : process_args . append ( \"--no-cache\" ) else : process_args . extend ([ \"--cache\" , str ( args . ots_cache )]) process_args . extend ([ \"upgrade\" , str ( ots_path )]) process = subprocess . run ( process_args , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , universal_newlines = True , ) message = f \">>> {shlex_join(process.args)}\\n{process.stdout}\" if process . returncode != 0 : logging . warning ( f \"OpenTimestamp upgrade failed with exit code {process.returncode}.\\n{message}\" ) elif not process . stdout . strip () == \"Success! Timestamp complete\" : logging . info ( message ) backup_path = ots_path . with_suffix ( \".ots.bak\" ) if backup_path . exists (): if process . returncode == 0 : backup_path . unlink () else : # Restore original timestamp if failure backup_path . rename ( ots_path )","title":"Webpage Command"},{"location":"reference/manubot/webpage/webpage_command/#module-manubotwebpagewebpage_command","text":"View Source import logging import pathlib import shutil import subprocess from manubot.util import shlex_join def cli_webpage ( args ): \"\"\" Execute manubot webpage commands. args should be an argparse.Namespace object created by parser.parse_args. \"\"\" configure_args ( args ) logging . debug ( f \"Running `manubot webpage` with the following args: \\n {args}\" ) if args . timestamp : ots_upgrade ( args ) create_version ( args ) def configure_args ( args ): \"\"\" Perform additional processing of arguments that is not handled by argparse. Derive additional variables and add them to args. For example, add directories to args and create them if neccessary. Note that versions_directory is the parent of version_directory. \"\"\" args_dict = vars ( args ) # If --timestamp specified, check that opentimestamps-client is installed if args . timestamp : ots_executable_path = shutil . which ( \"ots\" ) if not ots_executable_path : logging . error ( \"manubot webpage --timestamp was specified but opentimestamps-client not found on system. \" \"Setting --timestamp=False. \" \"Fix this by installing https://pypi.org/project/opentimestamps-client/\" ) args_dict [ \"timestamp\" ] = False # Directory where Manubot outputs reside args_dict [ \"output_directory\" ] = pathlib . Path ( \"output\" ) # Set webpage directory args_dict [ \"webpage_directory\" ] = pathlib . Path ( \"webpage\" ) args . webpage_directory . mkdir ( exist_ok = True ) # Create webpage/v directory (if it doesn't already exist) args_dict [ \"versions_directory\" ] = args . webpage_directory . joinpath ( \"v\" ) args . versions_directory . mkdir ( exist_ok = True ) # Checkout existing version directories checkout_existing_versions ( args ) # Apply --version argument defaults if args . version is None : from manubot.process.ci import get_continuous_integration_parameters ci_params = get_continuous_integration_parameters () if ci_params : args_dict [ \"version\" ] = ci_params . get ( \"commit\" , \"local\" ) else : args_dict [ \"version\" ] = \"local\" # Create empty webpage/v/version directory version_directory = args . versions_directory . joinpath ( args . version ) if version_directory . is_dir (): logging . warning ( f \"{version_directory} exists: replacing it with an empty directory\" ) shutil . rmtree ( version_directory ) version_directory . mkdir () args_dict [ \"version_directory\" ] = version_directory # Symlink webpage/v/latest to point to webpage/v/commit latest_directory = args . versions_directory . joinpath ( \"latest\" ) if latest_directory . is_symlink () or latest_directory . is_file (): latest_directory . unlink () elif latest_directory . is_dir (): shutil . rmtree ( latest_directory ) latest_directory . symlink_to ( args . version , target_is_directory = True ) args_dict [ \"latest_directory\" ] = latest_directory # Create freeze directory freeze_directory = args . versions_directory . joinpath ( \"freeze\" ) freeze_directory . mkdir ( exist_ok = True ) args_dict [ \"freeze_directory\" ] = freeze_directory return args def checkout_existing_versions ( args ): \"\"\" Must populate webpage/v from the gh-pages branch to get history References: http://clubmate.fi/git-checkout-file-or-directories-from-another-branch/ https://stackoverflow.com/a/2668947/4651668 https://stackoverflow.com/a/16493707/4651668 Command modeled after: git --work-tree=webpage checkout upstream/gh-pages -- v \"\"\" if not args . checkout : return command = [ \"git\" , f \"--work-tree={args.webpage_directory}\" , \"checkout\" , args . checkout , \"--\" , \"v\" , ] logging . info ( f \"Attempting checkout with the following command: \\n {shlex_join(command)}\" ) process = subprocess . run ( command , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) if process . returncode == 0 : # Addresses an odd behavior where git checkout stages v/* files that don't actually exist subprocess . run ([ \"git\" , \"add\" , \"v\" ], stdout = subprocess . PIPE ) else : output = process . stdout . decode () message = ( f \"Checkout returned a nonzero exit status. See output: \\n {output.rstrip()}\" ) if \"pathspec\" in output : message += ( \" \\n Manubot note: if there are no preexisting webpage versions (like for a newly created manuscript), \" \"the pathspec error above is expected and can be safely ignored.\" ) # see https://github.com/manubot/rootstock/issues/183 logging . warning ( message ) def create_version ( args ): \"\"\" Populate the version directory for a new version. \"\"\" # Copy content/images to webpage/v/commit/images images_src = pathlib . Path ( \"content/images\" ) if images_src . exists (): shutil . copytree ( src = images_src , dst = args . version_directory . joinpath ( \"images\" )) # Copy output files to to webpage/v/version/ renamer = { \"manuscript.html\" : \"index.html\" , \"manuscript.pdf\" : \"manuscript.pdf\" } for src , dst in renamer . items (): src_path = args . output_directory . joinpath ( src ) if not src_path . exists (): continue dst_path = args . version_directory . joinpath ( dst ) shutil . copy2 ( src = src_path , dst = dst_path ) if args . timestamp : ots_stamp ( dst_path ) # Create v/freeze to redirect to v/commit path = pathlib . Path ( __file__ ) . with_name ( \"redirect-template.html\" ) redirect_html = path . read_text () redirect_html = redirect_html . format ( url = f \"../{args.version}/\" ) args . freeze_directory . joinpath ( \"index.html\" ) . write_text ( redirect_html ) def get_versions ( args ): \"\"\" Extract versions from the webpage/v directory, which should each contain a manuscript. \"\"\" versions = { x . name for x in args . versions_directory . iterdir () if x . is_dir ()} versions -= { \"freeze\" , \"latest\" } versions = sorted ( versions ) return versions def ots_upgrade ( args ): \"\"\" Upgrade OpenTimestamps .ots files in versioned commit directory trees. Upgrades each .ots file with a separate ots upgrade subprocess call due to https://github.com/opentimestamps/opentimestamps-client/issues/71 \"\"\" ots_paths = list () for version in get_versions ( args ): ots_paths . extend ( args . versions_directory . joinpath ( version ) . glob ( \"**/*.ots\" )) ots_paths . sort () for ots_path in ots_paths : process_args = [ \"ots\" ] if args . no_ots_cache : process_args . append ( \"--no-cache\" ) else : process_args . extend ([ \"--cache\" , str ( args . ots_cache )]) process_args . extend ([ \"upgrade\" , str ( ots_path )]) process = subprocess . run ( process_args , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , universal_newlines = True , ) message = f \">>> {shlex_join(process.args)} \\n {process.stdout}\" if process . returncode != 0 : logging . warning ( f \"OpenTimestamp upgrade failed with exit code {process.returncode}. \\n {message}\" ) elif not process . stdout . strip () == \"Success! Timestamp complete\" : logging . info ( message ) backup_path = ots_path . with_suffix ( \".ots.bak\" ) if backup_path . exists (): if process . returncode == 0 : backup_path . unlink () else : # Restore original timestamp if failure backup_path . rename ( ots_path ) def ots_stamp ( path ): \"\"\" Timestamp a file using OpenTimestamps. This function calls `ots stamp path`. If `path` does not exist, this function does nothing. \"\"\" process_args = [ \"ots\" , \"stamp\" , str ( path )] process = subprocess . run ( process_args , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , universal_newlines = True , ) if process . returncode != 0 : logging . warning ( f \"OpenTimestamp command returned nonzero code ({process.returncode}). \\n \" f \">>> {shlex_join(process.args)} \\n \" f \"{process.stdout}\" )","title":"Module manubot.webpage.webpage_command"},{"location":"reference/manubot/webpage/webpage_command/#functions","text":"","title":"Functions"},{"location":"reference/manubot/webpage/webpage_command/#checkout_existing_versions","text":"def checkout_existing_versions ( args ) Must populate webpage/v from the gh-pages branch to get history References: http://clubmate.fi/git-checkout-file-or-directories-from-another-branch/ https://stackoverflow.com/a/2668947/4651668 https://stackoverflow.com/a/16493707/4651668 Command modeled after: git --work-tree=webpage checkout upstream/gh-pages -- v View Source def checkout_existing_versions ( args ): \"\"\" Must populate webpage/v from the gh-pages branch to get history References: http://clubmate.fi/git-checkout-file-or-directories-from-another-branch/ https://stackoverflow.com/a/2668947/4651668 https://stackoverflow.com/a/16493707/4651668 Command modeled after: git --work-tree=webpage checkout upstream/gh-pages -- v \"\"\" if not args . checkout : return command = [ \"git\" , f \"--work-tree={args.webpage_directory}\" , \"checkout\" , args . checkout , \"--\" , \"v\" , ] logging . info ( f \"Attempting checkout with the following command:\\n{shlex_join(command)}\" ) process = subprocess . run ( command , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) if process . returncode == 0 : # Addresses an odd behavior where git checkout stages v /* files that don't actually exist subprocess.run([\"git\", \"add\", \"v\"], stdout=subprocess.PIPE) else: output = process.stdout.decode() message = ( f\"Checkout returned a nonzero exit status. See output:\\n{output.rstrip()}\" ) if \"pathspec\" in output: message += ( \"\\nManubot note: if there are no preexisting webpage versions (like for a newly created manuscript), \" \"the pathspec error above is expected and can be safely ignored.\" ) # see https://github.com/manubot/rootstock/issues/183 logging.warning(message)","title":"checkout_existing_versions"},{"location":"reference/manubot/webpage/webpage_command/#cli_webpage","text":"def cli_webpage ( args ) Execute manubot webpage commands. args should be an argparse.Namespace object created by parser.parse_args. View Source def cli_webpage ( args ): \"\"\" Execute manubot webpage commands. args should be an argparse.Namespace object created by parser.parse_args. \"\"\" configure_args ( args ) logging . debug ( f \"Running `manubot webpage` with the following args:\\n{args}\" ) if args . timestamp : ots_upgrade ( args ) create_version ( args )","title":"cli_webpage"},{"location":"reference/manubot/webpage/webpage_command/#configure_args","text":"def configure_args ( args ) Perform additional processing of arguments that is not handled by argparse. Derive additional variables and add them to args. For example, add directories to args and create them if neccessary. Note that versions_directory is the parent of version_directory. View Source def configure_args ( args ): \"\"\" Perform additional processing of arguments that is not handled by argparse. Derive additional variables and add them to args. For example, add directories to args and create them if neccessary. Note that versions_directory is the parent of version_directory. \"\"\" args_dict = vars ( args ) # If --timestamp specified, check that opentimestamps-client is installed if args . timestamp : ots_executable_path = shutil . which ( \"ots\" ) if not ots_executable_path : logging . error ( \"manubot webpage --timestamp was specified but opentimestamps-client not found on system. \" \"Setting --timestamp=False. \" \"Fix this by installing https://pypi.org/project/opentimestamps-client/\" ) args_dict [ \"timestamp\" ] = False # Directory where Manubot outputs reside args_dict [ \"output_directory\" ] = pathlib . Path ( \"output\" ) # Set webpage directory args_dict [ \"webpage_directory\" ] = pathlib . Path ( \"webpage\" ) args . webpage_directory . mkdir ( exist_ok = True ) # Create webpage / v directory ( if it doesn ' t already exist ) args_dict [ \"versions_directory\" ] = args . webpage_directory . joinpath ( \"v\" ) args . versions_directory . mkdir ( exist_ok = True ) # Checkout existing version directories checkout_existing_versions ( args ) # Apply --version argument defaults if args . version is None : from manubot . process . ci import get_continuous_integration_parameters ci_params = get_continuous_integration_parameters () if ci_params : args_dict [ \"version\" ] = ci_params . get ( \"commit\" , \"local\" ) else : args_dict [ \"version\" ] = \"local\" # Create empty webpage / v / version directory version_directory = args . versions_directory . joinpath ( args . version ) if version_directory . is_dir (): logging . warning ( f \"{version_directory} exists: replacing it with an empty directory\" ) shutil . rmtree ( version_directory ) version_directory . mkdir () args_dict [ \"version_directory\" ] = version_directory # Symlink webpage / v / latest to point to webpage / v / commit latest_directory = args . versions_directory . joinpath ( \"latest\" ) if latest_directory . is_symlink () or latest_directory . is_file (): latest_directory . unlink () elif latest_directory . is_dir (): shutil . rmtree ( latest_directory ) latest_directory . symlink_to ( args . version , target_is_directory = True ) args_dict [ \"latest_directory\" ] = latest_directory # Create freeze directory freeze_directory = args . versions_directory . joinpath ( \"freeze\" ) freeze_directory . mkdir ( exist_ok = True ) args_dict [ \"freeze_directory\" ] = freeze_directory return args","title":"configure_args"},{"location":"reference/manubot/webpage/webpage_command/#create_version","text":"def create_version ( args ) Populate the version directory for a new version. View Source def create_version ( args ): \"\"\" Populate the version directory for a new version. \"\"\" # Copy content / images to webpage / v / commit / images images_src = pathlib . Path ( \"content/images\" ) if images_src . exists (): shutil . copytree ( src = images_src , dst = args . version_directory . joinpath ( \"images\" )) # Copy output files to to webpage / v / version / renamer = { \"manuscript.html\" : \"index.html\" , \"manuscript.pdf\" : \"manuscript.pdf\" } for src , dst in renamer . items (): src_path = args . output_directory . joinpath ( src ) if not src_path . exists (): continue dst_path = args . version_directory . joinpath ( dst ) shutil . copy2 ( src = src_path , dst = dst_path ) if args . timestamp : ots_stamp ( dst_path ) # Create v / freeze to redirect to v / commit path = pathlib . Path ( __file__ ). with_name ( \"redirect-template.html\" ) redirect_html = path . read_text () redirect_html = redirect_html . format ( url = f \"../{args.version}/\" ) args . freeze_directory . joinpath ( \"index.html\" ). write_text ( redirect_html )","title":"create_version"},{"location":"reference/manubot/webpage/webpage_command/#get_versions","text":"def get_versions ( args ) Extract versions from the webpage/v directory, which should each contain a manuscript. View Source def get_versions ( args ): \"\"\" Extract versions from the webpage/v directory, which should each contain a manuscript. \"\"\" versions = { x . name for x in args . versions_directory . iterdir () if x . is_dir () } versions -= { \"freeze\" , \"latest\" } versions = sorted ( versions ) return versions","title":"get_versions"},{"location":"reference/manubot/webpage/webpage_command/#ots_stamp","text":"def ots_stamp ( path ) Timestamp a file using OpenTimestamps. This function calls ots stamp path . If path does not exist, this function does nothing. View Source def ots_stamp ( path ): \"\"\" Timestamp a file using OpenTimestamps. This function calls `ots stamp path`. If `path` does not exist, this function does nothing. \"\"\" process_args = [ \"ots\" , \"stamp\" , str ( path )] process = subprocess . run ( process_args , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , universal_newlines = True , ) if process . returncode != 0 : logging . warning ( f \"OpenTimestamp command returned nonzero code ({process.returncode}).\\n\" f \">>> {shlex_join(process.args)}\\n\" f \"{process.stdout}\" )","title":"ots_stamp"},{"location":"reference/manubot/webpage/webpage_command/#ots_upgrade","text":"def ots_upgrade ( args ) Upgrade OpenTimestamps .ots files in versioned commit directory trees. Upgrades each .ots file with a separate ots upgrade subprocess call due to https://github.com/opentimestamps/opentimestamps-client/issues/71 View Source def ots_upgrade ( args ): \"\"\" Upgrade OpenTimestamps .ots files in versioned commit directory trees. Upgrades each .ots file with a separate ots upgrade subprocess call due to https://github.com/opentimestamps/opentimestamps-client/issues/71 \"\"\" ots_paths = list () for version in get_versions ( args ): ots_paths . extend ( args . versions_directory . joinpath ( version ). glob ( \"**/*.ots\" )) ots_paths . sort () for ots_path in ots_paths : process_args = [ \"ots\" ] if args . no_ots_cache : process_args . append ( \"--no-cache\" ) else : process_args . extend ([ \"--cache\" , str ( args . ots_cache )]) process_args . extend ([ \"upgrade\" , str ( ots_path )]) process = subprocess . run ( process_args , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , universal_newlines = True , ) message = f \">>> {shlex_join(process.args)}\\n{process.stdout}\" if process . returncode != 0 : logging . warning ( f \"OpenTimestamp upgrade failed with exit code {process.returncode}.\\n{message}\" ) elif not process . stdout . strip () == \"Success! Timestamp complete\" : logging . info ( message ) backup_path = ots_path . with_suffix ( \".ots.bak\" ) if backup_path . exists (): if process . returncode == 0 : backup_path . unlink () else : # Restore original timestamp if failure backup_path . rename ( ots_path )","title":"ots_upgrade"}]}